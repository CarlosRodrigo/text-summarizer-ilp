<?xml version="1.0" encoding="UTF-8"?>
<document id="0" name="LA030889-0163">
  <sentences>
    <sentence id="1" has_coreference="true">
      <content>Canadian Coach Charlie Francis, who claimed that sprinter Ben Johnson&amp;apost;s urine sample at the Seoul Olympics was spiked with a banned steroid, told an acquaintance in Seoul that Johnson had worried that he might test positive.</content>
      <tokens>
        <token id="1" string="Canadian" lemma="canadian" stem="canadian" pos="JJ" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="false" is_refers="false" />
        <token id="2" string="Coach" lemma="coach" stem="coach" pos="VBP" type="Word" isStopWord="false" ner="TITLE" is_referenced="false" is_refers="false" />
        <token id="3" string="Charlie" lemma="Charlie" stem="charli" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="4" string="Francis" lemma="Francis" stem="franci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="6" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="claimed" lemma="claim" stem="claim" pos="VBD" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="sprinter" lemma="sprinter" stem="sprinter" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="Ben" lemma="Ben" stem="ben" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="11" string="Johnson" lemma="Johnson" stem="johnson" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="12" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="urine" lemma="urine" stem="urin" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string="sample" lemma="sample" stem="sampl" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="16" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="17" string="Seoul" lemma="Seoul" stem="seoul" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="18" string="Olympics" lemma="Olympics" stem="olympic" pos="NNPS" type="Word" isStopWord="false" ner="MISC" is_referenced="true" is_refers="false" />
        <token id="19" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="20" string="spiked" lemma="spike" stem="spike" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="21" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="22" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="23" string="banned" lemma="ban" stem="ban" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="24" string="steroid" lemma="steroid" stem="steroid" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="25" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="told" lemma="tell" stem="told" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="acquaintance" lemma="acquaintance" stem="acquaint" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="Seoul" lemma="Seoul" stem="seoul" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="31" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="Johnson" lemma="Johnson" stem="johnson" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="33" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="worried" lemma="worry" stem="worri" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="might" lemma="might" stem="might" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="38" string="test" lemma="test" stem="test" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="positive" lemma="positive" stem="posit" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="40" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (JJ Canadian)) (VP (VBP Coach) (SBAR (S (NP (NP (NNP Charlie) (NNP Francis)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBD claimed) (SBAR (IN that) (S (NP (NP (NP (NN sprinter) (NNP Ben) (NNP Johnson) (POS 's)) (NN urine) (NN sample)) (PP (IN at) (NP (DT the) (NNP Seoul) (NNPS Olympics)))) (VP (VBD was) (VP (VBN spiked) (PP (IN with) (NP (DT a) (VBN banned) (NN steroid)))))))))) (, ,)) (VP (VBD told) (NP (NP (DT an) (NN acquaintance)) (PP (IN in) (NP (NNP Seoul))) (SBAR (WHNP (WDT that)) (S (NP (NNP Johnson)) (VP (VBD had) (VP (VBN worried) (SBAR (IN that) (S (NP (PRP he)) (VP (MD might) (VP (VB test) (ADJP (JJ positive))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Johnson" type="NP">
          <tokens>
            <token id="32" string="Johnson" />
          </tokens>
        </chunking>
        <chunking id="2" string="worried that he might test positive" type="VP">
          <tokens>
            <token id="34" string="worried" />
            <token id="35" string="that" />
            <token id="36" string="he" />
            <token id="37" string="might" />
            <token id="38" string="test" />
            <token id="39" string="positive" />
          </tokens>
        </chunking>
        <chunking id="3" string="Charlie Francis , who claimed that sprinter Ben Johnson 's urine sample at the Seoul Olympics was spiked with a banned steroid , told an acquaintance in Seoul that Johnson had worried that he might test positive" type="SBAR">
          <tokens>
            <token id="3" string="Charlie" />
            <token id="4" string="Francis" />
            <token id="5" string="," />
            <token id="6" string="who" />
            <token id="7" string="claimed" />
            <token id="8" string="that" />
            <token id="9" string="sprinter" />
            <token id="10" string="Ben" />
            <token id="11" string="Johnson" />
            <token id="12" string="'s" />
            <token id="13" string="urine" />
            <token id="14" string="sample" />
            <token id="15" string="at" />
            <token id="16" string="the" />
            <token id="17" string="Seoul" />
            <token id="18" string="Olympics" />
            <token id="19" string="was" />
            <token id="20" string="spiked" />
            <token id="21" string="with" />
            <token id="22" string="a" />
            <token id="23" string="banned" />
            <token id="24" string="steroid" />
            <token id="25" string="," />
            <token id="26" string="told" />
            <token id="27" string="an" />
            <token id="28" string="acquaintance" />
            <token id="29" string="in" />
            <token id="30" string="Seoul" />
            <token id="31" string="that" />
            <token id="32" string="Johnson" />
            <token id="33" string="had" />
            <token id="34" string="worried" />
            <token id="35" string="that" />
            <token id="36" string="he" />
            <token id="37" string="might" />
            <token id="38" string="test" />
            <token id="39" string="positive" />
          </tokens>
        </chunking>
        <chunking id="4" string="had worried that he might test positive" type="VP">
          <tokens>
            <token id="33" string="had" />
            <token id="34" string="worried" />
            <token id="35" string="that" />
            <token id="36" string="he" />
            <token id="37" string="might" />
            <token id="38" string="test" />
            <token id="39" string="positive" />
          </tokens>
        </chunking>
        <chunking id="5" string="positive" type="ADJP">
          <tokens>
            <token id="39" string="positive" />
          </tokens>
        </chunking>
        <chunking id="6" string="Charlie Francis , who claimed that sprinter Ben Johnson 's urine sample at the Seoul Olympics was spiked with a banned steroid ," type="NP">
          <tokens>
            <token id="3" string="Charlie" />
            <token id="4" string="Francis" />
            <token id="5" string="," />
            <token id="6" string="who" />
            <token id="7" string="claimed" />
            <token id="8" string="that" />
            <token id="9" string="sprinter" />
            <token id="10" string="Ben" />
            <token id="11" string="Johnson" />
            <token id="12" string="'s" />
            <token id="13" string="urine" />
            <token id="14" string="sample" />
            <token id="15" string="at" />
            <token id="16" string="the" />
            <token id="17" string="Seoul" />
            <token id="18" string="Olympics" />
            <token id="19" string="was" />
            <token id="20" string="spiked" />
            <token id="21" string="with" />
            <token id="22" string="a" />
            <token id="23" string="banned" />
            <token id="24" string="steroid" />
            <token id="25" string="," />
          </tokens>
        </chunking>
        <chunking id="7" string="claimed that sprinter Ben Johnson 's urine sample at the Seoul Olympics was spiked with a banned steroid" type="VP">
          <tokens>
            <token id="7" string="claimed" />
            <token id="8" string="that" />
            <token id="9" string="sprinter" />
            <token id="10" string="Ben" />
            <token id="11" string="Johnson" />
            <token id="12" string="'s" />
            <token id="13" string="urine" />
            <token id="14" string="sample" />
            <token id="15" string="at" />
            <token id="16" string="the" />
            <token id="17" string="Seoul" />
            <token id="18" string="Olympics" />
            <token id="19" string="was" />
            <token id="20" string="spiked" />
            <token id="21" string="with" />
            <token id="22" string="a" />
            <token id="23" string="banned" />
            <token id="24" string="steroid" />
          </tokens>
        </chunking>
        <chunking id="8" string="that he might test positive" type="SBAR">
          <tokens>
            <token id="35" string="that" />
            <token id="36" string="he" />
            <token id="37" string="might" />
            <token id="38" string="test" />
            <token id="39" string="positive" />
          </tokens>
        </chunking>
        <chunking id="9" string="told an acquaintance in Seoul that Johnson had worried that he might test positive" type="VP">
          <tokens>
            <token id="26" string="told" />
            <token id="27" string="an" />
            <token id="28" string="acquaintance" />
            <token id="29" string="in" />
            <token id="30" string="Seoul" />
            <token id="31" string="that" />
            <token id="32" string="Johnson" />
            <token id="33" string="had" />
            <token id="34" string="worried" />
            <token id="35" string="that" />
            <token id="36" string="he" />
            <token id="37" string="might" />
            <token id="38" string="test" />
            <token id="39" string="positive" />
          </tokens>
        </chunking>
        <chunking id="10" string="sprinter Ben Johnson 's urine sample" type="NP">
          <tokens>
            <token id="9" string="sprinter" />
            <token id="10" string="Ben" />
            <token id="11" string="Johnson" />
            <token id="12" string="'s" />
            <token id="13" string="urine" />
            <token id="14" string="sample" />
          </tokens>
        </chunking>
        <chunking id="11" string="he" type="NP">
          <tokens>
            <token id="36" string="he" />
          </tokens>
        </chunking>
        <chunking id="12" string="Canadian" type="NP">
          <tokens>
            <token id="1" string="Canadian" />
          </tokens>
        </chunking>
        <chunking id="13" string="the Seoul Olympics" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="Seoul" />
            <token id="18" string="Olympics" />
          </tokens>
        </chunking>
        <chunking id="14" string="might test positive" type="VP">
          <tokens>
            <token id="37" string="might" />
            <token id="38" string="test" />
            <token id="39" string="positive" />
          </tokens>
        </chunking>
        <chunking id="15" string="Charlie Francis" type="NP">
          <tokens>
            <token id="3" string="Charlie" />
            <token id="4" string="Francis" />
          </tokens>
        </chunking>
        <chunking id="16" string="who claimed that sprinter Ben Johnson 's urine sample at the Seoul Olympics was spiked with a banned steroid" type="SBAR">
          <tokens>
            <token id="6" string="who" />
            <token id="7" string="claimed" />
            <token id="8" string="that" />
            <token id="9" string="sprinter" />
            <token id="10" string="Ben" />
            <token id="11" string="Johnson" />
            <token id="12" string="'s" />
            <token id="13" string="urine" />
            <token id="14" string="sample" />
            <token id="15" string="at" />
            <token id="16" string="the" />
            <token id="17" string="Seoul" />
            <token id="18" string="Olympics" />
            <token id="19" string="was" />
            <token id="20" string="spiked" />
            <token id="21" string="with" />
            <token id="22" string="a" />
            <token id="23" string="banned" />
            <token id="24" string="steroid" />
          </tokens>
        </chunking>
        <chunking id="17" string="sprinter Ben Johnson 's urine sample at the Seoul Olympics" type="NP">
          <tokens>
            <token id="9" string="sprinter" />
            <token id="10" string="Ben" />
            <token id="11" string="Johnson" />
            <token id="12" string="'s" />
            <token id="13" string="urine" />
            <token id="14" string="sample" />
            <token id="15" string="at" />
            <token id="16" string="the" />
            <token id="17" string="Seoul" />
            <token id="18" string="Olympics" />
          </tokens>
        </chunking>
        <chunking id="18" string="a banned steroid" type="NP">
          <tokens>
            <token id="22" string="a" />
            <token id="23" string="banned" />
            <token id="24" string="steroid" />
          </tokens>
        </chunking>
        <chunking id="19" string="Seoul" type="NP">
          <tokens>
            <token id="30" string="Seoul" />
          </tokens>
        </chunking>
        <chunking id="20" string="sprinter Ben Johnson 's" type="NP">
          <tokens>
            <token id="9" string="sprinter" />
            <token id="10" string="Ben" />
            <token id="11" string="Johnson" />
            <token id="12" string="'s" />
          </tokens>
        </chunking>
        <chunking id="21" string="that Johnson had worried that he might test positive" type="SBAR">
          <tokens>
            <token id="31" string="that" />
            <token id="32" string="Johnson" />
            <token id="33" string="had" />
            <token id="34" string="worried" />
            <token id="35" string="that" />
            <token id="36" string="he" />
            <token id="37" string="might" />
            <token id="38" string="test" />
            <token id="39" string="positive" />
          </tokens>
        </chunking>
        <chunking id="22" string="spiked with a banned steroid" type="VP">
          <tokens>
            <token id="20" string="spiked" />
            <token id="21" string="with" />
            <token id="22" string="a" />
            <token id="23" string="banned" />
            <token id="24" string="steroid" />
          </tokens>
        </chunking>
        <chunking id="23" string="Coach Charlie Francis , who claimed that sprinter Ben Johnson 's urine sample at the Seoul Olympics was spiked with a banned steroid , told an acquaintance in Seoul that Johnson had worried that he might test positive" type="VP">
          <tokens>
            <token id="2" string="Coach" />
            <token id="3" string="Charlie" />
            <token id="4" string="Francis" />
            <token id="5" string="," />
            <token id="6" string="who" />
            <token id="7" string="claimed" />
            <token id="8" string="that" />
            <token id="9" string="sprinter" />
            <token id="10" string="Ben" />
            <token id="11" string="Johnson" />
            <token id="12" string="'s" />
            <token id="13" string="urine" />
            <token id="14" string="sample" />
            <token id="15" string="at" />
            <token id="16" string="the" />
            <token id="17" string="Seoul" />
            <token id="18" string="Olympics" />
            <token id="19" string="was" />
            <token id="20" string="spiked" />
            <token id="21" string="with" />
            <token id="22" string="a" />
            <token id="23" string="banned" />
            <token id="24" string="steroid" />
            <token id="25" string="," />
            <token id="26" string="told" />
            <token id="27" string="an" />
            <token id="28" string="acquaintance" />
            <token id="29" string="in" />
            <token id="30" string="Seoul" />
            <token id="31" string="that" />
            <token id="32" string="Johnson" />
            <token id="33" string="had" />
            <token id="34" string="worried" />
            <token id="35" string="that" />
            <token id="36" string="he" />
            <token id="37" string="might" />
            <token id="38" string="test" />
            <token id="39" string="positive" />
          </tokens>
        </chunking>
        <chunking id="24" string="that sprinter Ben Johnson 's urine sample at the Seoul Olympics was spiked with a banned steroid" type="SBAR">
          <tokens>
            <token id="8" string="that" />
            <token id="9" string="sprinter" />
            <token id="10" string="Ben" />
            <token id="11" string="Johnson" />
            <token id="12" string="'s" />
            <token id="13" string="urine" />
            <token id="14" string="sample" />
            <token id="15" string="at" />
            <token id="16" string="the" />
            <token id="17" string="Seoul" />
            <token id="18" string="Olympics" />
            <token id="19" string="was" />
            <token id="20" string="spiked" />
            <token id="21" string="with" />
            <token id="22" string="a" />
            <token id="23" string="banned" />
            <token id="24" string="steroid" />
          </tokens>
        </chunking>
        <chunking id="25" string="was spiked with a banned steroid" type="VP">
          <tokens>
            <token id="19" string="was" />
            <token id="20" string="spiked" />
            <token id="21" string="with" />
            <token id="22" string="a" />
            <token id="23" string="banned" />
            <token id="24" string="steroid" />
          </tokens>
        </chunking>
        <chunking id="26" string="an acquaintance" type="NP">
          <tokens>
            <token id="27" string="an" />
            <token id="28" string="acquaintance" />
          </tokens>
        </chunking>
        <chunking id="27" string="an acquaintance in Seoul that Johnson had worried that he might test positive" type="NP">
          <tokens>
            <token id="27" string="an" />
            <token id="28" string="acquaintance" />
            <token id="29" string="in" />
            <token id="30" string="Seoul" />
            <token id="31" string="that" />
            <token id="32" string="Johnson" />
            <token id="33" string="had" />
            <token id="34" string="worried" />
            <token id="35" string="that" />
            <token id="36" string="he" />
            <token id="37" string="might" />
            <token id="38" string="test" />
            <token id="39" string="positive" />
          </tokens>
        </chunking>
        <chunking id="28" string="test positive" type="VP">
          <tokens>
            <token id="38" string="test" />
            <token id="39" string="positive" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">Coach</governor>
          <dependent id="1">Canadian</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">Coach</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">Francis</governor>
          <dependent id="3">Charlie</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="26">told</governor>
          <dependent id="4">Francis</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">claimed</governor>
          <dependent id="6">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="4">Francis</governor>
          <dependent id="7">claimed</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="20">spiked</governor>
          <dependent id="8">that</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">Johnson</governor>
          <dependent id="9">sprinter</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">Johnson</governor>
          <dependent id="10">Ben</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="14">sample</governor>
          <dependent id="11">Johnson</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">Johnson</governor>
          <dependent id="12">'s</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">sample</governor>
          <dependent id="13">urine</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="20">spiked</governor>
          <dependent id="14">sample</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">Olympics</governor>
          <dependent id="15">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">Olympics</governor>
          <dependent id="16">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="18">Olympics</governor>
          <dependent id="17">Seoul</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">sample</governor>
          <dependent id="18">Olympics</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="20">spiked</governor>
          <dependent id="19">was</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="7">claimed</governor>
          <dependent id="20">spiked</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">steroid</governor>
          <dependent id="21">with</dependent>
        </dependency>
        <dependency type="det">
          <governor id="24">steroid</governor>
          <dependent id="22">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="24">steroid</governor>
          <dependent id="23">banned</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">spiked</governor>
          <dependent id="24">steroid</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="2">Coach</governor>
          <dependent id="26">told</dependent>
        </dependency>
        <dependency type="det">
          <governor id="28">acquaintance</governor>
          <dependent id="27">an</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="26">told</governor>
          <dependent id="28">acquaintance</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">Seoul</governor>
          <dependent id="29">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="28">acquaintance</governor>
          <dependent id="30">Seoul</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="34">worried</governor>
          <dependent id="31">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="34">worried</governor>
          <dependent id="32">Johnson</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="34">worried</governor>
          <dependent id="33">had</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="28">acquaintance</governor>
          <dependent id="34">worried</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="38">test</governor>
          <dependent id="35">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="38">test</governor>
          <dependent id="36">he</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="38">test</governor>
          <dependent id="37">might</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="34">worried</governor>
          <dependent id="38">test</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="38">test</governor>
          <dependent id="39">positive</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Ben Johnson" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="Ben" />
            <token id="11" string="Johnson" />
          </tokens>
        </entity>
        <entity id="2" string="Seoul" type="LOCATION" score="0.0">
          <tokens>
            <token id="17" string="Seoul" />
          </tokens>
        </entity>
        <entity id="3" string="Johnson" type="PERSON" score="0.0">
          <tokens>
            <token id="32" string="Johnson" />
          </tokens>
        </entity>
        <entity id="4" string="Canadian" type="NATIONALITY" score="0.0">
          <tokens>
            <token id="1" string="Canadian" />
          </tokens>
        </entity>
        <entity id="5" string="Olympics" type="MISC" score="0.0">
          <tokens>
            <token id="18" string="Olympics" />
          </tokens>
        </entity>
        <entity id="6" string="Charlie Francis" type="PERSON" score="0.0">
          <tokens>
            <token id="3" string="Charlie" />
            <token id="4" string="Francis" />
          </tokens>
        </entity>
        <entity id="7" string="Coach" type="TITLE" score="0.0">
          <tokens>
            <token id="2" string="Coach" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="2" has_coreference="true">
      <content>Lynda Huey, who was at Seoul working for NBC-TV and as a physical therapist for some American athletes, said Tuesday that Francis had bragged to her about Johnson&amp;apost;s preparations for a showdown against U.S. sprinter Carl Lewis.</content>
      <tokens>
        <token id="1" string="Lynda" lemma="Lynda" stem="lynda" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="2" string="Huey" lemma="Huey" stem="huei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="Seoul" lemma="Seoul" stem="seoul" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="8" string="working" lemma="work" stem="work" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="NBC-TV" lemma="NBC-TV" stem="nbc-tv" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="11" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="physical" lemma="physical" stem="physic" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="therapist" lemma="therapist" stem="therapist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="some" lemma="some" stem="some" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="American" lemma="american" stem="american" pos="JJ" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="false" is_refers="false" />
        <token id="19" string="athletes" lemma="athlete" stem="athlet" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="Tuesday" lemma="Tuesday" stem="tuesdai" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="23" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="Francis" lemma="Francis" stem="franci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="25" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="bragged" lemma="brag" stem="brag" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="Johnson" lemma="Johnson" stem="johnson" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="31" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="32" string="preparations" lemma="preparation" stem="prepar" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="showdown" lemma="showdown" stem="showdown" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="36" string="against" lemma="against" stem="against" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="U.S." lemma="U.S." stem="u.s." pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="38" string="sprinter" lemma="sprinter" stem="sprinter" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="39" string="Carl" lemma="Carl" stem="carl" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="40" string="Lewis" lemma="Lewis" stem="lewi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="41" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Lynda) (NNP Huey)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBD was) (PP (IN at) (NP (NP (NNP Seoul)) (VP (VBG working) (PP (PP (IN for) (NP (NNP NBC-TV))) (CC and) (PP (IN as) (NP (NP (DT a) (JJ physical) (NN therapist)) (PP (IN for) (NP (DT some) (JJ American) (NNS athletes)))))))))))) (, ,)) (VP (VBD said) (NP-TMP (NNP Tuesday)) (SBAR (IN that) (S (NP (NNP Francis)) (VP (VBD had) (VP (VBN bragged) (PP (TO to) (NP (PRP$ her))) (PP (IN about) (NP (NP (NNP Johnson) (POS 's)) (NNS preparations))) (PP (IN for) (NP (NP (DT a) (NN showdown)) (PP (IN against) (NP (NNP U.S.) (NN sprinter) (NNP Carl) (NNP Lewis)))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="some American athletes" type="NP">
          <tokens>
            <token id="17" string="some" />
            <token id="18" string="American" />
            <token id="19" string="athletes" />
          </tokens>
        </chunking>
        <chunking id="2" string="who was at Seoul working for NBC-TV and as a physical therapist for some American athletes" type="SBAR">
          <tokens>
            <token id="4" string="who" />
            <token id="5" string="was" />
            <token id="6" string="at" />
            <token id="7" string="Seoul" />
            <token id="8" string="working" />
            <token id="9" string="for" />
            <token id="10" string="NBC-TV" />
            <token id="11" string="and" />
            <token id="12" string="as" />
            <token id="13" string="a" />
            <token id="14" string="physical" />
            <token id="15" string="therapist" />
            <token id="16" string="for" />
            <token id="17" string="some" />
            <token id="18" string="American" />
            <token id="19" string="athletes" />
          </tokens>
        </chunking>
        <chunking id="3" string="was at Seoul working for NBC-TV and as a physical therapist for some American athletes" type="VP">
          <tokens>
            <token id="5" string="was" />
            <token id="6" string="at" />
            <token id="7" string="Seoul" />
            <token id="8" string="working" />
            <token id="9" string="for" />
            <token id="10" string="NBC-TV" />
            <token id="11" string="and" />
            <token id="12" string="as" />
            <token id="13" string="a" />
            <token id="14" string="physical" />
            <token id="15" string="therapist" />
            <token id="16" string="for" />
            <token id="17" string="some" />
            <token id="18" string="American" />
            <token id="19" string="athletes" />
          </tokens>
        </chunking>
        <chunking id="4" string="Lynda Huey" type="NP">
          <tokens>
            <token id="1" string="Lynda" />
            <token id="2" string="Huey" />
          </tokens>
        </chunking>
        <chunking id="5" string="NBC-TV" type="NP">
          <tokens>
            <token id="10" string="NBC-TV" />
          </tokens>
        </chunking>
        <chunking id="6" string="U.S. sprinter Carl Lewis" type="NP">
          <tokens>
            <token id="37" string="U.S." />
            <token id="38" string="sprinter" />
            <token id="39" string="Carl" />
            <token id="40" string="Lewis" />
          </tokens>
        </chunking>
        <chunking id="7" string="working for NBC-TV and as a physical therapist for some American athletes" type="VP">
          <tokens>
            <token id="8" string="working" />
            <token id="9" string="for" />
            <token id="10" string="NBC-TV" />
            <token id="11" string="and" />
            <token id="12" string="as" />
            <token id="13" string="a" />
            <token id="14" string="physical" />
            <token id="15" string="therapist" />
            <token id="16" string="for" />
            <token id="17" string="some" />
            <token id="18" string="American" />
            <token id="19" string="athletes" />
          </tokens>
        </chunking>
        <chunking id="8" string="a physical therapist for some American athletes" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="physical" />
            <token id="15" string="therapist" />
            <token id="16" string="for" />
            <token id="17" string="some" />
            <token id="18" string="American" />
            <token id="19" string="athletes" />
          </tokens>
        </chunking>
        <chunking id="9" string="a showdown" type="NP">
          <tokens>
            <token id="34" string="a" />
            <token id="35" string="showdown" />
          </tokens>
        </chunking>
        <chunking id="10" string="a showdown against U.S. sprinter Carl Lewis" type="NP">
          <tokens>
            <token id="34" string="a" />
            <token id="35" string="showdown" />
            <token id="36" string="against" />
            <token id="37" string="U.S." />
            <token id="38" string="sprinter" />
            <token id="39" string="Carl" />
            <token id="40" string="Lewis" />
          </tokens>
        </chunking>
        <chunking id="11" string="Seoul" type="NP">
          <tokens>
            <token id="7" string="Seoul" />
          </tokens>
        </chunking>
        <chunking id="12" string="Seoul working for NBC-TV and as a physical therapist for some American athletes" type="NP">
          <tokens>
            <token id="7" string="Seoul" />
            <token id="8" string="working" />
            <token id="9" string="for" />
            <token id="10" string="NBC-TV" />
            <token id="11" string="and" />
            <token id="12" string="as" />
            <token id="13" string="a" />
            <token id="14" string="physical" />
            <token id="15" string="therapist" />
            <token id="16" string="for" />
            <token id="17" string="some" />
            <token id="18" string="American" />
            <token id="19" string="athletes" />
          </tokens>
        </chunking>
        <chunking id="13" string="Francis" type="NP">
          <tokens>
            <token id="24" string="Francis" />
          </tokens>
        </chunking>
        <chunking id="14" string="said Tuesday that Francis had bragged to her about Johnson 's preparations for a showdown against U.S. sprinter Carl Lewis" type="VP">
          <tokens>
            <token id="21" string="said" />
            <token id="22" string="Tuesday" />
            <token id="23" string="that" />
            <token id="24" string="Francis" />
            <token id="25" string="had" />
            <token id="26" string="bragged" />
            <token id="27" string="to" />
            <token id="28" string="her" />
            <token id="29" string="about" />
            <token id="30" string="Johnson" />
            <token id="31" string="'s" />
            <token id="32" string="preparations" />
            <token id="33" string="for" />
            <token id="34" string="a" />
            <token id="35" string="showdown" />
            <token id="36" string="against" />
            <token id="37" string="U.S." />
            <token id="38" string="sprinter" />
            <token id="39" string="Carl" />
            <token id="40" string="Lewis" />
          </tokens>
        </chunking>
        <chunking id="15" string="her" type="NP">
          <tokens>
            <token id="28" string="her" />
          </tokens>
        </chunking>
        <chunking id="16" string="Lynda Huey , who was at Seoul working for NBC-TV and as a physical therapist for some American athletes ," type="NP">
          <tokens>
            <token id="1" string="Lynda" />
            <token id="2" string="Huey" />
            <token id="3" string="," />
            <token id="4" string="who" />
            <token id="5" string="was" />
            <token id="6" string="at" />
            <token id="7" string="Seoul" />
            <token id="8" string="working" />
            <token id="9" string="for" />
            <token id="10" string="NBC-TV" />
            <token id="11" string="and" />
            <token id="12" string="as" />
            <token id="13" string="a" />
            <token id="14" string="physical" />
            <token id="15" string="therapist" />
            <token id="16" string="for" />
            <token id="17" string="some" />
            <token id="18" string="American" />
            <token id="19" string="athletes" />
            <token id="20" string="," />
          </tokens>
        </chunking>
        <chunking id="17" string="Johnson 's preparations" type="NP">
          <tokens>
            <token id="30" string="Johnson" />
            <token id="31" string="'s" />
            <token id="32" string="preparations" />
          </tokens>
        </chunking>
        <chunking id="18" string="a physical therapist" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="physical" />
            <token id="15" string="therapist" />
          </tokens>
        </chunking>
        <chunking id="19" string="had bragged to her about Johnson 's preparations for a showdown against U.S. sprinter Carl Lewis" type="VP">
          <tokens>
            <token id="25" string="had" />
            <token id="26" string="bragged" />
            <token id="27" string="to" />
            <token id="28" string="her" />
            <token id="29" string="about" />
            <token id="30" string="Johnson" />
            <token id="31" string="'s" />
            <token id="32" string="preparations" />
            <token id="33" string="for" />
            <token id="34" string="a" />
            <token id="35" string="showdown" />
            <token id="36" string="against" />
            <token id="37" string="U.S." />
            <token id="38" string="sprinter" />
            <token id="39" string="Carl" />
            <token id="40" string="Lewis" />
          </tokens>
        </chunking>
        <chunking id="20" string="that Francis had bragged to her about Johnson 's preparations for a showdown against U.S. sprinter Carl Lewis" type="SBAR">
          <tokens>
            <token id="23" string="that" />
            <token id="24" string="Francis" />
            <token id="25" string="had" />
            <token id="26" string="bragged" />
            <token id="27" string="to" />
            <token id="28" string="her" />
            <token id="29" string="about" />
            <token id="30" string="Johnson" />
            <token id="31" string="'s" />
            <token id="32" string="preparations" />
            <token id="33" string="for" />
            <token id="34" string="a" />
            <token id="35" string="showdown" />
            <token id="36" string="against" />
            <token id="37" string="U.S." />
            <token id="38" string="sprinter" />
            <token id="39" string="Carl" />
            <token id="40" string="Lewis" />
          </tokens>
        </chunking>
        <chunking id="21" string="bragged to her about Johnson 's preparations for a showdown against U.S. sprinter Carl Lewis" type="VP">
          <tokens>
            <token id="26" string="bragged" />
            <token id="27" string="to" />
            <token id="28" string="her" />
            <token id="29" string="about" />
            <token id="30" string="Johnson" />
            <token id="31" string="'s" />
            <token id="32" string="preparations" />
            <token id="33" string="for" />
            <token id="34" string="a" />
            <token id="35" string="showdown" />
            <token id="36" string="against" />
            <token id="37" string="U.S." />
            <token id="38" string="sprinter" />
            <token id="39" string="Carl" />
            <token id="40" string="Lewis" />
          </tokens>
        </chunking>
        <chunking id="22" string="Johnson 's" type="NP">
          <tokens>
            <token id="30" string="Johnson" />
            <token id="31" string="'s" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">Huey</governor>
          <dependent id="1">Lynda</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">said</governor>
          <dependent id="2">Huey</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">Seoul</governor>
          <dependent id="4">who</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="7">Seoul</governor>
          <dependent id="5">was</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">Seoul</governor>
          <dependent id="6">at</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="2">Huey</governor>
          <dependent id="7">Seoul</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="7">Seoul</governor>
          <dependent id="8">working</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="8">working</governor>
          <dependent id="8">working</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">NBC-TV</governor>
          <dependent id="9">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">working</governor>
          <dependent id="10">NBC-TV</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="8">working</governor>
          <dependent id="11">and</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">therapist</governor>
          <dependent id="12">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">therapist</governor>
          <dependent id="13">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">therapist</governor>
          <dependent id="14">physical</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">working</governor>
          <dependent id="15">therapist</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">athletes</governor>
          <dependent id="16">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">athletes</governor>
          <dependent id="17">some</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">athletes</governor>
          <dependent id="18">American</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">therapist</governor>
          <dependent id="19">athletes</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="21">said</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="21">said</governor>
          <dependent id="22">Tuesday</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="26">bragged</governor>
          <dependent id="23">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="26">bragged</governor>
          <dependent id="24">Francis</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="26">bragged</governor>
          <dependent id="25">had</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="21">said</governor>
          <dependent id="26">bragged</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">her</governor>
          <dependent id="27">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">bragged</governor>
          <dependent id="28">her</dependent>
        </dependency>
        <dependency type="case">
          <governor id="32">preparations</governor>
          <dependent id="29">about</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="32">preparations</governor>
          <dependent id="30">Johnson</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">Johnson</governor>
          <dependent id="31">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">bragged</governor>
          <dependent id="32">preparations</dependent>
        </dependency>
        <dependency type="case">
          <governor id="35">showdown</governor>
          <dependent id="33">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="35">showdown</governor>
          <dependent id="34">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">bragged</governor>
          <dependent id="35">showdown</dependent>
        </dependency>
        <dependency type="case">
          <governor id="40">Lewis</governor>
          <dependent id="36">against</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="40">Lewis</governor>
          <dependent id="37">U.S.</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="40">Lewis</governor>
          <dependent id="38">sprinter</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="40">Lewis</governor>
          <dependent id="39">Carl</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="35">showdown</governor>
          <dependent id="40">Lewis</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Seoul" type="LOCATION" score="0.0">
          <tokens>
            <token id="7" string="Seoul" />
          </tokens>
        </entity>
        <entity id="2" string="Johnson" type="PERSON" score="0.0">
          <tokens>
            <token id="30" string="Johnson" />
          </tokens>
        </entity>
        <entity id="3" string="Lynda Huey" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Lynda" />
            <token id="2" string="Huey" />
          </tokens>
        </entity>
        <entity id="4" string="NBC-TV" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="10" string="NBC-TV" />
          </tokens>
        </entity>
        <entity id="5" string="Francis" type="PERSON" score="0.0">
          <tokens>
            <token id="24" string="Francis" />
          </tokens>
        </entity>
        <entity id="6" string="U.S." type="LOCATION" score="0.0">
          <tokens>
            <token id="37" string="U.S." />
          </tokens>
        </entity>
        <entity id="7" string="Carl Lewis" type="PERSON" score="0.0">
          <tokens>
            <token id="39" string="Carl" />
            <token id="40" string="Lewis" />
          </tokens>
        </entity>
        <entity id="8" string="American" type="NATIONALITY" score="0.0">
          <tokens>
            <token id="18" string="American" />
          </tokens>
        </entity>
        <entity id="9" string="Tuesday" type="DATE" score="0.0">
          <tokens>
            <token id="22" string="Tuesday" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="3" has_coreference="true">
      <content>Huey said she had known Francis since 1980 when he and sprinter Angella Taylor Issajenko stayed at her home in Los Angeles.</content>
      <tokens>
        <token id="1" string="Huey" lemma="Huey" stem="huei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="known" lemma="know" stem="known" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="Francis" lemma="Francis" stem="franci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="7" string="since" lemma="since" stem="sinc" pos="IN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="1980" lemma="1980" stem="1980" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="9" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="12" string="sprinter" lemma="sprinter" stem="sprinter" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="13" string="Angella" lemma="Angella" stem="angella" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="14" string="Taylor" lemma="Taylor" stem="taylor" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="15" string="Issajenko" lemma="Issajenko" stem="issajenko" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="16" string="stayed" lemma="stay" stem="stai" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="home" lemma="home" stem="home" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="Los" lemma="Los" stem="lo" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="22" string="Angeles" lemma="Angeles" stem="angele" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="23" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Huey)) (VP (VBD said) (SBAR (S (NP (PRP she)) (VP (VBD had) (VP (VBN known) (NP (NP (NNP Francis)) (PP (IN since) (NP (CD 1980)))) (SBAR (WHADVP (WRB when)) (S (NP (NP (PRP he)) (CC and) (NP (NN sprinter) (NNP Angella) (NNP Taylor) (NNP Issajenko))) (VP (VBD stayed) (PP (IN at) (NP (PRP$ her) (NN home))) (PP (IN in) (NP (NNP Los) (NNP Angeles))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Huey" type="NP">
          <tokens>
            <token id="1" string="Huey" />
          </tokens>
        </chunking>
        <chunking id="2" string="her home" type="NP">
          <tokens>
            <token id="18" string="her" />
            <token id="19" string="home" />
          </tokens>
        </chunking>
        <chunking id="3" string="had known Francis since 1980 when he and sprinter Angella Taylor Issajenko stayed at her home in Los Angeles" type="VP">
          <tokens>
            <token id="4" string="had" />
            <token id="5" string="known" />
            <token id="6" string="Francis" />
            <token id="7" string="since" />
            <token id="8" string="1980" />
            <token id="9" string="when" />
            <token id="10" string="he" />
            <token id="11" string="and" />
            <token id="12" string="sprinter" />
            <token id="13" string="Angella" />
            <token id="14" string="Taylor" />
            <token id="15" string="Issajenko" />
            <token id="16" string="stayed" />
            <token id="17" string="at" />
            <token id="18" string="her" />
            <token id="19" string="home" />
            <token id="20" string="in" />
            <token id="21" string="Los" />
            <token id="22" string="Angeles" />
          </tokens>
        </chunking>
        <chunking id="4" string="1980" type="NP">
          <tokens>
            <token id="8" string="1980" />
          </tokens>
        </chunking>
        <chunking id="5" string="stayed at her home in Los Angeles" type="VP">
          <tokens>
            <token id="16" string="stayed" />
            <token id="17" string="at" />
            <token id="18" string="her" />
            <token id="19" string="home" />
            <token id="20" string="in" />
            <token id="21" string="Los" />
            <token id="22" string="Angeles" />
          </tokens>
        </chunking>
        <chunking id="6" string="sprinter Angella Taylor Issajenko" type="NP">
          <tokens>
            <token id="12" string="sprinter" />
            <token id="13" string="Angella" />
            <token id="14" string="Taylor" />
            <token id="15" string="Issajenko" />
          </tokens>
        </chunking>
        <chunking id="7" string="she" type="NP">
          <tokens>
            <token id="3" string="she" />
          </tokens>
        </chunking>
        <chunking id="8" string="when" type="WHADVP">
          <tokens>
            <token id="9" string="when" />
          </tokens>
        </chunking>
        <chunking id="9" string="when he and sprinter Angella Taylor Issajenko stayed at her home in Los Angeles" type="SBAR">
          <tokens>
            <token id="9" string="when" />
            <token id="10" string="he" />
            <token id="11" string="and" />
            <token id="12" string="sprinter" />
            <token id="13" string="Angella" />
            <token id="14" string="Taylor" />
            <token id="15" string="Issajenko" />
            <token id="16" string="stayed" />
            <token id="17" string="at" />
            <token id="18" string="her" />
            <token id="19" string="home" />
            <token id="20" string="in" />
            <token id="21" string="Los" />
            <token id="22" string="Angeles" />
          </tokens>
        </chunking>
        <chunking id="10" string="Francis" type="NP">
          <tokens>
            <token id="6" string="Francis" />
          </tokens>
        </chunking>
        <chunking id="11" string="said she had known Francis since 1980 when he and sprinter Angella Taylor Issajenko stayed at her home in Los Angeles" type="VP">
          <tokens>
            <token id="2" string="said" />
            <token id="3" string="she" />
            <token id="4" string="had" />
            <token id="5" string="known" />
            <token id="6" string="Francis" />
            <token id="7" string="since" />
            <token id="8" string="1980" />
            <token id="9" string="when" />
            <token id="10" string="he" />
            <token id="11" string="and" />
            <token id="12" string="sprinter" />
            <token id="13" string="Angella" />
            <token id="14" string="Taylor" />
            <token id="15" string="Issajenko" />
            <token id="16" string="stayed" />
            <token id="17" string="at" />
            <token id="18" string="her" />
            <token id="19" string="home" />
            <token id="20" string="in" />
            <token id="21" string="Los" />
            <token id="22" string="Angeles" />
          </tokens>
        </chunking>
        <chunking id="12" string="he and sprinter Angella Taylor Issajenko" type="NP">
          <tokens>
            <token id="10" string="he" />
            <token id="11" string="and" />
            <token id="12" string="sprinter" />
            <token id="13" string="Angella" />
            <token id="14" string="Taylor" />
            <token id="15" string="Issajenko" />
          </tokens>
        </chunking>
        <chunking id="13" string="known Francis since 1980 when he and sprinter Angella Taylor Issajenko stayed at her home in Los Angeles" type="VP">
          <tokens>
            <token id="5" string="known" />
            <token id="6" string="Francis" />
            <token id="7" string="since" />
            <token id="8" string="1980" />
            <token id="9" string="when" />
            <token id="10" string="he" />
            <token id="11" string="and" />
            <token id="12" string="sprinter" />
            <token id="13" string="Angella" />
            <token id="14" string="Taylor" />
            <token id="15" string="Issajenko" />
            <token id="16" string="stayed" />
            <token id="17" string="at" />
            <token id="18" string="her" />
            <token id="19" string="home" />
            <token id="20" string="in" />
            <token id="21" string="Los" />
            <token id="22" string="Angeles" />
          </tokens>
        </chunking>
        <chunking id="14" string="Francis since 1980" type="NP">
          <tokens>
            <token id="6" string="Francis" />
            <token id="7" string="since" />
            <token id="8" string="1980" />
          </tokens>
        </chunking>
        <chunking id="15" string="Los Angeles" type="NP">
          <tokens>
            <token id="21" string="Los" />
            <token id="22" string="Angeles" />
          </tokens>
        </chunking>
        <chunking id="16" string="he" type="NP">
          <tokens>
            <token id="10" string="he" />
          </tokens>
        </chunking>
        <chunking id="17" string="she had known Francis since 1980 when he and sprinter Angella Taylor Issajenko stayed at her home in Los Angeles" type="SBAR">
          <tokens>
            <token id="3" string="she" />
            <token id="4" string="had" />
            <token id="5" string="known" />
            <token id="6" string="Francis" />
            <token id="7" string="since" />
            <token id="8" string="1980" />
            <token id="9" string="when" />
            <token id="10" string="he" />
            <token id="11" string="and" />
            <token id="12" string="sprinter" />
            <token id="13" string="Angella" />
            <token id="14" string="Taylor" />
            <token id="15" string="Issajenko" />
            <token id="16" string="stayed" />
            <token id="17" string="at" />
            <token id="18" string="her" />
            <token id="19" string="home" />
            <token id="20" string="in" />
            <token id="21" string="Los" />
            <token id="22" string="Angeles" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">said</governor>
          <dependent id="1">Huey</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">said</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">known</governor>
          <dependent id="3">she</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="5">known</governor>
          <dependent id="4">had</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="2">said</governor>
          <dependent id="5">known</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">known</governor>
          <dependent id="6">Francis</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">1980</governor>
          <dependent id="7">since</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">Francis</governor>
          <dependent id="8">1980</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="16">stayed</governor>
          <dependent id="9">when</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">stayed</governor>
          <dependent id="10">he</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="10">he</governor>
          <dependent id="11">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">Issajenko</governor>
          <dependent id="12">sprinter</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">Issajenko</governor>
          <dependent id="13">Angella</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">Issajenko</governor>
          <dependent id="14">Taylor</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="10">he</governor>
          <dependent id="15">Issajenko</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="5">known</governor>
          <dependent id="16">stayed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">home</governor>
          <dependent id="17">at</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="19">home</governor>
          <dependent id="18">her</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">stayed</governor>
          <dependent id="19">home</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">Angeles</governor>
          <dependent id="20">in</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="22">Angeles</governor>
          <dependent id="21">Los</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">stayed</governor>
          <dependent id="22">Angeles</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Huey" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Huey" />
          </tokens>
        </entity>
        <entity id="2" string="Francis" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Francis" />
          </tokens>
        </entity>
        <entity id="3" string="1980" type="DATE" score="0.0">
          <tokens>
            <token id="8" string="1980" />
          </tokens>
        </entity>
        <entity id="4" string="Angella Taylor Issajenko" type="PERSON" score="0.0">
          <tokens>
            <token id="13" string="Angella" />
            <token id="14" string="Taylor" />
            <token id="15" string="Issajenko" />
          </tokens>
        </entity>
        <entity id="5" string="Los Angeles" type="LOCATION" score="0.0">
          <tokens>
            <token id="21" string="Los" />
            <token id="22" string="Angeles" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="4" has_coreference="true">
      <content>Huey said she had seen Francis on a practice track at Seoul and he had greeted her as an old friend.</content>
      <tokens>
        <token id="1" string="Huey" lemma="Huey" stem="huei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="seen" lemma="see" stem="seen" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="Francis" lemma="Francis" stem="franci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="7" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="practice" lemma="practice" stem="practic" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="track" lemma="track" stem="track" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="Seoul" lemma="Seoul" stem="seoul" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="13" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="greeted" lemma="greet" stem="greet" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="old" lemma="old" stem="old" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="friend" lemma="friend" stem="friend" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (NNP Huey)) (VP (VBD said) (SBAR (S (NP (PRP she)) (VP (VBD had) (VP (VBN seen) (NP (NP (NNP Francis)) (PP (IN on) (NP (DT a) (NN practice) (NN track)))) (PP (IN at) (NP (NNP Seoul))))))))) (CC and) (S (NP (PRP he)) (VP (VBD had) (VP (VBN greeted) (NP (PRP$ her)) (PP (IN as) (NP (DT an) (JJ old) (NN friend)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Francis on a practice track" type="NP">
          <tokens>
            <token id="6" string="Francis" />
            <token id="7" string="on" />
            <token id="8" string="a" />
            <token id="9" string="practice" />
            <token id="10" string="track" />
          </tokens>
        </chunking>
        <chunking id="2" string="Huey" type="NP">
          <tokens>
            <token id="1" string="Huey" />
          </tokens>
        </chunking>
        <chunking id="3" string="an old friend" type="NP">
          <tokens>
            <token id="19" string="an" />
            <token id="20" string="old" />
            <token id="21" string="friend" />
          </tokens>
        </chunking>
        <chunking id="4" string="seen Francis on a practice track at Seoul" type="VP">
          <tokens>
            <token id="5" string="seen" />
            <token id="6" string="Francis" />
            <token id="7" string="on" />
            <token id="8" string="a" />
            <token id="9" string="practice" />
            <token id="10" string="track" />
            <token id="11" string="at" />
            <token id="12" string="Seoul" />
          </tokens>
        </chunking>
        <chunking id="5" string="a practice track" type="NP">
          <tokens>
            <token id="8" string="a" />
            <token id="9" string="practice" />
            <token id="10" string="track" />
          </tokens>
        </chunking>
        <chunking id="6" string="she" type="NP">
          <tokens>
            <token id="3" string="she" />
          </tokens>
        </chunking>
        <chunking id="7" string="had seen Francis on a practice track at Seoul" type="VP">
          <tokens>
            <token id="4" string="had" />
            <token id="5" string="seen" />
            <token id="6" string="Francis" />
            <token id="7" string="on" />
            <token id="8" string="a" />
            <token id="9" string="practice" />
            <token id="10" string="track" />
            <token id="11" string="at" />
            <token id="12" string="Seoul" />
          </tokens>
        </chunking>
        <chunking id="8" string="had greeted her as an old friend" type="VP">
          <tokens>
            <token id="15" string="had" />
            <token id="16" string="greeted" />
            <token id="17" string="her" />
            <token id="18" string="as" />
            <token id="19" string="an" />
            <token id="20" string="old" />
            <token id="21" string="friend" />
          </tokens>
        </chunking>
        <chunking id="9" string="Seoul" type="NP">
          <tokens>
            <token id="12" string="Seoul" />
          </tokens>
        </chunking>
        <chunking id="10" string="Francis" type="NP">
          <tokens>
            <token id="6" string="Francis" />
          </tokens>
        </chunking>
        <chunking id="11" string="her" type="NP">
          <tokens>
            <token id="17" string="her" />
          </tokens>
        </chunking>
        <chunking id="12" string="said she had seen Francis on a practice track at Seoul" type="VP">
          <tokens>
            <token id="2" string="said" />
            <token id="3" string="she" />
            <token id="4" string="had" />
            <token id="5" string="seen" />
            <token id="6" string="Francis" />
            <token id="7" string="on" />
            <token id="8" string="a" />
            <token id="9" string="practice" />
            <token id="10" string="track" />
            <token id="11" string="at" />
            <token id="12" string="Seoul" />
          </tokens>
        </chunking>
        <chunking id="13" string="greeted her as an old friend" type="VP">
          <tokens>
            <token id="16" string="greeted" />
            <token id="17" string="her" />
            <token id="18" string="as" />
            <token id="19" string="an" />
            <token id="20" string="old" />
            <token id="21" string="friend" />
          </tokens>
        </chunking>
        <chunking id="14" string="he" type="NP">
          <tokens>
            <token id="14" string="he" />
          </tokens>
        </chunking>
        <chunking id="15" string="she had seen Francis on a practice track at Seoul" type="SBAR">
          <tokens>
            <token id="3" string="she" />
            <token id="4" string="had" />
            <token id="5" string="seen" />
            <token id="6" string="Francis" />
            <token id="7" string="on" />
            <token id="8" string="a" />
            <token id="9" string="practice" />
            <token id="10" string="track" />
            <token id="11" string="at" />
            <token id="12" string="Seoul" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">said</governor>
          <dependent id="1">Huey</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">said</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">seen</governor>
          <dependent id="3">she</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="5">seen</governor>
          <dependent id="4">had</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="2">said</governor>
          <dependent id="5">seen</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">seen</governor>
          <dependent id="6">Francis</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">track</governor>
          <dependent id="7">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">track</governor>
          <dependent id="8">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">track</governor>
          <dependent id="9">practice</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">Francis</governor>
          <dependent id="10">track</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">Seoul</governor>
          <dependent id="11">at</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">seen</governor>
          <dependent id="12">Seoul</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="2">said</governor>
          <dependent id="13">and</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">greeted</governor>
          <dependent id="14">he</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="16">greeted</governor>
          <dependent id="15">had</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="2">said</governor>
          <dependent id="16">greeted</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="16">greeted</governor>
          <dependent id="17">her</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">friend</governor>
          <dependent id="18">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="21">friend</governor>
          <dependent id="19">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="21">friend</governor>
          <dependent id="20">old</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">greeted</governor>
          <dependent id="21">friend</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Huey" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Huey" />
          </tokens>
        </entity>
        <entity id="2" string="Seoul" type="LOCATION" score="0.0">
          <tokens>
            <token id="12" string="Seoul" />
          </tokens>
        </entity>
        <entity id="3" string="Francis" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Francis" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="5" has_coreference="true">
      <content>&amp;quot;Charlie came over to me and we started talking,&amp;quot; Huey said.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="Charlie" lemma="Charlie" stem="charli" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="came" lemma="come" stem="came" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="over" lemma="over" stem="over" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="me" lemma="I" stem="me" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="we" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="started" lemma="start" stem="start" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="talking" lemma="talk" stem="talk" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="Huey" lemma="Huey" stem="huei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="14" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (S (NP (NNP Charlie)) (VP (VBD came) (PP (IN over) (PP (TO to) (NP (PRP me)))))) (CC and) (S (NP (PRP we)) (VP (VBD started) (S (VP (VBG talking)))))) (, ,) ('' '') (NP (NNP Huey)) (VP (VBD said)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="came over to me" type="VP">
          <tokens>
            <token id="3" string="came" />
            <token id="4" string="over" />
            <token id="5" string="to" />
            <token id="6" string="me" />
          </tokens>
        </chunking>
        <chunking id="2" string="Huey" type="NP">
          <tokens>
            <token id="13" string="Huey" />
          </tokens>
        </chunking>
        <chunking id="3" string="Charlie" type="NP">
          <tokens>
            <token id="2" string="Charlie" />
          </tokens>
        </chunking>
        <chunking id="4" string="me" type="NP">
          <tokens>
            <token id="6" string="me" />
          </tokens>
        </chunking>
        <chunking id="5" string="talking" type="VP">
          <tokens>
            <token id="10" string="talking" />
          </tokens>
        </chunking>
        <chunking id="6" string="we" type="NP">
          <tokens>
            <token id="8" string="we" />
          </tokens>
        </chunking>
        <chunking id="7" string="said" type="VP">
          <tokens>
            <token id="14" string="said" />
          </tokens>
        </chunking>
        <chunking id="8" string="started talking" type="VP">
          <tokens>
            <token id="9" string="started" />
            <token id="10" string="talking" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">came</governor>
          <dependent id="2">Charlie</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="14">said</governor>
          <dependent id="3">came</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">me</governor>
          <dependent id="4">over</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">me</governor>
          <dependent id="5">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">came</governor>
          <dependent id="6">me</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="3">came</governor>
          <dependent id="7">and</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">started</governor>
          <dependent id="8">we</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="3">came</governor>
          <dependent id="9">started</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="9">started</governor>
          <dependent id="10">talking</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">said</governor>
          <dependent id="13">Huey</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="14">said</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Huey" type="PERSON" score="0.0">
          <tokens>
            <token id="13" string="Huey" />
          </tokens>
        </entity>
        <entity id="2" string="Charlie" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Charlie" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="6" has_coreference="true">
      <content>&amp;quot;We were talking about how Ben might do.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="We" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="talking" lemma="talk" stem="talk" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="how" lemma="how" stem="how" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="Ben" lemma="Ben" stem="ben" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="8" string="might" lemma="might" stem="might" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="do" lemma="do" stem="do" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (PRP We)) (VP (VBD were) (VP (VBG talking) (PP (IN about) (SBAR (WHADVP (WRB how)) (S (NP (NNP Ben)) (VP (MD might) (VP (VB do)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="were talking about how Ben might do" type="VP">
          <tokens>
            <token id="3" string="were" />
            <token id="4" string="talking" />
            <token id="5" string="about" />
            <token id="6" string="how" />
            <token id="7" string="Ben" />
            <token id="8" string="might" />
            <token id="9" string="do" />
          </tokens>
        </chunking>
        <chunking id="2" string="might do" type="VP">
          <tokens>
            <token id="8" string="might" />
            <token id="9" string="do" />
          </tokens>
        </chunking>
        <chunking id="3" string="how Ben might do" type="SBAR">
          <tokens>
            <token id="6" string="how" />
            <token id="7" string="Ben" />
            <token id="8" string="might" />
            <token id="9" string="do" />
          </tokens>
        </chunking>
        <chunking id="4" string="do" type="VP">
          <tokens>
            <token id="9" string="do" />
          </tokens>
        </chunking>
        <chunking id="5" string="Ben" type="NP">
          <tokens>
            <token id="7" string="Ben" />
          </tokens>
        </chunking>
        <chunking id="6" string="talking about how Ben might do" type="VP">
          <tokens>
            <token id="4" string="talking" />
            <token id="5" string="about" />
            <token id="6" string="how" />
            <token id="7" string="Ben" />
            <token id="8" string="might" />
            <token id="9" string="do" />
          </tokens>
        </chunking>
        <chunking id="7" string="We" type="NP">
          <tokens>
            <token id="2" string="We" />
          </tokens>
        </chunking>
        <chunking id="8" string="how" type="WHADVP">
          <tokens>
            <token id="6" string="how" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">talking</governor>
          <dependent id="2">We</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="4">talking</governor>
          <dependent id="3">were</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">talking</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="9">do</governor>
          <dependent id="5">about</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="9">do</governor>
          <dependent id="6">how</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">do</governor>
          <dependent id="7">Ben</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="9">do</governor>
          <dependent id="8">might</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="4">talking</governor>
          <dependent id="9">do</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Ben" type="PERSON" score="0.0">
          <tokens>
            <token id="7" string="Ben" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="7" has_coreference="true">
      <content>Charlie said, &amp;apost;Ben&amp;apost;s more afraid of failing the drug test than he is of Carl Lewis.&amp;apost;</content>
      <tokens>
        <token id="1" string="Charlie" lemma="Charlie" stem="charli" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="'" lemma="`" stem="'" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="Ben" lemma="Ben" stem="ben" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="6" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="7" string="more" lemma="more" stem="more" pos="JJR" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="afraid" lemma="afraid" stem="afraid" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="failing" lemma="fail" stem="fail" pos="VBG" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="12" string="drug" lemma="drug" stem="drug" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="13" string="test" lemma="test" stem="test" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string="than" lemma="than" stem="than" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="16" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="Carl" lemma="Carl" stem="carl" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="19" string="Lewis" lemma="Lewis" stem="lewi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="20" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Charlie)) (VP (VBD said) (, ,) (`` `) (S (NP (NP (NP (NNP Ben) (POS 's)) (ADJP (ADJP (JJR more) (JJ afraid)) (PP (IN of) (S (VP (VBG failing) (NP (DT the) (NN drug)))))) (NN test)) (PP (IN than) (NP (PRP he)))) (VP (VBZ is) (PP (IN of) (NP (NNP Carl) (NNP Lewis)))))) (. .) ('' ')))</syntactictree>
      <chunkings>
        <chunking id="1" string="said , ` Ben 's more afraid of failing the drug test than he is of Carl Lewis" type="VP">
          <tokens>
            <token id="2" string="said" />
            <token id="3" string="," />
            <token id="4" string="'" />
            <token id="5" string="Ben" />
            <token id="6" string="'s" />
            <token id="7" string="more" />
            <token id="8" string="afraid" />
            <token id="9" string="of" />
            <token id="10" string="failing" />
            <token id="11" string="the" />
            <token id="12" string="drug" />
            <token id="13" string="test" />
            <token id="14" string="than" />
            <token id="15" string="he" />
            <token id="16" string="is" />
            <token id="17" string="of" />
            <token id="18" string="Carl" />
            <token id="19" string="Lewis" />
          </tokens>
        </chunking>
        <chunking id="2" string="Ben 's" type="NP">
          <tokens>
            <token id="5" string="Ben" />
            <token id="6" string="'s" />
          </tokens>
        </chunking>
        <chunking id="3" string="more afraid" type="ADJP">
          <tokens>
            <token id="7" string="more" />
            <token id="8" string="afraid" />
          </tokens>
        </chunking>
        <chunking id="4" string="is of Carl Lewis" type="VP">
          <tokens>
            <token id="16" string="is" />
            <token id="17" string="of" />
            <token id="18" string="Carl" />
            <token id="19" string="Lewis" />
          </tokens>
        </chunking>
        <chunking id="5" string="failing the drug" type="VP">
          <tokens>
            <token id="10" string="failing" />
            <token id="11" string="the" />
            <token id="12" string="drug" />
          </tokens>
        </chunking>
        <chunking id="6" string="Charlie" type="NP">
          <tokens>
            <token id="1" string="Charlie" />
          </tokens>
        </chunking>
        <chunking id="7" string="Ben 's more afraid of failing the drug test than he" type="NP">
          <tokens>
            <token id="5" string="Ben" />
            <token id="6" string="'s" />
            <token id="7" string="more" />
            <token id="8" string="afraid" />
            <token id="9" string="of" />
            <token id="10" string="failing" />
            <token id="11" string="the" />
            <token id="12" string="drug" />
            <token id="13" string="test" />
            <token id="14" string="than" />
            <token id="15" string="he" />
          </tokens>
        </chunking>
        <chunking id="8" string="the drug" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="drug" />
          </tokens>
        </chunking>
        <chunking id="9" string="more afraid of failing the drug" type="ADJP">
          <tokens>
            <token id="7" string="more" />
            <token id="8" string="afraid" />
            <token id="9" string="of" />
            <token id="10" string="failing" />
            <token id="11" string="the" />
            <token id="12" string="drug" />
          </tokens>
        </chunking>
        <chunking id="10" string="Carl Lewis" type="NP">
          <tokens>
            <token id="18" string="Carl" />
            <token id="19" string="Lewis" />
          </tokens>
        </chunking>
        <chunking id="11" string="he" type="NP">
          <tokens>
            <token id="15" string="he" />
          </tokens>
        </chunking>
        <chunking id="12" string="Ben 's more afraid of failing the drug test" type="NP">
          <tokens>
            <token id="5" string="Ben" />
            <token id="6" string="'s" />
            <token id="7" string="more" />
            <token id="8" string="afraid" />
            <token id="9" string="of" />
            <token id="10" string="failing" />
            <token id="11" string="the" />
            <token id="12" string="drug" />
            <token id="13" string="test" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">said</governor>
          <dependent id="1">Charlie</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">said</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="13">test</governor>
          <dependent id="5">Ben</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">Ben</governor>
          <dependent id="6">'s</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="8">afraid</governor>
          <dependent id="7">more</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">test</governor>
          <dependent id="8">afraid</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="10">failing</governor>
          <dependent id="9">of</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="8">afraid</governor>
          <dependent id="10">failing</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">drug</governor>
          <dependent id="11">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="10">failing</governor>
          <dependent id="12">drug</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">Lewis</governor>
          <dependent id="13">test</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">he</governor>
          <dependent id="14">than</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">test</governor>
          <dependent id="15">he</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="19">Lewis</governor>
          <dependent id="16">is</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">Lewis</governor>
          <dependent id="17">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">Lewis</governor>
          <dependent id="18">Carl</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="2">said</governor>
          <dependent id="19">Lewis</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Charlie" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Charlie" />
          </tokens>
        </entity>
        <entity id="2" string="Carl Lewis" type="PERSON" score="0.0">
          <tokens>
            <token id="18" string="Carl" />
            <token id="19" string="Lewis" />
          </tokens>
        </entity>
        <entity id="3" string="Ben" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="Ben" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="8" has_coreference="true">
      <content>He was bragging.&amp;quot;</content>
      <tokens>
        <token id="1" string="He" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="bragging" lemma="brag" stem="brag" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP He)) (VP (VBD was) (VP (VBG bragging))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="was bragging" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="bragging" />
          </tokens>
        </chunking>
        <chunking id="2" string="bragging" type="VP">
          <tokens>
            <token id="3" string="bragging" />
          </tokens>
        </chunking>
        <chunking id="3" string="He" type="NP">
          <tokens>
            <token id="1" string="He" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">bragging</governor>
          <dependent id="1">He</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="3">bragging</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">bragging</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="9" has_coreference="true">
      <content>Huey said she is tired of hearing Francis, who has been in Toronto testifying at a Canadian inquiry into drug use in sport, say that Johnson was clean at the Olympics.</content>
      <tokens>
        <token id="1" string="Huey" lemma="Huey" stem="huei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="tired" lemma="tire" stem="tire" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="hearing" lemma="hearing" stem="hear" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="Francis" lemma="Francis" stem="franci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="Toronto" lemma="Toronto" stem="toronto" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="true" />
        <token id="15" string="testifying" lemma="testify" stem="testifi" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="16" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="Canadian" lemma="canadian" stem="canadian" pos="JJ" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="false" is_refers="true" />
        <token id="19" string="inquiry" lemma="inquiry" stem="inquiri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="20" string="into" lemma="into" stem="into" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="21" string="drug" lemma="drug" stem="drug" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="22" string="use" lemma="use" stem="us" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="23" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="24" string="sport" lemma="sport" stem="sport" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="25" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="say" lemma="say" stem="sai" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="Johnson" lemma="Johnson" stem="johnson" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="29" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="clean" lemma="clean" stem="clean" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="33" string="Olympics" lemma="Olympics" stem="olympic" pos="NNPS" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="true" />
        <token id="34" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Huey)) (VP (VBD said) (SBAR (S (NP (PRP she)) (VP (VBZ is) (ADJP (VBN tired) (PP (IN of) (NP (NP (NN hearing)) (SBAR (S (NP (NP (NNP Francis)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBZ has) (VP (VBN been) (PP (IN in) (NP (NP (NNP Toronto)) (VP (VBG testifying) (PP (IN at) (NP (DT a) (JJ Canadian) (NN inquiry)))))) (PP (IN into) (NP (NP (NN drug) (NN use)) (PP (IN in) (NP (NN sport))))))))) (, ,)) (VP (VBP say) (SBAR (IN that) (S (NP (NNP Johnson)) (VP (VBD was) (ADJP (JJ clean) (PP (IN at) (NP (DT the) (NNPS Olympics))))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Johnson" type="NP">
          <tokens>
            <token id="28" string="Johnson" />
          </tokens>
        </chunking>
        <chunking id="2" string="drug use in sport" type="NP">
          <tokens>
            <token id="21" string="drug" />
            <token id="22" string="use" />
            <token id="23" string="in" />
            <token id="24" string="sport" />
          </tokens>
        </chunking>
        <chunking id="3" string="say that Johnson was clean at the Olympics" type="VP">
          <tokens>
            <token id="26" string="say" />
            <token id="27" string="that" />
            <token id="28" string="Johnson" />
            <token id="29" string="was" />
            <token id="30" string="clean" />
            <token id="31" string="at" />
            <token id="32" string="the" />
            <token id="33" string="Olympics" />
          </tokens>
        </chunking>
        <chunking id="4" string="was clean at the Olympics" type="VP">
          <tokens>
            <token id="29" string="was" />
            <token id="30" string="clean" />
            <token id="31" string="at" />
            <token id="32" string="the" />
            <token id="33" string="Olympics" />
          </tokens>
        </chunking>
        <chunking id="5" string="testifying at a Canadian inquiry" type="VP">
          <tokens>
            <token id="15" string="testifying" />
            <token id="16" string="at" />
            <token id="17" string="a" />
            <token id="18" string="Canadian" />
            <token id="19" string="inquiry" />
          </tokens>
        </chunking>
        <chunking id="6" string="she" type="NP">
          <tokens>
            <token id="3" string="she" />
          </tokens>
        </chunking>
        <chunking id="7" string="Toronto testifying at a Canadian inquiry" type="NP">
          <tokens>
            <token id="14" string="Toronto" />
            <token id="15" string="testifying" />
            <token id="16" string="at" />
            <token id="17" string="a" />
            <token id="18" string="Canadian" />
            <token id="19" string="inquiry" />
          </tokens>
        </chunking>
        <chunking id="8" string="the Olympics" type="NP">
          <tokens>
            <token id="32" string="the" />
            <token id="33" string="Olympics" />
          </tokens>
        </chunking>
        <chunking id="9" string="that Johnson was clean at the Olympics" type="SBAR">
          <tokens>
            <token id="27" string="that" />
            <token id="28" string="Johnson" />
            <token id="29" string="was" />
            <token id="30" string="clean" />
            <token id="31" string="at" />
            <token id="32" string="the" />
            <token id="33" string="Olympics" />
          </tokens>
        </chunking>
        <chunking id="10" string="is tired of hearing Francis , who has been in Toronto testifying at a Canadian inquiry into drug use in sport , say that Johnson was clean at the Olympics" type="VP">
          <tokens>
            <token id="4" string="is" />
            <token id="5" string="tired" />
            <token id="6" string="of" />
            <token id="7" string="hearing" />
            <token id="8" string="Francis" />
            <token id="9" string="," />
            <token id="10" string="who" />
            <token id="11" string="has" />
            <token id="12" string="been" />
            <token id="13" string="in" />
            <token id="14" string="Toronto" />
            <token id="15" string="testifying" />
            <token id="16" string="at" />
            <token id="17" string="a" />
            <token id="18" string="Canadian" />
            <token id="19" string="inquiry" />
            <token id="20" string="into" />
            <token id="21" string="drug" />
            <token id="22" string="use" />
            <token id="23" string="in" />
            <token id="24" string="sport" />
            <token id="25" string="," />
            <token id="26" string="say" />
            <token id="27" string="that" />
            <token id="28" string="Johnson" />
            <token id="29" string="was" />
            <token id="30" string="clean" />
            <token id="31" string="at" />
            <token id="32" string="the" />
            <token id="33" string="Olympics" />
          </tokens>
        </chunking>
        <chunking id="11" string="sport" type="NP">
          <tokens>
            <token id="24" string="sport" />
          </tokens>
        </chunking>
        <chunking id="12" string="Toronto" type="NP">
          <tokens>
            <token id="14" string="Toronto" />
          </tokens>
        </chunking>
        <chunking id="13" string="she is tired of hearing Francis , who has been in Toronto testifying at a Canadian inquiry into drug use in sport , say that Johnson was clean at the Olympics" type="SBAR">
          <tokens>
            <token id="3" string="she" />
            <token id="4" string="is" />
            <token id="5" string="tired" />
            <token id="6" string="of" />
            <token id="7" string="hearing" />
            <token id="8" string="Francis" />
            <token id="9" string="," />
            <token id="10" string="who" />
            <token id="11" string="has" />
            <token id="12" string="been" />
            <token id="13" string="in" />
            <token id="14" string="Toronto" />
            <token id="15" string="testifying" />
            <token id="16" string="at" />
            <token id="17" string="a" />
            <token id="18" string="Canadian" />
            <token id="19" string="inquiry" />
            <token id="20" string="into" />
            <token id="21" string="drug" />
            <token id="22" string="use" />
            <token id="23" string="in" />
            <token id="24" string="sport" />
            <token id="25" string="," />
            <token id="26" string="say" />
            <token id="27" string="that" />
            <token id="28" string="Johnson" />
            <token id="29" string="was" />
            <token id="30" string="clean" />
            <token id="31" string="at" />
            <token id="32" string="the" />
            <token id="33" string="Olympics" />
          </tokens>
        </chunking>
        <chunking id="14" string="Huey" type="NP">
          <tokens>
            <token id="1" string="Huey" />
          </tokens>
        </chunking>
        <chunking id="15" string="has been in Toronto testifying at a Canadian inquiry into drug use in sport" type="VP">
          <tokens>
            <token id="11" string="has" />
            <token id="12" string="been" />
            <token id="13" string="in" />
            <token id="14" string="Toronto" />
            <token id="15" string="testifying" />
            <token id="16" string="at" />
            <token id="17" string="a" />
            <token id="18" string="Canadian" />
            <token id="19" string="inquiry" />
            <token id="20" string="into" />
            <token id="21" string="drug" />
            <token id="22" string="use" />
            <token id="23" string="in" />
            <token id="24" string="sport" />
          </tokens>
        </chunking>
        <chunking id="16" string="a Canadian inquiry" type="NP">
          <tokens>
            <token id="17" string="a" />
            <token id="18" string="Canadian" />
            <token id="19" string="inquiry" />
          </tokens>
        </chunking>
        <chunking id="17" string="Francis , who has been in Toronto testifying at a Canadian inquiry into drug use in sport ," type="NP">
          <tokens>
            <token id="8" string="Francis" />
            <token id="9" string="," />
            <token id="10" string="who" />
            <token id="11" string="has" />
            <token id="12" string="been" />
            <token id="13" string="in" />
            <token id="14" string="Toronto" />
            <token id="15" string="testifying" />
            <token id="16" string="at" />
            <token id="17" string="a" />
            <token id="18" string="Canadian" />
            <token id="19" string="inquiry" />
            <token id="20" string="into" />
            <token id="21" string="drug" />
            <token id="22" string="use" />
            <token id="23" string="in" />
            <token id="24" string="sport" />
            <token id="25" string="," />
          </tokens>
        </chunking>
        <chunking id="18" string="said she is tired of hearing Francis , who has been in Toronto testifying at a Canadian inquiry into drug use in sport , say that Johnson was clean at the Olympics" type="VP">
          <tokens>
            <token id="2" string="said" />
            <token id="3" string="she" />
            <token id="4" string="is" />
            <token id="5" string="tired" />
            <token id="6" string="of" />
            <token id="7" string="hearing" />
            <token id="8" string="Francis" />
            <token id="9" string="," />
            <token id="10" string="who" />
            <token id="11" string="has" />
            <token id="12" string="been" />
            <token id="13" string="in" />
            <token id="14" string="Toronto" />
            <token id="15" string="testifying" />
            <token id="16" string="at" />
            <token id="17" string="a" />
            <token id="18" string="Canadian" />
            <token id="19" string="inquiry" />
            <token id="20" string="into" />
            <token id="21" string="drug" />
            <token id="22" string="use" />
            <token id="23" string="in" />
            <token id="24" string="sport" />
            <token id="25" string="," />
            <token id="26" string="say" />
            <token id="27" string="that" />
            <token id="28" string="Johnson" />
            <token id="29" string="was" />
            <token id="30" string="clean" />
            <token id="31" string="at" />
            <token id="32" string="the" />
            <token id="33" string="Olympics" />
          </tokens>
        </chunking>
        <chunking id="19" string="been in Toronto testifying at a Canadian inquiry into drug use in sport" type="VP">
          <tokens>
            <token id="12" string="been" />
            <token id="13" string="in" />
            <token id="14" string="Toronto" />
            <token id="15" string="testifying" />
            <token id="16" string="at" />
            <token id="17" string="a" />
            <token id="18" string="Canadian" />
            <token id="19" string="inquiry" />
            <token id="20" string="into" />
            <token id="21" string="drug" />
            <token id="22" string="use" />
            <token id="23" string="in" />
            <token id="24" string="sport" />
          </tokens>
        </chunking>
        <chunking id="20" string="Francis , who has been in Toronto testifying at a Canadian inquiry into drug use in sport , say that Johnson was clean at the Olympics" type="SBAR">
          <tokens>
            <token id="8" string="Francis" />
            <token id="9" string="," />
            <token id="10" string="who" />
            <token id="11" string="has" />
            <token id="12" string="been" />
            <token id="13" string="in" />
            <token id="14" string="Toronto" />
            <token id="15" string="testifying" />
            <token id="16" string="at" />
            <token id="17" string="a" />
            <token id="18" string="Canadian" />
            <token id="19" string="inquiry" />
            <token id="20" string="into" />
            <token id="21" string="drug" />
            <token id="22" string="use" />
            <token id="23" string="in" />
            <token id="24" string="sport" />
            <token id="25" string="," />
            <token id="26" string="say" />
            <token id="27" string="that" />
            <token id="28" string="Johnson" />
            <token id="29" string="was" />
            <token id="30" string="clean" />
            <token id="31" string="at" />
            <token id="32" string="the" />
            <token id="33" string="Olympics" />
          </tokens>
        </chunking>
        <chunking id="21" string="tired of hearing Francis , who has been in Toronto testifying at a Canadian inquiry into drug use in sport , say that Johnson was clean at the Olympics" type="ADJP">
          <tokens>
            <token id="5" string="tired" />
            <token id="6" string="of" />
            <token id="7" string="hearing" />
            <token id="8" string="Francis" />
            <token id="9" string="," />
            <token id="10" string="who" />
            <token id="11" string="has" />
            <token id="12" string="been" />
            <token id="13" string="in" />
            <token id="14" string="Toronto" />
            <token id="15" string="testifying" />
            <token id="16" string="at" />
            <token id="17" string="a" />
            <token id="18" string="Canadian" />
            <token id="19" string="inquiry" />
            <token id="20" string="into" />
            <token id="21" string="drug" />
            <token id="22" string="use" />
            <token id="23" string="in" />
            <token id="24" string="sport" />
            <token id="25" string="," />
            <token id="26" string="say" />
            <token id="27" string="that" />
            <token id="28" string="Johnson" />
            <token id="29" string="was" />
            <token id="30" string="clean" />
            <token id="31" string="at" />
            <token id="32" string="the" />
            <token id="33" string="Olympics" />
          </tokens>
        </chunking>
        <chunking id="22" string="Francis" type="NP">
          <tokens>
            <token id="8" string="Francis" />
          </tokens>
        </chunking>
        <chunking id="23" string="drug use" type="NP">
          <tokens>
            <token id="21" string="drug" />
            <token id="22" string="use" />
          </tokens>
        </chunking>
        <chunking id="24" string="clean at the Olympics" type="ADJP">
          <tokens>
            <token id="30" string="clean" />
            <token id="31" string="at" />
            <token id="32" string="the" />
            <token id="33" string="Olympics" />
          </tokens>
        </chunking>
        <chunking id="25" string="hearing Francis , who has been in Toronto testifying at a Canadian inquiry into drug use in sport , say that Johnson was clean at the Olympics" type="NP">
          <tokens>
            <token id="7" string="hearing" />
            <token id="8" string="Francis" />
            <token id="9" string="," />
            <token id="10" string="who" />
            <token id="11" string="has" />
            <token id="12" string="been" />
            <token id="13" string="in" />
            <token id="14" string="Toronto" />
            <token id="15" string="testifying" />
            <token id="16" string="at" />
            <token id="17" string="a" />
            <token id="18" string="Canadian" />
            <token id="19" string="inquiry" />
            <token id="20" string="into" />
            <token id="21" string="drug" />
            <token id="22" string="use" />
            <token id="23" string="in" />
            <token id="24" string="sport" />
            <token id="25" string="," />
            <token id="26" string="say" />
            <token id="27" string="that" />
            <token id="28" string="Johnson" />
            <token id="29" string="was" />
            <token id="30" string="clean" />
            <token id="31" string="at" />
            <token id="32" string="the" />
            <token id="33" string="Olympics" />
          </tokens>
        </chunking>
        <chunking id="26" string="who has been in Toronto testifying at a Canadian inquiry into drug use in sport" type="SBAR">
          <tokens>
            <token id="10" string="who" />
            <token id="11" string="has" />
            <token id="12" string="been" />
            <token id="13" string="in" />
            <token id="14" string="Toronto" />
            <token id="15" string="testifying" />
            <token id="16" string="at" />
            <token id="17" string="a" />
            <token id="18" string="Canadian" />
            <token id="19" string="inquiry" />
            <token id="20" string="into" />
            <token id="21" string="drug" />
            <token id="22" string="use" />
            <token id="23" string="in" />
            <token id="24" string="sport" />
          </tokens>
        </chunking>
        <chunking id="27" string="hearing" type="NP">
          <tokens>
            <token id="7" string="hearing" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">said</governor>
          <dependent id="1">Huey</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">said</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="5">tired</governor>
          <dependent id="3">she</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="5">tired</governor>
          <dependent id="4">is</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="2">said</governor>
          <dependent id="5">tired</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">hearing</governor>
          <dependent id="6">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">tired</governor>
          <dependent id="7">hearing</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="26">say</governor>
          <dependent id="8">Francis</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">Toronto</governor>
          <dependent id="10">who</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="14">Toronto</governor>
          <dependent id="11">has</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="14">Toronto</governor>
          <dependent id="12">been</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">Toronto</governor>
          <dependent id="13">in</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="8">Francis</governor>
          <dependent id="14">Toronto</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="14">Toronto</governor>
          <dependent id="15">testifying</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">inquiry</governor>
          <dependent id="16">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">inquiry</governor>
          <dependent id="17">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">inquiry</governor>
          <dependent id="18">Canadian</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">testifying</governor>
          <dependent id="19">inquiry</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">use</governor>
          <dependent id="20">into</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="22">use</governor>
          <dependent id="21">drug</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">Toronto</governor>
          <dependent id="22">use</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">sport</governor>
          <dependent id="23">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="22">use</governor>
          <dependent id="24">sport</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="7">hearing</governor>
          <dependent id="26">say</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="30">clean</governor>
          <dependent id="27">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="30">clean</governor>
          <dependent id="28">Johnson</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="30">clean</governor>
          <dependent id="29">was</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="26">say</governor>
          <dependent id="30">clean</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">Olympics</governor>
          <dependent id="31">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="33">Olympics</governor>
          <dependent id="32">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="30">clean</governor>
          <dependent id="33">Olympics</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Huey" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Huey" />
          </tokens>
        </entity>
        <entity id="2" string="Johnson" type="PERSON" score="0.0">
          <tokens>
            <token id="28" string="Johnson" />
          </tokens>
        </entity>
        <entity id="3" string="Francis" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Francis" />
          </tokens>
        </entity>
        <entity id="4" string="Canadian" type="NATIONALITY" score="0.0">
          <tokens>
            <token id="18" string="Canadian" />
          </tokens>
        </entity>
        <entity id="5" string="Olympics" type="MISC" score="0.0">
          <tokens>
            <token id="33" string="Olympics" />
          </tokens>
        </entity>
        <entity id="6" string="Toronto" type="LOCATION" score="0.0">
          <tokens>
            <token id="14" string="Toronto" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="10" has_coreference="true">
      <content>Francis testified that Johnson was not taking the steroid, stanozolol, before the Games and that a mysterious person might have slipped something into Johnson&amp;apost;s beverage in the drug-testing area before the sprinter gave his urine specimen.</content>
      <tokens>
        <token id="1" string="Francis" lemma="Francis" stem="franci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="testified" lemma="testify" stem="testifi" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="Johnson" lemma="Johnson" stem="johnson" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="5" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="taking" lemma="take" stem="take" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="steroid" lemma="steroid" stem="steroid" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="stanozolol" lemma="stanozolol" stem="stanozolol" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="before" lemma="before" stem="befor" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="Games" lemma="Games" stem="game" pos="NNPS" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="16" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="mysterious" lemma="mysterious" stem="mysteri" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="person" lemma="person" stem="person" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="might" lemma="might" stem="might" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="have" lemma="have" stem="have" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="slipped" lemma="slip" stem="slip" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="something" lemma="something" stem="someth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="into" lemma="into" stem="into" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="Johnson" lemma="Johnson" stem="johnson" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="27" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="beverage" lemma="beverage" stem="beverag" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="drug-testing" lemma="drug-testing" stem="drug-test" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="area" lemma="area" stem="area" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="before" lemma="before" stem="befor" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="35" string="sprinter" lemma="sprinter" stem="sprinter" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="36" string="gave" lemma="give" stem="gave" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="37" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="urine" lemma="urine" stem="urin" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="specimen" lemma="specimen" stem="specimen" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="40" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Francis)) (VP (VBD testified) (SBAR (SBAR (IN that) (S (NP (NNP Johnson)) (VP (VBD was) (RB not) (VP (VBG taking) (NP (NP (NP (DT the) (NN steroid)) (, ,) (NP (NN stanozolol)) (, ,)) (PP (IN before) (NP (DT the) (NNPS Games)))))))) (CC and) (SBAR (IN that) (S (NP (DT a) (JJ mysterious) (NN person)) (VP (MD might) (VP (VB have) (VP (VBD slipped) (NP (NN something)) (PP (IN into) (NP (NP (NP (NNP Johnson) (POS 's)) (NN beverage)) (PP (IN in) (NP (DT the) (JJ drug-testing) (NN area))))) (SBAR (IN before) (S (NP (DT the) (NN sprinter)) (VP (VBD gave) (NP (PRP$ his) (NN urine) (NN specimen)))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Johnson 's beverage" type="NP">
          <tokens>
            <token id="26" string="Johnson" />
            <token id="27" string="'s" />
            <token id="28" string="beverage" />
          </tokens>
        </chunking>
        <chunking id="2" string="Johnson" type="NP">
          <tokens>
            <token id="4" string="Johnson" />
          </tokens>
        </chunking>
        <chunking id="3" string="stanozolol" type="NP">
          <tokens>
            <token id="11" string="stanozolol" />
          </tokens>
        </chunking>
        <chunking id="4" string="Johnson 's beverage in the drug-testing area" type="NP">
          <tokens>
            <token id="26" string="Johnson" />
            <token id="27" string="'s" />
            <token id="28" string="beverage" />
            <token id="29" string="in" />
            <token id="30" string="the" />
            <token id="31" string="drug-testing" />
            <token id="32" string="area" />
          </tokens>
        </chunking>
        <chunking id="5" string="something" type="NP">
          <tokens>
            <token id="24" string="something" />
          </tokens>
        </chunking>
        <chunking id="6" string="have slipped something into Johnson 's beverage in the drug-testing area before the sprinter gave his urine specimen" type="VP">
          <tokens>
            <token id="22" string="have" />
            <token id="23" string="slipped" />
            <token id="24" string="something" />
            <token id="25" string="into" />
            <token id="26" string="Johnson" />
            <token id="27" string="'s" />
            <token id="28" string="beverage" />
            <token id="29" string="in" />
            <token id="30" string="the" />
            <token id="31" string="drug-testing" />
            <token id="32" string="area" />
            <token id="33" string="before" />
            <token id="34" string="the" />
            <token id="35" string="sprinter" />
            <token id="36" string="gave" />
            <token id="37" string="his" />
            <token id="38" string="urine" />
            <token id="39" string="specimen" />
          </tokens>
        </chunking>
        <chunking id="7" string="the drug-testing area" type="NP">
          <tokens>
            <token id="30" string="the" />
            <token id="31" string="drug-testing" />
            <token id="32" string="area" />
          </tokens>
        </chunking>
        <chunking id="8" string="before the sprinter gave his urine specimen" type="SBAR">
          <tokens>
            <token id="33" string="before" />
            <token id="34" string="the" />
            <token id="35" string="sprinter" />
            <token id="36" string="gave" />
            <token id="37" string="his" />
            <token id="38" string="urine" />
            <token id="39" string="specimen" />
          </tokens>
        </chunking>
        <chunking id="9" string="taking the steroid , stanozolol , before the Games" type="VP">
          <tokens>
            <token id="7" string="taking" />
            <token id="8" string="the" />
            <token id="9" string="steroid" />
            <token id="10" string="," />
            <token id="11" string="stanozolol" />
            <token id="12" string="," />
            <token id="13" string="before" />
            <token id="14" string="the" />
            <token id="15" string="Games" />
          </tokens>
        </chunking>
        <chunking id="10" string="the Games" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="Games" />
          </tokens>
        </chunking>
        <chunking id="11" string="the steroid" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="steroid" />
          </tokens>
        </chunking>
        <chunking id="12" string="might have slipped something into Johnson 's beverage in the drug-testing area before the sprinter gave his urine specimen" type="VP">
          <tokens>
            <token id="21" string="might" />
            <token id="22" string="have" />
            <token id="23" string="slipped" />
            <token id="24" string="something" />
            <token id="25" string="into" />
            <token id="26" string="Johnson" />
            <token id="27" string="'s" />
            <token id="28" string="beverage" />
            <token id="29" string="in" />
            <token id="30" string="the" />
            <token id="31" string="drug-testing" />
            <token id="32" string="area" />
            <token id="33" string="before" />
            <token id="34" string="the" />
            <token id="35" string="sprinter" />
            <token id="36" string="gave" />
            <token id="37" string="his" />
            <token id="38" string="urine" />
            <token id="39" string="specimen" />
          </tokens>
        </chunking>
        <chunking id="13" string="the steroid , stanozolol , before the Games" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="steroid" />
            <token id="10" string="," />
            <token id="11" string="stanozolol" />
            <token id="12" string="," />
            <token id="13" string="before" />
            <token id="14" string="the" />
            <token id="15" string="Games" />
          </tokens>
        </chunking>
        <chunking id="14" string="Johnson 's" type="NP">
          <tokens>
            <token id="26" string="Johnson" />
            <token id="27" string="'s" />
          </tokens>
        </chunking>
        <chunking id="15" string="gave his urine specimen" type="VP">
          <tokens>
            <token id="36" string="gave" />
            <token id="37" string="his" />
            <token id="38" string="urine" />
            <token id="39" string="specimen" />
          </tokens>
        </chunking>
        <chunking id="16" string="the sprinter" type="NP">
          <tokens>
            <token id="34" string="the" />
            <token id="35" string="sprinter" />
          </tokens>
        </chunking>
        <chunking id="17" string="the steroid , stanozolol ," type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="steroid" />
            <token id="10" string="," />
            <token id="11" string="stanozolol" />
            <token id="12" string="," />
          </tokens>
        </chunking>
        <chunking id="18" string="testified that Johnson was not taking the steroid , stanozolol , before the Games and that a mysterious person might have slipped something into Johnson 's beverage in the drug-testing area before the sprinter gave his urine specimen" type="VP">
          <tokens>
            <token id="2" string="testified" />
            <token id="3" string="that" />
            <token id="4" string="Johnson" />
            <token id="5" string="was" />
            <token id="6" string="not" />
            <token id="7" string="taking" />
            <token id="8" string="the" />
            <token id="9" string="steroid" />
            <token id="10" string="," />
            <token id="11" string="stanozolol" />
            <token id="12" string="," />
            <token id="13" string="before" />
            <token id="14" string="the" />
            <token id="15" string="Games" />
            <token id="16" string="and" />
            <token id="17" string="that" />
            <token id="18" string="a" />
            <token id="19" string="mysterious" />
            <token id="20" string="person" />
            <token id="21" string="might" />
            <token id="22" string="have" />
            <token id="23" string="slipped" />
            <token id="24" string="something" />
            <token id="25" string="into" />
            <token id="26" string="Johnson" />
            <token id="27" string="'s" />
            <token id="28" string="beverage" />
            <token id="29" string="in" />
            <token id="30" string="the" />
            <token id="31" string="drug-testing" />
            <token id="32" string="area" />
            <token id="33" string="before" />
            <token id="34" string="the" />
            <token id="35" string="sprinter" />
            <token id="36" string="gave" />
            <token id="37" string="his" />
            <token id="38" string="urine" />
            <token id="39" string="specimen" />
          </tokens>
        </chunking>
        <chunking id="19" string="slipped something into Johnson 's beverage in the drug-testing area before the sprinter gave his urine specimen" type="VP">
          <tokens>
            <token id="23" string="slipped" />
            <token id="24" string="something" />
            <token id="25" string="into" />
            <token id="26" string="Johnson" />
            <token id="27" string="'s" />
            <token id="28" string="beverage" />
            <token id="29" string="in" />
            <token id="30" string="the" />
            <token id="31" string="drug-testing" />
            <token id="32" string="area" />
            <token id="33" string="before" />
            <token id="34" string="the" />
            <token id="35" string="sprinter" />
            <token id="36" string="gave" />
            <token id="37" string="his" />
            <token id="38" string="urine" />
            <token id="39" string="specimen" />
          </tokens>
        </chunking>
        <chunking id="20" string="that Johnson was not taking the steroid , stanozolol , before the Games" type="SBAR">
          <tokens>
            <token id="3" string="that" />
            <token id="4" string="Johnson" />
            <token id="5" string="was" />
            <token id="6" string="not" />
            <token id="7" string="taking" />
            <token id="8" string="the" />
            <token id="9" string="steroid" />
            <token id="10" string="," />
            <token id="11" string="stanozolol" />
            <token id="12" string="," />
            <token id="13" string="before" />
            <token id="14" string="the" />
            <token id="15" string="Games" />
          </tokens>
        </chunking>
        <chunking id="21" string="his urine specimen" type="NP">
          <tokens>
            <token id="37" string="his" />
            <token id="38" string="urine" />
            <token id="39" string="specimen" />
          </tokens>
        </chunking>
        <chunking id="22" string="Francis" type="NP">
          <tokens>
            <token id="1" string="Francis" />
          </tokens>
        </chunking>
        <chunking id="23" string="that Johnson was not taking the steroid , stanozolol , before the Games and that a mysterious person might have slipped something into Johnson 's beverage in the drug-testing area before the sprinter gave his urine specimen" type="SBAR">
          <tokens>
            <token id="3" string="that" />
            <token id="4" string="Johnson" />
            <token id="5" string="was" />
            <token id="6" string="not" />
            <token id="7" string="taking" />
            <token id="8" string="the" />
            <token id="9" string="steroid" />
            <token id="10" string="," />
            <token id="11" string="stanozolol" />
            <token id="12" string="," />
            <token id="13" string="before" />
            <token id="14" string="the" />
            <token id="15" string="Games" />
            <token id="16" string="and" />
            <token id="17" string="that" />
            <token id="18" string="a" />
            <token id="19" string="mysterious" />
            <token id="20" string="person" />
            <token id="21" string="might" />
            <token id="22" string="have" />
            <token id="23" string="slipped" />
            <token id="24" string="something" />
            <token id="25" string="into" />
            <token id="26" string="Johnson" />
            <token id="27" string="'s" />
            <token id="28" string="beverage" />
            <token id="29" string="in" />
            <token id="30" string="the" />
            <token id="31" string="drug-testing" />
            <token id="32" string="area" />
            <token id="33" string="before" />
            <token id="34" string="the" />
            <token id="35" string="sprinter" />
            <token id="36" string="gave" />
            <token id="37" string="his" />
            <token id="38" string="urine" />
            <token id="39" string="specimen" />
          </tokens>
        </chunking>
        <chunking id="24" string="that a mysterious person might have slipped something into Johnson 's beverage in the drug-testing area before the sprinter gave his urine specimen" type="SBAR">
          <tokens>
            <token id="17" string="that" />
            <token id="18" string="a" />
            <token id="19" string="mysterious" />
            <token id="20" string="person" />
            <token id="21" string="might" />
            <token id="22" string="have" />
            <token id="23" string="slipped" />
            <token id="24" string="something" />
            <token id="25" string="into" />
            <token id="26" string="Johnson" />
            <token id="27" string="'s" />
            <token id="28" string="beverage" />
            <token id="29" string="in" />
            <token id="30" string="the" />
            <token id="31" string="drug-testing" />
            <token id="32" string="area" />
            <token id="33" string="before" />
            <token id="34" string="the" />
            <token id="35" string="sprinter" />
            <token id="36" string="gave" />
            <token id="37" string="his" />
            <token id="38" string="urine" />
            <token id="39" string="specimen" />
          </tokens>
        </chunking>
        <chunking id="25" string="was not taking the steroid , stanozolol , before the Games" type="VP">
          <tokens>
            <token id="5" string="was" />
            <token id="6" string="not" />
            <token id="7" string="taking" />
            <token id="8" string="the" />
            <token id="9" string="steroid" />
            <token id="10" string="," />
            <token id="11" string="stanozolol" />
            <token id="12" string="," />
            <token id="13" string="before" />
            <token id="14" string="the" />
            <token id="15" string="Games" />
          </tokens>
        </chunking>
        <chunking id="26" string="a mysterious person" type="NP">
          <tokens>
            <token id="18" string="a" />
            <token id="19" string="mysterious" />
            <token id="20" string="person" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">testified</governor>
          <dependent id="1">Francis</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">testified</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="7">taking</governor>
          <dependent id="3">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">taking</governor>
          <dependent id="4">Johnson</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="7">taking</governor>
          <dependent id="5">was</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="7">taking</governor>
          <dependent id="6">not</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="2">testified</governor>
          <dependent id="7">taking</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">steroid</governor>
          <dependent id="8">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">taking</governor>
          <dependent id="9">steroid</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="9">steroid</governor>
          <dependent id="11">stanozolol</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">Games</governor>
          <dependent id="13">before</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">Games</governor>
          <dependent id="14">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">steroid</governor>
          <dependent id="15">Games</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">taking</governor>
          <dependent id="16">and</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="23">slipped</governor>
          <dependent id="17">that</dependent>
        </dependency>
        <dependency type="det">
          <governor id="20">person</governor>
          <dependent id="18">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="20">person</governor>
          <dependent id="19">mysterious</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="23">slipped</governor>
          <dependent id="20">person</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="23">slipped</governor>
          <dependent id="21">might</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="23">slipped</governor>
          <dependent id="22">have</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">taking</governor>
          <dependent id="23">slipped</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="23">slipped</governor>
          <dependent id="24">something</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">beverage</governor>
          <dependent id="25">into</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="28">beverage</governor>
          <dependent id="26">Johnson</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">Johnson</governor>
          <dependent id="27">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">slipped</governor>
          <dependent id="28">beverage</dependent>
        </dependency>
        <dependency type="case">
          <governor id="32">area</governor>
          <dependent id="29">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="32">area</governor>
          <dependent id="30">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="32">area</governor>
          <dependent id="31">drug-testing</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="28">beverage</governor>
          <dependent id="32">area</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="36">gave</governor>
          <dependent id="33">before</dependent>
        </dependency>
        <dependency type="det">
          <governor id="35">sprinter</governor>
          <dependent id="34">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="36">gave</governor>
          <dependent id="35">sprinter</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="23">slipped</governor>
          <dependent id="36">gave</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="39">specimen</governor>
          <dependent id="37">his</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="39">specimen</governor>
          <dependent id="38">urine</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="36">gave</governor>
          <dependent id="39">specimen</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Johnson" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="Johnson" />
          </tokens>
        </entity>
        <entity id="2" string="Francis" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Francis" />
          </tokens>
        </entity>
        <entity id="3" string="Games" type="MISC" score="0.0">
          <tokens>
            <token id="15" string="Games" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="11" has_coreference="true">
      <content>Clean or not, Huey said, &amp;quot;Francis must have had some reason to think Ben may not pass the test.&amp;quot;</content>
      <tokens>
        <token id="1" string="Clean" lemma="clean" stem="clean" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="or" lemma="or" stem="or" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="Huey" lemma="Huey" stem="huei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="6" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="Francis" lemma="Francis" stem="franci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="10" string="must" lemma="must" stem="must" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="have" lemma="have" stem="have" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="some" lemma="some" stem="some" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="reason" lemma="reason" stem="reason" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="think" lemma="think" stem="think" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="Ben" lemma="Ben" stem="ben" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="18" string="may" lemma="may" stem="mai" pos="MD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="pass" lemma="pass" stem="pass" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="22" string="test" lemma="test" stem="test" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="23" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (JJ Clean)) (CC or) (NP (RB not))) (PRN (, ,) (NP (NNP Huey)) (VP (VBD said)) (, ,)) (`` ``) (NP (NNP Francis)) (VP (MD must) (VP (VB have) (VP (VBD had) (NP (DT some) (NN reason) (S (VP (TO to) (VP (VB think) (SBAR (S (NP (NNP Ben)) (VP (MD may) (RB not) (VP (VB pass) (NP (DT the) (NN test))))))))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="may not pass the test" type="VP">
          <tokens>
            <token id="18" string="may" />
            <token id="19" string="not" />
            <token id="20" string="pass" />
            <token id="21" string="the" />
            <token id="22" string="test" />
          </tokens>
        </chunking>
        <chunking id="2" string="Huey" type="NP">
          <tokens>
            <token id="5" string="Huey" />
          </tokens>
        </chunking>
        <chunking id="3" string="pass the test" type="VP">
          <tokens>
            <token id="20" string="pass" />
            <token id="21" string="the" />
            <token id="22" string="test" />
          </tokens>
        </chunking>
        <chunking id="4" string="the test" type="NP">
          <tokens>
            <token id="21" string="the" />
            <token id="22" string="test" />
          </tokens>
        </chunking>
        <chunking id="5" string="had some reason to think Ben may not pass the test" type="VP">
          <tokens>
            <token id="12" string="had" />
            <token id="13" string="some" />
            <token id="14" string="reason" />
            <token id="15" string="to" />
            <token id="16" string="think" />
            <token id="17" string="Ben" />
            <token id="18" string="may" />
            <token id="19" string="not" />
            <token id="20" string="pass" />
            <token id="21" string="the" />
            <token id="22" string="test" />
          </tokens>
        </chunking>
        <chunking id="6" string="think Ben may not pass the test" type="VP">
          <tokens>
            <token id="16" string="think" />
            <token id="17" string="Ben" />
            <token id="18" string="may" />
            <token id="19" string="not" />
            <token id="20" string="pass" />
            <token id="21" string="the" />
            <token id="22" string="test" />
          </tokens>
        </chunking>
        <chunking id="7" string="Clean or not" type="NP">
          <tokens>
            <token id="1" string="Clean" />
            <token id="2" string="or" />
            <token id="3" string="not" />
          </tokens>
        </chunking>
        <chunking id="8" string="to think Ben may not pass the test" type="VP">
          <tokens>
            <token id="15" string="to" />
            <token id="16" string="think" />
            <token id="17" string="Ben" />
            <token id="18" string="may" />
            <token id="19" string="not" />
            <token id="20" string="pass" />
            <token id="21" string="the" />
            <token id="22" string="test" />
          </tokens>
        </chunking>
        <chunking id="9" string="have had some reason to think Ben may not pass the test" type="VP">
          <tokens>
            <token id="11" string="have" />
            <token id="12" string="had" />
            <token id="13" string="some" />
            <token id="14" string="reason" />
            <token id="15" string="to" />
            <token id="16" string="think" />
            <token id="17" string="Ben" />
            <token id="18" string="may" />
            <token id="19" string="not" />
            <token id="20" string="pass" />
            <token id="21" string="the" />
            <token id="22" string="test" />
          </tokens>
        </chunking>
        <chunking id="10" string="some reason to think Ben may not pass the test" type="NP">
          <tokens>
            <token id="13" string="some" />
            <token id="14" string="reason" />
            <token id="15" string="to" />
            <token id="16" string="think" />
            <token id="17" string="Ben" />
            <token id="18" string="may" />
            <token id="19" string="not" />
            <token id="20" string="pass" />
            <token id="21" string="the" />
            <token id="22" string="test" />
          </tokens>
        </chunking>
        <chunking id="11" string="Clean" type="NP">
          <tokens>
            <token id="1" string="Clean" />
          </tokens>
        </chunking>
        <chunking id="12" string="Ben" type="NP">
          <tokens>
            <token id="17" string="Ben" />
          </tokens>
        </chunking>
        <chunking id="13" string="must have had some reason to think Ben may not pass the test" type="VP">
          <tokens>
            <token id="10" string="must" />
            <token id="11" string="have" />
            <token id="12" string="had" />
            <token id="13" string="some" />
            <token id="14" string="reason" />
            <token id="15" string="to" />
            <token id="16" string="think" />
            <token id="17" string="Ben" />
            <token id="18" string="may" />
            <token id="19" string="not" />
            <token id="20" string="pass" />
            <token id="21" string="the" />
            <token id="22" string="test" />
          </tokens>
        </chunking>
        <chunking id="14" string="Ben may not pass the test" type="SBAR">
          <tokens>
            <token id="17" string="Ben" />
            <token id="18" string="may" />
            <token id="19" string="not" />
            <token id="20" string="pass" />
            <token id="21" string="the" />
            <token id="22" string="test" />
          </tokens>
        </chunking>
        <chunking id="15" string="not" type="NP">
          <tokens>
            <token id="3" string="not" />
          </tokens>
        </chunking>
        <chunking id="16" string="Francis" type="NP">
          <tokens>
            <token id="9" string="Francis" />
          </tokens>
        </chunking>
        <chunking id="17" string="said" type="VP">
          <tokens>
            <token id="6" string="said" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="12">had</governor>
          <dependent id="1">Clean</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="1">Clean</governor>
          <dependent id="2">or</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="1">Clean</governor>
          <dependent id="3">not</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">said</governor>
          <dependent id="5">Huey</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="12">had</governor>
          <dependent id="6">said</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">had</governor>
          <dependent id="9">Francis</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="12">had</governor>
          <dependent id="10">must</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="12">had</governor>
          <dependent id="11">have</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="12">had</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">reason</governor>
          <dependent id="13">some</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="12">had</governor>
          <dependent id="14">reason</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="16">think</governor>
          <dependent id="15">to</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="14">reason</governor>
          <dependent id="16">think</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="20">pass</governor>
          <dependent id="17">Ben</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="20">pass</governor>
          <dependent id="18">may</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="20">pass</governor>
          <dependent id="19">not</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="16">think</governor>
          <dependent id="20">pass</dependent>
        </dependency>
        <dependency type="det">
          <governor id="22">test</governor>
          <dependent id="21">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="20">pass</governor>
          <dependent id="22">test</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Huey" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="Huey" />
          </tokens>
        </entity>
        <entity id="2" string="Francis" type="PERSON" score="0.0">
          <tokens>
            <token id="9" string="Francis" />
          </tokens>
        </entity>
        <entity id="3" string="Ben" type="PERSON" score="0.0">
          <tokens>
            <token id="17" string="Ben" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="12" has_coreference="false">
      <content>JULIE CART</content>
      <tokens>
        <token id="1" string="JULIE" lemma="JULIE" stem="julie" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="2" string="CART" lemma="CART" stem="cart" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NNP JULIE) (NNP CART)))</syntactictree>
      <chunkings>
        <chunking id="1" string="JULIE CART" type="NP">
          <tokens>
            <token id="1" string="JULIE" />
            <token id="2" string="CART" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">CART</governor>
          <dependent id="1">JULIE</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">CART</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="JULIE CART" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="JULIE" />
            <token id="2" string="CART" />
          </tokens>
        </entity>
      </entities>
    </sentence>
  </sentences>
  <coreferences>
    <coreference id="1" type="PROPER">
      <referenced ids_tokens="3-4" string="Charlie Francis" id_sentence="1" />
      <mentions>
        <mention ids_tokens="24" string="Francis" id_sentence="2" />
        <mention ids_tokens="2" string="Charlie" id_sentence="5" />
        <mention ids_tokens="6" string="me" id_sentence="5" />
        <mention ids_tokens="8-24" string="Francis , who has been in Toronto testifying at a Canadian inquiry into drug use in sport" id_sentence="9" />
        <mention ids_tokens="8" string="Francis" id_sentence="9" />
        <mention ids_tokens="1" string="Francis" id_sentence="10" />
        <mention ids_tokens="9" string="Francis" id_sentence="11" />
      </mentions>
    </coreference>
    <coreference id="3" type="PROPER">
      <referenced ids_tokens="3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-21-22-23-24" string="Charlie Francis , who claimed that sprinter Ben Johnson 's urine sample at the Seoul Olympics was spiked with a banned steroid" id_sentence="1" />
      <mentions>
        <mention ids_tokens="7" string="Ben" id_sentence="6" />
        <mention ids_tokens="1" string="Charlie" id_sentence="7" />
        <mention ids_tokens="5-6" string="Ben's" id_sentence="7" />
        <mention ids_tokens="17" string="Ben" id_sentence="11" />
      </mentions>
    </coreference>
    <coreference id="4" type="PROPER">
      <referenced ids_tokens="9-10-11-12" string="sprinter Ben Johnson 's" id_sentence="1" />
      <mentions>
        <mention ids_tokens="30-31" string="Johnson's" id_sentence="2" />
        <mention ids_tokens="28" string="Johnson" id_sentence="9" />
        <mention ids_tokens="4" string="Johnson" id_sentence="10" />
        <mention ids_tokens="26-27" string="Johnson's" id_sentence="10" />
      </mentions>
    </coreference>
    <coreference id="5" type="PROPER">
      <referenced ids_tokens="16-17-18" string="the Seoul Olympics" id_sentence="1" />
      <mentions>
        <mention ids_tokens="32-33" string="the Olympics" id_sentence="9" />
      </mentions>
    </coreference>
    <coreference id="6" type="NOMINAL">
      <referenced ids_tokens="22-23-24" string="a banned steroid" id_sentence="1" />
      <mentions>
        <mention ids_tokens="8-9" string="the steroid" id_sentence="10" />
      </mentions>
    </coreference>
    <coreference id="7" type="PROPER">
      <referenced ids_tokens="1-2" string="Lynda Huey" id_sentence="2" />
      <mentions>
        <mention ids_tokens="1" string="Huey" id_sentence="3" />
        <mention ids_tokens="3" string="she" id_sentence="3" />
        <mention ids_tokens="18" string="her" id_sentence="3" />
        <mention ids_tokens="1" string="Huey" id_sentence="4" />
        <mention ids_tokens="3" string="she" id_sentence="4" />
        <mention ids_tokens="17" string="her" id_sentence="4" />
        <mention ids_tokens="13" string="Huey" id_sentence="5" />
        <mention ids_tokens="15" string="he" id_sentence="7" />
        <mention ids_tokens="1" string="He" id_sentence="8" />
        <mention ids_tokens="1" string="Huey" id_sentence="9" />
        <mention ids_tokens="3" string="she" id_sentence="9" />
        <mention ids_tokens="5" string="Huey" id_sentence="11" />
      </mentions>
    </coreference>
    <coreference id="9" type="PROPER">
      <referenced ids_tokens="37-38-39-40" string="U.S. sprinter Carl Lewis" id_sentence="2" />
      <mentions>
        <mention ids_tokens="18-19" string="Carl Lewis" id_sentence="7" />
      </mentions>
    </coreference>
    <coreference id="11" type="LIST">
      <referenced ids_tokens="10-11-12-13-14-15" string="he and sprinter Angella Taylor Issajenko" id_sentence="3" />
      <mentions>
        <mention ids_tokens="8" string="we" id_sentence="5" />
        <mention ids_tokens="2" string="We" id_sentence="6" />
      </mentions>
    </coreference>
    <coreference id="13" type="NOMINAL">
      <referenced ids_tokens="5-6-7-8-9-10-11-12-13-14-15" string="Ben 's more afraid of failing the drug test than he" id_sentence="7" />
      <mentions>
        <mention ids_tokens="21-22" string="the test" id_sentence="11" />
      </mentions>
    </coreference>
  </coreferences>
</document>
