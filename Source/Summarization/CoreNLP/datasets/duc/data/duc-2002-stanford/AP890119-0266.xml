<?xml version="1.0" encoding="UTF-8"?>
<document id="0" name="AP890119-0266">
  <sentences>
    <sentence id="1" has_coreference="true">
      <content>The image of a dour, shoeless English boy and his absent, carefree mother prompted Julia Baird and Geoffrey Giuliano to collaborate on a book.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="image" lemma="image" stem="imag" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="dour" lemma="dour" stem="dour" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="shoeless" lemma="shoeless" stem="shoeless" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="English" lemma="English" stem="english" pos="NNP" type="Word" isStopWord="false" ner="MISC" is_referenced="true" is_refers="false" />
        <token id="9" string="boy" lemma="boy" stem="boi" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="12" string="absent" lemma="absent" stem="absent" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="carefree" lemma="carefree" stem="carefre" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="16" string="prompted" lemma="prompt" stem="prompt" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="Julia" lemma="Julia" stem="julia" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="18" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="19" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="Geoffrey" lemma="Geoffrey" stem="geoffrei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="21" string="Giuliano" lemma="Giuliano" stem="giuliano" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="22" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="collaborate" lemma="collaborate" stem="collabor" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="26" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="27" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NP (DT The) (NN image)) (PP (IN of) (NP (DT a) (JJ dour) (, ,) (JJ shoeless) (NNP English) (NN boy)))) (CC and) (NP (PRP$ his) (ADJP (JJ absent) (, ,) (JJ carefree)) (NN mother))) (VP (VBD prompted) (S (NP (NNP Julia) (NNP Baird) (CC and) (NNP Geoffrey) (NNP Giuliano)) (VP (TO to) (VP (VB collaborate) (PP (IN on) (NP (DT a) (NN book))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="prompted Julia Baird and Geoffrey Giuliano to collaborate on a book" type="VP">
          <tokens>
            <token id="16" string="prompted" />
            <token id="17" string="Julia" />
            <token id="18" string="Baird" />
            <token id="19" string="and" />
            <token id="20" string="Geoffrey" />
            <token id="21" string="Giuliano" />
            <token id="22" string="to" />
            <token id="23" string="collaborate" />
            <token id="24" string="on" />
            <token id="25" string="a" />
            <token id="26" string="book" />
          </tokens>
        </chunking>
        <chunking id="2" string="a book" type="NP">
          <tokens>
            <token id="25" string="a" />
            <token id="26" string="book" />
          </tokens>
        </chunking>
        <chunking id="3" string="The image" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="image" />
          </tokens>
        </chunking>
        <chunking id="4" string="The image of a dour , shoeless English boy" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="image" />
            <token id="3" string="of" />
            <token id="4" string="a" />
            <token id="5" string="dour" />
            <token id="6" string="," />
            <token id="7" string="shoeless" />
            <token id="8" string="English" />
            <token id="9" string="boy" />
          </tokens>
        </chunking>
        <chunking id="5" string="to collaborate on a book" type="VP">
          <tokens>
            <token id="22" string="to" />
            <token id="23" string="collaborate" />
            <token id="24" string="on" />
            <token id="25" string="a" />
            <token id="26" string="book" />
          </tokens>
        </chunking>
        <chunking id="6" string="Julia Baird and Geoffrey Giuliano" type="NP">
          <tokens>
            <token id="17" string="Julia" />
            <token id="18" string="Baird" />
            <token id="19" string="and" />
            <token id="20" string="Geoffrey" />
            <token id="21" string="Giuliano" />
          </tokens>
        </chunking>
        <chunking id="7" string="a dour , shoeless English boy" type="NP">
          <tokens>
            <token id="4" string="a" />
            <token id="5" string="dour" />
            <token id="6" string="," />
            <token id="7" string="shoeless" />
            <token id="8" string="English" />
            <token id="9" string="boy" />
          </tokens>
        </chunking>
        <chunking id="8" string="collaborate on a book" type="VP">
          <tokens>
            <token id="23" string="collaborate" />
            <token id="24" string="on" />
            <token id="25" string="a" />
            <token id="26" string="book" />
          </tokens>
        </chunking>
        <chunking id="9" string="absent , carefree" type="ADJP">
          <tokens>
            <token id="12" string="absent" />
            <token id="13" string="," />
            <token id="14" string="carefree" />
          </tokens>
        </chunking>
        <chunking id="10" string="his absent , carefree mother" type="NP">
          <tokens>
            <token id="11" string="his" />
            <token id="12" string="absent" />
            <token id="13" string="," />
            <token id="14" string="carefree" />
            <token id="15" string="mother" />
          </tokens>
        </chunking>
        <chunking id="11" string="The image of a dour , shoeless English boy and his absent , carefree mother" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="image" />
            <token id="3" string="of" />
            <token id="4" string="a" />
            <token id="5" string="dour" />
            <token id="6" string="," />
            <token id="7" string="shoeless" />
            <token id="8" string="English" />
            <token id="9" string="boy" />
            <token id="10" string="and" />
            <token id="11" string="his" />
            <token id="12" string="absent" />
            <token id="13" string="," />
            <token id="14" string="carefree" />
            <token id="15" string="mother" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">image</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">prompted</governor>
          <dependent id="2">image</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">boy</governor>
          <dependent id="3">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">boy</governor>
          <dependent id="4">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">boy</governor>
          <dependent id="5">dour</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">boy</governor>
          <dependent id="7">shoeless</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">boy</governor>
          <dependent id="8">English</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">image</governor>
          <dependent id="9">boy</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="2">image</governor>
          <dependent id="10">and</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="15">mother</governor>
          <dependent id="11">his</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="14">carefree</governor>
          <dependent id="12">absent</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">mother</governor>
          <dependent id="14">carefree</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="2">image</governor>
          <dependent id="15">mother</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="16">prompted</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="18">Baird</governor>
          <dependent id="17">Julia</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="16">prompted</governor>
          <dependent id="18">Baird</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="18">Baird</governor>
          <dependent id="19">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="21">Giuliano</governor>
          <dependent id="20">Geoffrey</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="18">Baird</governor>
          <dependent id="21">Giuliano</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="23">collaborate</governor>
          <dependent id="22">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="16">prompted</governor>
          <dependent id="23">collaborate</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">book</governor>
          <dependent id="24">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="26">book</governor>
          <dependent id="25">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">collaborate</governor>
          <dependent id="26">book</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Julia Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="17" string="Julia" />
            <token id="18" string="Baird" />
          </tokens>
        </entity>
        <entity id="2" string="Geoffrey Giuliano" type="PERSON" score="0.0">
          <tokens>
            <token id="20" string="Geoffrey" />
            <token id="21" string="Giuliano" />
          </tokens>
        </entity>
        <entity id="3" string="English" type="MISC" score="0.0">
          <tokens>
            <token id="8" string="English" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="2" has_coreference="true">
      <content>The boy is the public image of young John Lennon, Baird&amp;apost;s half-brother and Giuliano&amp;apost;s idol.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="boy" lemma="boy" stem="boi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="public" lemma="public" stem="public" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="image" lemma="image" stem="imag" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="young" lemma="young" stem="young" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="10" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="12" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="13" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="half-brother" lemma="half-brother" stem="half-broth" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="16" string="Giuliano" lemma="Giuliano" stem="giuliano" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="17" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="18" string="idol" lemma="idol" stem="idol" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="19" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (NP (NP (DT the) (JJ public) (NN image)) (PP (IN of) (NP (NP (JJ young) (NNP John) (NNP Lennon)) (, ,) (NP (NP (NP (NNP Baird) (POS 's)) (NN half-brother)) (CC and) (NP (NP (NNP Giuliano) (POS 's)) (NN idol))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Baird 's half-brother" type="NP">
          <tokens>
            <token id="12" string="Baird" />
            <token id="13" string="'s" />
            <token id="14" string="half-brother" />
          </tokens>
        </chunking>
        <chunking id="2" string="Giuliano 's" type="NP">
          <tokens>
            <token id="16" string="Giuliano" />
            <token id="17" string="'s" />
          </tokens>
        </chunking>
        <chunking id="3" string="young John Lennon" type="NP">
          <tokens>
            <token id="8" string="young" />
            <token id="9" string="John" />
            <token id="10" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="4" string="the public image" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="public" />
            <token id="6" string="image" />
          </tokens>
        </chunking>
        <chunking id="5" string="Baird 's" type="NP">
          <tokens>
            <token id="12" string="Baird" />
            <token id="13" string="'s" />
          </tokens>
        </chunking>
        <chunking id="6" string="is the public image of young John Lennon , Baird 's half-brother and Giuliano 's idol" type="VP">
          <tokens>
            <token id="3" string="is" />
            <token id="4" string="the" />
            <token id="5" string="public" />
            <token id="6" string="image" />
            <token id="7" string="of" />
            <token id="8" string="young" />
            <token id="9" string="John" />
            <token id="10" string="Lennon" />
            <token id="11" string="," />
            <token id="12" string="Baird" />
            <token id="13" string="'s" />
            <token id="14" string="half-brother" />
            <token id="15" string="and" />
            <token id="16" string="Giuliano" />
            <token id="17" string="'s" />
            <token id="18" string="idol" />
          </tokens>
        </chunking>
        <chunking id="7" string="the public image of young John Lennon , Baird 's half-brother and Giuliano 's idol" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="public" />
            <token id="6" string="image" />
            <token id="7" string="of" />
            <token id="8" string="young" />
            <token id="9" string="John" />
            <token id="10" string="Lennon" />
            <token id="11" string="," />
            <token id="12" string="Baird" />
            <token id="13" string="'s" />
            <token id="14" string="half-brother" />
            <token id="15" string="and" />
            <token id="16" string="Giuliano" />
            <token id="17" string="'s" />
            <token id="18" string="idol" />
          </tokens>
        </chunking>
        <chunking id="8" string="Baird 's half-brother and Giuliano 's idol" type="NP">
          <tokens>
            <token id="12" string="Baird" />
            <token id="13" string="'s" />
            <token id="14" string="half-brother" />
            <token id="15" string="and" />
            <token id="16" string="Giuliano" />
            <token id="17" string="'s" />
            <token id="18" string="idol" />
          </tokens>
        </chunking>
        <chunking id="9" string="young John Lennon , Baird 's half-brother and Giuliano 's idol" type="NP">
          <tokens>
            <token id="8" string="young" />
            <token id="9" string="John" />
            <token id="10" string="Lennon" />
            <token id="11" string="," />
            <token id="12" string="Baird" />
            <token id="13" string="'s" />
            <token id="14" string="half-brother" />
            <token id="15" string="and" />
            <token id="16" string="Giuliano" />
            <token id="17" string="'s" />
            <token id="18" string="idol" />
          </tokens>
        </chunking>
        <chunking id="10" string="Giuliano 's idol" type="NP">
          <tokens>
            <token id="16" string="Giuliano" />
            <token id="17" string="'s" />
            <token id="18" string="idol" />
          </tokens>
        </chunking>
        <chunking id="11" string="The boy" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="boy" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">boy</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">image</governor>
          <dependent id="2">boy</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="6">image</governor>
          <dependent id="3">is</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">image</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">image</governor>
          <dependent id="5">public</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">image</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">Lennon</governor>
          <dependent id="7">of</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="10">Lennon</governor>
          <dependent id="8">young</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">Lennon</governor>
          <dependent id="9">John</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">image</governor>
          <dependent id="10">Lennon</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="14">half-brother</governor>
          <dependent id="12">Baird</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">Baird</governor>
          <dependent id="13">'s</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="10">Lennon</governor>
          <dependent id="14">half-brother</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="14">half-brother</governor>
          <dependent id="15">and</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="18">idol</governor>
          <dependent id="16">Giuliano</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">Giuliano</governor>
          <dependent id="17">'s</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">half-brother</governor>
          <dependent id="18">idol</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="9" string="John" />
            <token id="10" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="12" string="Baird" />
          </tokens>
        </entity>
        <entity id="3" string="Giuliano" type="PERSON" score="0.0">
          <tokens>
            <token id="16" string="Giuliano" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="3" has_coreference="true">
      <content>But the neglected urchin and his live-it-up mother _ who was also Baird&amp;apost;s mother _ never existed, the authors say.</content>
      <tokens>
        <token id="1" string="But" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="3" string="neglected" lemma="neglect" stem="neglect" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="urchin" lemma="urchin" stem="urchin" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="live-it-up" lemma="live-it-up" stem="live-it-up" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="_" lemma="_" stem="_" pos="NN" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="14" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="_" lemma="_" stem="_" pos="NN" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="never" lemma="never" stem="never" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="existed" lemma="exist" stem="exist" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="authors" lemma="author" stem="author" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="say" lemma="say" stem="sai" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (CC But) (NP (NP (NP (DT the) (VBN neglected) (NN urchin)) (CC and) (NP (PRP$ his) (JJ live-it-up) (NN mother) (NN _))) (SBAR (WHNP (WP who)) (S (VP (VBD was) (ADVP (RB also)) (NP (NP (NNP Baird) (POS 's)) (NN mother) (NN _)))))) (ADVP (RB never)) (VP (VBD existed) (, ,) (SBAR (S (NP (DT the) (NNS authors)) (VP (VBP say))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the neglected urchin and his live-it-up mother _" type="NP">
          <tokens>
            <token id="2" string="the" />
            <token id="3" string="neglected" />
            <token id="4" string="urchin" />
            <token id="5" string="and" />
            <token id="6" string="his" />
            <token id="7" string="live-it-up" />
            <token id="8" string="mother" />
            <token id="9" string="_" />
          </tokens>
        </chunking>
        <chunking id="2" string="his live-it-up mother _" type="NP">
          <tokens>
            <token id="6" string="his" />
            <token id="7" string="live-it-up" />
            <token id="8" string="mother" />
            <token id="9" string="_" />
          </tokens>
        </chunking>
        <chunking id="3" string="who was also Baird 's mother _" type="SBAR">
          <tokens>
            <token id="10" string="who" />
            <token id="11" string="was" />
            <token id="12" string="also" />
            <token id="13" string="Baird" />
            <token id="14" string="'s" />
            <token id="15" string="mother" />
            <token id="16" string="_" />
          </tokens>
        </chunking>
        <chunking id="4" string="the authors say" type="SBAR">
          <tokens>
            <token id="20" string="the" />
            <token id="21" string="authors" />
            <token id="22" string="say" />
          </tokens>
        </chunking>
        <chunking id="5" string="existed , the authors say" type="VP">
          <tokens>
            <token id="18" string="existed" />
            <token id="19" string="," />
            <token id="20" string="the" />
            <token id="21" string="authors" />
            <token id="22" string="say" />
          </tokens>
        </chunking>
        <chunking id="6" string="the neglected urchin" type="NP">
          <tokens>
            <token id="2" string="the" />
            <token id="3" string="neglected" />
            <token id="4" string="urchin" />
          </tokens>
        </chunking>
        <chunking id="7" string="Baird 's" type="NP">
          <tokens>
            <token id="13" string="Baird" />
            <token id="14" string="'s" />
          </tokens>
        </chunking>
        <chunking id="8" string="Baird 's mother _" type="NP">
          <tokens>
            <token id="13" string="Baird" />
            <token id="14" string="'s" />
            <token id="15" string="mother" />
            <token id="16" string="_" />
          </tokens>
        </chunking>
        <chunking id="9" string="the neglected urchin and his live-it-up mother _ who was also Baird 's mother _" type="NP">
          <tokens>
            <token id="2" string="the" />
            <token id="3" string="neglected" />
            <token id="4" string="urchin" />
            <token id="5" string="and" />
            <token id="6" string="his" />
            <token id="7" string="live-it-up" />
            <token id="8" string="mother" />
            <token id="9" string="_" />
            <token id="10" string="who" />
            <token id="11" string="was" />
            <token id="12" string="also" />
            <token id="13" string="Baird" />
            <token id="14" string="'s" />
            <token id="15" string="mother" />
            <token id="16" string="_" />
          </tokens>
        </chunking>
        <chunking id="10" string="was also Baird 's mother _" type="VP">
          <tokens>
            <token id="11" string="was" />
            <token id="12" string="also" />
            <token id="13" string="Baird" />
            <token id="14" string="'s" />
            <token id="15" string="mother" />
            <token id="16" string="_" />
          </tokens>
        </chunking>
        <chunking id="11" string="the authors" type="NP">
          <tokens>
            <token id="20" string="the" />
            <token id="21" string="authors" />
          </tokens>
        </chunking>
        <chunking id="12" string="say" type="VP">
          <tokens>
            <token id="22" string="say" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="cc">
          <governor id="18">existed</governor>
          <dependent id="1">But</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">urchin</governor>
          <dependent id="2">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="4">urchin</governor>
          <dependent id="3">neglected</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="18">existed</governor>
          <dependent id="4">urchin</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">urchin</governor>
          <dependent id="5">and</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="9">_</governor>
          <dependent id="6">his</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">_</governor>
          <dependent id="7">live-it-up</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">_</governor>
          <dependent id="8">mother</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">urchin</governor>
          <dependent id="9">_</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">_</governor>
          <dependent id="10">who</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="16">_</governor>
          <dependent id="11">was</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="16">_</governor>
          <dependent id="12">also</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="16">_</governor>
          <dependent id="13">Baird</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">Baird</governor>
          <dependent id="14">'s</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="16">_</governor>
          <dependent id="15">mother</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="4">urchin</governor>
          <dependent id="16">_</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="18">existed</governor>
          <dependent id="17">never</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="18">existed</dependent>
        </dependency>
        <dependency type="det">
          <governor id="21">authors</governor>
          <dependent id="20">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">say</governor>
          <dependent id="21">authors</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="18">existed</governor>
          <dependent id="22">say</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="13" string="Baird" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="4" has_coreference="true">
      <content>``That woman laughing, singing, dancing, saying goodbye to John, and John standing there alone crying `Mummy, Mummy!&amp;apost;</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="That" lemma="that" stem="that" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="3" string="woman" lemma="woman" stem="woman" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="laughing" lemma="laughing" stem="laugh" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="singing" lemma="singing" stem="sing" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="dancing" lemma="dancing" stem="danc" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="saying" lemma="say" stem="sai" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="goodbye" lemma="goodbye" stem="goodby" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="17" string="standing" lemma="stand" stem="stand" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="there" lemma="there" stem="there" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="alone" lemma="alone" stem="alon" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="crying" lemma="cry" stem="cry" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="`" lemma="`" stem="`" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="Mummy" lemma="Mummy" stem="mummi" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="Mummy" lemma="Mummy" stem="mummi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="25" string="!" lemma="!" stem="!" pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (NP (NP (DT That) (NN woman) (NN laughing)) (, ,) (NP (NP (NN singing)) (, ,) (NP (NN dancing))) (, ,)) (VP (VBG saying) (NP (NN goodbye)) (PP (TO to) (NP (NNP John))))) (, ,) (CC and) (S (NP (NNP John)) (VP (VBG standing) (ADVP (RB there)) (ADVP (RB alone)) (S (VP (VBG crying) (NP (`` `) (NP (NNP Mummy)) (, ,) (NP (NNP Mummy))))))) (. !) ('' ')))</syntactictree>
      <chunkings>
        <chunking id="1" string="standing there alone crying ` Mummy , Mummy" type="VP">
          <tokens>
            <token id="17" string="standing" />
            <token id="18" string="there" />
            <token id="19" string="alone" />
            <token id="20" string="crying" />
            <token id="21" string="`" />
            <token id="22" string="Mummy" />
            <token id="23" string="," />
            <token id="24" string="Mummy" />
          </tokens>
        </chunking>
        <chunking id="2" string="crying ` Mummy , Mummy" type="VP">
          <tokens>
            <token id="20" string="crying" />
            <token id="21" string="`" />
            <token id="22" string="Mummy" />
            <token id="23" string="," />
            <token id="24" string="Mummy" />
          </tokens>
        </chunking>
        <chunking id="3" string="` Mummy , Mummy" type="NP">
          <tokens>
            <token id="21" string="`" />
            <token id="22" string="Mummy" />
            <token id="23" string="," />
            <token id="24" string="Mummy" />
          </tokens>
        </chunking>
        <chunking id="4" string="Mummy" type="NP">
          <tokens>
            <token id="22" string="Mummy" />
          </tokens>
        </chunking>
        <chunking id="5" string="singing" type="NP">
          <tokens>
            <token id="6" string="singing" />
          </tokens>
        </chunking>
        <chunking id="6" string="goodbye" type="NP">
          <tokens>
            <token id="11" string="goodbye" />
          </tokens>
        </chunking>
        <chunking id="7" string="That woman laughing , singing , dancing ," type="NP">
          <tokens>
            <token id="2" string="That" />
            <token id="3" string="woman" />
            <token id="4" string="laughing" />
            <token id="5" string="," />
            <token id="6" string="singing" />
            <token id="7" string="," />
            <token id="8" string="dancing" />
            <token id="9" string="," />
          </tokens>
        </chunking>
        <chunking id="8" string="John" type="NP">
          <tokens>
            <token id="13" string="John" />
          </tokens>
        </chunking>
        <chunking id="9" string="That woman laughing" type="NP">
          <tokens>
            <token id="2" string="That" />
            <token id="3" string="woman" />
            <token id="4" string="laughing" />
          </tokens>
        </chunking>
        <chunking id="10" string="saying goodbye to John" type="VP">
          <tokens>
            <token id="10" string="saying" />
            <token id="11" string="goodbye" />
            <token id="12" string="to" />
            <token id="13" string="John" />
          </tokens>
        </chunking>
        <chunking id="11" string="singing , dancing" type="NP">
          <tokens>
            <token id="6" string="singing" />
            <token id="7" string="," />
            <token id="8" string="dancing" />
          </tokens>
        </chunking>
        <chunking id="12" string="dancing" type="NP">
          <tokens>
            <token id="8" string="dancing" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="4">laughing</governor>
          <dependent id="2">That</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">laughing</governor>
          <dependent id="3">woman</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">saying</governor>
          <dependent id="4">laughing</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="4">laughing</governor>
          <dependent id="6">singing</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="6">singing</governor>
          <dependent id="8">dancing</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="10">saying</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="10">saying</governor>
          <dependent id="11">goodbye</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">John</governor>
          <dependent id="12">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">saying</governor>
          <dependent id="13">John</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="10">saying</governor>
          <dependent id="15">and</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="17">standing</governor>
          <dependent id="16">John</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="10">saying</governor>
          <dependent id="17">standing</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="17">standing</governor>
          <dependent id="18">there</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="17">standing</governor>
          <dependent id="19">alone</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="17">standing</governor>
          <dependent id="20">crying</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="20">crying</governor>
          <dependent id="22">Mummy</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="22">Mummy</governor>
          <dependent id="24">Mummy</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Mummy" type="PERSON" score="0.0">
          <tokens>
            <token id="24" string="Mummy" />
          </tokens>
        </entity>
        <entity id="2" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="13" string="John" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="5" has_coreference="true">
      <content>That&amp;apost;s not my brother, that&amp;apost;s not my mother, that&amp;apost;s not my children&amp;apost;s grandmother,&amp;apost;&amp;apost; said Baird, who recently spent three weeks in the United States promoting her book, a family portrait of Lennon and his Liverpool roots.</content>
      <tokens>
        <token id="1" string="That" lemma="that" stem="that" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="my" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="5" string="brother" lemma="brother" stem="brother" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="that" lemma="that" stem="that" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="my" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="11" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="that" lemma="that" stem="that" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="16" string="my" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="17" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="18" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="19" string="grandmother" lemma="grandmother" stem="grandmoth" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="24" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="25" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="26" string="recently" lemma="recently" stem="recent" pos="RB" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="27" string="spent" lemma="spend" stem="spent" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="28" string="three" lemma="three" stem="three" pos="CD" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="true" />
        <token id="29" string="weeks" lemma="week" stem="week" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="true" />
        <token id="30" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="31" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="32" string="United" lemma="United" stem="unite" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="true" />
        <token id="33" string="States" lemma="States" stem="state" pos="NNPS" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="true" />
        <token id="34" string="promoting" lemma="promote" stem="promot" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="35" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="36" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="37" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="38" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="39" string="family" lemma="family" stem="famili" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="40" string="portrait" lemma="portrait" stem="portrait" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="41" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="42" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="43" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="44" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="45" string="Liverpool" lemma="Liverpool" stem="liverpool" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="true" />
        <token id="46" string="roots" lemma="root" stem="root" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="47" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (SINV (S (NP (DT That)) (VP (VBZ 's) (RB not) (NP (NP (PRP$ my) (NN brother)) (, ,) (SBAR (WHNP (DT that)) (S (VP (VBZ 's) (RB not) (NP (NP (PRP$ my) (NN mother)) (, ,) (SBAR (WHNP (DT that)) (S (VP (VBZ 's) (RB not) (NP (NP (PRP$ my) (NNS children) (POS 's)) (NN grandmother)))))))))))) (, ,) ('' '') (VP (VBD said)) (NP (NP (NNP Baird)) (, ,) (SBAR (WHNP (WP who)) (S (ADVP (RB recently)) (VP (VBD spent) (NP (CD three) (NNS weeks)) (PP (IN in) (NP (DT the) (NNP United) (NNPS States))) (S (VP (VBG promoting) (NP (NP (PRP$ her) (NN book)) (, ,) (NP (NP (DT a) (NN family) (NN portrait)) (PP (IN of) (NP (NNP Lennon)))) (CC and) (NP (PRP$ his) (NNP Liverpool) (NNS roots))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="That" type="NP">
          <tokens>
            <token id="1" string="That" />
          </tokens>
        </chunking>
        <chunking id="2" string="promoting her book , a family portrait of Lennon and his Liverpool roots" type="VP">
          <tokens>
            <token id="34" string="promoting" />
            <token id="35" string="her" />
            <token id="36" string="book" />
            <token id="37" string="," />
            <token id="38" string="a" />
            <token id="39" string="family" />
            <token id="40" string="portrait" />
            <token id="41" string="of" />
            <token id="42" string="Lennon" />
            <token id="43" string="and" />
            <token id="44" string="his" />
            <token id="45" string="Liverpool" />
            <token id="46" string="roots" />
          </tokens>
        </chunking>
        <chunking id="3" string="her book , a family portrait of Lennon and his Liverpool roots" type="NP">
          <tokens>
            <token id="35" string="her" />
            <token id="36" string="book" />
            <token id="37" string="," />
            <token id="38" string="a" />
            <token id="39" string="family" />
            <token id="40" string="portrait" />
            <token id="41" string="of" />
            <token id="42" string="Lennon" />
            <token id="43" string="and" />
            <token id="44" string="his" />
            <token id="45" string="Liverpool" />
            <token id="46" string="roots" />
          </tokens>
        </chunking>
        <chunking id="4" string="the United States" type="NP">
          <tokens>
            <token id="31" string="the" />
            <token id="32" string="United" />
            <token id="33" string="States" />
          </tokens>
        </chunking>
        <chunking id="5" string="'s not my mother , that 's not my children 's grandmother" type="VP">
          <tokens>
            <token id="8" string="'s" />
            <token id="9" string="not" />
            <token id="10" string="my" />
            <token id="11" string="mother" />
            <token id="12" string="," />
            <token id="13" string="that" />
            <token id="14" string="'s" />
            <token id="15" string="not" />
            <token id="16" string="my" />
            <token id="17" string="children" />
            <token id="18" string="'s" />
            <token id="19" string="grandmother" />
          </tokens>
        </chunking>
        <chunking id="6" string="'s not my brother , that 's not my mother , that 's not my children 's grandmother" type="VP">
          <tokens>
            <token id="2" string="'s" />
            <token id="3" string="not" />
            <token id="4" string="my" />
            <token id="5" string="brother" />
            <token id="6" string="," />
            <token id="7" string="that" />
            <token id="8" string="'s" />
            <token id="9" string="not" />
            <token id="10" string="my" />
            <token id="11" string="mother" />
            <token id="12" string="," />
            <token id="13" string="that" />
            <token id="14" string="'s" />
            <token id="15" string="not" />
            <token id="16" string="my" />
            <token id="17" string="children" />
            <token id="18" string="'s" />
            <token id="19" string="grandmother" />
          </tokens>
        </chunking>
        <chunking id="7" string="who recently spent three weeks in the United States promoting her book , a family portrait of Lennon and his Liverpool roots" type="SBAR">
          <tokens>
            <token id="25" string="who" />
            <token id="26" string="recently" />
            <token id="27" string="spent" />
            <token id="28" string="three" />
            <token id="29" string="weeks" />
            <token id="30" string="in" />
            <token id="31" string="the" />
            <token id="32" string="United" />
            <token id="33" string="States" />
            <token id="34" string="promoting" />
            <token id="35" string="her" />
            <token id="36" string="book" />
            <token id="37" string="," />
            <token id="38" string="a" />
            <token id="39" string="family" />
            <token id="40" string="portrait" />
            <token id="41" string="of" />
            <token id="42" string="Lennon" />
            <token id="43" string="and" />
            <token id="44" string="his" />
            <token id="45" string="Liverpool" />
            <token id="46" string="roots" />
          </tokens>
        </chunking>
        <chunking id="8" string="spent three weeks in the United States promoting her book , a family portrait of Lennon and his Liverpool roots" type="VP">
          <tokens>
            <token id="27" string="spent" />
            <token id="28" string="three" />
            <token id="29" string="weeks" />
            <token id="30" string="in" />
            <token id="31" string="the" />
            <token id="32" string="United" />
            <token id="33" string="States" />
            <token id="34" string="promoting" />
            <token id="35" string="her" />
            <token id="36" string="book" />
            <token id="37" string="," />
            <token id="38" string="a" />
            <token id="39" string="family" />
            <token id="40" string="portrait" />
            <token id="41" string="of" />
            <token id="42" string="Lennon" />
            <token id="43" string="and" />
            <token id="44" string="his" />
            <token id="45" string="Liverpool" />
            <token id="46" string="roots" />
          </tokens>
        </chunking>
        <chunking id="9" string="Baird , who recently spent three weeks in the United States promoting her book , a family portrait of Lennon and his Liverpool roots" type="NP">
          <tokens>
            <token id="23" string="Baird" />
            <token id="24" string="," />
            <token id="25" string="who" />
            <token id="26" string="recently" />
            <token id="27" string="spent" />
            <token id="28" string="three" />
            <token id="29" string="weeks" />
            <token id="30" string="in" />
            <token id="31" string="the" />
            <token id="32" string="United" />
            <token id="33" string="States" />
            <token id="34" string="promoting" />
            <token id="35" string="her" />
            <token id="36" string="book" />
            <token id="37" string="," />
            <token id="38" string="a" />
            <token id="39" string="family" />
            <token id="40" string="portrait" />
            <token id="41" string="of" />
            <token id="42" string="Lennon" />
            <token id="43" string="and" />
            <token id="44" string="his" />
            <token id="45" string="Liverpool" />
            <token id="46" string="roots" />
          </tokens>
        </chunking>
        <chunking id="10" string="a family portrait" type="NP">
          <tokens>
            <token id="38" string="a" />
            <token id="39" string="family" />
            <token id="40" string="portrait" />
          </tokens>
        </chunking>
        <chunking id="11" string="that 's not my children 's grandmother" type="SBAR">
          <tokens>
            <token id="13" string="that" />
            <token id="14" string="'s" />
            <token id="15" string="not" />
            <token id="16" string="my" />
            <token id="17" string="children" />
            <token id="18" string="'s" />
            <token id="19" string="grandmother" />
          </tokens>
        </chunking>
        <chunking id="12" string="my brother , that 's not my mother , that 's not my children 's grandmother" type="NP">
          <tokens>
            <token id="4" string="my" />
            <token id="5" string="brother" />
            <token id="6" string="," />
            <token id="7" string="that" />
            <token id="8" string="'s" />
            <token id="9" string="not" />
            <token id="10" string="my" />
            <token id="11" string="mother" />
            <token id="12" string="," />
            <token id="13" string="that" />
            <token id="14" string="'s" />
            <token id="15" string="not" />
            <token id="16" string="my" />
            <token id="17" string="children" />
            <token id="18" string="'s" />
            <token id="19" string="grandmother" />
          </tokens>
        </chunking>
        <chunking id="13" string="Lennon" type="NP">
          <tokens>
            <token id="42" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="14" string="'s not my children 's grandmother" type="VP">
          <tokens>
            <token id="14" string="'s" />
            <token id="15" string="not" />
            <token id="16" string="my" />
            <token id="17" string="children" />
            <token id="18" string="'s" />
            <token id="19" string="grandmother" />
          </tokens>
        </chunking>
        <chunking id="15" string="a family portrait of Lennon" type="NP">
          <tokens>
            <token id="38" string="a" />
            <token id="39" string="family" />
            <token id="40" string="portrait" />
            <token id="41" string="of" />
            <token id="42" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="16" string="his Liverpool roots" type="NP">
          <tokens>
            <token id="44" string="his" />
            <token id="45" string="Liverpool" />
            <token id="46" string="roots" />
          </tokens>
        </chunking>
        <chunking id="17" string="my brother" type="NP">
          <tokens>
            <token id="4" string="my" />
            <token id="5" string="brother" />
          </tokens>
        </chunking>
        <chunking id="18" string="three weeks" type="NP">
          <tokens>
            <token id="28" string="three" />
            <token id="29" string="weeks" />
          </tokens>
        </chunking>
        <chunking id="19" string="that 's not my mother , that 's not my children 's grandmother" type="SBAR">
          <tokens>
            <token id="7" string="that" />
            <token id="8" string="'s" />
            <token id="9" string="not" />
            <token id="10" string="my" />
            <token id="11" string="mother" />
            <token id="12" string="," />
            <token id="13" string="that" />
            <token id="14" string="'s" />
            <token id="15" string="not" />
            <token id="16" string="my" />
            <token id="17" string="children" />
            <token id="18" string="'s" />
            <token id="19" string="grandmother" />
          </tokens>
        </chunking>
        <chunking id="20" string="Baird" type="NP">
          <tokens>
            <token id="23" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="21" string="my children 's" type="NP">
          <tokens>
            <token id="16" string="my" />
            <token id="17" string="children" />
            <token id="18" string="'s" />
          </tokens>
        </chunking>
        <chunking id="22" string="her book" type="NP">
          <tokens>
            <token id="35" string="her" />
            <token id="36" string="book" />
          </tokens>
        </chunking>
        <chunking id="23" string="my mother , that 's not my children 's grandmother" type="NP">
          <tokens>
            <token id="10" string="my" />
            <token id="11" string="mother" />
            <token id="12" string="," />
            <token id="13" string="that" />
            <token id="14" string="'s" />
            <token id="15" string="not" />
            <token id="16" string="my" />
            <token id="17" string="children" />
            <token id="18" string="'s" />
            <token id="19" string="grandmother" />
          </tokens>
        </chunking>
        <chunking id="24" string="my children 's grandmother" type="NP">
          <tokens>
            <token id="16" string="my" />
            <token id="17" string="children" />
            <token id="18" string="'s" />
            <token id="19" string="grandmother" />
          </tokens>
        </chunking>
        <chunking id="25" string="my mother" type="NP">
          <tokens>
            <token id="10" string="my" />
            <token id="11" string="mother" />
          </tokens>
        </chunking>
        <chunking id="26" string="said" type="VP">
          <tokens>
            <token id="22" string="said" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="5">brother</governor>
          <dependent id="1">That</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="5">brother</governor>
          <dependent id="2">'s</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="5">brother</governor>
          <dependent id="3">not</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="5">brother</governor>
          <dependent id="4">my</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="22">said</governor>
          <dependent id="5">brother</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">mother</governor>
          <dependent id="7">that</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="11">mother</governor>
          <dependent id="8">'s</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="11">mother</governor>
          <dependent id="9">not</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="11">mother</governor>
          <dependent id="10">my</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="5">brother</governor>
          <dependent id="11">mother</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">grandmother</governor>
          <dependent id="13">that</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="19">grandmother</governor>
          <dependent id="14">'s</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="19">grandmother</governor>
          <dependent id="15">not</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="17">children</governor>
          <dependent id="16">my</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="19">grandmother</governor>
          <dependent id="17">children</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">children</governor>
          <dependent id="18">'s</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="11">mother</governor>
          <dependent id="19">grandmother</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="22">said</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">said</governor>
          <dependent id="23">Baird</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="27">spent</governor>
          <dependent id="25">who</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="27">spent</governor>
          <dependent id="26">recently</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="23">Baird</governor>
          <dependent id="27">spent</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="29">weeks</governor>
          <dependent id="28">three</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="27">spent</governor>
          <dependent id="29">weeks</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">States</governor>
          <dependent id="30">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="33">States</governor>
          <dependent id="31">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="33">States</governor>
          <dependent id="32">United</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">spent</governor>
          <dependent id="33">States</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="27">spent</governor>
          <dependent id="34">promoting</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="36">book</governor>
          <dependent id="35">her</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="34">promoting</governor>
          <dependent id="36">book</dependent>
        </dependency>
        <dependency type="det">
          <governor id="40">portrait</governor>
          <dependent id="38">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="40">portrait</governor>
          <dependent id="39">family</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="36">book</governor>
          <dependent id="40">portrait</dependent>
        </dependency>
        <dependency type="case">
          <governor id="42">Lennon</governor>
          <dependent id="41">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="40">portrait</governor>
          <dependent id="42">Lennon</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="36">book</governor>
          <dependent id="43">and</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="46">roots</governor>
          <dependent id="44">his</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="46">roots</governor>
          <dependent id="45">Liverpool</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="36">book</governor>
          <dependent id="46">roots</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Liverpool" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="45" string="Liverpool" />
          </tokens>
        </entity>
        <entity id="2" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="23" string="Baird" />
          </tokens>
        </entity>
        <entity id="3" string="United States" type="LOCATION" score="0.0">
          <tokens>
            <token id="32" string="United" />
            <token id="33" string="States" />
          </tokens>
        </entity>
        <entity id="4" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="42" string="Lennon" />
          </tokens>
        </entity>
        <entity id="5" string="recently" type="DATE" score="0.0">
          <tokens>
            <token id="26" string="recently" />
          </tokens>
        </entity>
        <entity id="6" string="three weeks" type="DURATION" score="0.0">
          <tokens>
            <token id="28" string="three" />
            <token id="29" string="weeks" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="6" has_coreference="true">
      <content>Published in North America last fall, ``John Lennon, My Brother&amp;apost;&amp;apost; was co-authored by Giuliano, a Beatles aficionado who lives in this Erie Canal city about 25 miles north of Buffalo.</content>
      <tokens>
        <token id="1" string="Published" lemma="publish" stem="publish" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="North" lemma="North" stem="north" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="4" string="America" lemma="America" stem="america" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="5" string="last" lemma="last" stem="last" pos="JJ" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="6" string="fall" lemma="fall" stem="fall" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="10" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="My" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="13" string="Brother" lemma="Brother" stem="brother" pos="NNP" type="Word" isStopWord="false" ner="TITLE" is_referenced="true" is_refers="false" />
        <token id="14" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="co-authored" lemma="co-author" stem="co-author" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="Giuliano" lemma="Giuliano" stem="giuliano" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="20" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="21" string="Beatles" lemma="Beatles" stem="beatl" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="22" string="aficionado" lemma="aficionado" stem="aficionado" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="23" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="24" string="lives" lemma="live" stem="live" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="25" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="26" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="27" string="Erie" lemma="Erie" stem="erie" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="true" />
        <token id="28" string="Canal" lemma="Canal" stem="canal" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="true" />
        <token id="29" string="city" lemma="city" stem="citi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="30" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="31" string="25" lemma="25" stem="25" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="true" />
        <token id="32" string="miles" lemma="mile" stem="mile" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="33" string="north" lemma="north" stem="north" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="34" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="35" string="Buffalo" lemma="Buffalo" stem="buffalo" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="true" />
        <token id="36" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (VP (VBN Published) (PP (IN in) (NP (NNP North) (NNP America))) (NP (NP (JJ last) (NN fall)) (, ,) (`` ``) (NP (NNP John) (NNP Lennon)) (, ,) (NP (PRP$ My) (NNP Brother)) ('' '')))) (VP (VBD was) (VP (VBN co-authored) (PP (IN by) (NP (NP (NNP Giuliano)) (, ,) (NP (NP (DT a) (NNP Beatles) (NN aficionado)) (SBAR (WHNP (WP who)) (S (VP (VBZ lives) (PP (IN in) (NP (NP (DT this) (NNP Erie) (NNP Canal) (NN city)) (PP (IN about) (ADVP (NP (CD 25) (NNS miles)) (RB north))) (PP (IN of) (NP (NNP Buffalo))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="My Brother" type="NP">
          <tokens>
            <token id="12" string="My" />
            <token id="13" string="Brother" />
          </tokens>
        </chunking>
        <chunking id="2" string="this Erie Canal city" type="NP">
          <tokens>
            <token id="26" string="this" />
            <token id="27" string="Erie" />
            <token id="28" string="Canal" />
            <token id="29" string="city" />
          </tokens>
        </chunking>
        <chunking id="3" string="this Erie Canal city about 25 miles north of Buffalo" type="NP">
          <tokens>
            <token id="26" string="this" />
            <token id="27" string="Erie" />
            <token id="28" string="Canal" />
            <token id="29" string="city" />
            <token id="30" string="about" />
            <token id="31" string="25" />
            <token id="32" string="miles" />
            <token id="33" string="north" />
            <token id="34" string="of" />
            <token id="35" string="Buffalo" />
          </tokens>
        </chunking>
        <chunking id="4" string="last fall" type="NP">
          <tokens>
            <token id="5" string="last" />
            <token id="6" string="fall" />
          </tokens>
        </chunking>
        <chunking id="5" string="who lives in this Erie Canal city about 25 miles north of Buffalo" type="SBAR">
          <tokens>
            <token id="23" string="who" />
            <token id="24" string="lives" />
            <token id="25" string="in" />
            <token id="26" string="this" />
            <token id="27" string="Erie" />
            <token id="28" string="Canal" />
            <token id="29" string="city" />
            <token id="30" string="about" />
            <token id="31" string="25" />
            <token id="32" string="miles" />
            <token id="33" string="north" />
            <token id="34" string="of" />
            <token id="35" string="Buffalo" />
          </tokens>
        </chunking>
        <chunking id="6" string="Published in North America last fall , `` John Lennon , My Brother ''" type="VP">
          <tokens>
            <token id="1" string="Published" />
            <token id="2" string="in" />
            <token id="3" string="North" />
            <token id="4" string="America" />
            <token id="5" string="last" />
            <token id="6" string="fall" />
            <token id="7" string="," />
            <token id="8" string="``" />
            <token id="9" string="John" />
            <token id="10" string="Lennon" />
            <token id="11" string="," />
            <token id="12" string="My" />
            <token id="13" string="Brother" />
            <token id="14" string="''" />
          </tokens>
        </chunking>
        <chunking id="7" string="North America" type="NP">
          <tokens>
            <token id="3" string="North" />
            <token id="4" string="America" />
          </tokens>
        </chunking>
        <chunking id="8" string="John Lennon" type="NP">
          <tokens>
            <token id="9" string="John" />
            <token id="10" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="9" string="25 miles" type="NP">
          <tokens>
            <token id="31" string="25" />
            <token id="32" string="miles" />
          </tokens>
        </chunking>
        <chunking id="10" string="a Beatles aficionado" type="NP">
          <tokens>
            <token id="20" string="a" />
            <token id="21" string="Beatles" />
            <token id="22" string="aficionado" />
          </tokens>
        </chunking>
        <chunking id="11" string="co-authored by Giuliano , a Beatles aficionado who lives in this Erie Canal city about 25 miles north of Buffalo" type="VP">
          <tokens>
            <token id="16" string="co-authored" />
            <token id="17" string="by" />
            <token id="18" string="Giuliano" />
            <token id="19" string="," />
            <token id="20" string="a" />
            <token id="21" string="Beatles" />
            <token id="22" string="aficionado" />
            <token id="23" string="who" />
            <token id="24" string="lives" />
            <token id="25" string="in" />
            <token id="26" string="this" />
            <token id="27" string="Erie" />
            <token id="28" string="Canal" />
            <token id="29" string="city" />
            <token id="30" string="about" />
            <token id="31" string="25" />
            <token id="32" string="miles" />
            <token id="33" string="north" />
            <token id="34" string="of" />
            <token id="35" string="Buffalo" />
          </tokens>
        </chunking>
        <chunking id="12" string="Giuliano , a Beatles aficionado who lives in this Erie Canal city about 25 miles north of Buffalo" type="NP">
          <tokens>
            <token id="18" string="Giuliano" />
            <token id="19" string="," />
            <token id="20" string="a" />
            <token id="21" string="Beatles" />
            <token id="22" string="aficionado" />
            <token id="23" string="who" />
            <token id="24" string="lives" />
            <token id="25" string="in" />
            <token id="26" string="this" />
            <token id="27" string="Erie" />
            <token id="28" string="Canal" />
            <token id="29" string="city" />
            <token id="30" string="about" />
            <token id="31" string="25" />
            <token id="32" string="miles" />
            <token id="33" string="north" />
            <token id="34" string="of" />
            <token id="35" string="Buffalo" />
          </tokens>
        </chunking>
        <chunking id="13" string="lives in this Erie Canal city about 25 miles north of Buffalo" type="VP">
          <tokens>
            <token id="24" string="lives" />
            <token id="25" string="in" />
            <token id="26" string="this" />
            <token id="27" string="Erie" />
            <token id="28" string="Canal" />
            <token id="29" string="city" />
            <token id="30" string="about" />
            <token id="31" string="25" />
            <token id="32" string="miles" />
            <token id="33" string="north" />
            <token id="34" string="of" />
            <token id="35" string="Buffalo" />
          </tokens>
        </chunking>
        <chunking id="14" string="last fall , `` John Lennon , My Brother ''" type="NP">
          <tokens>
            <token id="5" string="last" />
            <token id="6" string="fall" />
            <token id="7" string="," />
            <token id="8" string="``" />
            <token id="9" string="John" />
            <token id="10" string="Lennon" />
            <token id="11" string="," />
            <token id="12" string="My" />
            <token id="13" string="Brother" />
            <token id="14" string="''" />
          </tokens>
        </chunking>
        <chunking id="15" string="was co-authored by Giuliano , a Beatles aficionado who lives in this Erie Canal city about 25 miles north of Buffalo" type="VP">
          <tokens>
            <token id="15" string="was" />
            <token id="16" string="co-authored" />
            <token id="17" string="by" />
            <token id="18" string="Giuliano" />
            <token id="19" string="," />
            <token id="20" string="a" />
            <token id="21" string="Beatles" />
            <token id="22" string="aficionado" />
            <token id="23" string="who" />
            <token id="24" string="lives" />
            <token id="25" string="in" />
            <token id="26" string="this" />
            <token id="27" string="Erie" />
            <token id="28" string="Canal" />
            <token id="29" string="city" />
            <token id="30" string="about" />
            <token id="31" string="25" />
            <token id="32" string="miles" />
            <token id="33" string="north" />
            <token id="34" string="of" />
            <token id="35" string="Buffalo" />
          </tokens>
        </chunking>
        <chunking id="16" string="Giuliano" type="NP">
          <tokens>
            <token id="18" string="Giuliano" />
          </tokens>
        </chunking>
        <chunking id="17" string="a Beatles aficionado who lives in this Erie Canal city about 25 miles north of Buffalo" type="NP">
          <tokens>
            <token id="20" string="a" />
            <token id="21" string="Beatles" />
            <token id="22" string="aficionado" />
            <token id="23" string="who" />
            <token id="24" string="lives" />
            <token id="25" string="in" />
            <token id="26" string="this" />
            <token id="27" string="Erie" />
            <token id="28" string="Canal" />
            <token id="29" string="city" />
            <token id="30" string="about" />
            <token id="31" string="25" />
            <token id="32" string="miles" />
            <token id="33" string="north" />
            <token id="34" string="of" />
            <token id="35" string="Buffalo" />
          </tokens>
        </chunking>
        <chunking id="18" string="Buffalo" type="NP">
          <tokens>
            <token id="35" string="Buffalo" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="csubjpass">
          <governor id="16">co-authored</governor>
          <dependent id="1">Published</dependent>
        </dependency>
        <dependency type="case">
          <governor id="4">America</governor>
          <dependent id="2">in</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">America</governor>
          <dependent id="3">North</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="1">Published</governor>
          <dependent id="4">America</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">fall</governor>
          <dependent id="5">last</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="1">Published</governor>
          <dependent id="6">fall</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">Lennon</governor>
          <dependent id="9">John</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="6">fall</governor>
          <dependent id="10">Lennon</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="13">Brother</governor>
          <dependent id="12">My</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="6">fall</governor>
          <dependent id="13">Brother</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="16">co-authored</governor>
          <dependent id="15">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="16">co-authored</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">Giuliano</governor>
          <dependent id="17">by</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">co-authored</governor>
          <dependent id="18">Giuliano</dependent>
        </dependency>
        <dependency type="det">
          <governor id="22">aficionado</governor>
          <dependent id="20">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="22">aficionado</governor>
          <dependent id="21">Beatles</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="18">Giuliano</governor>
          <dependent id="22">aficionado</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="24">lives</governor>
          <dependent id="23">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="22">aficionado</governor>
          <dependent id="24">lives</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">city</governor>
          <dependent id="25">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="29">city</governor>
          <dependent id="26">this</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="29">city</governor>
          <dependent id="27">Erie</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="29">city</governor>
          <dependent id="28">Canal</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="24">lives</governor>
          <dependent id="29">city</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">north</governor>
          <dependent id="30">about</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="32">miles</governor>
          <dependent id="31">25</dependent>
        </dependency>
        <dependency type="nmod:npmod">
          <governor id="33">north</governor>
          <dependent id="32">miles</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="29">city</governor>
          <dependent id="33">north</dependent>
        </dependency>
        <dependency type="case">
          <governor id="35">Buffalo</governor>
          <dependent id="34">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="29">city</governor>
          <dependent id="35">Buffalo</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="9" string="John" />
            <token id="10" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="25" type="NUMBER" score="0.0">
          <tokens>
            <token id="31" string="25" />
          </tokens>
        </entity>
        <entity id="3" string="last fall" type="DATE" score="0.0">
          <tokens>
            <token id="5" string="last" />
            <token id="6" string="fall" />
          </tokens>
        </entity>
        <entity id="4" string="Brother" type="TITLE" score="0.0">
          <tokens>
            <token id="13" string="Brother" />
          </tokens>
        </entity>
        <entity id="5" string="Giuliano" type="PERSON" score="0.0">
          <tokens>
            <token id="18" string="Giuliano" />
          </tokens>
        </entity>
        <entity id="6" string="North America" type="LOCATION" score="0.0">
          <tokens>
            <token id="3" string="North" />
            <token id="4" string="America" />
          </tokens>
        </entity>
        <entity id="7" string="Erie Canal" type="LOCATION" score="0.0">
          <tokens>
            <token id="27" string="Erie" />
            <token id="28" string="Canal" />
          </tokens>
        </entity>
        <entity id="8" string="Buffalo" type="LOCATION" score="0.0">
          <tokens>
            <token id="35" string="Buffalo" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="7" has_coreference="false">
      <content>The two-year, cross-Atlantic collaboration came from two different motives.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="two-year" lemma="two-year" stem="two-year" pos="JJ" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="cross-Atlantic" lemma="cross-atlantic" stem="cross-atlant" pos="JJ" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="5" string="collaboration" lemma="collaboration" stem="collabor" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="came" lemma="come" stem="came" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="two" lemma="two" stem="two" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="9" string="different" lemma="different" stem="differ" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="motives" lemma="motive" stem="motiv" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (JJ two-year) (, ,) (JJ cross-Atlantic) (NN collaboration)) (VP (VBD came) (PP (IN from) (NP (CD two) (JJ different) (NNS motives)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="two different motives" type="NP">
          <tokens>
            <token id="8" string="two" />
            <token id="9" string="different" />
            <token id="10" string="motives" />
          </tokens>
        </chunking>
        <chunking id="2" string="came from two different motives" type="VP">
          <tokens>
            <token id="6" string="came" />
            <token id="7" string="from" />
            <token id="8" string="two" />
            <token id="9" string="different" />
            <token id="10" string="motives" />
          </tokens>
        </chunking>
        <chunking id="3" string="The two-year , cross-Atlantic collaboration" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="two-year" />
            <token id="3" string="," />
            <token id="4" string="cross-Atlantic" />
            <token id="5" string="collaboration" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="5">collaboration</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">collaboration</governor>
          <dependent id="2">two-year</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">collaboration</governor>
          <dependent id="4">cross-Atlantic</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">came</governor>
          <dependent id="5">collaboration</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">came</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">motives</governor>
          <dependent id="7">from</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="10">motives</governor>
          <dependent id="8">two</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="10">motives</governor>
          <dependent id="9">different</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">came</governor>
          <dependent id="10">motives</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="two-year" type="DURATION" score="0.0">
          <tokens>
            <token id="2" string="two-year" />
          </tokens>
        </entity>
        <entity id="2" string="two" type="NUMBER" score="0.0">
          <tokens>
            <token id="8" string="two" />
          </tokens>
        </entity>
        <entity id="3" string="cross-Atlantic" type="MISC" score="0.0">
          <tokens>
            <token id="4" string="cross-Atlantic" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="8" has_coreference="true">
      <content>Giuliano wanted to do a book about Lennon.</content>
      <tokens>
        <token id="1" string="Giuliano" lemma="Giuliano" stem="giuliano" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="wanted" lemma="want" stem="want" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="do" lemma="do" stem="do" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="9" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Giuliano)) (VP (VBD wanted) (S (VP (TO to) (VP (VB do) (NP (NP (DT a) (NN book)) (PP (IN about) (NP (NNP Lennon)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a book" type="NP">
          <tokens>
            <token id="5" string="a" />
            <token id="6" string="book" />
          </tokens>
        </chunking>
        <chunking id="2" string="to do a book about Lennon" type="VP">
          <tokens>
            <token id="3" string="to" />
            <token id="4" string="do" />
            <token id="5" string="a" />
            <token id="6" string="book" />
            <token id="7" string="about" />
            <token id="8" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="3" string="a book about Lennon" type="NP">
          <tokens>
            <token id="5" string="a" />
            <token id="6" string="book" />
            <token id="7" string="about" />
            <token id="8" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="4" string="Lennon" type="NP">
          <tokens>
            <token id="8" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="5" string="wanted to do a book about Lennon" type="VP">
          <tokens>
            <token id="2" string="wanted" />
            <token id="3" string="to" />
            <token id="4" string="do" />
            <token id="5" string="a" />
            <token id="6" string="book" />
            <token id="7" string="about" />
            <token id="8" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="6" string="Giuliano" type="NP">
          <tokens>
            <token id="1" string="Giuliano" />
          </tokens>
        </chunking>
        <chunking id="7" string="do a book about Lennon" type="VP">
          <tokens>
            <token id="4" string="do" />
            <token id="5" string="a" />
            <token id="6" string="book" />
            <token id="7" string="about" />
            <token id="8" string="Lennon" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">wanted</governor>
          <dependent id="1">Giuliano</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">wanted</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="4">do</governor>
          <dependent id="3">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="2">wanted</governor>
          <dependent id="4">do</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">book</governor>
          <dependent id="5">a</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">do</governor>
          <dependent id="6">book</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">Lennon</governor>
          <dependent id="7">about</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">book</governor>
          <dependent id="8">Lennon</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="Giuliano" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Giuliano" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="9" has_coreference="true">
      <content>Baird wanted to do a book about the ex-Beatle&amp;apost;s maligned mother, whose name also was Julia.</content>
      <tokens>
        <token id="1" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="wanted" lemma="want" stem="want" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="do" lemma="do" stem="do" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="ex-Beatle" lemma="ex-beatle" stem="ex-beatl" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="maligned" lemma="malign" stem="malign" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="whose" lemma="whose" stem="whose" pos="WP$" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="name" lemma="name" stem="name" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="Julia" lemma="Julia" stem="julia" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="19" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Baird)) (VP (VBD wanted) (S (VP (TO to) (VP (VB do) (NP (NP (DT a) (NN book)) (PP (IN about) (NP (DT the) (NN ex-Beatle) (POS 's))) (VP (VBN maligned) (NP (NP (NN mother)) (, ,) (SBAR (WHNP (WP$ whose) (NN name)) (S (ADVP (RB also)) (VP (VBD was) (NP (NNP Julia)))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="mother , whose name also was Julia" type="NP">
          <tokens>
            <token id="12" string="mother" />
            <token id="13" string="," />
            <token id="14" string="whose" />
            <token id="15" string="name" />
            <token id="16" string="also" />
            <token id="17" string="was" />
            <token id="18" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="2" string="whose name also was Julia" type="SBAR">
          <tokens>
            <token id="14" string="whose" />
            <token id="15" string="name" />
            <token id="16" string="also" />
            <token id="17" string="was" />
            <token id="18" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="3" string="do a book about the ex-Beatle 's maligned mother , whose name also was Julia" type="VP">
          <tokens>
            <token id="4" string="do" />
            <token id="5" string="a" />
            <token id="6" string="book" />
            <token id="7" string="about" />
            <token id="8" string="the" />
            <token id="9" string="ex-Beatle" />
            <token id="10" string="'s" />
            <token id="11" string="maligned" />
            <token id="12" string="mother" />
            <token id="13" string="," />
            <token id="14" string="whose" />
            <token id="15" string="name" />
            <token id="16" string="also" />
            <token id="17" string="was" />
            <token id="18" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="4" string="to do a book about the ex-Beatle 's maligned mother , whose name also was Julia" type="VP">
          <tokens>
            <token id="3" string="to" />
            <token id="4" string="do" />
            <token id="5" string="a" />
            <token id="6" string="book" />
            <token id="7" string="about" />
            <token id="8" string="the" />
            <token id="9" string="ex-Beatle" />
            <token id="10" string="'s" />
            <token id="11" string="maligned" />
            <token id="12" string="mother" />
            <token id="13" string="," />
            <token id="14" string="whose" />
            <token id="15" string="name" />
            <token id="16" string="also" />
            <token id="17" string="was" />
            <token id="18" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="5" string="maligned mother , whose name also was Julia" type="VP">
          <tokens>
            <token id="11" string="maligned" />
            <token id="12" string="mother" />
            <token id="13" string="," />
            <token id="14" string="whose" />
            <token id="15" string="name" />
            <token id="16" string="also" />
            <token id="17" string="was" />
            <token id="18" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="6" string="the ex-Beatle 's" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="ex-Beatle" />
            <token id="10" string="'s" />
          </tokens>
        </chunking>
        <chunking id="7" string="wanted to do a book about the ex-Beatle 's maligned mother , whose name also was Julia" type="VP">
          <tokens>
            <token id="2" string="wanted" />
            <token id="3" string="to" />
            <token id="4" string="do" />
            <token id="5" string="a" />
            <token id="6" string="book" />
            <token id="7" string="about" />
            <token id="8" string="the" />
            <token id="9" string="ex-Beatle" />
            <token id="10" string="'s" />
            <token id="11" string="maligned" />
            <token id="12" string="mother" />
            <token id="13" string="," />
            <token id="14" string="whose" />
            <token id="15" string="name" />
            <token id="16" string="also" />
            <token id="17" string="was" />
            <token id="18" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="8" string="a book" type="NP">
          <tokens>
            <token id="5" string="a" />
            <token id="6" string="book" />
          </tokens>
        </chunking>
        <chunking id="9" string="mother" type="NP">
          <tokens>
            <token id="12" string="mother" />
          </tokens>
        </chunking>
        <chunking id="10" string="Julia" type="NP">
          <tokens>
            <token id="18" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="11" string="was Julia" type="VP">
          <tokens>
            <token id="17" string="was" />
            <token id="18" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="12" string="Baird" type="NP">
          <tokens>
            <token id="1" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="13" string="a book about the ex-Beatle 's maligned mother , whose name also was Julia" type="NP">
          <tokens>
            <token id="5" string="a" />
            <token id="6" string="book" />
            <token id="7" string="about" />
            <token id="8" string="the" />
            <token id="9" string="ex-Beatle" />
            <token id="10" string="'s" />
            <token id="11" string="maligned" />
            <token id="12" string="mother" />
            <token id="13" string="," />
            <token id="14" string="whose" />
            <token id="15" string="name" />
            <token id="16" string="also" />
            <token id="17" string="was" />
            <token id="18" string="Julia" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">wanted</governor>
          <dependent id="1">Baird</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">wanted</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="4">do</governor>
          <dependent id="3">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="2">wanted</governor>
          <dependent id="4">do</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">book</governor>
          <dependent id="5">a</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">do</governor>
          <dependent id="6">book</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">ex-Beatle</governor>
          <dependent id="7">about</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">ex-Beatle</governor>
          <dependent id="8">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">book</governor>
          <dependent id="9">ex-Beatle</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">ex-Beatle</governor>
          <dependent id="10">'s</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="6">book</governor>
          <dependent id="11">maligned</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">maligned</governor>
          <dependent id="12">mother</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="15">name</governor>
          <dependent id="14">whose</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="18">Julia</governor>
          <dependent id="15">name</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="18">Julia</governor>
          <dependent id="16">also</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="18">Julia</governor>
          <dependent id="17">was</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="12">mother</governor>
          <dependent id="18">Julia</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Julia" type="PERSON" score="0.0">
          <tokens>
            <token id="18" string="Julia" />
          </tokens>
        </entity>
        <entity id="2" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Baird" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="10" has_coreference="true">
      <content>The result is a little of both, a mishmash of anecdotes showing a moody but generally happy Liverpool boy singing with his mother in a bathroom with good acoustics.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="result" lemma="result" stem="result" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="little" lemma="little" stem="littl" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="both" lemma="both" stem="both" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="mishmash" lemma="mishmash" stem="mishmash" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="anecdotes" lemma="anecdote" stem="anecdot" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="showing" lemma="show" stem="show" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="moody" lemma="moody" stem="moodi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="generally" lemma="generally" stem="gener" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="happy" lemma="happy" stem="happi" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="Liverpool" lemma="Liverpool" stem="liverpool" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="20" string="boy" lemma="boy" stem="boi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="singing" lemma="singing" stem="sing" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="24" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="25" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="bathroom" lemma="bathroom" stem="bathroom" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="good" lemma="good" stem="good" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="acoustics" lemma="acoustic" stem="acoust" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (DT The) (NN result)) (VP (VBZ is) (NP (NP (DT a) (JJ little)) (PP (IN of))))) (CC both) (, ,) (S (NP (NP (DT a) (NN mishmash)) (PP (IN of) (NP (NNS anecdotes)))) (VP (VBG showing) (NP (DT a) (UCP (NP (NN moody)) (CC but) (ADJP (RB generally) (JJ happy))) (NNP Liverpool)) (NP (NN boy) (NN singing)) (PP (IN with) (NP (PRP$ his) (NN mother))) (PP (IN in) (NP (NP (DT a) (NN bathroom)) (PP (IN with) (NP (JJ good) (NNS acoustics))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a mishmash" type="NP">
          <tokens>
            <token id="9" string="a" />
            <token id="10" string="mishmash" />
          </tokens>
        </chunking>
        <chunking id="2" string="anecdotes" type="NP">
          <tokens>
            <token id="12" string="anecdotes" />
          </tokens>
        </chunking>
        <chunking id="3" string="showing a moody but generally happy Liverpool boy singing with his mother in a bathroom with good acoustics" type="VP">
          <tokens>
            <token id="13" string="showing" />
            <token id="14" string="a" />
            <token id="15" string="moody" />
            <token id="16" string="but" />
            <token id="17" string="generally" />
            <token id="18" string="happy" />
            <token id="19" string="Liverpool" />
            <token id="20" string="boy" />
            <token id="21" string="singing" />
            <token id="22" string="with" />
            <token id="23" string="his" />
            <token id="24" string="mother" />
            <token id="25" string="in" />
            <token id="26" string="a" />
            <token id="27" string="bathroom" />
            <token id="28" string="with" />
            <token id="29" string="good" />
            <token id="30" string="acoustics" />
          </tokens>
        </chunking>
        <chunking id="4" string="boy singing" type="NP">
          <tokens>
            <token id="20" string="boy" />
            <token id="21" string="singing" />
          </tokens>
        </chunking>
        <chunking id="5" string="a moody but generally happy Liverpool" type="NP">
          <tokens>
            <token id="14" string="a" />
            <token id="15" string="moody" />
            <token id="16" string="but" />
            <token id="17" string="generally" />
            <token id="18" string="happy" />
            <token id="19" string="Liverpool" />
          </tokens>
        </chunking>
        <chunking id="6" string="a bathroom with good acoustics" type="NP">
          <tokens>
            <token id="26" string="a" />
            <token id="27" string="bathroom" />
            <token id="28" string="with" />
            <token id="29" string="good" />
            <token id="30" string="acoustics" />
          </tokens>
        </chunking>
        <chunking id="7" string="good acoustics" type="NP">
          <tokens>
            <token id="29" string="good" />
            <token id="30" string="acoustics" />
          </tokens>
        </chunking>
        <chunking id="8" string="a little of" type="NP">
          <tokens>
            <token id="4" string="a" />
            <token id="5" string="little" />
            <token id="6" string="of" />
          </tokens>
        </chunking>
        <chunking id="9" string="moody" type="NP">
          <tokens>
            <token id="15" string="moody" />
          </tokens>
        </chunking>
        <chunking id="10" string="is a little of" type="VP">
          <tokens>
            <token id="3" string="is" />
            <token id="4" string="a" />
            <token id="5" string="little" />
            <token id="6" string="of" />
          </tokens>
        </chunking>
        <chunking id="11" string="a little" type="NP">
          <tokens>
            <token id="4" string="a" />
            <token id="5" string="little" />
          </tokens>
        </chunking>
        <chunking id="12" string="his mother" type="NP">
          <tokens>
            <token id="23" string="his" />
            <token id="24" string="mother" />
          </tokens>
        </chunking>
        <chunking id="13" string="The result" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="result" />
          </tokens>
        </chunking>
        <chunking id="14" string="a mishmash of anecdotes" type="NP">
          <tokens>
            <token id="9" string="a" />
            <token id="10" string="mishmash" />
            <token id="11" string="of" />
            <token id="12" string="anecdotes" />
          </tokens>
        </chunking>
        <chunking id="15" string="generally happy" type="ADJP">
          <tokens>
            <token id="17" string="generally" />
            <token id="18" string="happy" />
          </tokens>
        </chunking>
        <chunking id="16" string="a bathroom" type="NP">
          <tokens>
            <token id="26" string="a" />
            <token id="27" string="bathroom" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">result</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">little</governor>
          <dependent id="2">result</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="5">little</governor>
          <dependent id="3">is</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">little</governor>
          <dependent id="4">a</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">little</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="5">little</governor>
          <dependent id="6">of</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="5">little</governor>
          <dependent id="7">both</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">mishmash</governor>
          <dependent id="9">a</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="13">showing</governor>
          <dependent id="10">mishmash</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">anecdotes</governor>
          <dependent id="11">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">mishmash</governor>
          <dependent id="12">anecdotes</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="5">little</governor>
          <dependent id="13">showing</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">Liverpool</governor>
          <dependent id="14">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">Liverpool</governor>
          <dependent id="15">moody</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="15">moody</governor>
          <dependent id="16">but</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="18">happy</governor>
          <dependent id="17">generally</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="15">moody</governor>
          <dependent id="18">happy</dependent>
        </dependency>
        <dependency type="iobj">
          <governor id="13">showing</governor>
          <dependent id="19">Liverpool</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="21">singing</governor>
          <dependent id="20">boy</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="13">showing</governor>
          <dependent id="21">singing</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">mother</governor>
          <dependent id="22">with</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="24">mother</governor>
          <dependent id="23">his</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">showing</governor>
          <dependent id="24">mother</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">bathroom</governor>
          <dependent id="25">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="27">bathroom</governor>
          <dependent id="26">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">showing</governor>
          <dependent id="27">bathroom</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">acoustics</governor>
          <dependent id="28">with</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="30">acoustics</governor>
          <dependent id="29">good</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">bathroom</governor>
          <dependent id="30">acoustics</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Liverpool" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="19" string="Liverpool" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="11" has_coreference="true">
      <content>Baird disagrees with other biographies which say that after his father decamped and Julia turned her son over to her sister, Mimi, Lennon was a grim youth who felt abandoned by his mother.</content>
      <tokens>
        <token id="1" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="disagrees" lemma="disagree" stem="disagre" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="other" lemma="other" stem="other" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="biographies" lemma="biography" stem="biographi" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="which" lemma="which" stem="which" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="say" lemma="say" stem="sai" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="after" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="11" string="father" lemma="father" stem="father" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="decamped" lemma="decamp" stem="decamp" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="Julia" lemma="Julia" stem="julia" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="15" string="turned" lemma="turn" stem="turn" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="son" lemma="son" stem="son" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="over" lemma="over" stem="over" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="21" string="sister" lemma="sister" stem="sister" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="Mimi" lemma="Mimi" stem="mimi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="24" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="25" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="26" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="grim" lemma="grim" stem="grim" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="youth" lemma="youth" stem="youth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="felt" lemma="feel" stem="felt" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="abandoned" lemma="abandon" stem="abandon" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="35" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="36" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Baird)) (VP (VBZ disagrees) (PP (IN with) (NP (NP (JJ other) (NNS biographies)) (SBAR (WHNP (WDT which)) (S (VP (VBP say) (SBAR (IN that) (S (SBAR (IN after) (S (S (NP (PRP$ his) (NN father)) (VP (VBD decamped))) (CC and) (S (NP (NNP Julia)) (VP (VBD turned) (NP (PRP$ her) (NN son)) (PP (IN over) (PP (TO to) (NP (PRP$ her) (NN sister)))))))) (, ,) (NP (NNP Mimi) (, ,) (NNP Lennon)) (VP (VBD was) (NP (NP (DT a) (JJ grim) (NN youth)) (SBAR (WHNP (WP who)) (S (VP (VBD felt) (VP (VBN abandoned) (PP (IN by) (NP (PRP$ his) (NN mother))))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="disagrees with other biographies which say that after his father decamped and Julia turned her son over to her sister , Mimi , Lennon was a grim youth who felt abandoned by his mother" type="VP">
          <tokens>
            <token id="2" string="disagrees" />
            <token id="3" string="with" />
            <token id="4" string="other" />
            <token id="5" string="biographies" />
            <token id="6" string="which" />
            <token id="7" string="say" />
            <token id="8" string="that" />
            <token id="9" string="after" />
            <token id="10" string="his" />
            <token id="11" string="father" />
            <token id="12" string="decamped" />
            <token id="13" string="and" />
            <token id="14" string="Julia" />
            <token id="15" string="turned" />
            <token id="16" string="her" />
            <token id="17" string="son" />
            <token id="18" string="over" />
            <token id="19" string="to" />
            <token id="20" string="her" />
            <token id="21" string="sister" />
            <token id="22" string="," />
            <token id="23" string="Mimi" />
            <token id="24" string="," />
            <token id="25" string="Lennon" />
            <token id="26" string="was" />
            <token id="27" string="a" />
            <token id="28" string="grim" />
            <token id="29" string="youth" />
            <token id="30" string="who" />
            <token id="31" string="felt" />
            <token id="32" string="abandoned" />
            <token id="33" string="by" />
            <token id="34" string="his" />
            <token id="35" string="mother" />
          </tokens>
        </chunking>
        <chunking id="2" string="decamped" type="VP">
          <tokens>
            <token id="12" string="decamped" />
          </tokens>
        </chunking>
        <chunking id="3" string="that after his father decamped and Julia turned her son over to her sister , Mimi , Lennon was a grim youth who felt abandoned by his mother" type="SBAR">
          <tokens>
            <token id="8" string="that" />
            <token id="9" string="after" />
            <token id="10" string="his" />
            <token id="11" string="father" />
            <token id="12" string="decamped" />
            <token id="13" string="and" />
            <token id="14" string="Julia" />
            <token id="15" string="turned" />
            <token id="16" string="her" />
            <token id="17" string="son" />
            <token id="18" string="over" />
            <token id="19" string="to" />
            <token id="20" string="her" />
            <token id="21" string="sister" />
            <token id="22" string="," />
            <token id="23" string="Mimi" />
            <token id="24" string="," />
            <token id="25" string="Lennon" />
            <token id="26" string="was" />
            <token id="27" string="a" />
            <token id="28" string="grim" />
            <token id="29" string="youth" />
            <token id="30" string="who" />
            <token id="31" string="felt" />
            <token id="32" string="abandoned" />
            <token id="33" string="by" />
            <token id="34" string="his" />
            <token id="35" string="mother" />
          </tokens>
        </chunking>
        <chunking id="4" string="a grim youth" type="NP">
          <tokens>
            <token id="27" string="a" />
            <token id="28" string="grim" />
            <token id="29" string="youth" />
          </tokens>
        </chunking>
        <chunking id="5" string="other biographies" type="NP">
          <tokens>
            <token id="4" string="other" />
            <token id="5" string="biographies" />
          </tokens>
        </chunking>
        <chunking id="6" string="a grim youth who felt abandoned by his mother" type="NP">
          <tokens>
            <token id="27" string="a" />
            <token id="28" string="grim" />
            <token id="29" string="youth" />
            <token id="30" string="who" />
            <token id="31" string="felt" />
            <token id="32" string="abandoned" />
            <token id="33" string="by" />
            <token id="34" string="his" />
            <token id="35" string="mother" />
          </tokens>
        </chunking>
        <chunking id="7" string="which say that after his father decamped and Julia turned her son over to her sister , Mimi , Lennon was a grim youth who felt abandoned by his mother" type="SBAR">
          <tokens>
            <token id="6" string="which" />
            <token id="7" string="say" />
            <token id="8" string="that" />
            <token id="9" string="after" />
            <token id="10" string="his" />
            <token id="11" string="father" />
            <token id="12" string="decamped" />
            <token id="13" string="and" />
            <token id="14" string="Julia" />
            <token id="15" string="turned" />
            <token id="16" string="her" />
            <token id="17" string="son" />
            <token id="18" string="over" />
            <token id="19" string="to" />
            <token id="20" string="her" />
            <token id="21" string="sister" />
            <token id="22" string="," />
            <token id="23" string="Mimi" />
            <token id="24" string="," />
            <token id="25" string="Lennon" />
            <token id="26" string="was" />
            <token id="27" string="a" />
            <token id="28" string="grim" />
            <token id="29" string="youth" />
            <token id="30" string="who" />
            <token id="31" string="felt" />
            <token id="32" string="abandoned" />
            <token id="33" string="by" />
            <token id="34" string="his" />
            <token id="35" string="mother" />
          </tokens>
        </chunking>
        <chunking id="8" string="after his father decamped and Julia turned her son over to her sister" type="SBAR">
          <tokens>
            <token id="9" string="after" />
            <token id="10" string="his" />
            <token id="11" string="father" />
            <token id="12" string="decamped" />
            <token id="13" string="and" />
            <token id="14" string="Julia" />
            <token id="15" string="turned" />
            <token id="16" string="her" />
            <token id="17" string="son" />
            <token id="18" string="over" />
            <token id="19" string="to" />
            <token id="20" string="her" />
            <token id="21" string="sister" />
          </tokens>
        </chunking>
        <chunking id="9" string="abandoned by his mother" type="VP">
          <tokens>
            <token id="32" string="abandoned" />
            <token id="33" string="by" />
            <token id="34" string="his" />
            <token id="35" string="mother" />
          </tokens>
        </chunking>
        <chunking id="10" string="other biographies which say that after his father decamped and Julia turned her son over to her sister , Mimi , Lennon was a grim youth who felt abandoned by his mother" type="NP">
          <tokens>
            <token id="4" string="other" />
            <token id="5" string="biographies" />
            <token id="6" string="which" />
            <token id="7" string="say" />
            <token id="8" string="that" />
            <token id="9" string="after" />
            <token id="10" string="his" />
            <token id="11" string="father" />
            <token id="12" string="decamped" />
            <token id="13" string="and" />
            <token id="14" string="Julia" />
            <token id="15" string="turned" />
            <token id="16" string="her" />
            <token id="17" string="son" />
            <token id="18" string="over" />
            <token id="19" string="to" />
            <token id="20" string="her" />
            <token id="21" string="sister" />
            <token id="22" string="," />
            <token id="23" string="Mimi" />
            <token id="24" string="," />
            <token id="25" string="Lennon" />
            <token id="26" string="was" />
            <token id="27" string="a" />
            <token id="28" string="grim" />
            <token id="29" string="youth" />
            <token id="30" string="who" />
            <token id="31" string="felt" />
            <token id="32" string="abandoned" />
            <token id="33" string="by" />
            <token id="34" string="his" />
            <token id="35" string="mother" />
          </tokens>
        </chunking>
        <chunking id="11" string="her sister" type="NP">
          <tokens>
            <token id="20" string="her" />
            <token id="21" string="sister" />
          </tokens>
        </chunking>
        <chunking id="12" string="Julia" type="NP">
          <tokens>
            <token id="14" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="13" string="Mimi , Lennon" type="NP">
          <tokens>
            <token id="23" string="Mimi" />
            <token id="24" string="," />
            <token id="25" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="14" string="who felt abandoned by his mother" type="SBAR">
          <tokens>
            <token id="30" string="who" />
            <token id="31" string="felt" />
            <token id="32" string="abandoned" />
            <token id="33" string="by" />
            <token id="34" string="his" />
            <token id="35" string="mother" />
          </tokens>
        </chunking>
        <chunking id="15" string="Baird" type="NP">
          <tokens>
            <token id="1" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="16" string="was a grim youth who felt abandoned by his mother" type="VP">
          <tokens>
            <token id="26" string="was" />
            <token id="27" string="a" />
            <token id="28" string="grim" />
            <token id="29" string="youth" />
            <token id="30" string="who" />
            <token id="31" string="felt" />
            <token id="32" string="abandoned" />
            <token id="33" string="by" />
            <token id="34" string="his" />
            <token id="35" string="mother" />
          </tokens>
        </chunking>
        <chunking id="17" string="say that after his father decamped and Julia turned her son over to her sister , Mimi , Lennon was a grim youth who felt abandoned by his mother" type="VP">
          <tokens>
            <token id="7" string="say" />
            <token id="8" string="that" />
            <token id="9" string="after" />
            <token id="10" string="his" />
            <token id="11" string="father" />
            <token id="12" string="decamped" />
            <token id="13" string="and" />
            <token id="14" string="Julia" />
            <token id="15" string="turned" />
            <token id="16" string="her" />
            <token id="17" string="son" />
            <token id="18" string="over" />
            <token id="19" string="to" />
            <token id="20" string="her" />
            <token id="21" string="sister" />
            <token id="22" string="," />
            <token id="23" string="Mimi" />
            <token id="24" string="," />
            <token id="25" string="Lennon" />
            <token id="26" string="was" />
            <token id="27" string="a" />
            <token id="28" string="grim" />
            <token id="29" string="youth" />
            <token id="30" string="who" />
            <token id="31" string="felt" />
            <token id="32" string="abandoned" />
            <token id="33" string="by" />
            <token id="34" string="his" />
            <token id="35" string="mother" />
          </tokens>
        </chunking>
        <chunking id="18" string="turned her son over to her sister" type="VP">
          <tokens>
            <token id="15" string="turned" />
            <token id="16" string="her" />
            <token id="17" string="son" />
            <token id="18" string="over" />
            <token id="19" string="to" />
            <token id="20" string="her" />
            <token id="21" string="sister" />
          </tokens>
        </chunking>
        <chunking id="19" string="his father" type="NP">
          <tokens>
            <token id="10" string="his" />
            <token id="11" string="father" />
          </tokens>
        </chunking>
        <chunking id="20" string="his mother" type="NP">
          <tokens>
            <token id="34" string="his" />
            <token id="35" string="mother" />
          </tokens>
        </chunking>
        <chunking id="21" string="her son" type="NP">
          <tokens>
            <token id="16" string="her" />
            <token id="17" string="son" />
          </tokens>
        </chunking>
        <chunking id="22" string="felt abandoned by his mother" type="VP">
          <tokens>
            <token id="31" string="felt" />
            <token id="32" string="abandoned" />
            <token id="33" string="by" />
            <token id="34" string="his" />
            <token id="35" string="mother" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">disagrees</governor>
          <dependent id="1">Baird</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">disagrees</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">biographies</governor>
          <dependent id="3">with</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">biographies</governor>
          <dependent id="4">other</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">disagrees</governor>
          <dependent id="5">biographies</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">say</governor>
          <dependent id="6">which</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="5">biographies</governor>
          <dependent id="7">say</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="29">youth</governor>
          <dependent id="8">that</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="12">decamped</governor>
          <dependent id="9">after</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="11">father</governor>
          <dependent id="10">his</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">decamped</governor>
          <dependent id="11">father</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="29">youth</governor>
          <dependent id="12">decamped</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="12">decamped</governor>
          <dependent id="13">and</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">turned</governor>
          <dependent id="14">Julia</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="12">decamped</governor>
          <dependent id="15">turned</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="17">son</governor>
          <dependent id="16">her</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">turned</governor>
          <dependent id="17">son</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">sister</governor>
          <dependent id="18">over</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">sister</governor>
          <dependent id="19">to</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="21">sister</governor>
          <dependent id="20">her</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">turned</governor>
          <dependent id="21">sister</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">Lennon</governor>
          <dependent id="23">Mimi</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="29">youth</governor>
          <dependent id="25">Lennon</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="29">youth</governor>
          <dependent id="26">was</dependent>
        </dependency>
        <dependency type="det">
          <governor id="29">youth</governor>
          <dependent id="27">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="29">youth</governor>
          <dependent id="28">grim</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="7">say</governor>
          <dependent id="29">youth</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="31">felt</governor>
          <dependent id="30">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="29">youth</governor>
          <dependent id="31">felt</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="31">felt</governor>
          <dependent id="32">abandoned</dependent>
        </dependency>
        <dependency type="case">
          <governor id="35">mother</governor>
          <dependent id="33">by</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="35">mother</governor>
          <dependent id="34">his</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="32">abandoned</governor>
          <dependent id="35">mother</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Julia" type="PERSON" score="0.0">
          <tokens>
            <token id="14" string="Julia" />
          </tokens>
        </entity>
        <entity id="2" string="Mimi" type="PERSON" score="0.0">
          <tokens>
            <token id="23" string="Mimi" />
          </tokens>
        </entity>
        <entity id="3" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Baird" />
          </tokens>
        </entity>
        <entity id="4" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="25" string="Lennon" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="12" has_coreference="true">
      <content>``I didn&amp;apost;t do it for John,&amp;apost;&amp;apost; said Baird.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="did" lemma="do" stem="did" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="n't" lemma="not" stem="n't" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="do" lemma="do" stem="do" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (SINV (`` ``) (S (NP (PRP I)) (VP (VBD did) (RB n't) (VP (VB do) (NP (PRP it)) (PP (IN for) (NP (NNP John)))))) (, ,) ('' '') (VP (VBD said)) (NP (NNP Baird)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="did n't do it for John" type="VP">
          <tokens>
            <token id="3" string="did" />
            <token id="4" string="n't" />
            <token id="5" string="do" />
            <token id="6" string="it" />
            <token id="7" string="for" />
            <token id="8" string="John" />
          </tokens>
        </chunking>
        <chunking id="2" string="do it for John" type="VP">
          <tokens>
            <token id="5" string="do" />
            <token id="6" string="it" />
            <token id="7" string="for" />
            <token id="8" string="John" />
          </tokens>
        </chunking>
        <chunking id="3" string="Baird" type="NP">
          <tokens>
            <token id="12" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="4" string="I" type="NP">
          <tokens>
            <token id="2" string="I" />
          </tokens>
        </chunking>
        <chunking id="5" string="John" type="NP">
          <tokens>
            <token id="8" string="John" />
          </tokens>
        </chunking>
        <chunking id="6" string="it" type="NP">
          <tokens>
            <token id="6" string="it" />
          </tokens>
        </chunking>
        <chunking id="7" string="said" type="VP">
          <tokens>
            <token id="11" string="said" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="5">do</governor>
          <dependent id="2">I</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="5">do</governor>
          <dependent id="3">did</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="5">do</governor>
          <dependent id="4">n't</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="11">said</governor>
          <dependent id="5">do</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">do</governor>
          <dependent id="6">it</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">John</governor>
          <dependent id="7">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">do</governor>
          <dependent id="8">John</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="11">said</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">said</governor>
          <dependent id="12">Baird</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="12" string="Baird" />
          </tokens>
        </entity>
        <entity id="2" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="John" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="13" has_coreference="true">
      <content>``I wanted to set the record straight about our mother, and that ties in with John.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="wanted" lemma="want" stem="want" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="set" lemma="set" stem="set" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="record" lemma="record" stem="record" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="straight" lemma="straight" stem="straight" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="our" lemma="we" stem="our" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="ties" lemma="tie" stem="ti" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="19" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (PRP I)) (VP (VBD wanted) (S (VP (TO to) (VP (VB set) (NP (DT the) (NN record)) (PP (RB straight) (PP (IN about) (NP (PRP$ our) (NN mother))) (, ,) (CC and) (PP (IN that) (NP (NNS ties)))) (PP (IN in) (PP (IN with) (NP (NNP John)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="ties" type="NP">
          <tokens>
            <token id="15" string="ties" />
          </tokens>
        </chunking>
        <chunking id="2" string="the record" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="record" />
          </tokens>
        </chunking>
        <chunking id="3" string="I" type="NP">
          <tokens>
            <token id="2" string="I" />
          </tokens>
        </chunking>
        <chunking id="4" string="wanted to set the record straight about our mother , and that ties in with John" type="VP">
          <tokens>
            <token id="3" string="wanted" />
            <token id="4" string="to" />
            <token id="5" string="set" />
            <token id="6" string="the" />
            <token id="7" string="record" />
            <token id="8" string="straight" />
            <token id="9" string="about" />
            <token id="10" string="our" />
            <token id="11" string="mother" />
            <token id="12" string="," />
            <token id="13" string="and" />
            <token id="14" string="that" />
            <token id="15" string="ties" />
            <token id="16" string="in" />
            <token id="17" string="with" />
            <token id="18" string="John" />
          </tokens>
        </chunking>
        <chunking id="5" string="to set the record straight about our mother , and that ties in with John" type="VP">
          <tokens>
            <token id="4" string="to" />
            <token id="5" string="set" />
            <token id="6" string="the" />
            <token id="7" string="record" />
            <token id="8" string="straight" />
            <token id="9" string="about" />
            <token id="10" string="our" />
            <token id="11" string="mother" />
            <token id="12" string="," />
            <token id="13" string="and" />
            <token id="14" string="that" />
            <token id="15" string="ties" />
            <token id="16" string="in" />
            <token id="17" string="with" />
            <token id="18" string="John" />
          </tokens>
        </chunking>
        <chunking id="6" string="our mother" type="NP">
          <tokens>
            <token id="10" string="our" />
            <token id="11" string="mother" />
          </tokens>
        </chunking>
        <chunking id="7" string="John" type="NP">
          <tokens>
            <token id="18" string="John" />
          </tokens>
        </chunking>
        <chunking id="8" string="set the record straight about our mother , and that ties in with John" type="VP">
          <tokens>
            <token id="5" string="set" />
            <token id="6" string="the" />
            <token id="7" string="record" />
            <token id="8" string="straight" />
            <token id="9" string="about" />
            <token id="10" string="our" />
            <token id="11" string="mother" />
            <token id="12" string="," />
            <token id="13" string="and" />
            <token id="14" string="that" />
            <token id="15" string="ties" />
            <token id="16" string="in" />
            <token id="17" string="with" />
            <token id="18" string="John" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">wanted</governor>
          <dependent id="2">I</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">wanted</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="5">set</governor>
          <dependent id="4">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="3">wanted</governor>
          <dependent id="5">set</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="5">set</governor>
          <dependent id="5">set</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">record</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">set</governor>
          <dependent id="7">record</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">mother</governor>
          <dependent id="8">straight</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">mother</governor>
          <dependent id="9">about</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="11">mother</governor>
          <dependent id="10">our</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">set</governor>
          <dependent id="11">mother</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="5">set</governor>
          <dependent id="13">and</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">ties</governor>
          <dependent id="14">that</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">set</governor>
          <dependent id="15">ties</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">John</governor>
          <dependent id="16">in</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">John</governor>
          <dependent id="17">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">set</governor>
          <dependent id="18">John</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="18" string="John" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="14" has_coreference="true">
      <content>I feel my mother has been badly misrepresented in the press.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="feel" lemma="feel" stem="feel" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="my" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="badly" lemma="badly" stem="badli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="misrepresented" lemma="misrepresent" stem="misrepres" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="press" lemma="press" stem="press" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP I)) (VP (VBP feel) (SBAR (S (NP (PRP$ my) (NN mother)) (VP (VBZ has) (VP (VBN been) (VP (ADVP (RB badly)) (VBN misrepresented) (PP (IN in) (NP (DT the) (NN press))))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="feel my mother has been badly misrepresented in the press" type="VP">
          <tokens>
            <token id="2" string="feel" />
            <token id="3" string="my" />
            <token id="4" string="mother" />
            <token id="5" string="has" />
            <token id="6" string="been" />
            <token id="7" string="badly" />
            <token id="8" string="misrepresented" />
            <token id="9" string="in" />
            <token id="10" string="the" />
            <token id="11" string="press" />
          </tokens>
        </chunking>
        <chunking id="2" string="has been badly misrepresented in the press" type="VP">
          <tokens>
            <token id="5" string="has" />
            <token id="6" string="been" />
            <token id="7" string="badly" />
            <token id="8" string="misrepresented" />
            <token id="9" string="in" />
            <token id="10" string="the" />
            <token id="11" string="press" />
          </tokens>
        </chunking>
        <chunking id="3" string="badly misrepresented in the press" type="VP">
          <tokens>
            <token id="7" string="badly" />
            <token id="8" string="misrepresented" />
            <token id="9" string="in" />
            <token id="10" string="the" />
            <token id="11" string="press" />
          </tokens>
        </chunking>
        <chunking id="4" string="I" type="NP">
          <tokens>
            <token id="1" string="I" />
          </tokens>
        </chunking>
        <chunking id="5" string="been badly misrepresented in the press" type="VP">
          <tokens>
            <token id="6" string="been" />
            <token id="7" string="badly" />
            <token id="8" string="misrepresented" />
            <token id="9" string="in" />
            <token id="10" string="the" />
            <token id="11" string="press" />
          </tokens>
        </chunking>
        <chunking id="6" string="my mother has been badly misrepresented in the press" type="SBAR">
          <tokens>
            <token id="3" string="my" />
            <token id="4" string="mother" />
            <token id="5" string="has" />
            <token id="6" string="been" />
            <token id="7" string="badly" />
            <token id="8" string="misrepresented" />
            <token id="9" string="in" />
            <token id="10" string="the" />
            <token id="11" string="press" />
          </tokens>
        </chunking>
        <chunking id="7" string="my mother" type="NP">
          <tokens>
            <token id="3" string="my" />
            <token id="4" string="mother" />
          </tokens>
        </chunking>
        <chunking id="8" string="the press" type="NP">
          <tokens>
            <token id="10" string="the" />
            <token id="11" string="press" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">feel</governor>
          <dependent id="1">I</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">feel</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="4">mother</governor>
          <dependent id="3">my</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="8">misrepresented</governor>
          <dependent id="4">mother</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="8">misrepresented</governor>
          <dependent id="5">has</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="8">misrepresented</governor>
          <dependent id="6">been</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="8">misrepresented</governor>
          <dependent id="7">badly</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="2">feel</governor>
          <dependent id="8">misrepresented</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">press</governor>
          <dependent id="9">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">press</governor>
          <dependent id="10">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">misrepresented</governor>
          <dependent id="11">press</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="15" has_coreference="true">
      <content>Baird, a 41-year-old teacher in Cheshire, England, was born seven years after Lennon to Julia and her common-law second husband, John Dykins.</content>
      <tokens>
        <token id="1" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="41-year-old" lemma="41-year-old" stem="41-year-old" pos="JJ" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="true" />
        <token id="5" string="teacher" lemma="teacher" stem="teacher" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="6" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="Cheshire" lemma="Cheshire" stem="cheshir" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="true" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="England" lemma="England" stem="england" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="true" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="born" lemma="bear" stem="born" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="seven" lemma="seven" stem="seven" pos="CD" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="14" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="15" string="after" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="17" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="Julia" lemma="Julia" stem="julia" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="19" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="21" string="common-law" lemma="common-law" stem="common-law" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="second" lemma="second" stem="second" pos="JJ" type="Word" isStopWord="false" ner="ORDINAL" is_referenced="false" is_refers="false" />
        <token id="23" string="husband" lemma="husband" stem="husband" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="26" string="Dykins" lemma="Dykins" stem="dykin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="27" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Baird)) (, ,) (NP (NP (DT a) (JJ 41-year-old) (NN teacher)) (PP (IN in) (NP (NNP Cheshire) (, ,) (NNP England)))) (, ,)) (VP (VBD was) (VP (VBN born) (PP (NP (CD seven) (NNS years)) (IN after) (NP (NNP Lennon))) (PP (TO to) (NP (NP (NNP Julia)) (CC and) (NP (NP (PRP$ her) (NN common-law) (JJ second) (NN husband)) (, ,) (NP (NNP John) (NNP Dykins))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a 41-year-old teacher" type="NP">
          <tokens>
            <token id="3" string="a" />
            <token id="4" string="41-year-old" />
            <token id="5" string="teacher" />
          </tokens>
        </chunking>
        <chunking id="2" string="John Dykins" type="NP">
          <tokens>
            <token id="25" string="John" />
            <token id="26" string="Dykins" />
          </tokens>
        </chunking>
        <chunking id="3" string="was born seven years after Lennon to Julia and her common-law second husband , John Dykins" type="VP">
          <tokens>
            <token id="11" string="was" />
            <token id="12" string="born" />
            <token id="13" string="seven" />
            <token id="14" string="years" />
            <token id="15" string="after" />
            <token id="16" string="Lennon" />
            <token id="17" string="to" />
            <token id="18" string="Julia" />
            <token id="19" string="and" />
            <token id="20" string="her" />
            <token id="21" string="common-law" />
            <token id="22" string="second" />
            <token id="23" string="husband" />
            <token id="24" string="," />
            <token id="25" string="John" />
            <token id="26" string="Dykins" />
          </tokens>
        </chunking>
        <chunking id="4" string="Lennon" type="NP">
          <tokens>
            <token id="16" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="5" string="Cheshire , England" type="NP">
          <tokens>
            <token id="7" string="Cheshire" />
            <token id="8" string="," />
            <token id="9" string="England" />
          </tokens>
        </chunking>
        <chunking id="6" string="Baird , a 41-year-old teacher in Cheshire , England ," type="NP">
          <tokens>
            <token id="1" string="Baird" />
            <token id="2" string="," />
            <token id="3" string="a" />
            <token id="4" string="41-year-old" />
            <token id="5" string="teacher" />
            <token id="6" string="in" />
            <token id="7" string="Cheshire" />
            <token id="8" string="," />
            <token id="9" string="England" />
            <token id="10" string="," />
          </tokens>
        </chunking>
        <chunking id="7" string="born seven years after Lennon to Julia and her common-law second husband , John Dykins" type="VP">
          <tokens>
            <token id="12" string="born" />
            <token id="13" string="seven" />
            <token id="14" string="years" />
            <token id="15" string="after" />
            <token id="16" string="Lennon" />
            <token id="17" string="to" />
            <token id="18" string="Julia" />
            <token id="19" string="and" />
            <token id="20" string="her" />
            <token id="21" string="common-law" />
            <token id="22" string="second" />
            <token id="23" string="husband" />
            <token id="24" string="," />
            <token id="25" string="John" />
            <token id="26" string="Dykins" />
          </tokens>
        </chunking>
        <chunking id="8" string="Julia" type="NP">
          <tokens>
            <token id="18" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="9" string="Baird" type="NP">
          <tokens>
            <token id="1" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="10" string="Julia and her common-law second husband , John Dykins" type="NP">
          <tokens>
            <token id="18" string="Julia" />
            <token id="19" string="and" />
            <token id="20" string="her" />
            <token id="21" string="common-law" />
            <token id="22" string="second" />
            <token id="23" string="husband" />
            <token id="24" string="," />
            <token id="25" string="John" />
            <token id="26" string="Dykins" />
          </tokens>
        </chunking>
        <chunking id="11" string="a 41-year-old teacher in Cheshire , England" type="NP">
          <tokens>
            <token id="3" string="a" />
            <token id="4" string="41-year-old" />
            <token id="5" string="teacher" />
            <token id="6" string="in" />
            <token id="7" string="Cheshire" />
            <token id="8" string="," />
            <token id="9" string="England" />
          </tokens>
        </chunking>
        <chunking id="12" string="seven years" type="NP">
          <tokens>
            <token id="13" string="seven" />
            <token id="14" string="years" />
          </tokens>
        </chunking>
        <chunking id="13" string="her common-law second husband , John Dykins" type="NP">
          <tokens>
            <token id="20" string="her" />
            <token id="21" string="common-law" />
            <token id="22" string="second" />
            <token id="23" string="husband" />
            <token id="24" string="," />
            <token id="25" string="John" />
            <token id="26" string="Dykins" />
          </tokens>
        </chunking>
        <chunking id="14" string="her common-law second husband" type="NP">
          <tokens>
            <token id="20" string="her" />
            <token id="21" string="common-law" />
            <token id="22" string="second" />
            <token id="23" string="husband" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="12">born</governor>
          <dependent id="1">Baird</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">teacher</governor>
          <dependent id="3">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">teacher</governor>
          <dependent id="4">41-year-old</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="1">Baird</governor>
          <dependent id="5">teacher</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">England</governor>
          <dependent id="6">in</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">England</governor>
          <dependent id="7">Cheshire</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">teacher</governor>
          <dependent id="9">England</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="12">born</governor>
          <dependent id="11">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="12">born</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="14">years</governor>
          <dependent id="13">seven</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">born</governor>
          <dependent id="14">years</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">years</governor>
          <dependent id="15">after</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="14">years</governor>
          <dependent id="16">Lennon</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">Julia</governor>
          <dependent id="17">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">born</governor>
          <dependent id="18">Julia</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="18">Julia</governor>
          <dependent id="19">and</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="23">husband</governor>
          <dependent id="20">her</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="23">husband</governor>
          <dependent id="21">common-law</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="23">husband</governor>
          <dependent id="22">second</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="18">Julia</governor>
          <dependent id="23">husband</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="26">Dykins</governor>
          <dependent id="25">John</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="23">husband</governor>
          <dependent id="26">Dykins</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Julia" type="PERSON" score="0.0">
          <tokens>
            <token id="18" string="Julia" />
          </tokens>
        </entity>
        <entity id="2" string="John Dykins" type="PERSON" score="0.0">
          <tokens>
            <token id="25" string="John" />
            <token id="26" string="Dykins" />
          </tokens>
        </entity>
        <entity id="3" string="Cheshire" type="LOCATION" score="0.0">
          <tokens>
            <token id="7" string="Cheshire" />
          </tokens>
        </entity>
        <entity id="4" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Baird" />
          </tokens>
        </entity>
        <entity id="5" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="16" string="Lennon" />
          </tokens>
        </entity>
        <entity id="6" string="41-year-old" type="DURATION" score="0.0">
          <tokens>
            <token id="4" string="41-year-old" />
          </tokens>
        </entity>
        <entity id="7" string="second" type="ORDINAL" score="0.0">
          <tokens>
            <token id="22" string="second" />
          </tokens>
        </entity>
        <entity id="8" string="England" type="LOCATION" score="0.0">
          <tokens>
            <token id="9" string="England" />
          </tokens>
        </entity>
        <entity id="9" string="seven years" type="DURATION" score="0.0">
          <tokens>
            <token id="13" string="seven" />
            <token id="14" string="years" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="16" has_coreference="true">
      <content>She and her younger sister, Jacqui, grew up with their parents amid an extended family of aunts, uncles, cousins _ and big brother John.</content>
      <tokens>
        <token id="1" string="She" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="2" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="3" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="4" string="younger" lemma="younger" stem="younger" pos="JJR" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="sister" lemma="sister" stem="sister" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="Jacqui" lemma="Jacqui" stem="jacqui" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="grew" lemma="grow" stem="grew" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="up" lemma="up" stem="up" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="parents" lemma="parent" stem="parent" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="amid" lemma="amid" stem="amid" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="16" string="extended" lemma="extend" stem="extend" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="17" string="family" lemma="family" stem="famili" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="18" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="19" string="aunts" lemma="aunt" stem="aunt" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="21" string="uncles" lemma="uncle" stem="uncl" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="22" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="23" string="cousins" lemma="cousin" stem="cousin" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="24" string="_" lemma="_" stem="_" pos="VBP" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="25" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="26" string="big" lemma="big" stem="big" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="27" string="brother" lemma="brother" stem="brother" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="28" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="29" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NP (PRP She)) (CC and) (NP (PRP$ her) (JJR younger) (NN sister))) (, ,) (NP (NNP Jacqui)) (, ,)) (VP (VBD grew) (PRT (RP up)) (PP (IN with) (NP (PRP$ their) (NNS parents))) (PP (IN amid) (NP (NP (DT an) (VBN extended) (NN family)) (PP (IN of) (NP (NP (NNS aunts)) (, ,) (NP (NNS uncles)) (, ,) (NP (NP (NNS cousins)) (SBAR (S (VP (VBP _))))) (CC and) (NP (JJ big) (NN brother) (NNP John))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="grew up with their parents amid an extended family of aunts , uncles , cousins _ and big brother John" type="VP">
          <tokens>
            <token id="9" string="grew" />
            <token id="10" string="up" />
            <token id="11" string="with" />
            <token id="12" string="their" />
            <token id="13" string="parents" />
            <token id="14" string="amid" />
            <token id="15" string="an" />
            <token id="16" string="extended" />
            <token id="17" string="family" />
            <token id="18" string="of" />
            <token id="19" string="aunts" />
            <token id="20" string="," />
            <token id="21" string="uncles" />
            <token id="22" string="," />
            <token id="23" string="cousins" />
            <token id="24" string="_" />
            <token id="25" string="and" />
            <token id="26" string="big" />
            <token id="27" string="brother" />
            <token id="28" string="John" />
          </tokens>
        </chunking>
        <chunking id="2" string="She and her younger sister" type="NP">
          <tokens>
            <token id="1" string="She" />
            <token id="2" string="and" />
            <token id="3" string="her" />
            <token id="4" string="younger" />
            <token id="5" string="sister" />
          </tokens>
        </chunking>
        <chunking id="3" string="an extended family of aunts , uncles , cousins _ and big brother John" type="NP">
          <tokens>
            <token id="15" string="an" />
            <token id="16" string="extended" />
            <token id="17" string="family" />
            <token id="18" string="of" />
            <token id="19" string="aunts" />
            <token id="20" string="," />
            <token id="21" string="uncles" />
            <token id="22" string="," />
            <token id="23" string="cousins" />
            <token id="24" string="_" />
            <token id="25" string="and" />
            <token id="26" string="big" />
            <token id="27" string="brother" />
            <token id="28" string="John" />
          </tokens>
        </chunking>
        <chunking id="4" string="cousins _" type="NP">
          <tokens>
            <token id="23" string="cousins" />
            <token id="24" string="_" />
          </tokens>
        </chunking>
        <chunking id="5" string="uncles" type="NP">
          <tokens>
            <token id="21" string="uncles" />
          </tokens>
        </chunking>
        <chunking id="6" string="She and her younger sister , Jacqui ," type="NP">
          <tokens>
            <token id="1" string="She" />
            <token id="2" string="and" />
            <token id="3" string="her" />
            <token id="4" string="younger" />
            <token id="5" string="sister" />
            <token id="6" string="," />
            <token id="7" string="Jacqui" />
            <token id="8" string="," />
          </tokens>
        </chunking>
        <chunking id="7" string="She" type="NP">
          <tokens>
            <token id="1" string="She" />
          </tokens>
        </chunking>
        <chunking id="8" string="big brother John" type="NP">
          <tokens>
            <token id="26" string="big" />
            <token id="27" string="brother" />
            <token id="28" string="John" />
          </tokens>
        </chunking>
        <chunking id="9" string="her younger sister" type="NP">
          <tokens>
            <token id="3" string="her" />
            <token id="4" string="younger" />
            <token id="5" string="sister" />
          </tokens>
        </chunking>
        <chunking id="10" string="an extended family" type="NP">
          <tokens>
            <token id="15" string="an" />
            <token id="16" string="extended" />
            <token id="17" string="family" />
          </tokens>
        </chunking>
        <chunking id="11" string="Jacqui" type="NP">
          <tokens>
            <token id="7" string="Jacqui" />
          </tokens>
        </chunking>
        <chunking id="12" string="aunts" type="NP">
          <tokens>
            <token id="19" string="aunts" />
          </tokens>
        </chunking>
        <chunking id="13" string="cousins" type="NP">
          <tokens>
            <token id="23" string="cousins" />
          </tokens>
        </chunking>
        <chunking id="14" string="their parents" type="NP">
          <tokens>
            <token id="12" string="their" />
            <token id="13" string="parents" />
          </tokens>
        </chunking>
        <chunking id="15" string="aunts , uncles , cousins _ and big brother John" type="NP">
          <tokens>
            <token id="19" string="aunts" />
            <token id="20" string="," />
            <token id="21" string="uncles" />
            <token id="22" string="," />
            <token id="23" string="cousins" />
            <token id="24" string="_" />
            <token id="25" string="and" />
            <token id="26" string="big" />
            <token id="27" string="brother" />
            <token id="28" string="John" />
          </tokens>
        </chunking>
        <chunking id="16" string="_" type="SBAR">
          <tokens>
            <token id="24" string="_" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="9">grew</governor>
          <dependent id="1">She</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="1">She</governor>
          <dependent id="2">and</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="5">sister</governor>
          <dependent id="3">her</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">sister</governor>
          <dependent id="4">younger</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="1">She</governor>
          <dependent id="5">sister</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="1">She</governor>
          <dependent id="7">Jacqui</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="9">grew</dependent>
        </dependency>
        <dependency type="compound:prt">
          <governor id="9">grew</governor>
          <dependent id="10">up</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">parents</governor>
          <dependent id="11">with</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="13">parents</governor>
          <dependent id="12">their</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">grew</governor>
          <dependent id="13">parents</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">family</governor>
          <dependent id="14">amid</dependent>
        </dependency>
        <dependency type="det">
          <governor id="17">family</governor>
          <dependent id="15">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="17">family</governor>
          <dependent id="16">extended</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">grew</governor>
          <dependent id="17">family</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">aunts</governor>
          <dependent id="18">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">family</governor>
          <dependent id="19">aunts</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="19">aunts</governor>
          <dependent id="21">uncles</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="19">aunts</governor>
          <dependent id="23">cousins</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="23">cousins</governor>
          <dependent id="24">_</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="19">aunts</governor>
          <dependent id="25">and</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="28">John</governor>
          <dependent id="26">big</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="28">John</governor>
          <dependent id="27">brother</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="19">aunts</governor>
          <dependent id="28">John</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Jacqui" type="PERSON" score="0.0">
          <tokens>
            <token id="7" string="Jacqui" />
          </tokens>
        </entity>
        <entity id="2" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="28" string="John" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="17" has_coreference="true">
      <content>The book tells of the sisters getting hauled out of the bathtub when teen-agers Lennon, Paul McCartney and George Harrison drop by for bathroom jam sessions with Julia.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="tells" lemma="tell" stem="tell" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="sisters" lemma="sister" stem="sister" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="getting" lemma="get" stem="get" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="hauled" lemma="haul" stem="haul" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="out" lemma="out" stem="out" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="bathtub" lemma="bathtub" stem="bathtub" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="teen-agers" lemma="teen-ager" stem="teen-ag" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="Paul" lemma="Paul" stem="paul" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="18" string="McCartney" lemma="McCartney" stem="mccartnei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="19" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="20" string="George" lemma="George" stem="georg" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="21" string="Harrison" lemma="Harrison" stem="harrison" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="22" string="drop" lemma="drop" stem="drop" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="23" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="bathroom" lemma="bathroom" stem="bathroom" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="jam" lemma="jam" stem="jam" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="sessions" lemma="session" stem="session" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="Julia" lemma="Julia" stem="julia" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="30" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN book)) (VP (VBZ tells) (PP (IN of) (NP (NP (DT the) (NNS sisters)) (VP (VBG getting) (VP (VBN hauled) (S (ADJP (IN out) (PP (IN of) (NP (DT the) (NN bathtub))))) (SBAR (WHADVP (WRB when)) (S (VP (NP (NNS teen-agers)) (NP (NP (NNP Lennon)) (, ,) (NP (NNP Paul) (NNP McCartney)) (CC and) (NP (NNP George) (NNP Harrison) (NN drop))) (PP (IN by) (IN for) (NP (NP (NN bathroom) (NN jam) (NNS sessions)) (PP (IN with) (NP (NNP Julia))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="bathroom jam sessions" type="NP">
          <tokens>
            <token id="25" string="bathroom" />
            <token id="26" string="jam" />
            <token id="27" string="sessions" />
          </tokens>
        </chunking>
        <chunking id="2" string="The book" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="book" />
          </tokens>
        </chunking>
        <chunking id="3" string="Lennon , Paul McCartney and George Harrison drop" type="NP">
          <tokens>
            <token id="15" string="Lennon" />
            <token id="16" string="," />
            <token id="17" string="Paul" />
            <token id="18" string="McCartney" />
            <token id="19" string="and" />
            <token id="20" string="George" />
            <token id="21" string="Harrison" />
            <token id="22" string="drop" />
          </tokens>
        </chunking>
        <chunking id="4" string="Lennon" type="NP">
          <tokens>
            <token id="15" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="5" string="the bathtub" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="bathtub" />
          </tokens>
        </chunking>
        <chunking id="6" string="hauled out of the bathtub when teen-agers Lennon , Paul McCartney and George Harrison drop by for bathroom jam sessions with Julia" type="VP">
          <tokens>
            <token id="8" string="hauled" />
            <token id="9" string="out" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="bathtub" />
            <token id="13" string="when" />
            <token id="14" string="teen-agers" />
            <token id="15" string="Lennon" />
            <token id="16" string="," />
            <token id="17" string="Paul" />
            <token id="18" string="McCartney" />
            <token id="19" string="and" />
            <token id="20" string="George" />
            <token id="21" string="Harrison" />
            <token id="22" string="drop" />
            <token id="23" string="by" />
            <token id="24" string="for" />
            <token id="25" string="bathroom" />
            <token id="26" string="jam" />
            <token id="27" string="sessions" />
            <token id="28" string="with" />
            <token id="29" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="7" string="when teen-agers Lennon , Paul McCartney and George Harrison drop by for bathroom jam sessions with Julia" type="SBAR">
          <tokens>
            <token id="13" string="when" />
            <token id="14" string="teen-agers" />
            <token id="15" string="Lennon" />
            <token id="16" string="," />
            <token id="17" string="Paul" />
            <token id="18" string="McCartney" />
            <token id="19" string="and" />
            <token id="20" string="George" />
            <token id="21" string="Harrison" />
            <token id="22" string="drop" />
            <token id="23" string="by" />
            <token id="24" string="for" />
            <token id="25" string="bathroom" />
            <token id="26" string="jam" />
            <token id="27" string="sessions" />
            <token id="28" string="with" />
            <token id="29" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="8" string="when" type="WHADVP">
          <tokens>
            <token id="13" string="when" />
          </tokens>
        </chunking>
        <chunking id="9" string="tells of the sisters getting hauled out of the bathtub when teen-agers Lennon , Paul McCartney and George Harrison drop by for bathroom jam sessions with Julia" type="VP">
          <tokens>
            <token id="3" string="tells" />
            <token id="4" string="of" />
            <token id="5" string="the" />
            <token id="6" string="sisters" />
            <token id="7" string="getting" />
            <token id="8" string="hauled" />
            <token id="9" string="out" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="bathtub" />
            <token id="13" string="when" />
            <token id="14" string="teen-agers" />
            <token id="15" string="Lennon" />
            <token id="16" string="," />
            <token id="17" string="Paul" />
            <token id="18" string="McCartney" />
            <token id="19" string="and" />
            <token id="20" string="George" />
            <token id="21" string="Harrison" />
            <token id="22" string="drop" />
            <token id="23" string="by" />
            <token id="24" string="for" />
            <token id="25" string="bathroom" />
            <token id="26" string="jam" />
            <token id="27" string="sessions" />
            <token id="28" string="with" />
            <token id="29" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="10" string="Paul McCartney" type="NP">
          <tokens>
            <token id="17" string="Paul" />
            <token id="18" string="McCartney" />
          </tokens>
        </chunking>
        <chunking id="11" string="George Harrison drop" type="NP">
          <tokens>
            <token id="20" string="George" />
            <token id="21" string="Harrison" />
            <token id="22" string="drop" />
          </tokens>
        </chunking>
        <chunking id="12" string="teen-agers" type="NP">
          <tokens>
            <token id="14" string="teen-agers" />
          </tokens>
        </chunking>
        <chunking id="13" string="teen-agers Lennon , Paul McCartney and George Harrison drop by for bathroom jam sessions with Julia" type="VP">
          <tokens>
            <token id="14" string="teen-agers" />
            <token id="15" string="Lennon" />
            <token id="16" string="," />
            <token id="17" string="Paul" />
            <token id="18" string="McCartney" />
            <token id="19" string="and" />
            <token id="20" string="George" />
            <token id="21" string="Harrison" />
            <token id="22" string="drop" />
            <token id="23" string="by" />
            <token id="24" string="for" />
            <token id="25" string="bathroom" />
            <token id="26" string="jam" />
            <token id="27" string="sessions" />
            <token id="28" string="with" />
            <token id="29" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="14" string="Julia" type="NP">
          <tokens>
            <token id="29" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="15" string="out of the bathtub" type="ADJP">
          <tokens>
            <token id="9" string="out" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="bathtub" />
          </tokens>
        </chunking>
        <chunking id="16" string="the sisters getting hauled out of the bathtub when teen-agers Lennon , Paul McCartney and George Harrison drop by for bathroom jam sessions with Julia" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="sisters" />
            <token id="7" string="getting" />
            <token id="8" string="hauled" />
            <token id="9" string="out" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="bathtub" />
            <token id="13" string="when" />
            <token id="14" string="teen-agers" />
            <token id="15" string="Lennon" />
            <token id="16" string="," />
            <token id="17" string="Paul" />
            <token id="18" string="McCartney" />
            <token id="19" string="and" />
            <token id="20" string="George" />
            <token id="21" string="Harrison" />
            <token id="22" string="drop" />
            <token id="23" string="by" />
            <token id="24" string="for" />
            <token id="25" string="bathroom" />
            <token id="26" string="jam" />
            <token id="27" string="sessions" />
            <token id="28" string="with" />
            <token id="29" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="17" string="getting hauled out of the bathtub when teen-agers Lennon , Paul McCartney and George Harrison drop by for bathroom jam sessions with Julia" type="VP">
          <tokens>
            <token id="7" string="getting" />
            <token id="8" string="hauled" />
            <token id="9" string="out" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="bathtub" />
            <token id="13" string="when" />
            <token id="14" string="teen-agers" />
            <token id="15" string="Lennon" />
            <token id="16" string="," />
            <token id="17" string="Paul" />
            <token id="18" string="McCartney" />
            <token id="19" string="and" />
            <token id="20" string="George" />
            <token id="21" string="Harrison" />
            <token id="22" string="drop" />
            <token id="23" string="by" />
            <token id="24" string="for" />
            <token id="25" string="bathroom" />
            <token id="26" string="jam" />
            <token id="27" string="sessions" />
            <token id="28" string="with" />
            <token id="29" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="18" string="bathroom jam sessions with Julia" type="NP">
          <tokens>
            <token id="25" string="bathroom" />
            <token id="26" string="jam" />
            <token id="27" string="sessions" />
            <token id="28" string="with" />
            <token id="29" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="19" string="the sisters" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="sisters" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">book</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">tells</governor>
          <dependent id="2">book</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">tells</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">sisters</governor>
          <dependent id="4">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">sisters</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">tells</governor>
          <dependent id="6">sisters</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="8">hauled</governor>
          <dependent id="7">getting</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="6">sisters</governor>
          <dependent id="8">hauled</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">bathtub</governor>
          <dependent id="9">out</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="9">out</governor>
          <dependent id="10">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">bathtub</governor>
          <dependent id="11">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">hauled</governor>
          <dependent id="12">bathtub</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="14">teen-agers</governor>
          <dependent id="13">when</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="8">hauled</governor>
          <dependent id="14">teen-agers</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="14">teen-agers</governor>
          <dependent id="15">Lennon</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="18">McCartney</governor>
          <dependent id="17">Paul</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="15">Lennon</governor>
          <dependent id="18">McCartney</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="15">Lennon</governor>
          <dependent id="19">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="22">drop</governor>
          <dependent id="20">George</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="22">drop</governor>
          <dependent id="21">Harrison</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="15">Lennon</governor>
          <dependent id="22">drop</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">sessions</governor>
          <dependent id="23">by</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">sessions</governor>
          <dependent id="24">for</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="27">sessions</governor>
          <dependent id="25">bathroom</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="27">sessions</governor>
          <dependent id="26">jam</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">teen-agers</governor>
          <dependent id="27">sessions</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">Julia</governor>
          <dependent id="28">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">sessions</governor>
          <dependent id="29">Julia</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="George Harrison" type="PERSON" score="0.0">
          <tokens>
            <token id="20" string="George" />
            <token id="21" string="Harrison" />
          </tokens>
        </entity>
        <entity id="2" string="Julia" type="PERSON" score="0.0">
          <tokens>
            <token id="29" string="Julia" />
          </tokens>
        </entity>
        <entity id="3" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="15" string="Lennon" />
          </tokens>
        </entity>
        <entity id="4" string="Paul McCartney" type="PERSON" score="0.0">
          <tokens>
            <token id="17" string="Paul" />
            <token id="18" string="McCartney" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="18" has_coreference="true">
      <content>It tells of an alternately thoughtless and protective brother who leaves his step-sisters at the movies for hours while he goes on a date, who between songs at a Beatles concert shouts for security guards to lift the Dykins girls out of the front row, away from screaming fans threatening to storm the stage.</content>
      <tokens>
        <token id="1" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="tells" lemma="tell" stem="tell" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="alternately" lemma="alternately" stem="altern" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="6" string="thoughtless" lemma="thoughtless" stem="thoughtless" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="protective" lemma="protective" stem="protect" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="9" string="brother" lemma="brother" stem="brother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="10" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="leaves" lemma="leave" stem="leav" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="12" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="step-sisters" lemma="step-sister" stem="step-sist" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="14" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="16" string="movies" lemma="movie" stem="movi" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="17" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="hours" lemma="hour" stem="hour" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="true" />
        <token id="19" string="while" lemma="while" stem="while" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="20" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="21" string="goes" lemma="go" stem="goe" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="22" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="23" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="24" string="date" lemma="date" stem="date" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="25" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="26" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="27" string="between" lemma="between" stem="between" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="songs" lemma="song" stem="song" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="29" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="30" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="31" string="Beatles" lemma="Beatles" stem="beatl" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="32" string="concert" lemma="concert" stem="concert" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="33" string="shouts" lemma="shout" stem="shout" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="34" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="35" string="security" lemma="security" stem="secur" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="36" string="guards" lemma="guard" stem="guard" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="37" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="38" string="lift" lemma="lift" stem="lift" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="39" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="40" string="Dykins" lemma="Dykins" stem="dykin" pos="NNP" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="true" />
        <token id="41" string="girls" lemma="girl" stem="girl" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="42" string="out" lemma="out" stem="out" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="43" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="44" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="45" string="front" lemma="front" stem="front" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="46" string="row" lemma="row" stem="row" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="47" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="48" string="away" lemma="away" stem="awai" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="49" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="50" string="screaming" lemma="scream" stem="scream" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="51" string="fans" lemma="fan" stem="fan" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="52" string="threatening" lemma="threaten" stem="threaten" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="53" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="54" string="storm" lemma="storm" stem="storm" pos="VB" type="Word" isStopWord="false" ner="CAUSE_OF_DEATH" is_referenced="false" is_refers="true" />
        <token id="55" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="56" string="stage" lemma="stage" stem="stage" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="57" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP It)) (VP (VBZ tells) (PP (IN of) (NP (NP (DT an) (ADJP (RB alternately) (JJ thoughtless) (CC and) (JJ protective)) (NN brother)) (SBAR (WHNP (WP who)) (S (VP (VBZ leaves) (NP (PRP$ his) (NNS step-sisters)) (PP (IN at) (NP (NP (DT the) (NNS movies)) (PP (IN for) (NP (NNS hours))))) (SBAR (IN while) (S (NP (PRP he)) (VP (VBZ goes) (PP (IN on) (NP (NP (DT a) (NN date)) (, ,) (SBAR (WHNP (WP who)) (S (PP (IN between) (NP (NP (NNS songs)) (PP (IN at) (NP (DT a) (NNP Beatles) (NN concert))))) (VP (VBZ shouts) (PP (IN for) (NP (NN security) (NNS guards))) (S (VP (TO to) (VP (VB lift) (NP (DT the) (NNP Dykins) (NNS girls)) (ADVP (IN out) (PP (IN of) (NP (DT the) (JJ front) (NN row)))) (, ,) (PP (RB away) (IN from) (S (VP (VBG screaming) (NP (NP (NNS fans)) (VP (VBG threatening) (S (VP (TO to) (VP (VB storm) (NP (DT the) (NN stage))))))))))))))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="hours" type="NP">
          <tokens>
            <token id="18" string="hours" />
          </tokens>
        </chunking>
        <chunking id="2" string="a date , who between songs at a Beatles concert shouts for security guards to lift the Dykins girls out of the front row , away from screaming fans threatening to storm the stage" type="NP">
          <tokens>
            <token id="23" string="a" />
            <token id="24" string="date" />
            <token id="25" string="," />
            <token id="26" string="who" />
            <token id="27" string="between" />
            <token id="28" string="songs" />
            <token id="29" string="at" />
            <token id="30" string="a" />
            <token id="31" string="Beatles" />
            <token id="32" string="concert" />
            <token id="33" string="shouts" />
            <token id="34" string="for" />
            <token id="35" string="security" />
            <token id="36" string="guards" />
            <token id="37" string="to" />
            <token id="38" string="lift" />
            <token id="39" string="the" />
            <token id="40" string="Dykins" />
            <token id="41" string="girls" />
            <token id="42" string="out" />
            <token id="43" string="of" />
            <token id="44" string="the" />
            <token id="45" string="front" />
            <token id="46" string="row" />
            <token id="47" string="," />
            <token id="48" string="away" />
            <token id="49" string="from" />
            <token id="50" string="screaming" />
            <token id="51" string="fans" />
            <token id="52" string="threatening" />
            <token id="53" string="to" />
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="3" string="the movies for hours" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="movies" />
            <token id="17" string="for" />
            <token id="18" string="hours" />
          </tokens>
        </chunking>
        <chunking id="4" string="security guards" type="NP">
          <tokens>
            <token id="35" string="security" />
            <token id="36" string="guards" />
          </tokens>
        </chunking>
        <chunking id="5" string="songs" type="NP">
          <tokens>
            <token id="28" string="songs" />
          </tokens>
        </chunking>
        <chunking id="6" string="an alternately thoughtless and protective brother" type="NP">
          <tokens>
            <token id="4" string="an" />
            <token id="5" string="alternately" />
            <token id="6" string="thoughtless" />
            <token id="7" string="and" />
            <token id="8" string="protective" />
            <token id="9" string="brother" />
          </tokens>
        </chunking>
        <chunking id="7" string="lift the Dykins girls out of the front row , away from screaming fans threatening to storm the stage" type="VP">
          <tokens>
            <token id="38" string="lift" />
            <token id="39" string="the" />
            <token id="40" string="Dykins" />
            <token id="41" string="girls" />
            <token id="42" string="out" />
            <token id="43" string="of" />
            <token id="44" string="the" />
            <token id="45" string="front" />
            <token id="46" string="row" />
            <token id="47" string="," />
            <token id="48" string="away" />
            <token id="49" string="from" />
            <token id="50" string="screaming" />
            <token id="51" string="fans" />
            <token id="52" string="threatening" />
            <token id="53" string="to" />
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="8" string="the stage" type="NP">
          <tokens>
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="9" string="screaming fans threatening to storm the stage" type="VP">
          <tokens>
            <token id="50" string="screaming" />
            <token id="51" string="fans" />
            <token id="52" string="threatening" />
            <token id="53" string="to" />
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="10" string="a date" type="NP">
          <tokens>
            <token id="23" string="a" />
            <token id="24" string="date" />
          </tokens>
        </chunking>
        <chunking id="11" string="he" type="NP">
          <tokens>
            <token id="20" string="he" />
          </tokens>
        </chunking>
        <chunking id="12" string="tells of an alternately thoughtless and protective brother who leaves his step-sisters at the movies for hours while he goes on a date , who between songs at a Beatles concert shouts for security guards to lift the Dykins girls out of the front row , away from screaming fans threatening to storm the stage" type="VP">
          <tokens>
            <token id="2" string="tells" />
            <token id="3" string="of" />
            <token id="4" string="an" />
            <token id="5" string="alternately" />
            <token id="6" string="thoughtless" />
            <token id="7" string="and" />
            <token id="8" string="protective" />
            <token id="9" string="brother" />
            <token id="10" string="who" />
            <token id="11" string="leaves" />
            <token id="12" string="his" />
            <token id="13" string="step-sisters" />
            <token id="14" string="at" />
            <token id="15" string="the" />
            <token id="16" string="movies" />
            <token id="17" string="for" />
            <token id="18" string="hours" />
            <token id="19" string="while" />
            <token id="20" string="he" />
            <token id="21" string="goes" />
            <token id="22" string="on" />
            <token id="23" string="a" />
            <token id="24" string="date" />
            <token id="25" string="," />
            <token id="26" string="who" />
            <token id="27" string="between" />
            <token id="28" string="songs" />
            <token id="29" string="at" />
            <token id="30" string="a" />
            <token id="31" string="Beatles" />
            <token id="32" string="concert" />
            <token id="33" string="shouts" />
            <token id="34" string="for" />
            <token id="35" string="security" />
            <token id="36" string="guards" />
            <token id="37" string="to" />
            <token id="38" string="lift" />
            <token id="39" string="the" />
            <token id="40" string="Dykins" />
            <token id="41" string="girls" />
            <token id="42" string="out" />
            <token id="43" string="of" />
            <token id="44" string="the" />
            <token id="45" string="front" />
            <token id="46" string="row" />
            <token id="47" string="," />
            <token id="48" string="away" />
            <token id="49" string="from" />
            <token id="50" string="screaming" />
            <token id="51" string="fans" />
            <token id="52" string="threatening" />
            <token id="53" string="to" />
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="13" string="songs at a Beatles concert" type="NP">
          <tokens>
            <token id="28" string="songs" />
            <token id="29" string="at" />
            <token id="30" string="a" />
            <token id="31" string="Beatles" />
            <token id="32" string="concert" />
          </tokens>
        </chunking>
        <chunking id="14" string="goes on a date , who between songs at a Beatles concert shouts for security guards to lift the Dykins girls out of the front row , away from screaming fans threatening to storm the stage" type="VP">
          <tokens>
            <token id="21" string="goes" />
            <token id="22" string="on" />
            <token id="23" string="a" />
            <token id="24" string="date" />
            <token id="25" string="," />
            <token id="26" string="who" />
            <token id="27" string="between" />
            <token id="28" string="songs" />
            <token id="29" string="at" />
            <token id="30" string="a" />
            <token id="31" string="Beatles" />
            <token id="32" string="concert" />
            <token id="33" string="shouts" />
            <token id="34" string="for" />
            <token id="35" string="security" />
            <token id="36" string="guards" />
            <token id="37" string="to" />
            <token id="38" string="lift" />
            <token id="39" string="the" />
            <token id="40" string="Dykins" />
            <token id="41" string="girls" />
            <token id="42" string="out" />
            <token id="43" string="of" />
            <token id="44" string="the" />
            <token id="45" string="front" />
            <token id="46" string="row" />
            <token id="47" string="," />
            <token id="48" string="away" />
            <token id="49" string="from" />
            <token id="50" string="screaming" />
            <token id="51" string="fans" />
            <token id="52" string="threatening" />
            <token id="53" string="to" />
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="15" string="who leaves his step-sisters at the movies for hours while he goes on a date , who between songs at a Beatles concert shouts for security guards to lift the Dykins girls out of the front row , away from screaming fans threatening to storm the stage" type="SBAR">
          <tokens>
            <token id="10" string="who" />
            <token id="11" string="leaves" />
            <token id="12" string="his" />
            <token id="13" string="step-sisters" />
            <token id="14" string="at" />
            <token id="15" string="the" />
            <token id="16" string="movies" />
            <token id="17" string="for" />
            <token id="18" string="hours" />
            <token id="19" string="while" />
            <token id="20" string="he" />
            <token id="21" string="goes" />
            <token id="22" string="on" />
            <token id="23" string="a" />
            <token id="24" string="date" />
            <token id="25" string="," />
            <token id="26" string="who" />
            <token id="27" string="between" />
            <token id="28" string="songs" />
            <token id="29" string="at" />
            <token id="30" string="a" />
            <token id="31" string="Beatles" />
            <token id="32" string="concert" />
            <token id="33" string="shouts" />
            <token id="34" string="for" />
            <token id="35" string="security" />
            <token id="36" string="guards" />
            <token id="37" string="to" />
            <token id="38" string="lift" />
            <token id="39" string="the" />
            <token id="40" string="Dykins" />
            <token id="41" string="girls" />
            <token id="42" string="out" />
            <token id="43" string="of" />
            <token id="44" string="the" />
            <token id="45" string="front" />
            <token id="46" string="row" />
            <token id="47" string="," />
            <token id="48" string="away" />
            <token id="49" string="from" />
            <token id="50" string="screaming" />
            <token id="51" string="fans" />
            <token id="52" string="threatening" />
            <token id="53" string="to" />
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="16" string="to lift the Dykins girls out of the front row , away from screaming fans threatening to storm the stage" type="VP">
          <tokens>
            <token id="37" string="to" />
            <token id="38" string="lift" />
            <token id="39" string="the" />
            <token id="40" string="Dykins" />
            <token id="41" string="girls" />
            <token id="42" string="out" />
            <token id="43" string="of" />
            <token id="44" string="the" />
            <token id="45" string="front" />
            <token id="46" string="row" />
            <token id="47" string="," />
            <token id="48" string="away" />
            <token id="49" string="from" />
            <token id="50" string="screaming" />
            <token id="51" string="fans" />
            <token id="52" string="threatening" />
            <token id="53" string="to" />
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="17" string="It" type="NP">
          <tokens>
            <token id="1" string="It" />
          </tokens>
        </chunking>
        <chunking id="18" string="the front row" type="NP">
          <tokens>
            <token id="44" string="the" />
            <token id="45" string="front" />
            <token id="46" string="row" />
          </tokens>
        </chunking>
        <chunking id="19" string="to storm the stage" type="VP">
          <tokens>
            <token id="53" string="to" />
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="20" string="an alternately thoughtless and protective brother who leaves his step-sisters at the movies for hours while he goes on a date , who between songs at a Beatles concert shouts for security guards to lift the Dykins girls out of the front row , away from screaming fans threatening to storm the stage" type="NP">
          <tokens>
            <token id="4" string="an" />
            <token id="5" string="alternately" />
            <token id="6" string="thoughtless" />
            <token id="7" string="and" />
            <token id="8" string="protective" />
            <token id="9" string="brother" />
            <token id="10" string="who" />
            <token id="11" string="leaves" />
            <token id="12" string="his" />
            <token id="13" string="step-sisters" />
            <token id="14" string="at" />
            <token id="15" string="the" />
            <token id="16" string="movies" />
            <token id="17" string="for" />
            <token id="18" string="hours" />
            <token id="19" string="while" />
            <token id="20" string="he" />
            <token id="21" string="goes" />
            <token id="22" string="on" />
            <token id="23" string="a" />
            <token id="24" string="date" />
            <token id="25" string="," />
            <token id="26" string="who" />
            <token id="27" string="between" />
            <token id="28" string="songs" />
            <token id="29" string="at" />
            <token id="30" string="a" />
            <token id="31" string="Beatles" />
            <token id="32" string="concert" />
            <token id="33" string="shouts" />
            <token id="34" string="for" />
            <token id="35" string="security" />
            <token id="36" string="guards" />
            <token id="37" string="to" />
            <token id="38" string="lift" />
            <token id="39" string="the" />
            <token id="40" string="Dykins" />
            <token id="41" string="girls" />
            <token id="42" string="out" />
            <token id="43" string="of" />
            <token id="44" string="the" />
            <token id="45" string="front" />
            <token id="46" string="row" />
            <token id="47" string="," />
            <token id="48" string="away" />
            <token id="49" string="from" />
            <token id="50" string="screaming" />
            <token id="51" string="fans" />
            <token id="52" string="threatening" />
            <token id="53" string="to" />
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="21" string="his step-sisters" type="NP">
          <tokens>
            <token id="12" string="his" />
            <token id="13" string="step-sisters" />
          </tokens>
        </chunking>
        <chunking id="22" string="who between songs at a Beatles concert shouts for security guards to lift the Dykins girls out of the front row , away from screaming fans threatening to storm the stage" type="SBAR">
          <tokens>
            <token id="26" string="who" />
            <token id="27" string="between" />
            <token id="28" string="songs" />
            <token id="29" string="at" />
            <token id="30" string="a" />
            <token id="31" string="Beatles" />
            <token id="32" string="concert" />
            <token id="33" string="shouts" />
            <token id="34" string="for" />
            <token id="35" string="security" />
            <token id="36" string="guards" />
            <token id="37" string="to" />
            <token id="38" string="lift" />
            <token id="39" string="the" />
            <token id="40" string="Dykins" />
            <token id="41" string="girls" />
            <token id="42" string="out" />
            <token id="43" string="of" />
            <token id="44" string="the" />
            <token id="45" string="front" />
            <token id="46" string="row" />
            <token id="47" string="," />
            <token id="48" string="away" />
            <token id="49" string="from" />
            <token id="50" string="screaming" />
            <token id="51" string="fans" />
            <token id="52" string="threatening" />
            <token id="53" string="to" />
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="23" string="fans" type="NP">
          <tokens>
            <token id="51" string="fans" />
          </tokens>
        </chunking>
        <chunking id="24" string="shouts for security guards to lift the Dykins girls out of the front row , away from screaming fans threatening to storm the stage" type="VP">
          <tokens>
            <token id="33" string="shouts" />
            <token id="34" string="for" />
            <token id="35" string="security" />
            <token id="36" string="guards" />
            <token id="37" string="to" />
            <token id="38" string="lift" />
            <token id="39" string="the" />
            <token id="40" string="Dykins" />
            <token id="41" string="girls" />
            <token id="42" string="out" />
            <token id="43" string="of" />
            <token id="44" string="the" />
            <token id="45" string="front" />
            <token id="46" string="row" />
            <token id="47" string="," />
            <token id="48" string="away" />
            <token id="49" string="from" />
            <token id="50" string="screaming" />
            <token id="51" string="fans" />
            <token id="52" string="threatening" />
            <token id="53" string="to" />
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="25" string="the Dykins girls" type="NP">
          <tokens>
            <token id="39" string="the" />
            <token id="40" string="Dykins" />
            <token id="41" string="girls" />
          </tokens>
        </chunking>
        <chunking id="26" string="leaves his step-sisters at the movies for hours while he goes on a date , who between songs at a Beatles concert shouts for security guards to lift the Dykins girls out of the front row , away from screaming fans threatening to storm the stage" type="VP">
          <tokens>
            <token id="11" string="leaves" />
            <token id="12" string="his" />
            <token id="13" string="step-sisters" />
            <token id="14" string="at" />
            <token id="15" string="the" />
            <token id="16" string="movies" />
            <token id="17" string="for" />
            <token id="18" string="hours" />
            <token id="19" string="while" />
            <token id="20" string="he" />
            <token id="21" string="goes" />
            <token id="22" string="on" />
            <token id="23" string="a" />
            <token id="24" string="date" />
            <token id="25" string="," />
            <token id="26" string="who" />
            <token id="27" string="between" />
            <token id="28" string="songs" />
            <token id="29" string="at" />
            <token id="30" string="a" />
            <token id="31" string="Beatles" />
            <token id="32" string="concert" />
            <token id="33" string="shouts" />
            <token id="34" string="for" />
            <token id="35" string="security" />
            <token id="36" string="guards" />
            <token id="37" string="to" />
            <token id="38" string="lift" />
            <token id="39" string="the" />
            <token id="40" string="Dykins" />
            <token id="41" string="girls" />
            <token id="42" string="out" />
            <token id="43" string="of" />
            <token id="44" string="the" />
            <token id="45" string="front" />
            <token id="46" string="row" />
            <token id="47" string="," />
            <token id="48" string="away" />
            <token id="49" string="from" />
            <token id="50" string="screaming" />
            <token id="51" string="fans" />
            <token id="52" string="threatening" />
            <token id="53" string="to" />
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="27" string="threatening to storm the stage" type="VP">
          <tokens>
            <token id="52" string="threatening" />
            <token id="53" string="to" />
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="28" string="fans threatening to storm the stage" type="NP">
          <tokens>
            <token id="51" string="fans" />
            <token id="52" string="threatening" />
            <token id="53" string="to" />
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="29" string="storm the stage" type="VP">
          <tokens>
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="30" string="alternately thoughtless and protective" type="ADJP">
          <tokens>
            <token id="5" string="alternately" />
            <token id="6" string="thoughtless" />
            <token id="7" string="and" />
            <token id="8" string="protective" />
          </tokens>
        </chunking>
        <chunking id="31" string="the movies" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="movies" />
          </tokens>
        </chunking>
        <chunking id="32" string="while he goes on a date , who between songs at a Beatles concert shouts for security guards to lift the Dykins girls out of the front row , away from screaming fans threatening to storm the stage" type="SBAR">
          <tokens>
            <token id="19" string="while" />
            <token id="20" string="he" />
            <token id="21" string="goes" />
            <token id="22" string="on" />
            <token id="23" string="a" />
            <token id="24" string="date" />
            <token id="25" string="," />
            <token id="26" string="who" />
            <token id="27" string="between" />
            <token id="28" string="songs" />
            <token id="29" string="at" />
            <token id="30" string="a" />
            <token id="31" string="Beatles" />
            <token id="32" string="concert" />
            <token id="33" string="shouts" />
            <token id="34" string="for" />
            <token id="35" string="security" />
            <token id="36" string="guards" />
            <token id="37" string="to" />
            <token id="38" string="lift" />
            <token id="39" string="the" />
            <token id="40" string="Dykins" />
            <token id="41" string="girls" />
            <token id="42" string="out" />
            <token id="43" string="of" />
            <token id="44" string="the" />
            <token id="45" string="front" />
            <token id="46" string="row" />
            <token id="47" string="," />
            <token id="48" string="away" />
            <token id="49" string="from" />
            <token id="50" string="screaming" />
            <token id="51" string="fans" />
            <token id="52" string="threatening" />
            <token id="53" string="to" />
            <token id="54" string="storm" />
            <token id="55" string="the" />
            <token id="56" string="stage" />
          </tokens>
        </chunking>
        <chunking id="33" string="a Beatles concert" type="NP">
          <tokens>
            <token id="30" string="a" />
            <token id="31" string="Beatles" />
            <token id="32" string="concert" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">tells</governor>
          <dependent id="1">It</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">tells</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">brother</governor>
          <dependent id="3">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">brother</governor>
          <dependent id="4">an</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="6">thoughtless</governor>
          <dependent id="5">alternately</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">brother</governor>
          <dependent id="6">thoughtless</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="6">thoughtless</governor>
          <dependent id="7">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="6">thoughtless</governor>
          <dependent id="8">protective</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">tells</governor>
          <dependent id="9">brother</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">leaves</governor>
          <dependent id="10">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="9">brother</governor>
          <dependent id="11">leaves</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="13">step-sisters</governor>
          <dependent id="12">his</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">leaves</governor>
          <dependent id="13">step-sisters</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">movies</governor>
          <dependent id="14">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="16">movies</governor>
          <dependent id="15">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">leaves</governor>
          <dependent id="16">movies</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">hours</governor>
          <dependent id="17">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">movies</governor>
          <dependent id="18">hours</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="21">goes</governor>
          <dependent id="19">while</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">goes</governor>
          <dependent id="20">he</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="11">leaves</governor>
          <dependent id="21">goes</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">date</governor>
          <dependent id="22">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="24">date</governor>
          <dependent id="23">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="21">goes</governor>
          <dependent id="24">date</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="33">shouts</governor>
          <dependent id="26">who</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">songs</governor>
          <dependent id="27">between</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="33">shouts</governor>
          <dependent id="28">songs</dependent>
        </dependency>
        <dependency type="case">
          <governor id="32">concert</governor>
          <dependent id="29">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="32">concert</governor>
          <dependent id="30">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="32">concert</governor>
          <dependent id="31">Beatles</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="28">songs</governor>
          <dependent id="32">concert</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="24">date</governor>
          <dependent id="33">shouts</dependent>
        </dependency>
        <dependency type="case">
          <governor id="36">guards</governor>
          <dependent id="34">for</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="36">guards</governor>
          <dependent id="35">security</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="33">shouts</governor>
          <dependent id="36">guards</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="38">lift</governor>
          <dependent id="37">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="33">shouts</governor>
          <dependent id="38">lift</dependent>
        </dependency>
        <dependency type="det">
          <governor id="41">girls</governor>
          <dependent id="39">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="41">girls</governor>
          <dependent id="40">Dykins</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="38">lift</governor>
          <dependent id="41">girls</dependent>
        </dependency>
        <dependency type="case">
          <governor id="46">row</governor>
          <dependent id="42">out</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="42">out</governor>
          <dependent id="43">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="46">row</governor>
          <dependent id="44">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="46">row</governor>
          <dependent id="45">front</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="38">lift</governor>
          <dependent id="46">row</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="50">screaming</governor>
          <dependent id="48">away</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="50">screaming</governor>
          <dependent id="49">from</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="38">lift</governor>
          <dependent id="50">screaming</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="50">screaming</governor>
          <dependent id="51">fans</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="51">fans</governor>
          <dependent id="52">threatening</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="54">storm</governor>
          <dependent id="53">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="52">threatening</governor>
          <dependent id="54">storm</dependent>
        </dependency>
        <dependency type="det">
          <governor id="56">stage</governor>
          <dependent id="55">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="54">storm</governor>
          <dependent id="56">stage</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="storm" type="CAUSE_OF_DEATH" score="0.0">
          <tokens>
            <token id="54" string="storm" />
          </tokens>
        </entity>
        <entity id="2" string="hours" type="DURATION" score="0.0">
          <tokens>
            <token id="18" string="hours" />
          </tokens>
        </entity>
        <entity id="3" string="Dykins" type="MISC" score="0.0">
          <tokens>
            <token id="40" string="Dykins" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="19" has_coreference="true">
      <content>But mostly it tells of a mother and son who, while living apart, had a loving relationship.</content>
      <tokens>
        <token id="1" string="But" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="mostly" lemma="mostly" stem="mostli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="tells" lemma="tell" stem="tell" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="son" lemma="son" stem="son" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="while" lemma="while" stem="while" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="living" lemma="live" stem="live" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="apart" lemma="apart" stem="apart" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="loving" lemma="loving" stem="love" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="relationship" lemma="relationship" stem="relationship" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (CC But) (ADVP (RB mostly)) (NP (PRP it)) (VP (VBZ tells) (PP (IN of) (NP (NP (DT a) (NN mother) (CC and) (NN son)) (SBAR (WHNP (WP who)) (S (, ,) (SBAR (IN while) (S (VP (VBG living) (ADVP (RB apart))))) (, ,) (VP (VBD had) (NP (DT a) (JJ loving) (NN relationship)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="tells of a mother and son who , while living apart , had a loving relationship" type="VP">
          <tokens>
            <token id="4" string="tells" />
            <token id="5" string="of" />
            <token id="6" string="a" />
            <token id="7" string="mother" />
            <token id="8" string="and" />
            <token id="9" string="son" />
            <token id="10" string="who" />
            <token id="11" string="," />
            <token id="12" string="while" />
            <token id="13" string="living" />
            <token id="14" string="apart" />
            <token id="15" string="," />
            <token id="16" string="had" />
            <token id="17" string="a" />
            <token id="18" string="loving" />
            <token id="19" string="relationship" />
          </tokens>
        </chunking>
        <chunking id="2" string="a loving relationship" type="NP">
          <tokens>
            <token id="17" string="a" />
            <token id="18" string="loving" />
            <token id="19" string="relationship" />
          </tokens>
        </chunking>
        <chunking id="3" string="a mother and son who , while living apart , had a loving relationship" type="NP">
          <tokens>
            <token id="6" string="a" />
            <token id="7" string="mother" />
            <token id="8" string="and" />
            <token id="9" string="son" />
            <token id="10" string="who" />
            <token id="11" string="," />
            <token id="12" string="while" />
            <token id="13" string="living" />
            <token id="14" string="apart" />
            <token id="15" string="," />
            <token id="16" string="had" />
            <token id="17" string="a" />
            <token id="18" string="loving" />
            <token id="19" string="relationship" />
          </tokens>
        </chunking>
        <chunking id="4" string="while living apart" type="SBAR">
          <tokens>
            <token id="12" string="while" />
            <token id="13" string="living" />
            <token id="14" string="apart" />
          </tokens>
        </chunking>
        <chunking id="5" string="living apart" type="VP">
          <tokens>
            <token id="13" string="living" />
            <token id="14" string="apart" />
          </tokens>
        </chunking>
        <chunking id="6" string="it" type="NP">
          <tokens>
            <token id="3" string="it" />
          </tokens>
        </chunking>
        <chunking id="7" string="who , while living apart , had a loving relationship" type="SBAR">
          <tokens>
            <token id="10" string="who" />
            <token id="11" string="," />
            <token id="12" string="while" />
            <token id="13" string="living" />
            <token id="14" string="apart" />
            <token id="15" string="," />
            <token id="16" string="had" />
            <token id="17" string="a" />
            <token id="18" string="loving" />
            <token id="19" string="relationship" />
          </tokens>
        </chunking>
        <chunking id="8" string="had a loving relationship" type="VP">
          <tokens>
            <token id="16" string="had" />
            <token id="17" string="a" />
            <token id="18" string="loving" />
            <token id="19" string="relationship" />
          </tokens>
        </chunking>
        <chunking id="9" string="a mother and son" type="NP">
          <tokens>
            <token id="6" string="a" />
            <token id="7" string="mother" />
            <token id="8" string="and" />
            <token id="9" string="son" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="cc">
          <governor id="4">tells</governor>
          <dependent id="1">But</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">tells</governor>
          <dependent id="2">mostly</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">tells</governor>
          <dependent id="3">it</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">tells</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">mother</governor>
          <dependent id="5">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">mother</governor>
          <dependent id="6">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">tells</governor>
          <dependent id="7">mother</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">mother</governor>
          <dependent id="8">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">mother</governor>
          <dependent id="9">son</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="16">had</governor>
          <dependent id="10">who</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="13">living</governor>
          <dependent id="12">while</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="16">had</governor>
          <dependent id="13">living</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="13">living</governor>
          <dependent id="14">apart</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="7">mother</governor>
          <dependent id="16">had</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">relationship</governor>
          <dependent id="17">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">relationship</governor>
          <dependent id="18">loving</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="16">had</governor>
          <dependent id="19">relationship</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="20" has_coreference="true">
      <content>The book got its start after Baird watched a British television special in 1985 marking the fifth anniversary of Lennon&amp;apost;s murder.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="got" lemma="get" stem="got" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="its" lemma="its" stem="it" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="start" lemma="start" stem="start" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="after" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="8" string="watched" lemma="watch" stem="watch" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="British" lemma="british" stem="british" pos="JJ" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="false" is_refers="false" />
        <token id="11" string="television" lemma="television" stem="televis" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="special" lemma="special" stem="special" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="1985" lemma="1985" stem="1985" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="15" string="marking" lemma="mark" stem="mark" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="fifth" lemma="fifth" stem="fifth" pos="JJ" type="Word" isStopWord="false" ner="ORDINAL" is_referenced="false" is_refers="false" />
        <token id="18" string="anniversary" lemma="anniversary" stem="anniversari" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="21" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="22" string="murder" lemma="murder" stem="murder" pos="NN" type="Word" isStopWord="false" ner="CRIMINAL_CHARGE" is_referenced="false" is_refers="false" />
        <token id="23" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN book)) (VP (VBD got) (NP (PRP$ its) (NN start)) (SBAR (IN after) (S (NP (NNP Baird)) (VP (VBD watched) (NP (DT a) (JJ British) (NN television) (NN special)) (PP (IN in) (NP (CD 1985))) (S (VP (VBG marking) (NP (NP (DT the) (JJ fifth) (NN anniversary)) (PP (IN of) (NP (NP (NNP Lennon) (POS 's)) (NN murder)))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="watched a British television special in 1985 marking the fifth anniversary of Lennon 's murder" type="VP">
          <tokens>
            <token id="8" string="watched" />
            <token id="9" string="a" />
            <token id="10" string="British" />
            <token id="11" string="television" />
            <token id="12" string="special" />
            <token id="13" string="in" />
            <token id="14" string="1985" />
            <token id="15" string="marking" />
            <token id="16" string="the" />
            <token id="17" string="fifth" />
            <token id="18" string="anniversary" />
            <token id="19" string="of" />
            <token id="20" string="Lennon" />
            <token id="21" string="'s" />
            <token id="22" string="murder" />
          </tokens>
        </chunking>
        <chunking id="2" string="Lennon 's murder" type="NP">
          <tokens>
            <token id="20" string="Lennon" />
            <token id="21" string="'s" />
            <token id="22" string="murder" />
          </tokens>
        </chunking>
        <chunking id="3" string="The book" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="book" />
          </tokens>
        </chunking>
        <chunking id="4" string="its start" type="NP">
          <tokens>
            <token id="4" string="its" />
            <token id="5" string="start" />
          </tokens>
        </chunking>
        <chunking id="5" string="Lennon 's" type="NP">
          <tokens>
            <token id="20" string="Lennon" />
            <token id="21" string="'s" />
          </tokens>
        </chunking>
        <chunking id="6" string="the fifth anniversary of Lennon 's murder" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="fifth" />
            <token id="18" string="anniversary" />
            <token id="19" string="of" />
            <token id="20" string="Lennon" />
            <token id="21" string="'s" />
            <token id="22" string="murder" />
          </tokens>
        </chunking>
        <chunking id="7" string="1985" type="NP">
          <tokens>
            <token id="14" string="1985" />
          </tokens>
        </chunking>
        <chunking id="8" string="Baird" type="NP">
          <tokens>
            <token id="7" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="9" string="marking the fifth anniversary of Lennon 's murder" type="VP">
          <tokens>
            <token id="15" string="marking" />
            <token id="16" string="the" />
            <token id="17" string="fifth" />
            <token id="18" string="anniversary" />
            <token id="19" string="of" />
            <token id="20" string="Lennon" />
            <token id="21" string="'s" />
            <token id="22" string="murder" />
          </tokens>
        </chunking>
        <chunking id="10" string="a British television special" type="NP">
          <tokens>
            <token id="9" string="a" />
            <token id="10" string="British" />
            <token id="11" string="television" />
            <token id="12" string="special" />
          </tokens>
        </chunking>
        <chunking id="11" string="the fifth anniversary" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="fifth" />
            <token id="18" string="anniversary" />
          </tokens>
        </chunking>
        <chunking id="12" string="after Baird watched a British television special in 1985 marking the fifth anniversary of Lennon 's murder" type="SBAR">
          <tokens>
            <token id="6" string="after" />
            <token id="7" string="Baird" />
            <token id="8" string="watched" />
            <token id="9" string="a" />
            <token id="10" string="British" />
            <token id="11" string="television" />
            <token id="12" string="special" />
            <token id="13" string="in" />
            <token id="14" string="1985" />
            <token id="15" string="marking" />
            <token id="16" string="the" />
            <token id="17" string="fifth" />
            <token id="18" string="anniversary" />
            <token id="19" string="of" />
            <token id="20" string="Lennon" />
            <token id="21" string="'s" />
            <token id="22" string="murder" />
          </tokens>
        </chunking>
        <chunking id="13" string="got its start after Baird watched a British television special in 1985 marking the fifth anniversary of Lennon 's murder" type="VP">
          <tokens>
            <token id="3" string="got" />
            <token id="4" string="its" />
            <token id="5" string="start" />
            <token id="6" string="after" />
            <token id="7" string="Baird" />
            <token id="8" string="watched" />
            <token id="9" string="a" />
            <token id="10" string="British" />
            <token id="11" string="television" />
            <token id="12" string="special" />
            <token id="13" string="in" />
            <token id="14" string="1985" />
            <token id="15" string="marking" />
            <token id="16" string="the" />
            <token id="17" string="fifth" />
            <token id="18" string="anniversary" />
            <token id="19" string="of" />
            <token id="20" string="Lennon" />
            <token id="21" string="'s" />
            <token id="22" string="murder" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">book</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">got</governor>
          <dependent id="2">book</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">got</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="5">start</governor>
          <dependent id="4">its</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">got</governor>
          <dependent id="5">start</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="8">watched</governor>
          <dependent id="6">after</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">watched</governor>
          <dependent id="7">Baird</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="3">got</governor>
          <dependent id="8">watched</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">special</governor>
          <dependent id="9">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">special</governor>
          <dependent id="10">British</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">special</governor>
          <dependent id="11">television</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="8">watched</governor>
          <dependent id="12">special</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">1985</governor>
          <dependent id="13">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">watched</governor>
          <dependent id="14">1985</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="8">watched</governor>
          <dependent id="15">marking</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">anniversary</governor>
          <dependent id="16">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="18">anniversary</governor>
          <dependent id="17">fifth</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">marking</governor>
          <dependent id="18">anniversary</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">murder</governor>
          <dependent id="19">of</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="22">murder</governor>
          <dependent id="20">Lennon</dependent>
        </dependency>
        <dependency type="case">
          <governor id="20">Lennon</governor>
          <dependent id="21">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="18">anniversary</governor>
          <dependent id="22">murder</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="murder" type="CRIMINAL_CHARGE" score="0.0">
          <tokens>
            <token id="22" string="murder" />
          </tokens>
        </entity>
        <entity id="2" string="British" type="NATIONALITY" score="0.0">
          <tokens>
            <token id="10" string="British" />
          </tokens>
        </entity>
        <entity id="3" string="1985" type="DATE" score="0.0">
          <tokens>
            <token id="14" string="1985" />
          </tokens>
        </entity>
        <entity id="4" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="7" string="Baird" />
          </tokens>
        </entity>
        <entity id="5" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="20" string="Lennon" />
          </tokens>
        </entity>
        <entity id="6" string="fifth" type="ORDINAL" score="0.0">
          <tokens>
            <token id="17" string="fifth" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="21" has_coreference="true">
      <content>The show presented Julia ``as an unfit mother, no mother at all,&amp;apost;&amp;apost; Baird said.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="show" lemma="show" stem="show" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="presented" lemma="present" stem="present" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="Julia" lemma="Julia" stem="julia" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="5" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="unfit" lemma="unfit" stem="unfit" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="no" lemma="no" stem="no" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="18" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (DT The) (NN show)) (VP (VBD presented) (NP (NNP Julia)) (`` ``) (PP (IN as) (NP (NP (DT an) (ADJP (JJ unfit)) (NN mother)) (, ,) (NP (DT no) (NN mother)))) (ADVP (IN at) (DT all)))) (, ,) ('' '') (NP (NNP Baird)) (VP (VBD said)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="an unfit mother" type="NP">
          <tokens>
            <token id="7" string="an" />
            <token id="8" string="unfit" />
            <token id="9" string="mother" />
          </tokens>
        </chunking>
        <chunking id="2" string="Julia" type="NP">
          <tokens>
            <token id="4" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="3" string="unfit" type="ADJP">
          <tokens>
            <token id="8" string="unfit" />
          </tokens>
        </chunking>
        <chunking id="4" string="Baird" type="NP">
          <tokens>
            <token id="17" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="5" string="an unfit mother , no mother" type="NP">
          <tokens>
            <token id="7" string="an" />
            <token id="8" string="unfit" />
            <token id="9" string="mother" />
            <token id="10" string="," />
            <token id="11" string="no" />
            <token id="12" string="mother" />
          </tokens>
        </chunking>
        <chunking id="6" string="no mother" type="NP">
          <tokens>
            <token id="11" string="no" />
            <token id="12" string="mother" />
          </tokens>
        </chunking>
        <chunking id="7" string="The show" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="show" />
          </tokens>
        </chunking>
        <chunking id="8" string="presented Julia `` as an unfit mother , no mother at all" type="VP">
          <tokens>
            <token id="3" string="presented" />
            <token id="4" string="Julia" />
            <token id="5" string="``" />
            <token id="6" string="as" />
            <token id="7" string="an" />
            <token id="8" string="unfit" />
            <token id="9" string="mother" />
            <token id="10" string="," />
            <token id="11" string="no" />
            <token id="12" string="mother" />
            <token id="13" string="at" />
            <token id="14" string="all" />
          </tokens>
        </chunking>
        <chunking id="9" string="said" type="VP">
          <tokens>
            <token id="18" string="said" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">show</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">presented</governor>
          <dependent id="2">show</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="18">said</governor>
          <dependent id="3">presented</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">presented</governor>
          <dependent id="4">Julia</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">mother</governor>
          <dependent id="6">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">mother</governor>
          <dependent id="7">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">mother</governor>
          <dependent id="8">unfit</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">presented</governor>
          <dependent id="9">mother</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="12">mother</governor>
          <dependent id="11">no</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="9">mother</governor>
          <dependent id="12">mother</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">all</governor>
          <dependent id="13">at</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">presented</governor>
          <dependent id="14">all</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="18">said</governor>
          <dependent id="17">Baird</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="18">said</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Julia" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="Julia" />
          </tokens>
        </entity>
        <entity id="2" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="17" string="Baird" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="22" has_coreference="true">
      <content>Baird complained and was urged to tell her story.</content>
      <tokens>
        <token id="1" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="complained" lemma="complain" stem="complain" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="urged" lemma="urge" stem="urg" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="tell" lemma="tell" stem="tell" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="story" lemma="story" stem="stori" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Baird)) (VP (VP (VBD complained)) (CC and) (VP (VBD was) (VP (VBN urged) (S (VP (TO to) (VP (VB tell) (NP (PRP$ her) (NN story)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="urged to tell her story" type="VP">
          <tokens>
            <token id="5" string="urged" />
            <token id="6" string="to" />
            <token id="7" string="tell" />
            <token id="8" string="her" />
            <token id="9" string="story" />
          </tokens>
        </chunking>
        <chunking id="2" string="Baird" type="NP">
          <tokens>
            <token id="1" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="3" string="was urged to tell her story" type="VP">
          <tokens>
            <token id="4" string="was" />
            <token id="5" string="urged" />
            <token id="6" string="to" />
            <token id="7" string="tell" />
            <token id="8" string="her" />
            <token id="9" string="story" />
          </tokens>
        </chunking>
        <chunking id="4" string="tell her story" type="VP">
          <tokens>
            <token id="7" string="tell" />
            <token id="8" string="her" />
            <token id="9" string="story" />
          </tokens>
        </chunking>
        <chunking id="5" string="complained" type="VP">
          <tokens>
            <token id="2" string="complained" />
          </tokens>
        </chunking>
        <chunking id="6" string="complained and was urged to tell her story" type="VP">
          <tokens>
            <token id="2" string="complained" />
            <token id="3" string="and" />
            <token id="4" string="was" />
            <token id="5" string="urged" />
            <token id="6" string="to" />
            <token id="7" string="tell" />
            <token id="8" string="her" />
            <token id="9" string="story" />
          </tokens>
        </chunking>
        <chunking id="7" string="her story" type="NP">
          <tokens>
            <token id="8" string="her" />
            <token id="9" string="story" />
          </tokens>
        </chunking>
        <chunking id="8" string="to tell her story" type="VP">
          <tokens>
            <token id="6" string="to" />
            <token id="7" string="tell" />
            <token id="8" string="her" />
            <token id="9" string="story" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">complained</governor>
          <dependent id="1">Baird</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">complained</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="2">complained</governor>
          <dependent id="3">and</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="5">urged</governor>
          <dependent id="4">was</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="2">complained</governor>
          <dependent id="5">urged</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="7">tell</governor>
          <dependent id="6">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="5">urged</governor>
          <dependent id="7">tell</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="9">story</governor>
          <dependent id="8">her</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">tell</governor>
          <dependent id="9">story</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Baird" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="23" has_coreference="true">
      <content>She wrote an essay and arranged to publish it in Liverpool.</content>
      <tokens>
        <token id="1" string="She" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="wrote" lemma="write" stem="wrote" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="4" string="essay" lemma="essay" stem="essai" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="arranged" lemma="arrange" stem="arrang" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="publish" lemma="publish" stem="publish" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="Liverpool" lemma="Liverpool" stem="liverpool" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP She)) (VP (VP (VBD wrote) (NP (DT an) (NN essay))) (CC and) (VP (VBN arranged) (S (VP (TO to) (VP (VB publish) (NP (PRP it)) (PP (IN in) (NP (NNP Liverpool)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Liverpool" type="NP">
          <tokens>
            <token id="11" string="Liverpool" />
          </tokens>
        </chunking>
        <chunking id="2" string="wrote an essay" type="VP">
          <tokens>
            <token id="2" string="wrote" />
            <token id="3" string="an" />
            <token id="4" string="essay" />
          </tokens>
        </chunking>
        <chunking id="3" string="an essay" type="NP">
          <tokens>
            <token id="3" string="an" />
            <token id="4" string="essay" />
          </tokens>
        </chunking>
        <chunking id="4" string="publish it in Liverpool" type="VP">
          <tokens>
            <token id="8" string="publish" />
            <token id="9" string="it" />
            <token id="10" string="in" />
            <token id="11" string="Liverpool" />
          </tokens>
        </chunking>
        <chunking id="5" string="wrote an essay and arranged to publish it in Liverpool" type="VP">
          <tokens>
            <token id="2" string="wrote" />
            <token id="3" string="an" />
            <token id="4" string="essay" />
            <token id="5" string="and" />
            <token id="6" string="arranged" />
            <token id="7" string="to" />
            <token id="8" string="publish" />
            <token id="9" string="it" />
            <token id="10" string="in" />
            <token id="11" string="Liverpool" />
          </tokens>
        </chunking>
        <chunking id="6" string="arranged to publish it in Liverpool" type="VP">
          <tokens>
            <token id="6" string="arranged" />
            <token id="7" string="to" />
            <token id="8" string="publish" />
            <token id="9" string="it" />
            <token id="10" string="in" />
            <token id="11" string="Liverpool" />
          </tokens>
        </chunking>
        <chunking id="7" string="it" type="NP">
          <tokens>
            <token id="9" string="it" />
          </tokens>
        </chunking>
        <chunking id="8" string="to publish it in Liverpool" type="VP">
          <tokens>
            <token id="7" string="to" />
            <token id="8" string="publish" />
            <token id="9" string="it" />
            <token id="10" string="in" />
            <token id="11" string="Liverpool" />
          </tokens>
        </chunking>
        <chunking id="9" string="She" type="NP">
          <tokens>
            <token id="1" string="She" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">wrote</governor>
          <dependent id="1">She</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">wrote</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">essay</governor>
          <dependent id="3">an</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">wrote</governor>
          <dependent id="4">essay</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="2">wrote</governor>
          <dependent id="5">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="2">wrote</governor>
          <dependent id="6">arranged</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="8">publish</governor>
          <dependent id="7">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="6">arranged</governor>
          <dependent id="8">publish</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="8">publish</governor>
          <dependent id="9">it</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">Liverpool</governor>
          <dependent id="10">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">publish</governor>
          <dependent id="11">Liverpool</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Liverpool" type="LOCATION" score="0.0">
          <tokens>
            <token id="11" string="Liverpool" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="24" has_coreference="true">
      <content>Then she met Giuliano, who was in Liverpool to promote his first book, ``The Beatles: A Celebration.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="Then" lemma="then" stem="then" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="met" lemma="meet" stem="met" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="Giuliano" lemma="Giuliano" stem="giuliano" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="Liverpool" lemma="Liverpool" stem="liverpool" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="true" />
        <token id="10" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="promote" lemma="promote" stem="promot" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="12" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="first" lemma="first" stem="first" pos="JJ" type="Word" isStopWord="false" ner="ORDINAL" is_referenced="false" is_refers="true" />
        <token id="14" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="15" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="16" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="Beatles" lemma="Beatles" stem="beatl" pos="NNPS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="19" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="20" string="A" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="21" string="Celebration" lemma="celebration" stem="celebr" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="22" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (RB Then) (NP (PRP she)) (VP (VBD met) (NP (NP (NNP Giuliano)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBD was) (PP (IN in) (NP (NNP Liverpool))) (S (VP (TO to) (VP (VB promote) (NP (PRP$ his) (JJ first) (NN book)))))))) (, ,) (`` ``) (NP (DT The) (NNPS Beatles)) (: :) (NP (DT A) (NN Celebration)))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="was in Liverpool to promote his first book" type="VP">
          <tokens>
            <token id="7" string="was" />
            <token id="8" string="in" />
            <token id="9" string="Liverpool" />
            <token id="10" string="to" />
            <token id="11" string="promote" />
            <token id="12" string="his" />
            <token id="13" string="first" />
            <token id="14" string="book" />
          </tokens>
        </chunking>
        <chunking id="2" string="who was in Liverpool to promote his first book" type="SBAR">
          <tokens>
            <token id="6" string="who" />
            <token id="7" string="was" />
            <token id="8" string="in" />
            <token id="9" string="Liverpool" />
            <token id="10" string="to" />
            <token id="11" string="promote" />
            <token id="12" string="his" />
            <token id="13" string="first" />
            <token id="14" string="book" />
          </tokens>
        </chunking>
        <chunking id="3" string="Liverpool" type="NP">
          <tokens>
            <token id="9" string="Liverpool" />
          </tokens>
        </chunking>
        <chunking id="4" string="A Celebration" type="NP">
          <tokens>
            <token id="20" string="A" />
            <token id="21" string="Celebration" />
          </tokens>
        </chunking>
        <chunking id="5" string="his first book" type="NP">
          <tokens>
            <token id="12" string="his" />
            <token id="13" string="first" />
            <token id="14" string="book" />
          </tokens>
        </chunking>
        <chunking id="6" string="to promote his first book" type="VP">
          <tokens>
            <token id="10" string="to" />
            <token id="11" string="promote" />
            <token id="12" string="his" />
            <token id="13" string="first" />
            <token id="14" string="book" />
          </tokens>
        </chunking>
        <chunking id="7" string="promote his first book" type="VP">
          <tokens>
            <token id="11" string="promote" />
            <token id="12" string="his" />
            <token id="13" string="first" />
            <token id="14" string="book" />
          </tokens>
        </chunking>
        <chunking id="8" string="Giuliano" type="NP">
          <tokens>
            <token id="4" string="Giuliano" />
          </tokens>
        </chunking>
        <chunking id="9" string="Giuliano , who was in Liverpool to promote his first book , `` The Beatles : A Celebration" type="NP">
          <tokens>
            <token id="4" string="Giuliano" />
            <token id="5" string="," />
            <token id="6" string="who" />
            <token id="7" string="was" />
            <token id="8" string="in" />
            <token id="9" string="Liverpool" />
            <token id="10" string="to" />
            <token id="11" string="promote" />
            <token id="12" string="his" />
            <token id="13" string="first" />
            <token id="14" string="book" />
            <token id="15" string="," />
            <token id="16" string="``" />
            <token id="17" string="The" />
            <token id="18" string="Beatles" />
            <token id="19" string=":" />
            <token id="20" string="A" />
            <token id="21" string="Celebration" />
          </tokens>
        </chunking>
        <chunking id="10" string="met Giuliano , who was in Liverpool to promote his first book , `` The Beatles : A Celebration" type="VP">
          <tokens>
            <token id="3" string="met" />
            <token id="4" string="Giuliano" />
            <token id="5" string="," />
            <token id="6" string="who" />
            <token id="7" string="was" />
            <token id="8" string="in" />
            <token id="9" string="Liverpool" />
            <token id="10" string="to" />
            <token id="11" string="promote" />
            <token id="12" string="his" />
            <token id="13" string="first" />
            <token id="14" string="book" />
            <token id="15" string="," />
            <token id="16" string="``" />
            <token id="17" string="The" />
            <token id="18" string="Beatles" />
            <token id="19" string=":" />
            <token id="20" string="A" />
            <token id="21" string="Celebration" />
          </tokens>
        </chunking>
        <chunking id="11" string="The Beatles" type="NP">
          <tokens>
            <token id="17" string="The" />
            <token id="18" string="Beatles" />
          </tokens>
        </chunking>
        <chunking id="12" string="she" type="NP">
          <tokens>
            <token id="2" string="she" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="3">met</governor>
          <dependent id="1">Then</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">met</governor>
          <dependent id="2">she</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">met</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">met</governor>
          <dependent id="4">Giuliano</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">Liverpool</governor>
          <dependent id="6">who</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="9">Liverpool</governor>
          <dependent id="7">was</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">Liverpool</governor>
          <dependent id="8">in</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="4">Giuliano</governor>
          <dependent id="9">Liverpool</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="11">promote</governor>
          <dependent id="10">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="9">Liverpool</governor>
          <dependent id="11">promote</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="14">book</governor>
          <dependent id="12">his</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="14">book</governor>
          <dependent id="13">first</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">promote</governor>
          <dependent id="14">book</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">Beatles</governor>
          <dependent id="17">The</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="4">Giuliano</governor>
          <dependent id="18">Beatles</dependent>
        </dependency>
        <dependency type="det">
          <governor id="21">Celebration</governor>
          <dependent id="20">A</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="4">Giuliano</governor>
          <dependent id="21">Celebration</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Liverpool" type="LOCATION" score="0.0">
          <tokens>
            <token id="9" string="Liverpool" />
          </tokens>
        </entity>
        <entity id="2" string="first" type="ORDINAL" score="0.0">
          <tokens>
            <token id="13" string="first" />
          </tokens>
        </entity>
        <entity id="3" string="Giuliano" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="Giuliano" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="25" has_coreference="true">
      <content>Giuliano, 35, a self-proclaimed huckster, convinced Baird to work with him on a book, which he pieced together from interviews with Baird in England.</content>
      <tokens>
        <token id="1" string="Giuliano" lemma="Giuliano" stem="giuliano" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="35" lemma="35" stem="35" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="true" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="self-proclaimed" lemma="self-proclaimed" stem="self-proclaim" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="huckster" lemma="huckster" stem="huckster" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="convinced" lemma="convinced" stem="convinc" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="11" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="work" lemma="work" stem="work" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="him" lemma="he" stem="him" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="18" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="which" lemma="which" stem="which" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="20" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="21" string="pieced" lemma="piece" stem="piec" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="22" string="together" lemma="together" stem="togeth" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="23" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="24" string="interviews" lemma="interview" stem="interview" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="25" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="26" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="27" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="England" lemma="England" stem="england" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="true" />
        <token id="29" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Giuliano)) (, ,) (NP (NP (CD 35)) (, ,) (NP (DT a) (JJ self-proclaimed) (NN huckster)) (, ,))) (VP (JJ convinced) (S (NP (NNP Baird)) (VP (TO to) (VP (VB work) (PP (IN with) (NP (PRP him))) (PP (IN on) (NP (NP (DT a) (NN book)) (, ,) (SBAR (WHNP (WDT which)) (S (NP (PRP he)) (VP (VBD pieced) (ADVP (RB together)) (PP (IN from) (NP (NP (NNS interviews)) (PP (IN with) (NP (NNP Baird))))) (PP (IN in) (NP (NNP England)))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="35" type="NP">
          <tokens>
            <token id="3" string="35" />
          </tokens>
        </chunking>
        <chunking id="2" string="35 , a self-proclaimed huckster ," type="NP">
          <tokens>
            <token id="3" string="35" />
            <token id="4" string="," />
            <token id="5" string="a" />
            <token id="6" string="self-proclaimed" />
            <token id="7" string="huckster" />
            <token id="8" string="," />
          </tokens>
        </chunking>
        <chunking id="3" string="convinced Baird to work with him on a book , which he pieced together from interviews with Baird in England" type="VP">
          <tokens>
            <token id="9" string="convinced" />
            <token id="10" string="Baird" />
            <token id="11" string="to" />
            <token id="12" string="work" />
            <token id="13" string="with" />
            <token id="14" string="him" />
            <token id="15" string="on" />
            <token id="16" string="a" />
            <token id="17" string="book" />
            <token id="18" string="," />
            <token id="19" string="which" />
            <token id="20" string="he" />
            <token id="21" string="pieced" />
            <token id="22" string="together" />
            <token id="23" string="from" />
            <token id="24" string="interviews" />
            <token id="25" string="with" />
            <token id="26" string="Baird" />
            <token id="27" string="in" />
            <token id="28" string="England" />
          </tokens>
        </chunking>
        <chunking id="4" string="work with him on a book , which he pieced together from interviews with Baird in England" type="VP">
          <tokens>
            <token id="12" string="work" />
            <token id="13" string="with" />
            <token id="14" string="him" />
            <token id="15" string="on" />
            <token id="16" string="a" />
            <token id="17" string="book" />
            <token id="18" string="," />
            <token id="19" string="which" />
            <token id="20" string="he" />
            <token id="21" string="pieced" />
            <token id="22" string="together" />
            <token id="23" string="from" />
            <token id="24" string="interviews" />
            <token id="25" string="with" />
            <token id="26" string="Baird" />
            <token id="27" string="in" />
            <token id="28" string="England" />
          </tokens>
        </chunking>
        <chunking id="5" string="England" type="NP">
          <tokens>
            <token id="28" string="England" />
          </tokens>
        </chunking>
        <chunking id="6" string="a self-proclaimed huckster" type="NP">
          <tokens>
            <token id="5" string="a" />
            <token id="6" string="self-proclaimed" />
            <token id="7" string="huckster" />
          </tokens>
        </chunking>
        <chunking id="7" string="him" type="NP">
          <tokens>
            <token id="14" string="him" />
          </tokens>
        </chunking>
        <chunking id="8" string="pieced together from interviews with Baird in England" type="VP">
          <tokens>
            <token id="21" string="pieced" />
            <token id="22" string="together" />
            <token id="23" string="from" />
            <token id="24" string="interviews" />
            <token id="25" string="with" />
            <token id="26" string="Baird" />
            <token id="27" string="in" />
            <token id="28" string="England" />
          </tokens>
        </chunking>
        <chunking id="9" string="interviews with Baird" type="NP">
          <tokens>
            <token id="24" string="interviews" />
            <token id="25" string="with" />
            <token id="26" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="10" string="interviews" type="NP">
          <tokens>
            <token id="24" string="interviews" />
          </tokens>
        </chunking>
        <chunking id="11" string="to work with him on a book , which he pieced together from interviews with Baird in England" type="VP">
          <tokens>
            <token id="11" string="to" />
            <token id="12" string="work" />
            <token id="13" string="with" />
            <token id="14" string="him" />
            <token id="15" string="on" />
            <token id="16" string="a" />
            <token id="17" string="book" />
            <token id="18" string="," />
            <token id="19" string="which" />
            <token id="20" string="he" />
            <token id="21" string="pieced" />
            <token id="22" string="together" />
            <token id="23" string="from" />
            <token id="24" string="interviews" />
            <token id="25" string="with" />
            <token id="26" string="Baird" />
            <token id="27" string="in" />
            <token id="28" string="England" />
          </tokens>
        </chunking>
        <chunking id="12" string="a book" type="NP">
          <tokens>
            <token id="16" string="a" />
            <token id="17" string="book" />
          </tokens>
        </chunking>
        <chunking id="13" string="Giuliano , 35 , a self-proclaimed huckster ," type="NP">
          <tokens>
            <token id="1" string="Giuliano" />
            <token id="2" string="," />
            <token id="3" string="35" />
            <token id="4" string="," />
            <token id="5" string="a" />
            <token id="6" string="self-proclaimed" />
            <token id="7" string="huckster" />
            <token id="8" string="," />
          </tokens>
        </chunking>
        <chunking id="14" string="Baird" type="NP">
          <tokens>
            <token id="10" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="15" string="which he pieced together from interviews with Baird in England" type="SBAR">
          <tokens>
            <token id="19" string="which" />
            <token id="20" string="he" />
            <token id="21" string="pieced" />
            <token id="22" string="together" />
            <token id="23" string="from" />
            <token id="24" string="interviews" />
            <token id="25" string="with" />
            <token id="26" string="Baird" />
            <token id="27" string="in" />
            <token id="28" string="England" />
          </tokens>
        </chunking>
        <chunking id="16" string="a book , which he pieced together from interviews with Baird in England" type="NP">
          <tokens>
            <token id="16" string="a" />
            <token id="17" string="book" />
            <token id="18" string="," />
            <token id="19" string="which" />
            <token id="20" string="he" />
            <token id="21" string="pieced" />
            <token id="22" string="together" />
            <token id="23" string="from" />
            <token id="24" string="interviews" />
            <token id="25" string="with" />
            <token id="26" string="Baird" />
            <token id="27" string="in" />
            <token id="28" string="England" />
          </tokens>
        </chunking>
        <chunking id="17" string="Giuliano" type="NP">
          <tokens>
            <token id="1" string="Giuliano" />
          </tokens>
        </chunking>
        <chunking id="18" string="he" type="NP">
          <tokens>
            <token id="20" string="he" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="9">convinced</governor>
          <dependent id="1">Giuliano</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="1">Giuliano</governor>
          <dependent id="3">35</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">huckster</governor>
          <dependent id="5">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">huckster</governor>
          <dependent id="6">self-proclaimed</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="3">35</governor>
          <dependent id="7">huckster</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="9">convinced</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="9">convinced</governor>
          <dependent id="10">Baird</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="12">work</governor>
          <dependent id="11">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="9">convinced</governor>
          <dependent id="12">work</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">him</governor>
          <dependent id="13">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">work</governor>
          <dependent id="14">him</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">book</governor>
          <dependent id="15">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="17">book</governor>
          <dependent id="16">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">work</governor>
          <dependent id="17">book</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="21">pieced</governor>
          <dependent id="19">which</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">pieced</governor>
          <dependent id="20">he</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="17">book</governor>
          <dependent id="21">pieced</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="21">pieced</governor>
          <dependent id="22">together</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">interviews</governor>
          <dependent id="23">from</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="21">pieced</governor>
          <dependent id="24">interviews</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">Baird</governor>
          <dependent id="25">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="24">interviews</governor>
          <dependent id="26">Baird</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">England</governor>
          <dependent id="27">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="21">pieced</governor>
          <dependent id="28">England</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="35" type="NUMBER" score="0.0">
          <tokens>
            <token id="3" string="35" />
          </tokens>
        </entity>
        <entity id="2" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="Baird" />
          </tokens>
        </entity>
        <entity id="3" string="England" type="LOCATION" score="0.0">
          <tokens>
            <token id="28" string="England" />
          </tokens>
        </entity>
        <entity id="4" string="Giuliano" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Giuliano" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="26" has_coreference="true">
      <content>While proofreading the text, Baird sent a copy to McCartney.</content>
      <tokens>
        <token id="1" string="While" lemma="while" stem="while" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="proofreading" lemma="proofread" stem="proofread" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="text" lemma="text" stem="text" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="7" string="sent" lemma="send" stem="sent" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="copy" lemma="copy" stem="copi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="McCartney" lemma="McCartney" stem="mccartnei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (SBAR (IN While) (S (VP (VBG proofreading) (NP (DT the) (NN text))))) (, ,) (NP (NNP Baird)) (VP (VBD sent) (NP (DT a) (NN copy)) (PP (TO to) (NP (NNP McCartney)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="sent a copy to McCartney" type="VP">
          <tokens>
            <token id="7" string="sent" />
            <token id="8" string="a" />
            <token id="9" string="copy" />
            <token id="10" string="to" />
            <token id="11" string="McCartney" />
          </tokens>
        </chunking>
        <chunking id="2" string="While proofreading the text" type="SBAR">
          <tokens>
            <token id="1" string="While" />
            <token id="2" string="proofreading" />
            <token id="3" string="the" />
            <token id="4" string="text" />
          </tokens>
        </chunking>
        <chunking id="3" string="Baird" type="NP">
          <tokens>
            <token id="6" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="4" string="a copy" type="NP">
          <tokens>
            <token id="8" string="a" />
            <token id="9" string="copy" />
          </tokens>
        </chunking>
        <chunking id="5" string="McCartney" type="NP">
          <tokens>
            <token id="11" string="McCartney" />
          </tokens>
        </chunking>
        <chunking id="6" string="the text" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="text" />
          </tokens>
        </chunking>
        <chunking id="7" string="proofreading the text" type="VP">
          <tokens>
            <token id="2" string="proofreading" />
            <token id="3" string="the" />
            <token id="4" string="text" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="mark">
          <governor id="2">proofreading</governor>
          <dependent id="1">While</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="7">sent</governor>
          <dependent id="2">proofreading</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">text</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">proofreading</governor>
          <dependent id="4">text</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">sent</governor>
          <dependent id="6">Baird</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">sent</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">copy</governor>
          <dependent id="8">a</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">sent</governor>
          <dependent id="9">copy</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">McCartney</governor>
          <dependent id="10">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">sent</governor>
          <dependent id="11">McCartney</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Baird" />
          </tokens>
        </entity>
        <entity id="2" string="McCartney" type="PERSON" score="0.0">
          <tokens>
            <token id="11" string="McCartney" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="27" has_coreference="true">
      <content>He sent his blessing, plus a bonus: a brief foreword for the book.</content>
      <tokens>
        <token id="1" string="He" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="sent" lemma="send" stem="sent" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="blessing" lemma="blessing" stem="bless" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="plus" lemma="plus" stem="plu" pos="CC" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="bonus" lemma="bonus" stem="bonu" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="brief" lemma="brief" stem="brief" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="foreword" lemma="foreword" stem="foreword" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="16" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP He)) (VP (VBD sent) (NP (NP (PRP$ his) (NN blessing)) (, ,) (CC plus) (NP (DT a) (NN bonus)) (: :) (NP (NP (DT a) (JJ brief) (NN foreword)) (PP (IN for) (NP (DT the) (NN book)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="his blessing , plus a bonus : a brief foreword for the book" type="NP">
          <tokens>
            <token id="3" string="his" />
            <token id="4" string="blessing" />
            <token id="5" string="," />
            <token id="6" string="plus" />
            <token id="7" string="a" />
            <token id="8" string="bonus" />
            <token id="9" string=":" />
            <token id="10" string="a" />
            <token id="11" string="brief" />
            <token id="12" string="foreword" />
            <token id="13" string="for" />
            <token id="14" string="the" />
            <token id="15" string="book" />
          </tokens>
        </chunking>
        <chunking id="2" string="a brief foreword for the book" type="NP">
          <tokens>
            <token id="10" string="a" />
            <token id="11" string="brief" />
            <token id="12" string="foreword" />
            <token id="13" string="for" />
            <token id="14" string="the" />
            <token id="15" string="book" />
          </tokens>
        </chunking>
        <chunking id="3" string="his blessing" type="NP">
          <tokens>
            <token id="3" string="his" />
            <token id="4" string="blessing" />
          </tokens>
        </chunking>
        <chunking id="4" string="sent his blessing , plus a bonus : a brief foreword for the book" type="VP">
          <tokens>
            <token id="2" string="sent" />
            <token id="3" string="his" />
            <token id="4" string="blessing" />
            <token id="5" string="," />
            <token id="6" string="plus" />
            <token id="7" string="a" />
            <token id="8" string="bonus" />
            <token id="9" string=":" />
            <token id="10" string="a" />
            <token id="11" string="brief" />
            <token id="12" string="foreword" />
            <token id="13" string="for" />
            <token id="14" string="the" />
            <token id="15" string="book" />
          </tokens>
        </chunking>
        <chunking id="5" string="He" type="NP">
          <tokens>
            <token id="1" string="He" />
          </tokens>
        </chunking>
        <chunking id="6" string="the book" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="book" />
          </tokens>
        </chunking>
        <chunking id="7" string="a bonus" type="NP">
          <tokens>
            <token id="7" string="a" />
            <token id="8" string="bonus" />
          </tokens>
        </chunking>
        <chunking id="8" string="a brief foreword" type="NP">
          <tokens>
            <token id="10" string="a" />
            <token id="11" string="brief" />
            <token id="12" string="foreword" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">sent</governor>
          <dependent id="1">He</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">sent</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="4">blessing</governor>
          <dependent id="3">his</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">sent</governor>
          <dependent id="4">blessing</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">blessing</governor>
          <dependent id="6">plus</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">bonus</governor>
          <dependent id="7">a</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">blessing</governor>
          <dependent id="8">bonus</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">foreword</governor>
          <dependent id="10">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">foreword</governor>
          <dependent id="11">brief</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">blessing</governor>
          <dependent id="12">foreword</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">book</governor>
          <dependent id="13">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">book</governor>
          <dependent id="14">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">foreword</governor>
          <dependent id="15">book</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="28" has_coreference="true">
      <content>With a collection of rare early photos, the book is more a family album than a biography.</content>
      <tokens>
        <token id="1" string="With" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="collection" lemma="collection" stem="collect" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="rare" lemma="rare" stem="rare" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="early" lemma="early" stem="earli" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="photos" lemma="photo" stem="photo" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="more" lemma="more" stem="more" pos="RBR" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="family" lemma="family" stem="famili" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="album" lemma="album" stem="album" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="16" string="than" lemma="than" stem="than" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="17" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="18" string="biography" lemma="biography" stem="biographi" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="19" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (IN With) (NP (NP (DT a) (NN collection)) (PP (IN of) (NP (JJ rare) (JJ early) (NNS photos))))) (, ,) (NP (DT the) (NN book)) (VP (VBZ is) (ADVP (RBR more)) (NP (NP (DT a) (NN family) (NN album)) (PP (IN than) (NP (DT a) (NN biography))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a collection of rare early photos" type="NP">
          <tokens>
            <token id="2" string="a" />
            <token id="3" string="collection" />
            <token id="4" string="of" />
            <token id="5" string="rare" />
            <token id="6" string="early" />
            <token id="7" string="photos" />
          </tokens>
        </chunking>
        <chunking id="2" string="a collection" type="NP">
          <tokens>
            <token id="2" string="a" />
            <token id="3" string="collection" />
          </tokens>
        </chunking>
        <chunking id="3" string="is more a family album than a biography" type="VP">
          <tokens>
            <token id="11" string="is" />
            <token id="12" string="more" />
            <token id="13" string="a" />
            <token id="14" string="family" />
            <token id="15" string="album" />
            <token id="16" string="than" />
            <token id="17" string="a" />
            <token id="18" string="biography" />
          </tokens>
        </chunking>
        <chunking id="4" string="a family album" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="family" />
            <token id="15" string="album" />
          </tokens>
        </chunking>
        <chunking id="5" string="rare early photos" type="NP">
          <tokens>
            <token id="5" string="rare" />
            <token id="6" string="early" />
            <token id="7" string="photos" />
          </tokens>
        </chunking>
        <chunking id="6" string="the book" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="book" />
          </tokens>
        </chunking>
        <chunking id="7" string="a family album than a biography" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="family" />
            <token id="15" string="album" />
            <token id="16" string="than" />
            <token id="17" string="a" />
            <token id="18" string="biography" />
          </tokens>
        </chunking>
        <chunking id="8" string="a biography" type="NP">
          <tokens>
            <token id="17" string="a" />
            <token id="18" string="biography" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="case">
          <governor id="3">collection</governor>
          <dependent id="1">With</dependent>
        </dependency>
        <dependency type="det">
          <governor id="3">collection</governor>
          <dependent id="2">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">album</governor>
          <dependent id="3">collection</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">photos</governor>
          <dependent id="4">of</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">photos</governor>
          <dependent id="5">rare</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">photos</governor>
          <dependent id="6">early</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">collection</governor>
          <dependent id="7">photos</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">book</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">album</governor>
          <dependent id="10">book</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="15">album</governor>
          <dependent id="11">is</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="15">album</governor>
          <dependent id="12">more</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">album</governor>
          <dependent id="13">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">album</governor>
          <dependent id="14">family</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="15">album</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">biography</governor>
          <dependent id="16">than</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">biography</governor>
          <dependent id="17">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">album</governor>
          <dependent id="18">biography</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="29" has_coreference="true">
      <content>Giuliano&amp;apost;s text reads like oral history, as if a British schoolteacher had spread the family scrapbook across the reader&amp;apost;s knees to boast about her brother, the musician.</content>
      <tokens>
        <token id="1" string="Giuliano" lemma="Giuliano" stem="giuliano" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="2" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="text" lemma="text" stem="text" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="reads" lemma="read" stem="read" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="like" lemma="like" stem="like" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="oral" lemma="oral" stem="oral" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="history" lemma="history" stem="histori" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="if" lemma="if" stem="if" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="12" string="British" lemma="british" stem="british" pos="JJ" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="true" is_refers="false" />
        <token id="13" string="schoolteacher" lemma="schoolteacher" stem="schoolteach" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="spread" lemma="spread" stem="spread" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="family" lemma="family" stem="famili" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="scrapbook" lemma="scrapbook" stem="scrapbook" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="across" lemma="across" stem="across" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="reader" lemma="reader" stem="reader" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="knees" lemma="knee" stem="knee" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="boast" lemma="boast" stem="boast" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="28" string="brother" lemma="brother" stem="brother" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="29" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="30" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="31" string="musician" lemma="musician" stem="musician" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="32" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Giuliano) (POS 's)) (NN text)) (VP (VBZ reads) (PP (IN like) (NP (JJ oral) (NN history))) (, ,) (SBAR (IN as) (IN if) (S (NP (DT a) (JJ British) (NN schoolteacher)) (VP (VBD had) (VP (VBN spread) (NP (DT the) (NN family) (NN scrapbook)) (PP (IN across) (NP (NP (DT the) (NN reader) (POS 's)) (NNS knees) (S (VP (TO to) (VP (VB boast) (PP (IN about) (NP (NP (PRP$ her) (NN brother)) (, ,) (NP (DT the) (NN musician)))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the musician" type="NP">
          <tokens>
            <token id="30" string="the" />
            <token id="31" string="musician" />
          </tokens>
        </chunking>
        <chunking id="2" string="a British schoolteacher" type="NP">
          <tokens>
            <token id="11" string="a" />
            <token id="12" string="British" />
            <token id="13" string="schoolteacher" />
          </tokens>
        </chunking>
        <chunking id="3" string="her brother" type="NP">
          <tokens>
            <token id="27" string="her" />
            <token id="28" string="brother" />
          </tokens>
        </chunking>
        <chunking id="4" string="had spread the family scrapbook across the reader 's knees to boast about her brother , the musician" type="VP">
          <tokens>
            <token id="14" string="had" />
            <token id="15" string="spread" />
            <token id="16" string="the" />
            <token id="17" string="family" />
            <token id="18" string="scrapbook" />
            <token id="19" string="across" />
            <token id="20" string="the" />
            <token id="21" string="reader" />
            <token id="22" string="'s" />
            <token id="23" string="knees" />
            <token id="24" string="to" />
            <token id="25" string="boast" />
            <token id="26" string="about" />
            <token id="27" string="her" />
            <token id="28" string="brother" />
            <token id="29" string="," />
            <token id="30" string="the" />
            <token id="31" string="musician" />
          </tokens>
        </chunking>
        <chunking id="5" string="her brother , the musician" type="NP">
          <tokens>
            <token id="27" string="her" />
            <token id="28" string="brother" />
            <token id="29" string="," />
            <token id="30" string="the" />
            <token id="31" string="musician" />
          </tokens>
        </chunking>
        <chunking id="6" string="the reader 's knees to boast about her brother , the musician" type="NP">
          <tokens>
            <token id="20" string="the" />
            <token id="21" string="reader" />
            <token id="22" string="'s" />
            <token id="23" string="knees" />
            <token id="24" string="to" />
            <token id="25" string="boast" />
            <token id="26" string="about" />
            <token id="27" string="her" />
            <token id="28" string="brother" />
            <token id="29" string="," />
            <token id="30" string="the" />
            <token id="31" string="musician" />
          </tokens>
        </chunking>
        <chunking id="7" string="Giuliano 's" type="NP">
          <tokens>
            <token id="1" string="Giuliano" />
            <token id="2" string="'s" />
          </tokens>
        </chunking>
        <chunking id="8" string="to boast about her brother , the musician" type="VP">
          <tokens>
            <token id="24" string="to" />
            <token id="25" string="boast" />
            <token id="26" string="about" />
            <token id="27" string="her" />
            <token id="28" string="brother" />
            <token id="29" string="," />
            <token id="30" string="the" />
            <token id="31" string="musician" />
          </tokens>
        </chunking>
        <chunking id="9" string="as if a British schoolteacher had spread the family scrapbook across the reader 's knees to boast about her brother , the musician" type="SBAR">
          <tokens>
            <token id="9" string="as" />
            <token id="10" string="if" />
            <token id="11" string="a" />
            <token id="12" string="British" />
            <token id="13" string="schoolteacher" />
            <token id="14" string="had" />
            <token id="15" string="spread" />
            <token id="16" string="the" />
            <token id="17" string="family" />
            <token id="18" string="scrapbook" />
            <token id="19" string="across" />
            <token id="20" string="the" />
            <token id="21" string="reader" />
            <token id="22" string="'s" />
            <token id="23" string="knees" />
            <token id="24" string="to" />
            <token id="25" string="boast" />
            <token id="26" string="about" />
            <token id="27" string="her" />
            <token id="28" string="brother" />
            <token id="29" string="," />
            <token id="30" string="the" />
            <token id="31" string="musician" />
          </tokens>
        </chunking>
        <chunking id="10" string="Giuliano 's text" type="NP">
          <tokens>
            <token id="1" string="Giuliano" />
            <token id="2" string="'s" />
            <token id="3" string="text" />
          </tokens>
        </chunking>
        <chunking id="11" string="oral history" type="NP">
          <tokens>
            <token id="6" string="oral" />
            <token id="7" string="history" />
          </tokens>
        </chunking>
        <chunking id="12" string="the family scrapbook" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="family" />
            <token id="18" string="scrapbook" />
          </tokens>
        </chunking>
        <chunking id="13" string="boast about her brother , the musician" type="VP">
          <tokens>
            <token id="25" string="boast" />
            <token id="26" string="about" />
            <token id="27" string="her" />
            <token id="28" string="brother" />
            <token id="29" string="," />
            <token id="30" string="the" />
            <token id="31" string="musician" />
          </tokens>
        </chunking>
        <chunking id="14" string="spread the family scrapbook across the reader 's knees to boast about her brother , the musician" type="VP">
          <tokens>
            <token id="15" string="spread" />
            <token id="16" string="the" />
            <token id="17" string="family" />
            <token id="18" string="scrapbook" />
            <token id="19" string="across" />
            <token id="20" string="the" />
            <token id="21" string="reader" />
            <token id="22" string="'s" />
            <token id="23" string="knees" />
            <token id="24" string="to" />
            <token id="25" string="boast" />
            <token id="26" string="about" />
            <token id="27" string="her" />
            <token id="28" string="brother" />
            <token id="29" string="," />
            <token id="30" string="the" />
            <token id="31" string="musician" />
          </tokens>
        </chunking>
        <chunking id="15" string="the reader 's" type="NP">
          <tokens>
            <token id="20" string="the" />
            <token id="21" string="reader" />
            <token id="22" string="'s" />
          </tokens>
        </chunking>
        <chunking id="16" string="reads like oral history , as if a British schoolteacher had spread the family scrapbook across the reader 's knees to boast about her brother , the musician" type="VP">
          <tokens>
            <token id="4" string="reads" />
            <token id="5" string="like" />
            <token id="6" string="oral" />
            <token id="7" string="history" />
            <token id="8" string="," />
            <token id="9" string="as" />
            <token id="10" string="if" />
            <token id="11" string="a" />
            <token id="12" string="British" />
            <token id="13" string="schoolteacher" />
            <token id="14" string="had" />
            <token id="15" string="spread" />
            <token id="16" string="the" />
            <token id="17" string="family" />
            <token id="18" string="scrapbook" />
            <token id="19" string="across" />
            <token id="20" string="the" />
            <token id="21" string="reader" />
            <token id="22" string="'s" />
            <token id="23" string="knees" />
            <token id="24" string="to" />
            <token id="25" string="boast" />
            <token id="26" string="about" />
            <token id="27" string="her" />
            <token id="28" string="brother" />
            <token id="29" string="," />
            <token id="30" string="the" />
            <token id="31" string="musician" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="3">text</governor>
          <dependent id="1">Giuliano</dependent>
        </dependency>
        <dependency type="case">
          <governor id="1">Giuliano</governor>
          <dependent id="2">'s</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">reads</governor>
          <dependent id="3">text</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">reads</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">history</governor>
          <dependent id="5">like</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">history</governor>
          <dependent id="6">oral</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">reads</governor>
          <dependent id="7">history</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="15">spread</governor>
          <dependent id="9">as</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="9">as</governor>
          <dependent id="10">if</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">schoolteacher</governor>
          <dependent id="11">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">schoolteacher</governor>
          <dependent id="12">British</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">spread</governor>
          <dependent id="13">schoolteacher</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="15">spread</governor>
          <dependent id="14">had</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="4">reads</governor>
          <dependent id="15">spread</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">scrapbook</governor>
          <dependent id="16">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="18">scrapbook</governor>
          <dependent id="17">family</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">spread</governor>
          <dependent id="18">scrapbook</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">knees</governor>
          <dependent id="19">across</dependent>
        </dependency>
        <dependency type="det">
          <governor id="21">reader</governor>
          <dependent id="20">the</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="23">knees</governor>
          <dependent id="21">reader</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">reader</governor>
          <dependent id="22">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">spread</governor>
          <dependent id="23">knees</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="25">boast</governor>
          <dependent id="24">to</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="23">knees</governor>
          <dependent id="25">boast</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">brother</governor>
          <dependent id="26">about</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="28">brother</governor>
          <dependent id="27">her</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="25">boast</governor>
          <dependent id="28">brother</dependent>
        </dependency>
        <dependency type="det">
          <governor id="31">musician</governor>
          <dependent id="30">the</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="28">brother</governor>
          <dependent id="31">musician</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="British" type="NATIONALITY" score="0.0">
          <tokens>
            <token id="12" string="British" />
          </tokens>
        </entity>
        <entity id="2" string="Giuliano" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Giuliano" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="30" has_coreference="true">
      <content>However, the book is a narrow view of Lennon, a child&amp;apost;s recollections of an aspiring brother.</content>
      <tokens>
        <token id="1" string="However" lemma="however" stem="howev" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="narrow" lemma="narrow" stem="narrow" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="view" lemma="view" stem="view" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="9" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="child" lemma="child" stem="child" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="14" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="recollections" lemma="recollection" stem="recollect" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="16" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="aspiring" lemma="aspiring" stem="aspir" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="19" string="brother" lemma="brother" stem="brother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="20" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (ADVP (RB However)) (, ,) (NP (DT the) (NN book)) (VP (VBZ is) (NP (NP (DT a) (JJ narrow) (NN view)) (PP (IN of) (NP (NP (NNP Lennon)) (, ,) (NP (NP (NP (DT a) (NN child) (POS 's)) (NNS recollections)) (PP (IN of) (NP (DT an) (JJ aspiring) (NN brother)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a child 's recollections of an aspiring brother" type="NP">
          <tokens>
            <token id="12" string="a" />
            <token id="13" string="child" />
            <token id="14" string="'s" />
            <token id="15" string="recollections" />
            <token id="16" string="of" />
            <token id="17" string="an" />
            <token id="18" string="aspiring" />
            <token id="19" string="brother" />
          </tokens>
        </chunking>
        <chunking id="2" string="a child 's recollections" type="NP">
          <tokens>
            <token id="12" string="a" />
            <token id="13" string="child" />
            <token id="14" string="'s" />
            <token id="15" string="recollections" />
          </tokens>
        </chunking>
        <chunking id="3" string="is a narrow view of Lennon , a child 's recollections of an aspiring brother" type="VP">
          <tokens>
            <token id="5" string="is" />
            <token id="6" string="a" />
            <token id="7" string="narrow" />
            <token id="8" string="view" />
            <token id="9" string="of" />
            <token id="10" string="Lennon" />
            <token id="11" string="," />
            <token id="12" string="a" />
            <token id="13" string="child" />
            <token id="14" string="'s" />
            <token id="15" string="recollections" />
            <token id="16" string="of" />
            <token id="17" string="an" />
            <token id="18" string="aspiring" />
            <token id="19" string="brother" />
          </tokens>
        </chunking>
        <chunking id="4" string="a narrow view of Lennon , a child 's recollections of an aspiring brother" type="NP">
          <tokens>
            <token id="6" string="a" />
            <token id="7" string="narrow" />
            <token id="8" string="view" />
            <token id="9" string="of" />
            <token id="10" string="Lennon" />
            <token id="11" string="," />
            <token id="12" string="a" />
            <token id="13" string="child" />
            <token id="14" string="'s" />
            <token id="15" string="recollections" />
            <token id="16" string="of" />
            <token id="17" string="an" />
            <token id="18" string="aspiring" />
            <token id="19" string="brother" />
          </tokens>
        </chunking>
        <chunking id="5" string="Lennon" type="NP">
          <tokens>
            <token id="10" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="6" string="a child 's" type="NP">
          <tokens>
            <token id="12" string="a" />
            <token id="13" string="child" />
            <token id="14" string="'s" />
          </tokens>
        </chunking>
        <chunking id="7" string="an aspiring brother" type="NP">
          <tokens>
            <token id="17" string="an" />
            <token id="18" string="aspiring" />
            <token id="19" string="brother" />
          </tokens>
        </chunking>
        <chunking id="8" string="Lennon , a child 's recollections of an aspiring brother" type="NP">
          <tokens>
            <token id="10" string="Lennon" />
            <token id="11" string="," />
            <token id="12" string="a" />
            <token id="13" string="child" />
            <token id="14" string="'s" />
            <token id="15" string="recollections" />
            <token id="16" string="of" />
            <token id="17" string="an" />
            <token id="18" string="aspiring" />
            <token id="19" string="brother" />
          </tokens>
        </chunking>
        <chunking id="9" string="a narrow view" type="NP">
          <tokens>
            <token id="6" string="a" />
            <token id="7" string="narrow" />
            <token id="8" string="view" />
          </tokens>
        </chunking>
        <chunking id="10" string="the book" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="book" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="8">view</governor>
          <dependent id="1">However</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">book</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">view</governor>
          <dependent id="4">book</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="8">view</governor>
          <dependent id="5">is</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">view</governor>
          <dependent id="6">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">view</governor>
          <dependent id="7">narrow</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="8">view</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">Lennon</governor>
          <dependent id="9">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">view</governor>
          <dependent id="10">Lennon</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">child</governor>
          <dependent id="12">a</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="15">recollections</governor>
          <dependent id="13">child</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">child</governor>
          <dependent id="14">'s</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="10">Lennon</governor>
          <dependent id="15">recollections</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">brother</governor>
          <dependent id="16">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">brother</governor>
          <dependent id="17">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">brother</governor>
          <dependent id="18">aspiring</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">recollections</governor>
          <dependent id="19">brother</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="Lennon" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="31" has_coreference="true">
      <content>After Julia was killed in a traffic accident in 1958, Baird saw less and less of Lennon, losing contact with him for years when he and wife Yoko Ono moved to the United States.</content>
      <tokens>
        <token id="1" string="After" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="Julia" lemma="Julia" stem="julia" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="killed" lemma="kill" stem="kill" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="traffic" lemma="traffic" stem="traffic" pos="NN" type="Word" isStopWord="false" ner="CAUSE_OF_DEATH" is_referenced="false" is_refers="false" />
        <token id="8" string="accident" lemma="accident" stem="accid" pos="NN" type="Word" isStopWord="false" ner="CAUSE_OF_DEATH" is_referenced="false" is_refers="false" />
        <token id="9" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="1958" lemma="1958" stem="1958" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="13" string="saw" lemma="see" stem="saw" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="less" lemma="less" stem="less" pos="JJR" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="less" lemma="less" stem="less" pos="JJR" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="losing" lemma="lose" stem="lose" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="contact" lemma="contact" stem="contact" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="him" lemma="he" stem="him" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="24" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="26" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="wife" lemma="wife" stem="wife" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="Yoko" lemma="Yoko" stem="yoko" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="31" string="Ono" lemma="Ono" stem="ono" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="32" string="moved" lemma="move" stem="move" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="United" lemma="United" stem="unite" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="36" string="States" lemma="States" stem="state" pos="NNPS" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="37" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (SBAR (IN After) (S (NP (NNP Julia)) (VP (VBD was) (VP (VBN killed) (PP (IN in) (NP (NP (DT a) (NN traffic) (NN accident)) (PP (IN in) (NP (CD 1958))))))))) (, ,) (NP (NNP Baird)) (VP (VBD saw) (NP (NP (JJR less) (CC and) (JJR less)) (PP (IN of) (NP (NNP Lennon)))) (, ,) (S (VP (VBG losing) (NP (NN contact)) (PP (IN with) (NP (PRP him))) (PP (IN for) (NP (NP (NNS years)) (SBAR (WHADVP (WRB when)) (S (NP (NP (PRP he)) (CC and) (NP (NN wife) (NNP Yoko) (NNP Ono))) (VP (VBD moved) (PP (TO to) (NP (DT the) (NNP United) (NNPS States))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="was killed in a traffic accident in 1958" type="VP">
          <tokens>
            <token id="3" string="was" />
            <token id="4" string="killed" />
            <token id="5" string="in" />
            <token id="6" string="a" />
            <token id="7" string="traffic" />
            <token id="8" string="accident" />
            <token id="9" string="in" />
            <token id="10" string="1958" />
          </tokens>
        </chunking>
        <chunking id="2" string="less and less of Lennon" type="NP">
          <tokens>
            <token id="14" string="less" />
            <token id="15" string="and" />
            <token id="16" string="less" />
            <token id="17" string="of" />
            <token id="18" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="3" string="After Julia was killed in a traffic accident in 1958" type="SBAR">
          <tokens>
            <token id="1" string="After" />
            <token id="2" string="Julia" />
            <token id="3" string="was" />
            <token id="4" string="killed" />
            <token id="5" string="in" />
            <token id="6" string="a" />
            <token id="7" string="traffic" />
            <token id="8" string="accident" />
            <token id="9" string="in" />
            <token id="10" string="1958" />
          </tokens>
        </chunking>
        <chunking id="4" string="Lennon" type="NP">
          <tokens>
            <token id="18" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="5" string="contact" type="NP">
          <tokens>
            <token id="21" string="contact" />
          </tokens>
        </chunking>
        <chunking id="6" string="a traffic accident" type="NP">
          <tokens>
            <token id="6" string="a" />
            <token id="7" string="traffic" />
            <token id="8" string="accident" />
          </tokens>
        </chunking>
        <chunking id="7" string="him" type="NP">
          <tokens>
            <token id="23" string="him" />
          </tokens>
        </chunking>
        <chunking id="8" string="killed in a traffic accident in 1958" type="VP">
          <tokens>
            <token id="4" string="killed" />
            <token id="5" string="in" />
            <token id="6" string="a" />
            <token id="7" string="traffic" />
            <token id="8" string="accident" />
            <token id="9" string="in" />
            <token id="10" string="1958" />
          </tokens>
        </chunking>
        <chunking id="9" string="years" type="NP">
          <tokens>
            <token id="25" string="years" />
          </tokens>
        </chunking>
        <chunking id="10" string="when" type="WHADVP">
          <tokens>
            <token id="26" string="when" />
          </tokens>
        </chunking>
        <chunking id="11" string="saw less and less of Lennon , losing contact with him for years when he and wife Yoko Ono moved to the United States" type="VP">
          <tokens>
            <token id="13" string="saw" />
            <token id="14" string="less" />
            <token id="15" string="and" />
            <token id="16" string="less" />
            <token id="17" string="of" />
            <token id="18" string="Lennon" />
            <token id="19" string="," />
            <token id="20" string="losing" />
            <token id="21" string="contact" />
            <token id="22" string="with" />
            <token id="23" string="him" />
            <token id="24" string="for" />
            <token id="25" string="years" />
            <token id="26" string="when" />
            <token id="27" string="he" />
            <token id="28" string="and" />
            <token id="29" string="wife" />
            <token id="30" string="Yoko" />
            <token id="31" string="Ono" />
            <token id="32" string="moved" />
            <token id="33" string="to" />
            <token id="34" string="the" />
            <token id="35" string="United" />
            <token id="36" string="States" />
          </tokens>
        </chunking>
        <chunking id="12" string="years when he and wife Yoko Ono moved to the United States" type="NP">
          <tokens>
            <token id="25" string="years" />
            <token id="26" string="when" />
            <token id="27" string="he" />
            <token id="28" string="and" />
            <token id="29" string="wife" />
            <token id="30" string="Yoko" />
            <token id="31" string="Ono" />
            <token id="32" string="moved" />
            <token id="33" string="to" />
            <token id="34" string="the" />
            <token id="35" string="United" />
            <token id="36" string="States" />
          </tokens>
        </chunking>
        <chunking id="13" string="the United States" type="NP">
          <tokens>
            <token id="34" string="the" />
            <token id="35" string="United" />
            <token id="36" string="States" />
          </tokens>
        </chunking>
        <chunking id="14" string="Julia" type="NP">
          <tokens>
            <token id="2" string="Julia" />
          </tokens>
        </chunking>
        <chunking id="15" string="wife Yoko Ono" type="NP">
          <tokens>
            <token id="29" string="wife" />
            <token id="30" string="Yoko" />
            <token id="31" string="Ono" />
          </tokens>
        </chunking>
        <chunking id="16" string="Baird" type="NP">
          <tokens>
            <token id="12" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="17" string="a traffic accident in 1958" type="NP">
          <tokens>
            <token id="6" string="a" />
            <token id="7" string="traffic" />
            <token id="8" string="accident" />
            <token id="9" string="in" />
            <token id="10" string="1958" />
          </tokens>
        </chunking>
        <chunking id="18" string="less and less" type="NP">
          <tokens>
            <token id="14" string="less" />
            <token id="15" string="and" />
            <token id="16" string="less" />
          </tokens>
        </chunking>
        <chunking id="19" string="when he and wife Yoko Ono moved to the United States" type="SBAR">
          <tokens>
            <token id="26" string="when" />
            <token id="27" string="he" />
            <token id="28" string="and" />
            <token id="29" string="wife" />
            <token id="30" string="Yoko" />
            <token id="31" string="Ono" />
            <token id="32" string="moved" />
            <token id="33" string="to" />
            <token id="34" string="the" />
            <token id="35" string="United" />
            <token id="36" string="States" />
          </tokens>
        </chunking>
        <chunking id="20" string="moved to the United States" type="VP">
          <tokens>
            <token id="32" string="moved" />
            <token id="33" string="to" />
            <token id="34" string="the" />
            <token id="35" string="United" />
            <token id="36" string="States" />
          </tokens>
        </chunking>
        <chunking id="21" string="he and wife Yoko Ono" type="NP">
          <tokens>
            <token id="27" string="he" />
            <token id="28" string="and" />
            <token id="29" string="wife" />
            <token id="30" string="Yoko" />
            <token id="31" string="Ono" />
          </tokens>
        </chunking>
        <chunking id="22" string="he" type="NP">
          <tokens>
            <token id="27" string="he" />
          </tokens>
        </chunking>
        <chunking id="23" string="1958" type="NP">
          <tokens>
            <token id="10" string="1958" />
          </tokens>
        </chunking>
        <chunking id="24" string="losing contact with him for years when he and wife Yoko Ono moved to the United States" type="VP">
          <tokens>
            <token id="20" string="losing" />
            <token id="21" string="contact" />
            <token id="22" string="with" />
            <token id="23" string="him" />
            <token id="24" string="for" />
            <token id="25" string="years" />
            <token id="26" string="when" />
            <token id="27" string="he" />
            <token id="28" string="and" />
            <token id="29" string="wife" />
            <token id="30" string="Yoko" />
            <token id="31" string="Ono" />
            <token id="32" string="moved" />
            <token id="33" string="to" />
            <token id="34" string="the" />
            <token id="35" string="United" />
            <token id="36" string="States" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="mark">
          <governor id="4">killed</governor>
          <dependent id="1">After</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="4">killed</governor>
          <dependent id="2">Julia</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="4">killed</governor>
          <dependent id="3">was</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="13">saw</governor>
          <dependent id="4">killed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">accident</governor>
          <dependent id="5">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">accident</governor>
          <dependent id="6">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">accident</governor>
          <dependent id="7">traffic</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">killed</governor>
          <dependent id="8">accident</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">1958</governor>
          <dependent id="9">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">accident</governor>
          <dependent id="10">1958</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="13">saw</governor>
          <dependent id="12">Baird</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="13">saw</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="13">saw</governor>
          <dependent id="14">less</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="14">less</governor>
          <dependent id="15">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">less</governor>
          <dependent id="16">less</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">Lennon</governor>
          <dependent id="17">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">less</governor>
          <dependent id="18">Lennon</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="13">saw</governor>
          <dependent id="20">losing</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="20">losing</governor>
          <dependent id="21">contact</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">him</governor>
          <dependent id="22">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">losing</governor>
          <dependent id="23">him</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">years</governor>
          <dependent id="24">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">losing</governor>
          <dependent id="25">years</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="32">moved</governor>
          <dependent id="26">when</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="32">moved</governor>
          <dependent id="27">he</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="27">he</governor>
          <dependent id="28">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="31">Ono</governor>
          <dependent id="29">wife</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="31">Ono</governor>
          <dependent id="30">Yoko</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="27">he</governor>
          <dependent id="31">Ono</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="25">years</governor>
          <dependent id="32">moved</dependent>
        </dependency>
        <dependency type="case">
          <governor id="36">States</governor>
          <dependent id="33">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="36">States</governor>
          <dependent id="34">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="36">States</governor>
          <dependent id="35">United</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="32">moved</governor>
          <dependent id="36">States</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Julia" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Julia" />
          </tokens>
        </entity>
        <entity id="2" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="12" string="Baird" />
          </tokens>
        </entity>
        <entity id="3" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="18" string="Lennon" />
          </tokens>
        </entity>
        <entity id="4" string="United States" type="LOCATION" score="0.0">
          <tokens>
            <token id="35" string="United" />
            <token id="36" string="States" />
          </tokens>
        </entity>
        <entity id="5" string="traffic accident" type="CAUSE_OF_DEATH" score="0.0">
          <tokens>
            <token id="7" string="traffic" />
            <token id="8" string="accident" />
          </tokens>
        </entity>
        <entity id="6" string="Yoko Ono" type="PERSON" score="0.0">
          <tokens>
            <token id="30" string="Yoko" />
            <token id="31" string="Ono" />
          </tokens>
        </entity>
        <entity id="7" string="1958" type="DATE" score="0.0">
          <tokens>
            <token id="10" string="1958" />
          </tokens>
        </entity>
        <entity id="8" string="years" type="DURATION" score="0.0">
          <tokens>
            <token id="25" string="years" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="32" has_coreference="true">
      <content>The end of the book describes what Baird calls Lennon&amp;apost;s desire to regain his lost Liverpool roots, when she and her step-brother exchanged letters and phone calls during 1975-80.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="end" lemma="end" stem="end" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="6" string="describes" lemma="describe" stem="describ" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="what" lemma="what" stem="what" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="9" string="calls" lemma="call" stem="call" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="11" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="desire" lemma="desire" stem="desir" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="regain" lemma="regain" stem="regain" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="16" string="lost" lemma="lost" stem="lost" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="Liverpool" lemma="Liverpool" stem="liverpool" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="18" string="roots" lemma="root" stem="root" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="22" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="24" string="step-brother" lemma="step-brother" stem="step-broth" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="25" string="exchanged" lemma="exchange" stem="exchang" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="letters" lemma="letter" stem="letter" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="phone" lemma="phone" stem="phone" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="calls" lemma="call" stem="call" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="during" lemma="during" stem="dure" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="1975-80" lemma="1975-80" stem="1975-80" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="32" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT The) (NN end)) (PP (IN of) (NP (DT the) (NN book)))) (VP (VBZ describes) (SBAR (WHNP (WP what)) (S (NP (NNP Baird)) (VP (VBZ calls) (NP (NP (NNP Lennon) (POS 's)) (NN desire)) (S (VP (TO to) (VP (VB regain) (NP (PRP$ his) (JJ lost) (NNP Liverpool) (NNS roots))))) (, ,) (SBAR (WHADVP (WRB when)) (S (NP (NP (PRP she)) (CC and) (NP (PRP$ her) (NN step-brother))) (VP (VBD exchanged) (NP (NNS letters) (CC and) (NN phone) (NNS calls)) (PP (IN during) (NP (CD 1975-80)))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Lennon 's desire" type="NP">
          <tokens>
            <token id="10" string="Lennon" />
            <token id="11" string="'s" />
            <token id="12" string="desire" />
          </tokens>
        </chunking>
        <chunking id="2" string="regain his lost Liverpool roots" type="VP">
          <tokens>
            <token id="14" string="regain" />
            <token id="15" string="his" />
            <token id="16" string="lost" />
            <token id="17" string="Liverpool" />
            <token id="18" string="roots" />
          </tokens>
        </chunking>
        <chunking id="3" string="to regain his lost Liverpool roots" type="VP">
          <tokens>
            <token id="13" string="to" />
            <token id="14" string="regain" />
            <token id="15" string="his" />
            <token id="16" string="lost" />
            <token id="17" string="Liverpool" />
            <token id="18" string="roots" />
          </tokens>
        </chunking>
        <chunking id="4" string="letters and phone calls" type="NP">
          <tokens>
            <token id="26" string="letters" />
            <token id="27" string="and" />
            <token id="28" string="phone" />
            <token id="29" string="calls" />
          </tokens>
        </chunking>
        <chunking id="5" string="his lost Liverpool roots" type="NP">
          <tokens>
            <token id="15" string="his" />
            <token id="16" string="lost" />
            <token id="17" string="Liverpool" />
            <token id="18" string="roots" />
          </tokens>
        </chunking>
        <chunking id="6" string="The end" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="end" />
          </tokens>
        </chunking>
        <chunking id="7" string="Lennon 's" type="NP">
          <tokens>
            <token id="10" string="Lennon" />
            <token id="11" string="'s" />
          </tokens>
        </chunking>
        <chunking id="8" string="her step-brother" type="NP">
          <tokens>
            <token id="23" string="her" />
            <token id="24" string="step-brother" />
          </tokens>
        </chunking>
        <chunking id="9" string="1975-80" type="NP">
          <tokens>
            <token id="31" string="1975-80" />
          </tokens>
        </chunking>
        <chunking id="10" string="the book" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="book" />
          </tokens>
        </chunking>
        <chunking id="11" string="The end of the book" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="end" />
            <token id="3" string="of" />
            <token id="4" string="the" />
            <token id="5" string="book" />
          </tokens>
        </chunking>
        <chunking id="12" string="when" type="WHADVP">
          <tokens>
            <token id="20" string="when" />
          </tokens>
        </chunking>
        <chunking id="13" string="she" type="NP">
          <tokens>
            <token id="21" string="she" />
          </tokens>
        </chunking>
        <chunking id="14" string="Baird" type="NP">
          <tokens>
            <token id="8" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="15" string="what Baird calls Lennon 's desire to regain his lost Liverpool roots , when she and her step-brother exchanged letters and phone calls during 1975-80" type="SBAR">
          <tokens>
            <token id="7" string="what" />
            <token id="8" string="Baird" />
            <token id="9" string="calls" />
            <token id="10" string="Lennon" />
            <token id="11" string="'s" />
            <token id="12" string="desire" />
            <token id="13" string="to" />
            <token id="14" string="regain" />
            <token id="15" string="his" />
            <token id="16" string="lost" />
            <token id="17" string="Liverpool" />
            <token id="18" string="roots" />
            <token id="19" string="," />
            <token id="20" string="when" />
            <token id="21" string="she" />
            <token id="22" string="and" />
            <token id="23" string="her" />
            <token id="24" string="step-brother" />
            <token id="25" string="exchanged" />
            <token id="26" string="letters" />
            <token id="27" string="and" />
            <token id="28" string="phone" />
            <token id="29" string="calls" />
            <token id="30" string="during" />
            <token id="31" string="1975-80" />
          </tokens>
        </chunking>
        <chunking id="16" string="exchanged letters and phone calls during 1975-80" type="VP">
          <tokens>
            <token id="25" string="exchanged" />
            <token id="26" string="letters" />
            <token id="27" string="and" />
            <token id="28" string="phone" />
            <token id="29" string="calls" />
            <token id="30" string="during" />
            <token id="31" string="1975-80" />
          </tokens>
        </chunking>
        <chunking id="17" string="describes what Baird calls Lennon 's desire to regain his lost Liverpool roots , when she and her step-brother exchanged letters and phone calls during 1975-80" type="VP">
          <tokens>
            <token id="6" string="describes" />
            <token id="7" string="what" />
            <token id="8" string="Baird" />
            <token id="9" string="calls" />
            <token id="10" string="Lennon" />
            <token id="11" string="'s" />
            <token id="12" string="desire" />
            <token id="13" string="to" />
            <token id="14" string="regain" />
            <token id="15" string="his" />
            <token id="16" string="lost" />
            <token id="17" string="Liverpool" />
            <token id="18" string="roots" />
            <token id="19" string="," />
            <token id="20" string="when" />
            <token id="21" string="she" />
            <token id="22" string="and" />
            <token id="23" string="her" />
            <token id="24" string="step-brother" />
            <token id="25" string="exchanged" />
            <token id="26" string="letters" />
            <token id="27" string="and" />
            <token id="28" string="phone" />
            <token id="29" string="calls" />
            <token id="30" string="during" />
            <token id="31" string="1975-80" />
          </tokens>
        </chunking>
        <chunking id="18" string="when she and her step-brother exchanged letters and phone calls during 1975-80" type="SBAR">
          <tokens>
            <token id="20" string="when" />
            <token id="21" string="she" />
            <token id="22" string="and" />
            <token id="23" string="her" />
            <token id="24" string="step-brother" />
            <token id="25" string="exchanged" />
            <token id="26" string="letters" />
            <token id="27" string="and" />
            <token id="28" string="phone" />
            <token id="29" string="calls" />
            <token id="30" string="during" />
            <token id="31" string="1975-80" />
          </tokens>
        </chunking>
        <chunking id="19" string="calls Lennon 's desire to regain his lost Liverpool roots , when she and her step-brother exchanged letters and phone calls during 1975-80" type="VP">
          <tokens>
            <token id="9" string="calls" />
            <token id="10" string="Lennon" />
            <token id="11" string="'s" />
            <token id="12" string="desire" />
            <token id="13" string="to" />
            <token id="14" string="regain" />
            <token id="15" string="his" />
            <token id="16" string="lost" />
            <token id="17" string="Liverpool" />
            <token id="18" string="roots" />
            <token id="19" string="," />
            <token id="20" string="when" />
            <token id="21" string="she" />
            <token id="22" string="and" />
            <token id="23" string="her" />
            <token id="24" string="step-brother" />
            <token id="25" string="exchanged" />
            <token id="26" string="letters" />
            <token id="27" string="and" />
            <token id="28" string="phone" />
            <token id="29" string="calls" />
            <token id="30" string="during" />
            <token id="31" string="1975-80" />
          </tokens>
        </chunking>
        <chunking id="20" string="she and her step-brother" type="NP">
          <tokens>
            <token id="21" string="she" />
            <token id="22" string="and" />
            <token id="23" string="her" />
            <token id="24" string="step-brother" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">end</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">describes</governor>
          <dependent id="2">end</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">book</governor>
          <dependent id="3">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">book</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">end</governor>
          <dependent id="5">book</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">describes</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="9">calls</governor>
          <dependent id="7">what</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">calls</governor>
          <dependent id="8">Baird</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="6">describes</governor>
          <dependent id="9">calls</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="12">desire</governor>
          <dependent id="10">Lennon</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">Lennon</governor>
          <dependent id="11">'s</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="9">calls</governor>
          <dependent id="12">desire</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="14">regain</governor>
          <dependent id="13">to</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="9">calls</governor>
          <dependent id="14">regain</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="18">roots</governor>
          <dependent id="15">his</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="18">roots</governor>
          <dependent id="16">lost</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="18">roots</governor>
          <dependent id="17">Liverpool</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="14">regain</governor>
          <dependent id="18">roots</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="25">exchanged</governor>
          <dependent id="20">when</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="25">exchanged</governor>
          <dependent id="21">she</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="21">she</governor>
          <dependent id="22">and</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="24">step-brother</governor>
          <dependent id="23">her</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="21">she</governor>
          <dependent id="24">step-brother</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="9">calls</governor>
          <dependent id="25">exchanged</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="25">exchanged</governor>
          <dependent id="26">letters</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="26">letters</governor>
          <dependent id="27">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="29">calls</governor>
          <dependent id="28">phone</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="26">letters</governor>
          <dependent id="29">calls</dependent>
        </dependency>
        <dependency type="case">
          <governor id="31">1975-80</governor>
          <dependent id="30">during</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="25">exchanged</governor>
          <dependent id="31">1975-80</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Liverpool" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="17" string="Liverpool" />
          </tokens>
        </entity>
        <entity id="2" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Baird" />
          </tokens>
        </entity>
        <entity id="3" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="Lennon" />
          </tokens>
        </entity>
        <entity id="4" string="1975-80" type="NUMBER" score="0.0">
          <tokens>
            <token id="31" string="1975-80" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="33" has_coreference="true">
      <content>Baird has been accused of cashing in on her step-brother but insists she just wants to tell the family&amp;apost;s story of Lennon&amp;apost;s early years.</content>
      <tokens>
        <token id="1" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="accused" lemma="accuse" stem="accus" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="cashing" lemma="cash" stem="cash" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="step-brother" lemma="step-brother" stem="step-broth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="insists" lemma="insist" stem="insist" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="just" lemma="just" stem="just" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="wants" lemma="want" stem="want" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="tell" lemma="tell" stem="tell" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="19" string="family" lemma="family" stem="famili" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="20" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="21" string="story" lemma="story" stem="stori" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="22" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="23" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="24" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="25" string="early" lemma="early" stem="earli" pos="JJ" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="26" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="27" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Baird)) (VP (VP (VBZ has) (VP (VBN been) (VP (VBN accused) (PP (IN of) (S (VP (VBG cashing) (PP (IN in) (PP (IN on) (NP (PRP$ her) (NN step-brother)))))))))) (CC but) (VP (VBZ insists) (SBAR (S (NP (PRP she)) (ADVP (RB just)) (VP (VBZ wants) (S (VP (TO to) (VP (VB tell) (NP (NP (NP (DT the) (NN family) (POS 's)) (NN story)) (PP (IN of) (NP (NP (NNP Lennon) (POS 's)) (JJ early) (NNS years)))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="she just wants to tell the family 's story of Lennon 's early years" type="SBAR">
          <tokens>
            <token id="13" string="she" />
            <token id="14" string="just" />
            <token id="15" string="wants" />
            <token id="16" string="to" />
            <token id="17" string="tell" />
            <token id="18" string="the" />
            <token id="19" string="family" />
            <token id="20" string="'s" />
            <token id="21" string="story" />
            <token id="22" string="of" />
            <token id="23" string="Lennon" />
            <token id="24" string="'s" />
            <token id="25" string="early" />
            <token id="26" string="years" />
          </tokens>
        </chunking>
        <chunking id="2" string="tell the family 's story of Lennon 's early years" type="VP">
          <tokens>
            <token id="17" string="tell" />
            <token id="18" string="the" />
            <token id="19" string="family" />
            <token id="20" string="'s" />
            <token id="21" string="story" />
            <token id="22" string="of" />
            <token id="23" string="Lennon" />
            <token id="24" string="'s" />
            <token id="25" string="early" />
            <token id="26" string="years" />
          </tokens>
        </chunking>
        <chunking id="3" string="wants to tell the family 's story of Lennon 's early years" type="VP">
          <tokens>
            <token id="15" string="wants" />
            <token id="16" string="to" />
            <token id="17" string="tell" />
            <token id="18" string="the" />
            <token id="19" string="family" />
            <token id="20" string="'s" />
            <token id="21" string="story" />
            <token id="22" string="of" />
            <token id="23" string="Lennon" />
            <token id="24" string="'s" />
            <token id="25" string="early" />
            <token id="26" string="years" />
          </tokens>
        </chunking>
        <chunking id="4" string="accused of cashing in on her step-brother" type="VP">
          <tokens>
            <token id="4" string="accused" />
            <token id="5" string="of" />
            <token id="6" string="cashing" />
            <token id="7" string="in" />
            <token id="8" string="on" />
            <token id="9" string="her" />
            <token id="10" string="step-brother" />
          </tokens>
        </chunking>
        <chunking id="5" string="her step-brother" type="NP">
          <tokens>
            <token id="9" string="her" />
            <token id="10" string="step-brother" />
          </tokens>
        </chunking>
        <chunking id="6" string="Lennon 's" type="NP">
          <tokens>
            <token id="23" string="Lennon" />
            <token id="24" string="'s" />
          </tokens>
        </chunking>
        <chunking id="7" string="has been accused of cashing in on her step-brother but insists she just wants to tell the family 's story of Lennon 's early years" type="VP">
          <tokens>
            <token id="2" string="has" />
            <token id="3" string="been" />
            <token id="4" string="accused" />
            <token id="5" string="of" />
            <token id="6" string="cashing" />
            <token id="7" string="in" />
            <token id="8" string="on" />
            <token id="9" string="her" />
            <token id="10" string="step-brother" />
            <token id="11" string="but" />
            <token id="12" string="insists" />
            <token id="13" string="she" />
            <token id="14" string="just" />
            <token id="15" string="wants" />
            <token id="16" string="to" />
            <token id="17" string="tell" />
            <token id="18" string="the" />
            <token id="19" string="family" />
            <token id="20" string="'s" />
            <token id="21" string="story" />
            <token id="22" string="of" />
            <token id="23" string="Lennon" />
            <token id="24" string="'s" />
            <token id="25" string="early" />
            <token id="26" string="years" />
          </tokens>
        </chunking>
        <chunking id="8" string="to tell the family 's story of Lennon 's early years" type="VP">
          <tokens>
            <token id="16" string="to" />
            <token id="17" string="tell" />
            <token id="18" string="the" />
            <token id="19" string="family" />
            <token id="20" string="'s" />
            <token id="21" string="story" />
            <token id="22" string="of" />
            <token id="23" string="Lennon" />
            <token id="24" string="'s" />
            <token id="25" string="early" />
            <token id="26" string="years" />
          </tokens>
        </chunking>
        <chunking id="9" string="she" type="NP">
          <tokens>
            <token id="13" string="she" />
          </tokens>
        </chunking>
        <chunking id="10" string="Baird" type="NP">
          <tokens>
            <token id="1" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="11" string="cashing in on her step-brother" type="VP">
          <tokens>
            <token id="6" string="cashing" />
            <token id="7" string="in" />
            <token id="8" string="on" />
            <token id="9" string="her" />
            <token id="10" string="step-brother" />
          </tokens>
        </chunking>
        <chunking id="12" string="the family 's story" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="family" />
            <token id="20" string="'s" />
            <token id="21" string="story" />
          </tokens>
        </chunking>
        <chunking id="13" string="been accused of cashing in on her step-brother" type="VP">
          <tokens>
            <token id="3" string="been" />
            <token id="4" string="accused" />
            <token id="5" string="of" />
            <token id="6" string="cashing" />
            <token id="7" string="in" />
            <token id="8" string="on" />
            <token id="9" string="her" />
            <token id="10" string="step-brother" />
          </tokens>
        </chunking>
        <chunking id="14" string="has been accused of cashing in on her step-brother" type="VP">
          <tokens>
            <token id="2" string="has" />
            <token id="3" string="been" />
            <token id="4" string="accused" />
            <token id="5" string="of" />
            <token id="6" string="cashing" />
            <token id="7" string="in" />
            <token id="8" string="on" />
            <token id="9" string="her" />
            <token id="10" string="step-brother" />
          </tokens>
        </chunking>
        <chunking id="15" string="Lennon 's early years" type="NP">
          <tokens>
            <token id="23" string="Lennon" />
            <token id="24" string="'s" />
            <token id="25" string="early" />
            <token id="26" string="years" />
          </tokens>
        </chunking>
        <chunking id="16" string="insists she just wants to tell the family 's story of Lennon 's early years" type="VP">
          <tokens>
            <token id="12" string="insists" />
            <token id="13" string="she" />
            <token id="14" string="just" />
            <token id="15" string="wants" />
            <token id="16" string="to" />
            <token id="17" string="tell" />
            <token id="18" string="the" />
            <token id="19" string="family" />
            <token id="20" string="'s" />
            <token id="21" string="story" />
            <token id="22" string="of" />
            <token id="23" string="Lennon" />
            <token id="24" string="'s" />
            <token id="25" string="early" />
            <token id="26" string="years" />
          </tokens>
        </chunking>
        <chunking id="17" string="the family 's" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="family" />
            <token id="20" string="'s" />
          </tokens>
        </chunking>
        <chunking id="18" string="the family 's story of Lennon 's early years" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="family" />
            <token id="20" string="'s" />
            <token id="21" string="story" />
            <token id="22" string="of" />
            <token id="23" string="Lennon" />
            <token id="24" string="'s" />
            <token id="25" string="early" />
            <token id="26" string="years" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="4">accused</governor>
          <dependent id="1">Baird</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="4">accused</governor>
          <dependent id="2">has</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="4">accused</governor>
          <dependent id="3">been</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">accused</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="6">cashing</governor>
          <dependent id="5">of</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="4">accused</governor>
          <dependent id="6">cashing</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">step-brother</governor>
          <dependent id="7">in</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">step-brother</governor>
          <dependent id="8">on</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="10">step-brother</governor>
          <dependent id="9">her</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">cashing</governor>
          <dependent id="10">step-brother</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">accused</governor>
          <dependent id="11">but</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">accused</governor>
          <dependent id="12">insists</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">wants</governor>
          <dependent id="13">she</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="15">wants</governor>
          <dependent id="14">just</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="12">insists</governor>
          <dependent id="15">wants</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="17">tell</governor>
          <dependent id="16">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="15">wants</governor>
          <dependent id="17">tell</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">family</governor>
          <dependent id="18">the</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="21">story</governor>
          <dependent id="19">family</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">family</governor>
          <dependent id="20">'s</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="17">tell</governor>
          <dependent id="21">story</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">years</governor>
          <dependent id="22">of</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="26">years</governor>
          <dependent id="23">Lennon</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">Lennon</governor>
          <dependent id="24">'s</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="26">years</governor>
          <dependent id="25">early</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="21">story</governor>
          <dependent id="26">years</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Baird" />
          </tokens>
        </entity>
        <entity id="2" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="23" string="Lennon" />
          </tokens>
        </entity>
        <entity id="3" string="early years" type="DURATION" score="0.0">
          <tokens>
            <token id="25" string="early" />
            <token id="26" string="years" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="34" has_coreference="true">
      <content>``I could have cashed in when John was alive,&amp;apost;&amp;apost; Baird said.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="could" lemma="could" stem="could" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="have" lemma="have" stem="have" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="cashed" lemma="cash" stem="cash" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="9" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="alive" lemma="alive" stem="aliv" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="14" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (NP (PRP I)) (VP (MD could) (VP (VB have) (VP (VBN cashed) (PP (IN in) (SBAR (WHADVP (WRB when)) (S (NP (NNP John)) (VP (VBD was) (ADJP (JJ alive)))))))))) (, ,) ('' '') (NP (NNP Baird)) (VP (VBD said)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="could have cashed in when John was alive" type="VP">
          <tokens>
            <token id="3" string="could" />
            <token id="4" string="have" />
            <token id="5" string="cashed" />
            <token id="6" string="in" />
            <token id="7" string="when" />
            <token id="8" string="John" />
            <token id="9" string="was" />
            <token id="10" string="alive" />
          </tokens>
        </chunking>
        <chunking id="2" string="Baird" type="NP">
          <tokens>
            <token id="13" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="3" string="cashed in when John was alive" type="VP">
          <tokens>
            <token id="5" string="cashed" />
            <token id="6" string="in" />
            <token id="7" string="when" />
            <token id="8" string="John" />
            <token id="9" string="was" />
            <token id="10" string="alive" />
          </tokens>
        </chunking>
        <chunking id="4" string="when John was alive" type="SBAR">
          <tokens>
            <token id="7" string="when" />
            <token id="8" string="John" />
            <token id="9" string="was" />
            <token id="10" string="alive" />
          </tokens>
        </chunking>
        <chunking id="5" string="alive" type="ADJP">
          <tokens>
            <token id="10" string="alive" />
          </tokens>
        </chunking>
        <chunking id="6" string="was alive" type="VP">
          <tokens>
            <token id="9" string="was" />
            <token id="10" string="alive" />
          </tokens>
        </chunking>
        <chunking id="7" string="I" type="NP">
          <tokens>
            <token id="2" string="I" />
          </tokens>
        </chunking>
        <chunking id="8" string="John" type="NP">
          <tokens>
            <token id="8" string="John" />
          </tokens>
        </chunking>
        <chunking id="9" string="have cashed in when John was alive" type="VP">
          <tokens>
            <token id="4" string="have" />
            <token id="5" string="cashed" />
            <token id="6" string="in" />
            <token id="7" string="when" />
            <token id="8" string="John" />
            <token id="9" string="was" />
            <token id="10" string="alive" />
          </tokens>
        </chunking>
        <chunking id="10" string="said" type="VP">
          <tokens>
            <token id="14" string="said" />
          </tokens>
        </chunking>
        <chunking id="11" string="when" type="WHADVP">
          <tokens>
            <token id="7" string="when" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="5">cashed</governor>
          <dependent id="2">I</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="5">cashed</governor>
          <dependent id="3">could</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="5">cashed</governor>
          <dependent id="4">have</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="14">said</governor>
          <dependent id="5">cashed</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="10">alive</governor>
          <dependent id="6">in</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="10">alive</governor>
          <dependent id="7">when</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">alive</governor>
          <dependent id="8">John</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="10">alive</governor>
          <dependent id="9">was</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="5">cashed</governor>
          <dependent id="10">alive</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">said</governor>
          <dependent id="13">Baird</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="14">said</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="13" string="Baird" />
          </tokens>
        </entity>
        <entity id="2" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="John" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="35" has_coreference="true">
      <content>``When John died the tabloids in England were knocking on the door saying, `Write your story, name your price.&amp;apost;</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="When" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="4" string="died" lemma="die" stem="di" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="tabloids" lemma="tabloid" stem="tabloid" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="England" lemma="England" stem="england" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="9" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="knocking" lemma="knock" stem="knock" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="door" lemma="door" stem="door" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string="saying" lemma="say" stem="sai" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="`" lemma="`" stem="`" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="Write" lemma="write" stem="write" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="your" lemma="you" stem="your" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="19" string="story" lemma="story" stem="stori" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="name" lemma="name" stem="name" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="your" lemma="you" stem="your" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="price" lemma="price" stem="price" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (SBAR (WHADVP (WRB When)) (S (NP (NNP John)) (VP (VBD died) (SBAR (S (NP (NP (DT the) (NNS tabloids)) (PP (IN in) (NP (NNP England)))) (VP (VBD were) (VP (VBG knocking) (PP (IN on) (NP (DT the) (NN door))) (S (VP (VBG saying) (, ,) (`` `) (S (VP (VB Write) (NP (PRP$ your) (NN story))))))))))))) (, ,) (VP (VB name) (NP (PRP$ your) (NN price))) (. .) ('' ')))</syntactictree>
      <chunkings>
        <chunking id="1" string="were knocking on the door saying , ` Write your story" type="VP">
          <tokens>
            <token id="9" string="were" />
            <token id="10" string="knocking" />
            <token id="11" string="on" />
            <token id="12" string="the" />
            <token id="13" string="door" />
            <token id="14" string="saying" />
            <token id="15" string="," />
            <token id="16" string="`" />
            <token id="17" string="Write" />
            <token id="18" string="your" />
            <token id="19" string="story" />
          </tokens>
        </chunking>
        <chunking id="2" string="name your price" type="VP">
          <tokens>
            <token id="21" string="name" />
            <token id="22" string="your" />
            <token id="23" string="price" />
          </tokens>
        </chunking>
        <chunking id="3" string="the door" type="NP">
          <tokens>
            <token id="12" string="the" />
            <token id="13" string="door" />
          </tokens>
        </chunking>
        <chunking id="4" string="John" type="NP">
          <tokens>
            <token id="3" string="John" />
          </tokens>
        </chunking>
        <chunking id="5" string="England" type="NP">
          <tokens>
            <token id="8" string="England" />
          </tokens>
        </chunking>
        <chunking id="6" string="the tabloids in England were knocking on the door saying , ` Write your story" type="SBAR">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="tabloids" />
            <token id="7" string="in" />
            <token id="8" string="England" />
            <token id="9" string="were" />
            <token id="10" string="knocking" />
            <token id="11" string="on" />
            <token id="12" string="the" />
            <token id="13" string="door" />
            <token id="14" string="saying" />
            <token id="15" string="," />
            <token id="16" string="`" />
            <token id="17" string="Write" />
            <token id="18" string="your" />
            <token id="19" string="story" />
          </tokens>
        </chunking>
        <chunking id="7" string="Write your story" type="VP">
          <tokens>
            <token id="17" string="Write" />
            <token id="18" string="your" />
            <token id="19" string="story" />
          </tokens>
        </chunking>
        <chunking id="8" string="saying , ` Write your story" type="VP">
          <tokens>
            <token id="14" string="saying" />
            <token id="15" string="," />
            <token id="16" string="`" />
            <token id="17" string="Write" />
            <token id="18" string="your" />
            <token id="19" string="story" />
          </tokens>
        </chunking>
        <chunking id="9" string="knocking on the door saying , ` Write your story" type="VP">
          <tokens>
            <token id="10" string="knocking" />
            <token id="11" string="on" />
            <token id="12" string="the" />
            <token id="13" string="door" />
            <token id="14" string="saying" />
            <token id="15" string="," />
            <token id="16" string="`" />
            <token id="17" string="Write" />
            <token id="18" string="your" />
            <token id="19" string="story" />
          </tokens>
        </chunking>
        <chunking id="10" string="When" type="WHADVP">
          <tokens>
            <token id="2" string="When" />
          </tokens>
        </chunking>
        <chunking id="11" string="died the tabloids in England were knocking on the door saying , ` Write your story" type="VP">
          <tokens>
            <token id="4" string="died" />
            <token id="5" string="the" />
            <token id="6" string="tabloids" />
            <token id="7" string="in" />
            <token id="8" string="England" />
            <token id="9" string="were" />
            <token id="10" string="knocking" />
            <token id="11" string="on" />
            <token id="12" string="the" />
            <token id="13" string="door" />
            <token id="14" string="saying" />
            <token id="15" string="," />
            <token id="16" string="`" />
            <token id="17" string="Write" />
            <token id="18" string="your" />
            <token id="19" string="story" />
          </tokens>
        </chunking>
        <chunking id="12" string="your price" type="NP">
          <tokens>
            <token id="22" string="your" />
            <token id="23" string="price" />
          </tokens>
        </chunking>
        <chunking id="13" string="When John died the tabloids in England were knocking on the door saying , ` Write your story" type="SBAR">
          <tokens>
            <token id="2" string="When" />
            <token id="3" string="John" />
            <token id="4" string="died" />
            <token id="5" string="the" />
            <token id="6" string="tabloids" />
            <token id="7" string="in" />
            <token id="8" string="England" />
            <token id="9" string="were" />
            <token id="10" string="knocking" />
            <token id="11" string="on" />
            <token id="12" string="the" />
            <token id="13" string="door" />
            <token id="14" string="saying" />
            <token id="15" string="," />
            <token id="16" string="`" />
            <token id="17" string="Write" />
            <token id="18" string="your" />
            <token id="19" string="story" />
          </tokens>
        </chunking>
        <chunking id="14" string="the tabloids in England" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="tabloids" />
            <token id="7" string="in" />
            <token id="8" string="England" />
          </tokens>
        </chunking>
        <chunking id="15" string="the tabloids" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="tabloids" />
          </tokens>
        </chunking>
        <chunking id="16" string="your story" type="NP">
          <tokens>
            <token id="18" string="your" />
            <token id="19" string="story" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="4">died</governor>
          <dependent id="2">When</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">died</governor>
          <dependent id="3">John</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="21">name</governor>
          <dependent id="4">died</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">tabloids</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">knocking</governor>
          <dependent id="6">tabloids</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">England</governor>
          <dependent id="7">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">tabloids</governor>
          <dependent id="8">England</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="10">knocking</governor>
          <dependent id="9">were</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="4">died</governor>
          <dependent id="10">knocking</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">door</governor>
          <dependent id="11">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">door</governor>
          <dependent id="12">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">knocking</governor>
          <dependent id="13">door</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="10">knocking</governor>
          <dependent id="14">saying</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="14">saying</governor>
          <dependent id="17">Write</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="19">story</governor>
          <dependent id="18">your</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="17">Write</governor>
          <dependent id="19">story</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="21">name</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="23">price</governor>
          <dependent id="22">your</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="21">name</governor>
          <dependent id="23">price</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="3" string="John" />
          </tokens>
        </entity>
        <entity id="2" string="England" type="LOCATION" score="0.0">
          <tokens>
            <token id="8" string="England" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="36" has_coreference="true">
      <content>We quickly showed them the door.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="We" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="quickly" lemma="quickly" stem="quickli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="showed" lemma="show" stem="show" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="them" lemma="they" stem="them" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="door" lemma="door" stem="door" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP We)) (VP (ADVP (RB quickly)) (VBD showed) (NP (PRP them)) (NP (DT the) (NN door))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="the door" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="door" />
          </tokens>
        </chunking>
        <chunking id="2" string="quickly showed them the door" type="VP">
          <tokens>
            <token id="2" string="quickly" />
            <token id="3" string="showed" />
            <token id="4" string="them" />
            <token id="5" string="the" />
            <token id="6" string="door" />
          </tokens>
        </chunking>
        <chunking id="3" string="We" type="NP">
          <tokens>
            <token id="1" string="We" />
          </tokens>
        </chunking>
        <chunking id="4" string="them" type="NP">
          <tokens>
            <token id="4" string="them" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">showed</governor>
          <dependent id="1">We</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="3">showed</governor>
          <dependent id="2">quickly</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">showed</dependent>
        </dependency>
        <dependency type="iobj">
          <governor id="3">showed</governor>
          <dependent id="4">them</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">door</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">showed</governor>
          <dependent id="6">door</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="37" has_coreference="true">
      <content>Also shown the door was Albert Goldman, whose sordid biography, ``The Lives of John Lennon,&amp;apost;&amp;apost; hit bookstores about the same time as Baird&amp;apost;s book.</content>
      <tokens>
        <token id="1" string="Also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="shown" lemma="show" stem="shown" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="door" lemma="door" stem="door" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Albert" lemma="Albert" stem="albert" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="7" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="whose" lemma="whose" stem="whose" pos="WP$" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="sordid" lemma="sordid" stem="sordid" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="biography" lemma="biography" stem="biographi" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="Lives" lemma="life" stem="live" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="16" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="17" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="18" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="20" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="21" string="hit" lemma="hit" stem="hit" pos="VBD" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="22" string="bookstores" lemma="bookstore" stem="bookstor" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="23" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="24" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="25" string="same" lemma="same" stem="same" pos="JJ" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="26" string="time" lemma="time" stem="time" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="27" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="28" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="29" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="30" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="31" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (SINV (RB Also) (VP (VBN shown) (NP (DT the) (NN door))) (VP (VBD was)) (NP (NP (NNP Albert) (NNP Goldman)) (, ,) (SBAR (WP$ whose) (S (NP (NP (JJ sordid) (NN biography)) (, ,) (`` ``) (NP (NP (DT The) (NNS Lives)) (PP (IN of) (NP (NNP John) (NNP Lennon)))) (, ,) ('' '')) (VP (VBD hit) (NP (NNS bookstores)) (PP (IN about) (NP (DT the) (JJ same) (NN time))) (PP (IN as) (NP (NP (NNP Baird) (POS 's)) (NN book))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="bookstores" type="NP">
          <tokens>
            <token id="22" string="bookstores" />
          </tokens>
        </chunking>
        <chunking id="2" string="shown the door" type="VP">
          <tokens>
            <token id="2" string="shown" />
            <token id="3" string="the" />
            <token id="4" string="door" />
          </tokens>
        </chunking>
        <chunking id="3" string="Baird 's book" type="NP">
          <tokens>
            <token id="28" string="Baird" />
            <token id="29" string="'s" />
            <token id="30" string="book" />
          </tokens>
        </chunking>
        <chunking id="4" string="Baird 's" type="NP">
          <tokens>
            <token id="28" string="Baird" />
            <token id="29" string="'s" />
          </tokens>
        </chunking>
        <chunking id="5" string="whose sordid biography , `` The Lives of John Lennon , '' hit bookstores about the same time as Baird 's book" type="SBAR">
          <tokens>
            <token id="9" string="whose" />
            <token id="10" string="sordid" />
            <token id="11" string="biography" />
            <token id="12" string="," />
            <token id="13" string="``" />
            <token id="14" string="The" />
            <token id="15" string="Lives" />
            <token id="16" string="of" />
            <token id="17" string="John" />
            <token id="18" string="Lennon" />
            <token id="19" string="," />
            <token id="20" string="''" />
            <token id="21" string="hit" />
            <token id="22" string="bookstores" />
            <token id="23" string="about" />
            <token id="24" string="the" />
            <token id="25" string="same" />
            <token id="26" string="time" />
            <token id="27" string="as" />
            <token id="28" string="Baird" />
            <token id="29" string="'s" />
            <token id="30" string="book" />
          </tokens>
        </chunking>
        <chunking id="6" string="the door" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="door" />
          </tokens>
        </chunking>
        <chunking id="7" string="The Lives" type="NP">
          <tokens>
            <token id="14" string="The" />
            <token id="15" string="Lives" />
          </tokens>
        </chunking>
        <chunking id="8" string="was" type="VP">
          <tokens>
            <token id="5" string="was" />
          </tokens>
        </chunking>
        <chunking id="9" string="the same time" type="NP">
          <tokens>
            <token id="24" string="the" />
            <token id="25" string="same" />
            <token id="26" string="time" />
          </tokens>
        </chunking>
        <chunking id="10" string="sordid biography" type="NP">
          <tokens>
            <token id="10" string="sordid" />
            <token id="11" string="biography" />
          </tokens>
        </chunking>
        <chunking id="11" string="John Lennon" type="NP">
          <tokens>
            <token id="17" string="John" />
            <token id="18" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="12" string="sordid biography , `` The Lives of John Lennon , ''" type="NP">
          <tokens>
            <token id="10" string="sordid" />
            <token id="11" string="biography" />
            <token id="12" string="," />
            <token id="13" string="``" />
            <token id="14" string="The" />
            <token id="15" string="Lives" />
            <token id="16" string="of" />
            <token id="17" string="John" />
            <token id="18" string="Lennon" />
            <token id="19" string="," />
            <token id="20" string="''" />
          </tokens>
        </chunking>
        <chunking id="13" string="The Lives of John Lennon" type="NP">
          <tokens>
            <token id="14" string="The" />
            <token id="15" string="Lives" />
            <token id="16" string="of" />
            <token id="17" string="John" />
            <token id="18" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="14" string="hit bookstores about the same time as Baird 's book" type="VP">
          <tokens>
            <token id="21" string="hit" />
            <token id="22" string="bookstores" />
            <token id="23" string="about" />
            <token id="24" string="the" />
            <token id="25" string="same" />
            <token id="26" string="time" />
            <token id="27" string="as" />
            <token id="28" string="Baird" />
            <token id="29" string="'s" />
            <token id="30" string="book" />
          </tokens>
        </chunking>
        <chunking id="15" string="Albert Goldman" type="NP">
          <tokens>
            <token id="6" string="Albert" />
            <token id="7" string="Goldman" />
          </tokens>
        </chunking>
        <chunking id="16" string="Albert Goldman , whose sordid biography , `` The Lives of John Lennon , '' hit bookstores about the same time as Baird 's book" type="NP">
          <tokens>
            <token id="6" string="Albert" />
            <token id="7" string="Goldman" />
            <token id="8" string="," />
            <token id="9" string="whose" />
            <token id="10" string="sordid" />
            <token id="11" string="biography" />
            <token id="12" string="," />
            <token id="13" string="``" />
            <token id="14" string="The" />
            <token id="15" string="Lives" />
            <token id="16" string="of" />
            <token id="17" string="John" />
            <token id="18" string="Lennon" />
            <token id="19" string="," />
            <token id="20" string="''" />
            <token id="21" string="hit" />
            <token id="22" string="bookstores" />
            <token id="23" string="about" />
            <token id="24" string="the" />
            <token id="25" string="same" />
            <token id="26" string="time" />
            <token id="27" string="as" />
            <token id="28" string="Baird" />
            <token id="29" string="'s" />
            <token id="30" string="book" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="2">shown</governor>
          <dependent id="1">Also</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">shown</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">door</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">shown</governor>
          <dependent id="4">door</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="2">shown</governor>
          <dependent id="5">was</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">Goldman</governor>
          <dependent id="6">Albert</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="2">shown</governor>
          <dependent id="7">Goldman</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="21">hit</governor>
          <dependent id="9">whose</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">biography</governor>
          <dependent id="10">sordid</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">hit</governor>
          <dependent id="11">biography</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">Lives</governor>
          <dependent id="14">The</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="11">biography</governor>
          <dependent id="15">Lives</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">Lennon</governor>
          <dependent id="16">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="18">Lennon</governor>
          <dependent id="17">John</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">Lives</governor>
          <dependent id="18">Lennon</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="7">Goldman</governor>
          <dependent id="21">hit</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="21">hit</governor>
          <dependent id="22">bookstores</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">time</governor>
          <dependent id="23">about</dependent>
        </dependency>
        <dependency type="det">
          <governor id="26">time</governor>
          <dependent id="24">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="26">time</governor>
          <dependent id="25">same</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="21">hit</governor>
          <dependent id="26">time</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">book</governor>
          <dependent id="27">as</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="30">book</governor>
          <dependent id="28">Baird</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">Baird</governor>
          <dependent id="29">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="21">hit</governor>
          <dependent id="30">book</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="17" string="John" />
            <token id="18" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="28" string="Baird" />
          </tokens>
        </entity>
        <entity id="3" string="Albert Goldman" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Albert" />
            <token id="7" string="Goldman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="38" has_coreference="true">
      <content>Baird, who read Goldman&amp;apost;s scathing biography of Elvis Presley, said she turned down an interview with Goldman, who brands Lennon homosexual, anorexic and possibly a murderer.</content>
      <tokens>
        <token id="1" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="read" lemma="read" stem="read" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="6" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="scathing" lemma="scathing" stem="scath" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="biography" lemma="biography" stem="biographi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="9" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="Elvis" lemma="Elvis" stem="elvi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="11" string="Presley" lemma="Presley" stem="preslei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="turned" lemma="turn" stem="turn" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="down" lemma="down" stem="down" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="interview" lemma="interview" stem="interview" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="21" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="22" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="23" string="brands" lemma="brand" stem="brand" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="24" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="25" string="homosexual" lemma="homosexual" stem="homosexu" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="26" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="27" string="anorexic" lemma="anorexic" stem="anorex" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="28" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="29" string="possibly" lemma="possibly" stem="possibli" pos="RB" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="30" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="31" string="murderer" lemma="murderer" stem="murder" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="32" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Baird)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBD read) (NP (NP (NP (NNP Goldman) (POS 's)) (JJ scathing) (NN biography)) (PP (IN of) (NP (NNP Elvis) (NNP Presley))))))) (, ,)) (VP (VBD said) (SBAR (S (NP (PRP she)) (VP (VBD turned) (PRT (RP down)) (NP (DT an) (NN interview)) (PP (IN with) (NP (NP (NNP Goldman)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBZ brands) (NP (NP (NNP Lennon) (NN homosexual)) (, ,) (NP (JJ anorexic)) (CC and) (NP (RB possibly) (DT a) (NN murderer)))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="who read Goldman 's scathing biography of Elvis Presley" type="SBAR">
          <tokens>
            <token id="3" string="who" />
            <token id="4" string="read" />
            <token id="5" string="Goldman" />
            <token id="6" string="'s" />
            <token id="7" string="scathing" />
            <token id="8" string="biography" />
            <token id="9" string="of" />
            <token id="10" string="Elvis" />
            <token id="11" string="Presley" />
          </tokens>
        </chunking>
        <chunking id="2" string="turned down an interview with Goldman , who brands Lennon homosexual , anorexic and possibly a murderer" type="VP">
          <tokens>
            <token id="15" string="turned" />
            <token id="16" string="down" />
            <token id="17" string="an" />
            <token id="18" string="interview" />
            <token id="19" string="with" />
            <token id="20" string="Goldman" />
            <token id="21" string="," />
            <token id="22" string="who" />
            <token id="23" string="brands" />
            <token id="24" string="Lennon" />
            <token id="25" string="homosexual" />
            <token id="26" string="," />
            <token id="27" string="anorexic" />
            <token id="28" string="and" />
            <token id="29" string="possibly" />
            <token id="30" string="a" />
            <token id="31" string="murderer" />
          </tokens>
        </chunking>
        <chunking id="3" string="possibly a murderer" type="NP">
          <tokens>
            <token id="29" string="possibly" />
            <token id="30" string="a" />
            <token id="31" string="murderer" />
          </tokens>
        </chunking>
        <chunking id="4" string="anorexic" type="NP">
          <tokens>
            <token id="27" string="anorexic" />
          </tokens>
        </chunking>
        <chunking id="5" string="Baird , who read Goldman 's scathing biography of Elvis Presley ," type="NP">
          <tokens>
            <token id="1" string="Baird" />
            <token id="2" string="," />
            <token id="3" string="who" />
            <token id="4" string="read" />
            <token id="5" string="Goldman" />
            <token id="6" string="'s" />
            <token id="7" string="scathing" />
            <token id="8" string="biography" />
            <token id="9" string="of" />
            <token id="10" string="Elvis" />
            <token id="11" string="Presley" />
            <token id="12" string="," />
          </tokens>
        </chunking>
        <chunking id="6" string="Elvis Presley" type="NP">
          <tokens>
            <token id="10" string="Elvis" />
            <token id="11" string="Presley" />
          </tokens>
        </chunking>
        <chunking id="7" string="read Goldman 's scathing biography of Elvis Presley" type="VP">
          <tokens>
            <token id="4" string="read" />
            <token id="5" string="Goldman" />
            <token id="6" string="'s" />
            <token id="7" string="scathing" />
            <token id="8" string="biography" />
            <token id="9" string="of" />
            <token id="10" string="Elvis" />
            <token id="11" string="Presley" />
          </tokens>
        </chunking>
        <chunking id="8" string="brands Lennon homosexual , anorexic and possibly a murderer" type="VP">
          <tokens>
            <token id="23" string="brands" />
            <token id="24" string="Lennon" />
            <token id="25" string="homosexual" />
            <token id="26" string="," />
            <token id="27" string="anorexic" />
            <token id="28" string="and" />
            <token id="29" string="possibly" />
            <token id="30" string="a" />
            <token id="31" string="murderer" />
          </tokens>
        </chunking>
        <chunking id="9" string="she" type="NP">
          <tokens>
            <token id="14" string="she" />
          </tokens>
        </chunking>
        <chunking id="10" string="Goldman , who brands Lennon homosexual , anorexic and possibly a murderer" type="NP">
          <tokens>
            <token id="20" string="Goldman" />
            <token id="21" string="," />
            <token id="22" string="who" />
            <token id="23" string="brands" />
            <token id="24" string="Lennon" />
            <token id="25" string="homosexual" />
            <token id="26" string="," />
            <token id="27" string="anorexic" />
            <token id="28" string="and" />
            <token id="29" string="possibly" />
            <token id="30" string="a" />
            <token id="31" string="murderer" />
          </tokens>
        </chunking>
        <chunking id="11" string="Lennon homosexual" type="NP">
          <tokens>
            <token id="24" string="Lennon" />
            <token id="25" string="homosexual" />
          </tokens>
        </chunking>
        <chunking id="12" string="an interview" type="NP">
          <tokens>
            <token id="17" string="an" />
            <token id="18" string="interview" />
          </tokens>
        </chunking>
        <chunking id="13" string="Goldman 's scathing biography of Elvis Presley" type="NP">
          <tokens>
            <token id="5" string="Goldman" />
            <token id="6" string="'s" />
            <token id="7" string="scathing" />
            <token id="8" string="biography" />
            <token id="9" string="of" />
            <token id="10" string="Elvis" />
            <token id="11" string="Presley" />
          </tokens>
        </chunking>
        <chunking id="14" string="Goldman 's" type="NP">
          <tokens>
            <token id="5" string="Goldman" />
            <token id="6" string="'s" />
          </tokens>
        </chunking>
        <chunking id="15" string="Baird" type="NP">
          <tokens>
            <token id="1" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="16" string="she turned down an interview with Goldman , who brands Lennon homosexual , anorexic and possibly a murderer" type="SBAR">
          <tokens>
            <token id="14" string="she" />
            <token id="15" string="turned" />
            <token id="16" string="down" />
            <token id="17" string="an" />
            <token id="18" string="interview" />
            <token id="19" string="with" />
            <token id="20" string="Goldman" />
            <token id="21" string="," />
            <token id="22" string="who" />
            <token id="23" string="brands" />
            <token id="24" string="Lennon" />
            <token id="25" string="homosexual" />
            <token id="26" string="," />
            <token id="27" string="anorexic" />
            <token id="28" string="and" />
            <token id="29" string="possibly" />
            <token id="30" string="a" />
            <token id="31" string="murderer" />
          </tokens>
        </chunking>
        <chunking id="17" string="said she turned down an interview with Goldman , who brands Lennon homosexual , anorexic and possibly a murderer" type="VP">
          <tokens>
            <token id="13" string="said" />
            <token id="14" string="she" />
            <token id="15" string="turned" />
            <token id="16" string="down" />
            <token id="17" string="an" />
            <token id="18" string="interview" />
            <token id="19" string="with" />
            <token id="20" string="Goldman" />
            <token id="21" string="," />
            <token id="22" string="who" />
            <token id="23" string="brands" />
            <token id="24" string="Lennon" />
            <token id="25" string="homosexual" />
            <token id="26" string="," />
            <token id="27" string="anorexic" />
            <token id="28" string="and" />
            <token id="29" string="possibly" />
            <token id="30" string="a" />
            <token id="31" string="murderer" />
          </tokens>
        </chunking>
        <chunking id="18" string="Goldman" type="NP">
          <tokens>
            <token id="20" string="Goldman" />
          </tokens>
        </chunking>
        <chunking id="19" string="Lennon homosexual , anorexic and possibly a murderer" type="NP">
          <tokens>
            <token id="24" string="Lennon" />
            <token id="25" string="homosexual" />
            <token id="26" string="," />
            <token id="27" string="anorexic" />
            <token id="28" string="and" />
            <token id="29" string="possibly" />
            <token id="30" string="a" />
            <token id="31" string="murderer" />
          </tokens>
        </chunking>
        <chunking id="20" string="Goldman 's scathing biography" type="NP">
          <tokens>
            <token id="5" string="Goldman" />
            <token id="6" string="'s" />
            <token id="7" string="scathing" />
            <token id="8" string="biography" />
          </tokens>
        </chunking>
        <chunking id="21" string="who brands Lennon homosexual , anorexic and possibly a murderer" type="SBAR">
          <tokens>
            <token id="22" string="who" />
            <token id="23" string="brands" />
            <token id="24" string="Lennon" />
            <token id="25" string="homosexual" />
            <token id="26" string="," />
            <token id="27" string="anorexic" />
            <token id="28" string="and" />
            <token id="29" string="possibly" />
            <token id="30" string="a" />
            <token id="31" string="murderer" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="13">said</governor>
          <dependent id="1">Baird</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">read</governor>
          <dependent id="3">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="1">Baird</governor>
          <dependent id="4">read</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="8">biography</governor>
          <dependent id="5">Goldman</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">Goldman</governor>
          <dependent id="6">'s</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">biography</governor>
          <dependent id="7">scathing</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">read</governor>
          <dependent id="8">biography</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">Presley</governor>
          <dependent id="9">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">Presley</governor>
          <dependent id="10">Elvis</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">biography</governor>
          <dependent id="11">Presley</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="13">said</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">turned</governor>
          <dependent id="14">she</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="13">said</governor>
          <dependent id="15">turned</dependent>
        </dependency>
        <dependency type="compound:prt">
          <governor id="15">turned</governor>
          <dependent id="16">down</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">interview</governor>
          <dependent id="17">an</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">turned</governor>
          <dependent id="18">interview</dependent>
        </dependency>
        <dependency type="case">
          <governor id="20">Goldman</governor>
          <dependent id="19">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">turned</governor>
          <dependent id="20">Goldman</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="23">brands</governor>
          <dependent id="22">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="20">Goldman</governor>
          <dependent id="23">brands</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">homosexual</governor>
          <dependent id="24">Lennon</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="23">brands</governor>
          <dependent id="25">homosexual</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="25">homosexual</governor>
          <dependent id="27">anorexic</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="25">homosexual</governor>
          <dependent id="28">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="31">murderer</governor>
          <dependent id="29">possibly</dependent>
        </dependency>
        <dependency type="det">
          <governor id="31">murderer</governor>
          <dependent id="30">a</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="25">homosexual</governor>
          <dependent id="31">murderer</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Baird" />
          </tokens>
        </entity>
        <entity id="2" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="24" string="Lennon" />
          </tokens>
        </entity>
        <entity id="3" string="Elvis Presley" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="Elvis" />
            <token id="11" string="Presley" />
          </tokens>
        </entity>
        <entity id="4" string="Goldman" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="5" string="Goldman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="39" has_coreference="true">
      <content>``I had heard Goldman was doing the book six years ago, and I knew what the tone would be,&amp;apost;&amp;apost; Baird said.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="heard" lemma="hear" stem="heard" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="6" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="doing" lemma="do" stem="do" pos="VBG" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="10" string="six" lemma="six" stem="six" pos="CD" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="11" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="12" string="ago" lemma="ago" stem="ago" pos="RB" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="16" string="knew" lemma="know" stem="knew" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="what" lemma="what" stem="what" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="tone" lemma="tone" stem="tone" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="would" lemma="would" stem="would" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="be" lemma="be" stem="be" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="25" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (S (NP (PRP I)) (VP (VBD had) (VP (VBN heard) (SBAR (S (NP (NNP Goldman)) (VP (VBD was) (VP (VBG doing) (NP (DT the) (NN book)) (ADVP (NP (CD six) (NNS years)) (RB ago))))))))) (, ,) (CC and) (S (NP (PRP I)) (VP (VBD knew) (SBAR (WHNP (WP what)) (S (NP (DT the) (NN tone)) (VP (MD would) (VP (VB be)))))))) (, ,) ('' '') (NP (NNP Baird)) (VP (VBD said)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="be" type="VP">
          <tokens>
            <token id="21" string="be" />
          </tokens>
        </chunking>
        <chunking id="2" string="the tone" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="tone" />
          </tokens>
        </chunking>
        <chunking id="3" string="I" type="NP">
          <tokens>
            <token id="2" string="I" />
          </tokens>
        </chunking>
        <chunking id="4" string="doing the book six years ago" type="VP">
          <tokens>
            <token id="7" string="doing" />
            <token id="8" string="the" />
            <token id="9" string="book" />
            <token id="10" string="six" />
            <token id="11" string="years" />
            <token id="12" string="ago" />
          </tokens>
        </chunking>
        <chunking id="5" string="would be" type="VP">
          <tokens>
            <token id="20" string="would" />
            <token id="21" string="be" />
          </tokens>
        </chunking>
        <chunking id="6" string="the book" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="book" />
          </tokens>
        </chunking>
        <chunking id="7" string="heard Goldman was doing the book six years ago" type="VP">
          <tokens>
            <token id="4" string="heard" />
            <token id="5" string="Goldman" />
            <token id="6" string="was" />
            <token id="7" string="doing" />
            <token id="8" string="the" />
            <token id="9" string="book" />
            <token id="10" string="six" />
            <token id="11" string="years" />
            <token id="12" string="ago" />
          </tokens>
        </chunking>
        <chunking id="8" string="was doing the book six years ago" type="VP">
          <tokens>
            <token id="6" string="was" />
            <token id="7" string="doing" />
            <token id="8" string="the" />
            <token id="9" string="book" />
            <token id="10" string="six" />
            <token id="11" string="years" />
            <token id="12" string="ago" />
          </tokens>
        </chunking>
        <chunking id="9" string="six years" type="NP">
          <tokens>
            <token id="10" string="six" />
            <token id="11" string="years" />
          </tokens>
        </chunking>
        <chunking id="10" string="Baird" type="NP">
          <tokens>
            <token id="24" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="11" string="knew what the tone would be" type="VP">
          <tokens>
            <token id="16" string="knew" />
            <token id="17" string="what" />
            <token id="18" string="the" />
            <token id="19" string="tone" />
            <token id="20" string="would" />
            <token id="21" string="be" />
          </tokens>
        </chunking>
        <chunking id="12" string="Goldman was doing the book six years ago" type="SBAR">
          <tokens>
            <token id="5" string="Goldman" />
            <token id="6" string="was" />
            <token id="7" string="doing" />
            <token id="8" string="the" />
            <token id="9" string="book" />
            <token id="10" string="six" />
            <token id="11" string="years" />
            <token id="12" string="ago" />
          </tokens>
        </chunking>
        <chunking id="13" string="Goldman" type="NP">
          <tokens>
            <token id="5" string="Goldman" />
          </tokens>
        </chunking>
        <chunking id="14" string="had heard Goldman was doing the book six years ago" type="VP">
          <tokens>
            <token id="3" string="had" />
            <token id="4" string="heard" />
            <token id="5" string="Goldman" />
            <token id="6" string="was" />
            <token id="7" string="doing" />
            <token id="8" string="the" />
            <token id="9" string="book" />
            <token id="10" string="six" />
            <token id="11" string="years" />
            <token id="12" string="ago" />
          </tokens>
        </chunking>
        <chunking id="15" string="what the tone would be" type="SBAR">
          <tokens>
            <token id="17" string="what" />
            <token id="18" string="the" />
            <token id="19" string="tone" />
            <token id="20" string="would" />
            <token id="21" string="be" />
          </tokens>
        </chunking>
        <chunking id="16" string="said" type="VP">
          <tokens>
            <token id="25" string="said" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">heard</governor>
          <dependent id="2">I</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="4">heard</governor>
          <dependent id="3">had</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="25">said</governor>
          <dependent id="4">heard</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">doing</governor>
          <dependent id="5">Goldman</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="7">doing</governor>
          <dependent id="6">was</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="4">heard</governor>
          <dependent id="7">doing</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">book</governor>
          <dependent id="8">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">doing</governor>
          <dependent id="9">book</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="11">years</governor>
          <dependent id="10">six</dependent>
        </dependency>
        <dependency type="nmod:npmod">
          <governor id="12">ago</governor>
          <dependent id="11">years</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="7">doing</governor>
          <dependent id="12">ago</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">heard</governor>
          <dependent id="14">and</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">knew</governor>
          <dependent id="15">I</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">heard</governor>
          <dependent id="16">knew</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="21">be</governor>
          <dependent id="17">what</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">tone</governor>
          <dependent id="18">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">be</governor>
          <dependent id="19">tone</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="21">be</governor>
          <dependent id="20">would</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="16">knew</governor>
          <dependent id="21">be</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="25">said</governor>
          <dependent id="24">Baird</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="25">said</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="six years ago" type="DATE" score="0.0">
          <tokens>
            <token id="10" string="six" />
            <token id="11" string="years" />
            <token id="12" string="ago" />
          </tokens>
        </entity>
        <entity id="2" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="24" string="Baird" />
          </tokens>
        </entity>
        <entity id="3" string="Goldman" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="Goldman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="40" has_coreference="true">
      <content>``Most people, including Goldman, write books based on other books that have it all wrong.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="Most" lemma="most" stem="most" pos="JJS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="people" lemma="people" stem="peopl" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="including" lemma="include" stem="includ" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="write" lemma="write" stem="write" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="books" lemma="book" stem="book" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="based" lemma="base" stem="base" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="other" lemma="other" stem="other" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="books" lemma="book" stem="book" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="have" lemma="have" stem="have" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="wrong" lemma="wrong" stem="wrong" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (NP (NP (JJS Most) (NNS people)) (, ,) (PP (VBG including) (NP (NNP Goldman))) (, ,)) (VP (VB write) (NP (NNS books)) (PP (VBN based) (PP (IN on) (NP (NP (JJ other) (NNS books)) (SBAR (WHNP (WDT that)) (S (VP (VBP have) (S (NP (PRP it)) (ADJP (DT all) (JJ wrong))))))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="write books based on other books that have it all wrong" type="VP">
          <tokens>
            <token id="8" string="write" />
            <token id="9" string="books" />
            <token id="10" string="based" />
            <token id="11" string="on" />
            <token id="12" string="other" />
            <token id="13" string="books" />
            <token id="14" string="that" />
            <token id="15" string="have" />
            <token id="16" string="it" />
            <token id="17" string="all" />
            <token id="18" string="wrong" />
          </tokens>
        </chunking>
        <chunking id="2" string="books" type="NP">
          <tokens>
            <token id="9" string="books" />
          </tokens>
        </chunking>
        <chunking id="3" string="have it all wrong" type="VP">
          <tokens>
            <token id="15" string="have" />
            <token id="16" string="it" />
            <token id="17" string="all" />
            <token id="18" string="wrong" />
          </tokens>
        </chunking>
        <chunking id="4" string="that have it all wrong" type="SBAR">
          <tokens>
            <token id="14" string="that" />
            <token id="15" string="have" />
            <token id="16" string="it" />
            <token id="17" string="all" />
            <token id="18" string="wrong" />
          </tokens>
        </chunking>
        <chunking id="5" string="other books" type="NP">
          <tokens>
            <token id="12" string="other" />
            <token id="13" string="books" />
          </tokens>
        </chunking>
        <chunking id="6" string="all wrong" type="ADJP">
          <tokens>
            <token id="17" string="all" />
            <token id="18" string="wrong" />
          </tokens>
        </chunking>
        <chunking id="7" string="Most people , including Goldman ," type="NP">
          <tokens>
            <token id="2" string="Most" />
            <token id="3" string="people" />
            <token id="4" string="," />
            <token id="5" string="including" />
            <token id="6" string="Goldman" />
            <token id="7" string="," />
          </tokens>
        </chunking>
        <chunking id="8" string="Most people" type="NP">
          <tokens>
            <token id="2" string="Most" />
            <token id="3" string="people" />
          </tokens>
        </chunking>
        <chunking id="9" string="other books that have it all wrong" type="NP">
          <tokens>
            <token id="12" string="other" />
            <token id="13" string="books" />
            <token id="14" string="that" />
            <token id="15" string="have" />
            <token id="16" string="it" />
            <token id="17" string="all" />
            <token id="18" string="wrong" />
          </tokens>
        </chunking>
        <chunking id="10" string="it" type="NP">
          <tokens>
            <token id="16" string="it" />
          </tokens>
        </chunking>
        <chunking id="11" string="Goldman" type="NP">
          <tokens>
            <token id="6" string="Goldman" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="amod">
          <governor id="3">people</governor>
          <dependent id="2">Most</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">write</governor>
          <dependent id="3">people</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">Goldman</governor>
          <dependent id="5">including</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">people</governor>
          <dependent id="6">Goldman</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="8">write</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="8">write</governor>
          <dependent id="9">books</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">books</governor>
          <dependent id="10">based</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="10">based</governor>
          <dependent id="11">on</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">books</governor>
          <dependent id="12">other</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="8">write</governor>
          <dependent id="13">books</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">have</governor>
          <dependent id="14">that</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="13">books</governor>
          <dependent id="15">have</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="18">wrong</governor>
          <dependent id="16">it</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">wrong</governor>
          <dependent id="17">all</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="15">have</governor>
          <dependent id="18">wrong</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Goldman" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="6" string="Goldman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="41" has_coreference="true">
      <content>Giuliano was harsher.</content>
      <tokens>
        <token id="1" string="Giuliano" lemma="Giuliano" stem="giuliano" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="harsher" lemma="harsher" stem="harsher" pos="JJR" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Giuliano)) (VP (VBD was) (ADJP (JJR harsher))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="harsher" type="ADJP">
          <tokens>
            <token id="3" string="harsher" />
          </tokens>
        </chunking>
        <chunking id="2" string="was harsher" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="harsher" />
          </tokens>
        </chunking>
        <chunking id="3" string="Giuliano" type="NP">
          <tokens>
            <token id="1" string="Giuliano" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">harsher</governor>
          <dependent id="1">Giuliano</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="3">harsher</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">harsher</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Giuliano" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Giuliano" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="42" has_coreference="true">
      <content>``Goldman was in it for the money from day one,&amp;apost;&amp;apost; Giuliano said.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="money" lemma="money" stem="monei" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="day" lemma="day" stem="dai" pos="NN" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="11" string="one" lemma="one" stem="on" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="Giuliano" lemma="Giuliano" stem="giuliano" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="15" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (NP (NNP Goldman)) (VP (VBD was) (PP (IN in) (NP (PRP it))) (PP (IN for) (NP (NP (DT the) (NN money)) (PP (IN from) (NP (NN day) (CD one))))))) (, ,) ('' '') (NP (NNP Giuliano)) (VP (VBD said)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the money" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="money" />
          </tokens>
        </chunking>
        <chunking id="2" string="the money from day one" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="money" />
            <token id="9" string="from" />
            <token id="10" string="day" />
            <token id="11" string="one" />
          </tokens>
        </chunking>
        <chunking id="3" string="was in it for the money from day one" type="VP">
          <tokens>
            <token id="3" string="was" />
            <token id="4" string="in" />
            <token id="5" string="it" />
            <token id="6" string="for" />
            <token id="7" string="the" />
            <token id="8" string="money" />
            <token id="9" string="from" />
            <token id="10" string="day" />
            <token id="11" string="one" />
          </tokens>
        </chunking>
        <chunking id="4" string="it" type="NP">
          <tokens>
            <token id="5" string="it" />
          </tokens>
        </chunking>
        <chunking id="5" string="Giuliano" type="NP">
          <tokens>
            <token id="14" string="Giuliano" />
          </tokens>
        </chunking>
        <chunking id="6" string="Goldman" type="NP">
          <tokens>
            <token id="2" string="Goldman" />
          </tokens>
        </chunking>
        <chunking id="7" string="said" type="VP">
          <tokens>
            <token id="15" string="said" />
          </tokens>
        </chunking>
        <chunking id="8" string="day one" type="NP">
          <tokens>
            <token id="10" string="day" />
            <token id="11" string="one" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="5">it</governor>
          <dependent id="2">Goldman</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="5">it</governor>
          <dependent id="3">was</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">it</governor>
          <dependent id="4">in</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="15">said</governor>
          <dependent id="5">it</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">money</governor>
          <dependent id="6">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">money</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">it</governor>
          <dependent id="8">money</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">day</governor>
          <dependent id="9">from</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">money</governor>
          <dependent id="10">day</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="10">day</governor>
          <dependent id="11">one</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">said</governor>
          <dependent id="14">Giuliano</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="15">said</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="one" type="NUMBER" score="0.0">
          <tokens>
            <token id="11" string="one" />
          </tokens>
        </entity>
        <entity id="2" string="day" type="DURATION" score="0.0">
          <tokens>
            <token id="10" string="day" />
          </tokens>
        </entity>
        <entity id="3" string="Giuliano" type="PERSON" score="0.0">
          <tokens>
            <token id="14" string="Giuliano" />
          </tokens>
        </entity>
        <entity id="4" string="Goldman" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Goldman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="43" has_coreference="true">
      <content>``Goldman knows the formula: the more sleaze, the more dough.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="3" string="knows" lemma="know" stem="know" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="formula" lemma="formula" stem="formula" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="more" lemma="more" stem="more" pos="JJR" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="sleaze" lemma="sleaze" stem="sleaz" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="12" string="more" lemma="more" stem="more" pos="JJR" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="dough" lemma="dough" stem="dough" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (NNP Goldman)) (VP (VBZ knows) (NP (NP (DT the) (NN formula)) (: :) (NP (NP (DT the) (JJR more) (NN sleaze)) (, ,) (NP (DT the) (JJR more) (NN dough))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the more sleaze" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="more" />
            <token id="9" string="sleaze" />
          </tokens>
        </chunking>
        <chunking id="2" string="knows the formula : the more sleaze , the more dough" type="VP">
          <tokens>
            <token id="3" string="knows" />
            <token id="4" string="the" />
            <token id="5" string="formula" />
            <token id="6" string=":" />
            <token id="7" string="the" />
            <token id="8" string="more" />
            <token id="9" string="sleaze" />
            <token id="10" string="," />
            <token id="11" string="the" />
            <token id="12" string="more" />
            <token id="13" string="dough" />
          </tokens>
        </chunking>
        <chunking id="3" string="the more sleaze , the more dough" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="more" />
            <token id="9" string="sleaze" />
            <token id="10" string="," />
            <token id="11" string="the" />
            <token id="12" string="more" />
            <token id="13" string="dough" />
          </tokens>
        </chunking>
        <chunking id="4" string="the formula : the more sleaze , the more dough" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="formula" />
            <token id="6" string=":" />
            <token id="7" string="the" />
            <token id="8" string="more" />
            <token id="9" string="sleaze" />
            <token id="10" string="," />
            <token id="11" string="the" />
            <token id="12" string="more" />
            <token id="13" string="dough" />
          </tokens>
        </chunking>
        <chunking id="5" string="Goldman" type="NP">
          <tokens>
            <token id="2" string="Goldman" />
          </tokens>
        </chunking>
        <chunking id="6" string="the formula" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="formula" />
          </tokens>
        </chunking>
        <chunking id="7" string="the more dough" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="more" />
            <token id="13" string="dough" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">knows</governor>
          <dependent id="2">Goldman</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">knows</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">formula</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">knows</governor>
          <dependent id="5">formula</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">sleaze</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">sleaze</governor>
          <dependent id="8">more</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="5">formula</governor>
          <dependent id="9">sleaze</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">dough</governor>
          <dependent id="11">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">dough</governor>
          <dependent id="12">more</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="9">sleaze</governor>
          <dependent id="13">dough</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Goldman" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="2" string="Goldman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="44" has_coreference="true">
      <content>So what&amp;apost;s his motive?</content>
      <tokens>
        <token id="1" string="So" lemma="so" stem="so" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="what" lemma="what" stem="what" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="motive" lemma="motive" stem="motiv" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="?" lemma="?" stem="?" pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (SBARQ (RB So) (WHNP (WP what)) (SQ (VBZ 's) (NP (PRP$ his) (NN motive))) (. ?)))</syntactictree>
      <chunkings>
        <chunking id="1" string="his motive" type="NP">
          <tokens>
            <token id="4" string="his" />
            <token id="5" string="motive" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="2">what</governor>
          <dependent id="1">So</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">what</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="2">what</governor>
          <dependent id="3">'s</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="5">motive</governor>
          <dependent id="4">his</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="2">what</governor>
          <dependent id="5">motive</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="45" has_coreference="true">
      <content>If he writes the sleaziest book he makes the most dough.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="If" lemma="if" stem="if" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="writes" lemma="write" stem="write" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="sleaziest" lemma="sleaziest" stem="sleaziest" pos="JJS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="makes" lemma="make" stem="make" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="most" lemma="most" stem="most" pos="RBS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="dough" lemma="dough" stem="dough" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (SBAR (IN If) (S (NP (PRP he)) (VP (VBZ writes) (NP (DT the) (JJS sleaziest) (NN book))))) (NP (PRP he)) (VP (VBZ makes) (NP (DT the) (RBS most) (JJ dough))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="the sleaziest book" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="sleaziest" />
            <token id="6" string="book" />
          </tokens>
        </chunking>
        <chunking id="2" string="writes the sleaziest book" type="VP">
          <tokens>
            <token id="3" string="writes" />
            <token id="4" string="the" />
            <token id="5" string="sleaziest" />
            <token id="6" string="book" />
          </tokens>
        </chunking>
        <chunking id="3" string="he" type="NP">
          <tokens>
            <token id="2" string="he" />
          </tokens>
        </chunking>
        <chunking id="4" string="makes the most dough" type="VP">
          <tokens>
            <token id="8" string="makes" />
            <token id="9" string="the" />
            <token id="10" string="most" />
            <token id="11" string="dough" />
          </tokens>
        </chunking>
        <chunking id="5" string="the most dough" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="most" />
            <token id="11" string="dough" />
          </tokens>
        </chunking>
        <chunking id="6" string="If he writes the sleaziest book" type="SBAR">
          <tokens>
            <token id="1" string="If" />
            <token id="2" string="he" />
            <token id="3" string="writes" />
            <token id="4" string="the" />
            <token id="5" string="sleaziest" />
            <token id="6" string="book" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="mark">
          <governor id="3">writes</governor>
          <dependent id="1">If</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">writes</governor>
          <dependent id="2">he</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="8">makes</governor>
          <dependent id="3">writes</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">book</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">book</governor>
          <dependent id="5">sleaziest</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">writes</governor>
          <dependent id="6">book</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">makes</governor>
          <dependent id="7">he</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="8">makes</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">dough</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">dough</governor>
          <dependent id="10">most</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="8">makes</governor>
          <dependent id="11">dough</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="46" has_coreference="true">
      <content>Baird herself wasn&amp;apost;t blind to the business end of publishing.</content>
      <tokens>
        <token id="1" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="herself" lemma="herself" stem="herself" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="n't" lemma="not" stem="n't" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="blind" lemma="blind" stem="blind" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="business" lemma="business" stem="busi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="end" lemma="end" stem="end" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="publishing" lemma="publishing" stem="publish" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Baird) (PRP herself)) (VP (VBD was) (RB n't) (ADJP (JJ blind) (PP (TO to) (NP (NP (DT the) (NN business) (NN end)) (PP (IN of) (NP (NN publishing))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="was n't blind to the business end of publishing" type="VP">
          <tokens>
            <token id="3" string="was" />
            <token id="4" string="n't" />
            <token id="5" string="blind" />
            <token id="6" string="to" />
            <token id="7" string="the" />
            <token id="8" string="business" />
            <token id="9" string="end" />
            <token id="10" string="of" />
            <token id="11" string="publishing" />
          </tokens>
        </chunking>
        <chunking id="2" string="the business end" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="business" />
            <token id="9" string="end" />
          </tokens>
        </chunking>
        <chunking id="3" string="the business end of publishing" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="business" />
            <token id="9" string="end" />
            <token id="10" string="of" />
            <token id="11" string="publishing" />
          </tokens>
        </chunking>
        <chunking id="4" string="Baird herself" type="NP">
          <tokens>
            <token id="1" string="Baird" />
            <token id="2" string="herself" />
          </tokens>
        </chunking>
        <chunking id="5" string="blind to the business end of publishing" type="ADJP">
          <tokens>
            <token id="5" string="blind" />
            <token id="6" string="to" />
            <token id="7" string="the" />
            <token id="8" string="business" />
            <token id="9" string="end" />
            <token id="10" string="of" />
            <token id="11" string="publishing" />
          </tokens>
        </chunking>
        <chunking id="6" string="publishing" type="NP">
          <tokens>
            <token id="11" string="publishing" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="5">blind</governor>
          <dependent id="1">Baird</dependent>
        </dependency>
        <dependency type="nmod:npmod">
          <governor id="1">Baird</governor>
          <dependent id="2">herself</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="5">blind</governor>
          <dependent id="3">was</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="5">blind</governor>
          <dependent id="4">n't</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">blind</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">end</governor>
          <dependent id="6">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">end</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">end</governor>
          <dependent id="8">business</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">blind</governor>
          <dependent id="9">end</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">publishing</governor>
          <dependent id="10">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">end</governor>
          <dependent id="11">publishing</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Baird" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="47" has_coreference="true">
      <content>She agreed to change the title from ``My Brother John&amp;apost;&amp;apost; to ``John Lennon, My Brother.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="She" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="agreed" lemma="agree" stem="agre" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="change" lemma="change" stem="chang" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="title" lemma="title" stem="titl" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="My" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="10" string="Brother" lemma="Brother" stem="brother" pos="NNP" type="Word" isStopWord="false" ner="TITLE" is_referenced="true" is_refers="true" />
        <token id="11" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="12" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="16" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="17" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="My" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="Brother" lemma="Brother" stem="brother" pos="NNP" type="Word" isStopWord="false" ner="TITLE" is_referenced="false" is_refers="false" />
        <token id="20" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP She)) (VP (VBD agreed) (S (VP (TO to) (VP (VB change) (NP (DT the) (NN title)) (PP (IN from) (`` ``) (NP (PRP$ My) (NNP Brother) (NNP John)) ('' '')) (PP (TO to) (`` ``) (NP (NP (NNP John) (NNP Lennon)) (, ,) (NP (PRP$ My) (NNP Brother)))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="John Lennon" type="NP">
          <tokens>
            <token id="15" string="John" />
            <token id="16" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="2" string="My Brother" type="NP">
          <tokens>
            <token id="18" string="My" />
            <token id="19" string="Brother" />
          </tokens>
        </chunking>
        <chunking id="3" string="change the title from `` My Brother John '' to `` John Lennon , My Brother" type="VP">
          <tokens>
            <token id="4" string="change" />
            <token id="5" string="the" />
            <token id="6" string="title" />
            <token id="7" string="from" />
            <token id="8" string="``" />
            <token id="9" string="My" />
            <token id="10" string="Brother" />
            <token id="11" string="John" />
            <token id="12" string="''" />
            <token id="13" string="to" />
            <token id="14" string="``" />
            <token id="15" string="John" />
            <token id="16" string="Lennon" />
            <token id="17" string="," />
            <token id="18" string="My" />
            <token id="19" string="Brother" />
          </tokens>
        </chunking>
        <chunking id="4" string="agreed to change the title from `` My Brother John '' to `` John Lennon , My Brother" type="VP">
          <tokens>
            <token id="2" string="agreed" />
            <token id="3" string="to" />
            <token id="4" string="change" />
            <token id="5" string="the" />
            <token id="6" string="title" />
            <token id="7" string="from" />
            <token id="8" string="``" />
            <token id="9" string="My" />
            <token id="10" string="Brother" />
            <token id="11" string="John" />
            <token id="12" string="''" />
            <token id="13" string="to" />
            <token id="14" string="``" />
            <token id="15" string="John" />
            <token id="16" string="Lennon" />
            <token id="17" string="," />
            <token id="18" string="My" />
            <token id="19" string="Brother" />
          </tokens>
        </chunking>
        <chunking id="5" string="John Lennon , My Brother" type="NP">
          <tokens>
            <token id="15" string="John" />
            <token id="16" string="Lennon" />
            <token id="17" string="," />
            <token id="18" string="My" />
            <token id="19" string="Brother" />
          </tokens>
        </chunking>
        <chunking id="6" string="the title" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="title" />
          </tokens>
        </chunking>
        <chunking id="7" string="to change the title from `` My Brother John '' to `` John Lennon , My Brother" type="VP">
          <tokens>
            <token id="3" string="to" />
            <token id="4" string="change" />
            <token id="5" string="the" />
            <token id="6" string="title" />
            <token id="7" string="from" />
            <token id="8" string="``" />
            <token id="9" string="My" />
            <token id="10" string="Brother" />
            <token id="11" string="John" />
            <token id="12" string="''" />
            <token id="13" string="to" />
            <token id="14" string="``" />
            <token id="15" string="John" />
            <token id="16" string="Lennon" />
            <token id="17" string="," />
            <token id="18" string="My" />
            <token id="19" string="Brother" />
          </tokens>
        </chunking>
        <chunking id="8" string="She" type="NP">
          <tokens>
            <token id="1" string="She" />
          </tokens>
        </chunking>
        <chunking id="9" string="My Brother John" type="NP">
          <tokens>
            <token id="9" string="My" />
            <token id="10" string="Brother" />
            <token id="11" string="John" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">agreed</governor>
          <dependent id="1">She</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">agreed</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="4">change</governor>
          <dependent id="3">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="2">agreed</governor>
          <dependent id="4">change</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">title</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">change</governor>
          <dependent id="6">title</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">John</governor>
          <dependent id="7">from</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="11">John</governor>
          <dependent id="9">My</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">John</governor>
          <dependent id="10">Brother</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">change</governor>
          <dependent id="11">John</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">Lennon</governor>
          <dependent id="13">to</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="16">Lennon</governor>
          <dependent id="15">John</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">change</governor>
          <dependent id="16">Lennon</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="19">Brother</governor>
          <dependent id="18">My</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="16">Lennon</governor>
          <dependent id="19">Brother</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="15" string="John" />
            <token id="16" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="Brother" type="TITLE" score="0.0">
          <tokens>
            <token id="10" string="Brother" />
          </tokens>
        </entity>
        <entity id="3" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="11" string="John" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="48" has_coreference="true">
      <content>The cover&amp;apost;s look-alike photos of Baird and Lennon in granny specs also help, but she said she didn&amp;apost;t don them to sell books.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="cover" lemma="cover" stem="cover" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="look-alike" lemma="look-alike" stem="look-alik" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="photos" lemma="photo" stem="photo" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="8" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="10" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="granny" lemma="granny" stem="granni" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="specs" lemma="spec" stem="spec" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="help" lemma="help" stem="help" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="20" string="did" lemma="do" stem="did" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="n't" lemma="not" stem="n't" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="don" lemma="don" stem="don" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="them" lemma="they" stem="them" pos="PRP" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="24" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="sell" lemma="sell" stem="sell" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="books" lemma="book" stem="book" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (NP (NP (DT The) (NN cover) (POS 's)) (JJ look-alike) (NNS photos)) (PP (IN of) (NP (NNP Baird) (CC and) (NNP Lennon))) (PP (IN in) (NP (NN granny) (NNS specs)))) (ADVP (RB also)) (VP (VBP help))) (, ,) (CC but) (S (NP (PRP she)) (VP (VBD said) (SBAR (S (NP (PRP she)) (VP (VBD did) (RB n't) (VP (VB don) (S (NP (PRP them)) (VP (TO to) (VP (VB sell) (NP (NNS books))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="The cover 's" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="cover" />
            <token id="3" string="'s" />
          </tokens>
        </chunking>
        <chunking id="2" string="said she did n't don them to sell books" type="VP">
          <tokens>
            <token id="18" string="said" />
            <token id="19" string="she" />
            <token id="20" string="did" />
            <token id="21" string="n't" />
            <token id="22" string="don" />
            <token id="23" string="them" />
            <token id="24" string="to" />
            <token id="25" string="sell" />
            <token id="26" string="books" />
          </tokens>
        </chunking>
        <chunking id="3" string="sell books" type="VP">
          <tokens>
            <token id="25" string="sell" />
            <token id="26" string="books" />
          </tokens>
        </chunking>
        <chunking id="4" string="them" type="NP">
          <tokens>
            <token id="23" string="them" />
          </tokens>
        </chunking>
        <chunking id="5" string="to sell books" type="VP">
          <tokens>
            <token id="24" string="to" />
            <token id="25" string="sell" />
            <token id="26" string="books" />
          </tokens>
        </chunking>
        <chunking id="6" string="she" type="NP">
          <tokens>
            <token id="17" string="she" />
          </tokens>
        </chunking>
        <chunking id="7" string="help" type="VP">
          <tokens>
            <token id="14" string="help" />
          </tokens>
        </chunking>
        <chunking id="8" string="books" type="NP">
          <tokens>
            <token id="26" string="books" />
          </tokens>
        </chunking>
        <chunking id="9" string="Baird and Lennon" type="NP">
          <tokens>
            <token id="7" string="Baird" />
            <token id="8" string="and" />
            <token id="9" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="10" string="The cover 's look-alike photos of Baird and Lennon in granny specs" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="cover" />
            <token id="3" string="'s" />
            <token id="4" string="look-alike" />
            <token id="5" string="photos" />
            <token id="6" string="of" />
            <token id="7" string="Baird" />
            <token id="8" string="and" />
            <token id="9" string="Lennon" />
            <token id="10" string="in" />
            <token id="11" string="granny" />
            <token id="12" string="specs" />
          </tokens>
        </chunking>
        <chunking id="11" string="The cover 's look-alike photos" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="cover" />
            <token id="3" string="'s" />
            <token id="4" string="look-alike" />
            <token id="5" string="photos" />
          </tokens>
        </chunking>
        <chunking id="12" string="did n't don them to sell books" type="VP">
          <tokens>
            <token id="20" string="did" />
            <token id="21" string="n't" />
            <token id="22" string="don" />
            <token id="23" string="them" />
            <token id="24" string="to" />
            <token id="25" string="sell" />
            <token id="26" string="books" />
          </tokens>
        </chunking>
        <chunking id="13" string="don them to sell books" type="VP">
          <tokens>
            <token id="22" string="don" />
            <token id="23" string="them" />
            <token id="24" string="to" />
            <token id="25" string="sell" />
            <token id="26" string="books" />
          </tokens>
        </chunking>
        <chunking id="14" string="she did n't don them to sell books" type="SBAR">
          <tokens>
            <token id="19" string="she" />
            <token id="20" string="did" />
            <token id="21" string="n't" />
            <token id="22" string="don" />
            <token id="23" string="them" />
            <token id="24" string="to" />
            <token id="25" string="sell" />
            <token id="26" string="books" />
          </tokens>
        </chunking>
        <chunking id="15" string="granny specs" type="NP">
          <tokens>
            <token id="11" string="granny" />
            <token id="12" string="specs" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">cover</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="5">photos</governor>
          <dependent id="2">cover</dependent>
        </dependency>
        <dependency type="case">
          <governor id="2">cover</governor>
          <dependent id="3">'s</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">photos</governor>
          <dependent id="4">look-alike</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">help</governor>
          <dependent id="5">photos</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">Baird</governor>
          <dependent id="6">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">photos</governor>
          <dependent id="7">Baird</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">Baird</governor>
          <dependent id="8">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">Baird</governor>
          <dependent id="9">Lennon</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">specs</governor>
          <dependent id="10">in</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">specs</governor>
          <dependent id="11">granny</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">photos</governor>
          <dependent id="12">specs</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="14">help</governor>
          <dependent id="13">also</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="14">help</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="14">help</governor>
          <dependent id="16">but</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="18">said</governor>
          <dependent id="17">she</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">help</governor>
          <dependent id="18">said</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">don</governor>
          <dependent id="19">she</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="22">don</governor>
          <dependent id="20">did</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="22">don</governor>
          <dependent id="21">n't</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="18">said</governor>
          <dependent id="22">don</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="22">don</governor>
          <dependent id="23">them</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="25">sell</governor>
          <dependent id="24">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="22">don</governor>
          <dependent id="25">sell</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="25">sell</governor>
          <dependent id="26">books</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="7" string="Baird" />
          </tokens>
        </entity>
        <entity id="2" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="9" string="Lennon" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="49" has_coreference="true">
      <content>They&amp;apost;re on the National Health, Britain&amp;apost;s medical plan, and she and Lennon inherited their mother&amp;apost;s short-sightedness, Baird said.</content>
      <tokens>
        <token id="1" string="They" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="'re" lemma="be" stem="'re" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="National" lemma="National" stem="nation" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="6" string="Health" lemma="Health" stem="health" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Britain" lemma="Britain" stem="britain" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="9" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="medical" lemma="medical" stem="medic" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="plan" lemma="plan" stem="plan" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="15" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="16" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="17" string="inherited" lemma="inherit" stem="inherit" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="20" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="21" string="short-sightedness" lemma="short-sightedness" stem="short-sighted" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="Baird" lemma="Baird" stem="baird" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="24" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (S (NP (PRP They)) (VP (VBP 're) (PP (IN on) (NP (NP (DT the) (NNP National) (NNP Health)) (, ,) (NP (NP (NNP Britain) (POS 's)) (JJ medical) (NN plan)))))) (, ,) (CC and) (S (NP (PRP she) (CC and) (NNP Lennon)) (VP (VBD inherited) (NP (NP (PRP$ their) (NN mother) (POS 's)) (NN short-sightedness))))) (, ,) (NP (NNP Baird)) (VP (VBD said)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="They" type="NP">
          <tokens>
            <token id="1" string="They" />
          </tokens>
        </chunking>
        <chunking id="2" string="inherited their mother 's short-sightedness" type="VP">
          <tokens>
            <token id="17" string="inherited" />
            <token id="18" string="their" />
            <token id="19" string="mother" />
            <token id="20" string="'s" />
            <token id="21" string="short-sightedness" />
          </tokens>
        </chunking>
        <chunking id="3" string="'re on the National Health , Britain 's medical plan" type="VP">
          <tokens>
            <token id="2" string="'re" />
            <token id="3" string="on" />
            <token id="4" string="the" />
            <token id="5" string="National" />
            <token id="6" string="Health" />
            <token id="7" string="," />
            <token id="8" string="Britain" />
            <token id="9" string="'s" />
            <token id="10" string="medical" />
            <token id="11" string="plan" />
          </tokens>
        </chunking>
        <chunking id="4" string="the National Health" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="National" />
            <token id="6" string="Health" />
          </tokens>
        </chunking>
        <chunking id="5" string="she and Lennon" type="NP">
          <tokens>
            <token id="14" string="she" />
            <token id="15" string="and" />
            <token id="16" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="6" string="Baird" type="NP">
          <tokens>
            <token id="23" string="Baird" />
          </tokens>
        </chunking>
        <chunking id="7" string="Britain 's" type="NP">
          <tokens>
            <token id="8" string="Britain" />
            <token id="9" string="'s" />
          </tokens>
        </chunking>
        <chunking id="8" string="their mother 's short-sightedness" type="NP">
          <tokens>
            <token id="18" string="their" />
            <token id="19" string="mother" />
            <token id="20" string="'s" />
            <token id="21" string="short-sightedness" />
          </tokens>
        </chunking>
        <chunking id="9" string="their mother 's" type="NP">
          <tokens>
            <token id="18" string="their" />
            <token id="19" string="mother" />
            <token id="20" string="'s" />
          </tokens>
        </chunking>
        <chunking id="10" string="said" type="VP">
          <tokens>
            <token id="24" string="said" />
          </tokens>
        </chunking>
        <chunking id="11" string="the National Health , Britain 's medical plan" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="National" />
            <token id="6" string="Health" />
            <token id="7" string="," />
            <token id="8" string="Britain" />
            <token id="9" string="'s" />
            <token id="10" string="medical" />
            <token id="11" string="plan" />
          </tokens>
        </chunking>
        <chunking id="12" string="Britain 's medical plan" type="NP">
          <tokens>
            <token id="8" string="Britain" />
            <token id="9" string="'s" />
            <token id="10" string="medical" />
            <token id="11" string="plan" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="6">Health</governor>
          <dependent id="1">They</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="6">Health</governor>
          <dependent id="2">'re</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">Health</governor>
          <dependent id="3">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">Health</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">Health</governor>
          <dependent id="5">National</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="24">said</governor>
          <dependent id="6">Health</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="11">plan</governor>
          <dependent id="8">Britain</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">Britain</governor>
          <dependent id="9">'s</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">plan</governor>
          <dependent id="10">medical</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="6">Health</governor>
          <dependent id="11">plan</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="6">Health</governor>
          <dependent id="13">and</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="17">inherited</governor>
          <dependent id="14">she</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="14">she</governor>
          <dependent id="15">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">she</governor>
          <dependent id="16">Lennon</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="6">Health</governor>
          <dependent id="17">inherited</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="19">mother</governor>
          <dependent id="18">their</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="21">short-sightedness</governor>
          <dependent id="19">mother</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">mother</governor>
          <dependent id="20">'s</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="17">inherited</governor>
          <dependent id="21">short-sightedness</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="24">said</governor>
          <dependent id="23">Baird</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="24">said</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="16" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="Baird" type="PERSON" score="0.0">
          <tokens>
            <token id="23" string="Baird" />
          </tokens>
        </entity>
        <entity id="3" string="Britain" type="LOCATION" score="0.0">
          <tokens>
            <token id="8" string="Britain" />
          </tokens>
        </entity>
        <entity id="4" string="National Health" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="5" string="National" />
            <token id="6" string="Health" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="50" has_coreference="true">
      <content>The book itself is a loving look through rose-colored specs of a brother whose art touched millions.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="itself" lemma="itself" stem="itself" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="loving" lemma="loving" stem="love" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="look" lemma="look" stem="look" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="through" lemma="through" stem="through" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="rose-colored" lemma="rose-colored" stem="rose-color" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="10" string="specs" lemma="spec" stem="spec" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="11" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="brother" lemma="brother" stem="brother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="14" string="whose" lemma="whose" stem="whose" pos="WP$" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="15" string="art" lemma="art" stem="art" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="16" string="touched" lemma="touch" stem="touch" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="17" string="millions" lemma="million" stem="million" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="18" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN book)) (ADVP (PRP itself)) (VP (VBZ is) (NP (NP (DT a) (JJ loving) (NN look)) (PP (IN through) (NP (NP (JJ rose-colored) (NNS specs)) (PP (IN of) (NP (NP (DT a) (NN brother)) (SBAR (WHNP (WP$ whose) (NN art)) (S (VP (VBD touched) (NP (NNS millions))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="rose-colored specs" type="NP">
          <tokens>
            <token id="9" string="rose-colored" />
            <token id="10" string="specs" />
          </tokens>
        </chunking>
        <chunking id="2" string="is a loving look through rose-colored specs of a brother whose art touched millions" type="VP">
          <tokens>
            <token id="4" string="is" />
            <token id="5" string="a" />
            <token id="6" string="loving" />
            <token id="7" string="look" />
            <token id="8" string="through" />
            <token id="9" string="rose-colored" />
            <token id="10" string="specs" />
            <token id="11" string="of" />
            <token id="12" string="a" />
            <token id="13" string="brother" />
            <token id="14" string="whose" />
            <token id="15" string="art" />
            <token id="16" string="touched" />
            <token id="17" string="millions" />
          </tokens>
        </chunking>
        <chunking id="3" string="a brother" type="NP">
          <tokens>
            <token id="12" string="a" />
            <token id="13" string="brother" />
          </tokens>
        </chunking>
        <chunking id="4" string="The book" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="book" />
          </tokens>
        </chunking>
        <chunking id="5" string="a brother whose art touched millions" type="NP">
          <tokens>
            <token id="12" string="a" />
            <token id="13" string="brother" />
            <token id="14" string="whose" />
            <token id="15" string="art" />
            <token id="16" string="touched" />
            <token id="17" string="millions" />
          </tokens>
        </chunking>
        <chunking id="6" string="a loving look" type="NP">
          <tokens>
            <token id="5" string="a" />
            <token id="6" string="loving" />
            <token id="7" string="look" />
          </tokens>
        </chunking>
        <chunking id="7" string="millions" type="NP">
          <tokens>
            <token id="17" string="millions" />
          </tokens>
        </chunking>
        <chunking id="8" string="a loving look through rose-colored specs of a brother whose art touched millions" type="NP">
          <tokens>
            <token id="5" string="a" />
            <token id="6" string="loving" />
            <token id="7" string="look" />
            <token id="8" string="through" />
            <token id="9" string="rose-colored" />
            <token id="10" string="specs" />
            <token id="11" string="of" />
            <token id="12" string="a" />
            <token id="13" string="brother" />
            <token id="14" string="whose" />
            <token id="15" string="art" />
            <token id="16" string="touched" />
            <token id="17" string="millions" />
          </tokens>
        </chunking>
        <chunking id="9" string="whose art touched millions" type="SBAR">
          <tokens>
            <token id="14" string="whose" />
            <token id="15" string="art" />
            <token id="16" string="touched" />
            <token id="17" string="millions" />
          </tokens>
        </chunking>
        <chunking id="10" string="touched millions" type="VP">
          <tokens>
            <token id="16" string="touched" />
            <token id="17" string="millions" />
          </tokens>
        </chunking>
        <chunking id="11" string="rose-colored specs of a brother whose art touched millions" type="NP">
          <tokens>
            <token id="9" string="rose-colored" />
            <token id="10" string="specs" />
            <token id="11" string="of" />
            <token id="12" string="a" />
            <token id="13" string="brother" />
            <token id="14" string="whose" />
            <token id="15" string="art" />
            <token id="16" string="touched" />
            <token id="17" string="millions" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">book</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">look</governor>
          <dependent id="2">book</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="7">look</governor>
          <dependent id="3">itself</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="7">look</governor>
          <dependent id="4">is</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">look</governor>
          <dependent id="5">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">look</governor>
          <dependent id="6">loving</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">look</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">specs</governor>
          <dependent id="8">through</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="10">specs</governor>
          <dependent id="9">rose-colored</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">look</governor>
          <dependent id="10">specs</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">brother</governor>
          <dependent id="11">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">brother</governor>
          <dependent id="12">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">specs</governor>
          <dependent id="13">brother</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="15">art</governor>
          <dependent id="14">whose</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">touched</governor>
          <dependent id="15">art</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="13">brother</governor>
          <dependent id="16">touched</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="16">touched</governor>
          <dependent id="17">millions</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="51" has_coreference="true">
      <content>``It&amp;apost;s not a major biography,&amp;apost;&amp;apost; Giuliano said.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="major" lemma="major" stem="major" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="biography" lemma="biography" stem="biographi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="Giuliano" lemma="Giuliano" stem="giuliano" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="11" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (NP (PRP It)) (VP (VBZ 's) (RB not) (NP (DT a) (JJ major) (NN biography)))) (, ,) ('' '') (NP (NNP Giuliano)) (VP (VBD said)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a major biography" type="NP">
          <tokens>
            <token id="5" string="a" />
            <token id="6" string="major" />
            <token id="7" string="biography" />
          </tokens>
        </chunking>
        <chunking id="2" string="It" type="NP">
          <tokens>
            <token id="2" string="It" />
          </tokens>
        </chunking>
        <chunking id="3" string="Giuliano" type="NP">
          <tokens>
            <token id="10" string="Giuliano" />
          </tokens>
        </chunking>
        <chunking id="4" string="'s not a major biography" type="VP">
          <tokens>
            <token id="3" string="'s" />
            <token id="4" string="not" />
            <token id="5" string="a" />
            <token id="6" string="major" />
            <token id="7" string="biography" />
          </tokens>
        </chunking>
        <chunking id="5" string="said" type="VP">
          <tokens>
            <token id="11" string="said" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="7">biography</governor>
          <dependent id="2">It</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="7">biography</governor>
          <dependent id="3">'s</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="7">biography</governor>
          <dependent id="4">not</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">biography</governor>
          <dependent id="5">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">biography</governor>
          <dependent id="6">major</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="11">said</governor>
          <dependent id="7">biography</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">said</governor>
          <dependent id="10">Giuliano</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="11">said</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Giuliano" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="Giuliano" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="52" has_coreference="true">
      <content>``This book is a little slice out of a big pie, out of a big life.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="This" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="6" string="little" lemma="little" stem="littl" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="slice" lemma="slice" stem="slice" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="out" lemma="out" stem="out" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="big" lemma="big" stem="big" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="pie" lemma="pie" stem="pie" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="out" lemma="out" stem="out" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="big" lemma="big" stem="big" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="life" lemma="life" stem="life" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (DT This) (NN book)) (VP (VBZ is) (NP (DT a) (JJ little) (NN slice)) (ADVP (IN out) (PP (IN of) (NP (DT a) (JJ big) (NN pie)))) (, ,) (ADJP (IN out) (PP (IN of) (NP (DT a) (JJ big) (NN life))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="out of a big life" type="ADJP">
          <tokens>
            <token id="14" string="out" />
            <token id="15" string="of" />
            <token id="16" string="a" />
            <token id="17" string="big" />
            <token id="18" string="life" />
          </tokens>
        </chunking>
        <chunking id="2" string="This book" type="NP">
          <tokens>
            <token id="2" string="This" />
            <token id="3" string="book" />
          </tokens>
        </chunking>
        <chunking id="3" string="a big life" type="NP">
          <tokens>
            <token id="16" string="a" />
            <token id="17" string="big" />
            <token id="18" string="life" />
          </tokens>
        </chunking>
        <chunking id="4" string="a big pie" type="NP">
          <tokens>
            <token id="10" string="a" />
            <token id="11" string="big" />
            <token id="12" string="pie" />
          </tokens>
        </chunking>
        <chunking id="5" string="is a little slice out of a big pie , out of a big life" type="VP">
          <tokens>
            <token id="4" string="is" />
            <token id="5" string="a" />
            <token id="6" string="little" />
            <token id="7" string="slice" />
            <token id="8" string="out" />
            <token id="9" string="of" />
            <token id="10" string="a" />
            <token id="11" string="big" />
            <token id="12" string="pie" />
            <token id="13" string="," />
            <token id="14" string="out" />
            <token id="15" string="of" />
            <token id="16" string="a" />
            <token id="17" string="big" />
            <token id="18" string="life" />
          </tokens>
        </chunking>
        <chunking id="6" string="a little slice" type="NP">
          <tokens>
            <token id="5" string="a" />
            <token id="6" string="little" />
            <token id="7" string="slice" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">book</governor>
          <dependent id="2">This</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="18">life</governor>
          <dependent id="3">book</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="18">life</governor>
          <dependent id="4">is</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">slice</governor>
          <dependent id="5">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">slice</governor>
          <dependent id="6">little</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="18">life</governor>
          <dependent id="7">slice</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">pie</governor>
          <dependent id="8">out</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="8">out</governor>
          <dependent id="9">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">pie</governor>
          <dependent id="10">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">pie</governor>
          <dependent id="11">big</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="18">life</governor>
          <dependent id="12">pie</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">life</governor>
          <dependent id="14">out</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="14">out</governor>
          <dependent id="15">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">life</governor>
          <dependent id="16">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="18">life</governor>
          <dependent id="17">big</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="18">life</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="53" has_coreference="true">
      <content>It offers a lot of previously unknown details on Lennon&amp;apost;s early life.</content>
      <tokens>
        <token id="1" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="offers" lemma="offer" stem="offer" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="lot" lemma="lot" stem="lot" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="previously" lemma="previously" stem="previous" pos="RB" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="7" string="unknown" lemma="unknown" stem="unknown" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="details" lemma="detail" stem="detail" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="11" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="early" lemma="early" stem="earli" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="life" lemma="life" stem="life" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP It)) (VP (VBZ offers) (NP (NP (DT a) (NN lot)) (PP (IN of) (NP (NP (ADJP (RB previously) (JJ unknown)) (NNS details)) (PP (IN on) (NP (NP (NNP Lennon) (POS 's)) (JJ early) (NN life))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="previously unknown details on Lennon 's early life" type="NP">
          <tokens>
            <token id="6" string="previously" />
            <token id="7" string="unknown" />
            <token id="8" string="details" />
            <token id="9" string="on" />
            <token id="10" string="Lennon" />
            <token id="11" string="'s" />
            <token id="12" string="early" />
            <token id="13" string="life" />
          </tokens>
        </chunking>
        <chunking id="2" string="previously unknown details" type="NP">
          <tokens>
            <token id="6" string="previously" />
            <token id="7" string="unknown" />
            <token id="8" string="details" />
          </tokens>
        </chunking>
        <chunking id="3" string="a lot of previously unknown details on Lennon 's early life" type="NP">
          <tokens>
            <token id="3" string="a" />
            <token id="4" string="lot" />
            <token id="5" string="of" />
            <token id="6" string="previously" />
            <token id="7" string="unknown" />
            <token id="8" string="details" />
            <token id="9" string="on" />
            <token id="10" string="Lennon" />
            <token id="11" string="'s" />
            <token id="12" string="early" />
            <token id="13" string="life" />
          </tokens>
        </chunking>
        <chunking id="4" string="a lot" type="NP">
          <tokens>
            <token id="3" string="a" />
            <token id="4" string="lot" />
          </tokens>
        </chunking>
        <chunking id="5" string="previously unknown" type="ADJP">
          <tokens>
            <token id="6" string="previously" />
            <token id="7" string="unknown" />
          </tokens>
        </chunking>
        <chunking id="6" string="offers a lot of previously unknown details on Lennon 's early life" type="VP">
          <tokens>
            <token id="2" string="offers" />
            <token id="3" string="a" />
            <token id="4" string="lot" />
            <token id="5" string="of" />
            <token id="6" string="previously" />
            <token id="7" string="unknown" />
            <token id="8" string="details" />
            <token id="9" string="on" />
            <token id="10" string="Lennon" />
            <token id="11" string="'s" />
            <token id="12" string="early" />
            <token id="13" string="life" />
          </tokens>
        </chunking>
        <chunking id="7" string="It" type="NP">
          <tokens>
            <token id="1" string="It" />
          </tokens>
        </chunking>
        <chunking id="8" string="Lennon 's" type="NP">
          <tokens>
            <token id="10" string="Lennon" />
            <token id="11" string="'s" />
          </tokens>
        </chunking>
        <chunking id="9" string="Lennon 's early life" type="NP">
          <tokens>
            <token id="10" string="Lennon" />
            <token id="11" string="'s" />
            <token id="12" string="early" />
            <token id="13" string="life" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">offers</governor>
          <dependent id="1">It</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">offers</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">lot</governor>
          <dependent id="3">a</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">offers</governor>
          <dependent id="4">lot</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">details</governor>
          <dependent id="5">of</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="7">unknown</governor>
          <dependent id="6">previously</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">details</governor>
          <dependent id="7">unknown</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">lot</governor>
          <dependent id="8">details</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">life</governor>
          <dependent id="9">on</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="13">life</governor>
          <dependent id="10">Lennon</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">Lennon</governor>
          <dependent id="11">'s</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">life</governor>
          <dependent id="12">early</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">details</governor>
          <dependent id="13">life</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="previously" type="DATE" score="0.0">
          <tokens>
            <token id="6" string="previously" />
          </tokens>
        </entity>
        <entity id="2" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="Lennon" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="54" has_coreference="true">
      <content>It&amp;apost;s a sympathetic view, sure, but what&amp;apost;s not to love?&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="sympathetic" lemma="sympathetic" stem="sympathet" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="view" lemma="view" stem="view" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="sure" lemma="sure" stem="sure" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="what" lemma="what" stem="what" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="love" lemma="love" stem="love" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="?" lemma="?" stem="?" pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (PRP It)) (VP (VBZ 's) (NP (DT a) (JJ sympathetic) (NN view)) (, ,) (ADVP (RB sure)))) (, ,) (CC but) (S (NP (WP what)) (VP (VBZ 's) (RB not) (S (VP (TO to) (VP (VB love)))))) (. ?) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="love" type="VP">
          <tokens>
            <token id="14" string="love" />
          </tokens>
        </chunking>
        <chunking id="2" string="what" type="NP">
          <tokens>
            <token id="10" string="what" />
          </tokens>
        </chunking>
        <chunking id="3" string="'s not to love" type="VP">
          <tokens>
            <token id="11" string="'s" />
            <token id="12" string="not" />
            <token id="13" string="to" />
            <token id="14" string="love" />
          </tokens>
        </chunking>
        <chunking id="4" string="to love" type="VP">
          <tokens>
            <token id="13" string="to" />
            <token id="14" string="love" />
          </tokens>
        </chunking>
        <chunking id="5" string="a sympathetic view" type="NP">
          <tokens>
            <token id="3" string="a" />
            <token id="4" string="sympathetic" />
            <token id="5" string="view" />
          </tokens>
        </chunking>
        <chunking id="6" string="It" type="NP">
          <tokens>
            <token id="1" string="It" />
          </tokens>
        </chunking>
        <chunking id="7" string="'s a sympathetic view , sure" type="VP">
          <tokens>
            <token id="2" string="'s" />
            <token id="3" string="a" />
            <token id="4" string="sympathetic" />
            <token id="5" string="view" />
            <token id="6" string="," />
            <token id="7" string="sure" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="5">view</governor>
          <dependent id="1">It</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="5">view</governor>
          <dependent id="2">'s</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">view</governor>
          <dependent id="3">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">view</governor>
          <dependent id="4">sympathetic</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">view</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">view</governor>
          <dependent id="7">sure</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="5">view</governor>
          <dependent id="9">but</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">'s</governor>
          <dependent id="10">what</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="5">view</governor>
          <dependent id="11">'s</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="11">'s</governor>
          <dependent id="12">not</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="14">love</governor>
          <dependent id="13">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="11">'s</governor>
          <dependent id="14">love</dependent>
        </dependency>
      </dependencies>
    </sentence>
  </sentences>
  <coreferences>
    <coreference id="1" type="PROPER">
      <referenced ids_tokens="17-18" string="Julia Baird" id_sentence="1" />
      <mentions>
        <mention ids_tokens="18" string="Julia" id_sentence="9" />
        <mention ids_tokens="14" string="Julia" id_sentence="11" />
        <mention ids_tokens="16" string="her" id_sentence="11" />
        <mention ids_tokens="20" string="her" id_sentence="11" />
        <mention ids_tokens="18" string="Julia" id_sentence="15" />
        <mention ids_tokens="20" string="her" id_sentence="15" />
        <mention ids_tokens="1" string="She" id_sentence="16" />
        <mention ids_tokens="3" string="her" id_sentence="16" />
        <mention ids_tokens="29" string="Julia" id_sentence="17" />
        <mention ids_tokens="4" string="Julia" id_sentence="21" />
        <mention ids_tokens="8" string="her" id_sentence="22" />
        <mention ids_tokens="1" string="She" id_sentence="23" />
        <mention ids_tokens="2" string="she" id_sentence="24" />
        <mention ids_tokens="2" string="Julia" id_sentence="31" />
        <mention ids_tokens="21" string="she" id_sentence="32" />
        <mention ids_tokens="23" string="her" id_sentence="32" />
        <mention ids_tokens="9" string="her" id_sentence="33" />
        <mention ids_tokens="13" string="she" id_sentence="33" />
      </mentions>
    </coreference>
    <coreference id="2" type="NOMINAL">
      <referenced ids_tokens="4-5-6-7-8-9" string="a dour , shoeless English boy" id_sentence="1" />
      <mentions>
        <mention ids_tokens="1-2" string="The boy" id_sentence="2" />
      </mentions>
    </coreference>
    <coreference id="3" type="NOMINAL">
      <referenced ids_tokens="11-12-13-14-15" string="his absent , carefree mother" id_sentence="1" />
      <mentions>
        <mention ids_tokens="10-11" string="my mother" id_sentence="5" />
        <mention ids_tokens="35" string="her" id_sentence="5" />
        <mention ids_tokens="23-24" string="his mother" id_sentence="10" />
        <mention ids_tokens="34-35" string="his mother" id_sentence="11" />
        <mention ids_tokens="10-11" string="our mother" id_sentence="13" />
        <mention ids_tokens="3-4" string="my mother" id_sentence="14" />
      </mentions>
    </coreference>
    <coreference id="4" type="NOMINAL">
      <referenced ids_tokens="28-29-30" string="Baird 's book" id_sentence="37" />
      <mentions>
        <mention ids_tokens="25-26" string="a book" id_sentence="1" />
        <mention ids_tokens="35-36" string="her book" id_sentence="5" />
        <mention ids_tokens="16-28" string="a book , which he pieced together from interviews with Baird in England" id_sentence="25" />
        <mention ids_tokens="16-17" string="a book" id_sentence="25" />
      </mentions>
    </coreference>
    <coreference id="5" type="PROPER">
      <referenced ids_tokens="9-10-11" string="My Brother John" id_sentence="47" />
      <mentions>
        <mention ids_tokens="9-10" string="John Lennon" id_sentence="2" />
        <mention ids_tokens="42" string="Lennon" id_sentence="5" />
        <mention ids_tokens="44" string="his" id_sentence="5" />
        <mention ids_tokens="9-10" string="John Lennon" id_sentence="6" />
        <mention ids_tokens="8" string="Lennon" id_sentence="8" />
        <mention ids_tokens="16" string="Lennon" id_sentence="15" />
        <mention ids_tokens="15" string="Lennon" id_sentence="17" />
        <mention ids_tokens="12" string="his" id_sentence="18" />
        <mention ids_tokens="20" string="he" id_sentence="18" />
        <mention ids_tokens="20-21" string="Lennon's" id_sentence="20" />
        <mention ids_tokens="10-19" string="Lennon , a child's recollections of an aspiring brother" id_sentence="30" />
        <mention ids_tokens="10" string="Lennon" id_sentence="30" />
        <mention ids_tokens="18" string="Lennon" id_sentence="31" />
        <mention ids_tokens="10-11" string="Lennon's" id_sentence="32" />
        <mention ids_tokens="23-24" string="Lennon's" id_sentence="33" />
        <mention ids_tokens="8" string="John" id_sentence="34" />
        <mention ids_tokens="3" string="John" id_sentence="35" />
        <mention ids_tokens="17-18" string="John Lennon" id_sentence="37" />
        <mention ids_tokens="24-25" string="Lennon homosexual" id_sentence="38" />
        <mention ids_tokens="24" string="Lennon" id_sentence="38" />
        <mention ids_tokens="9" string="Lennon" id_sentence="48" />
        <mention ids_tokens="16" string="Lennon" id_sentence="49" />
        <mention ids_tokens="10-11" string="Lennon's" id_sentence="53" />
      </mentions>
    </coreference>
    <coreference id="6" type="PROPER">
      <referenced ids_tokens="12-13" string="Baird 's" id_sentence="2" />
      <mentions>
        <mention ids_tokens="23-46" string="Baird , who recently spent three weeks in the United States promoting her book , a family portrait of Lennon and his Liverpool roots" id_sentence="5" />
        <mention ids_tokens="23" string="Baird" id_sentence="5" />
        <mention ids_tokens="1" string="Baird" id_sentence="9" />
        <mention ids_tokens="23" string="his" id_sentence="10" />
        <mention ids_tokens="1" string="Baird" id_sentence="11" />
        <mention ids_tokens="10" string="his" id_sentence="11" />
        <mention ids_tokens="12" string="Baird" id_sentence="12" />
        <mention ids_tokens="10" string="our" id_sentence="13" />
        <mention ids_tokens="1-9" string="Baird , a 41-year-old teacher in Cheshire , England" id_sentence="15" />
        <mention ids_tokens="1" string="Baird" id_sentence="15" />
        <mention ids_tokens="3-9" string="a 41-year-old teacher in Cheshire , England" id_sentence="15" />
        <mention ids_tokens="7" string="Baird" id_sentence="20" />
        <mention ids_tokens="17" string="Baird" id_sentence="21" />
        <mention ids_tokens="1" string="Baird" id_sentence="22" />
        <mention ids_tokens="10" string="Baird" id_sentence="25" />
        <mention ids_tokens="14" string="him" id_sentence="25" />
        <mention ids_tokens="20" string="he" id_sentence="25" />
        <mention ids_tokens="26" string="Baird" id_sentence="25" />
        <mention ids_tokens="6" string="Baird" id_sentence="26" />
        <mention ids_tokens="1" string="He" id_sentence="27" />
        <mention ids_tokens="3" string="his" id_sentence="27" />
        <mention ids_tokens="12" string="Baird" id_sentence="31" />
        <mention ids_tokens="23" string="him" id_sentence="31" />
        <mention ids_tokens="27" string="he" id_sentence="31" />
        <mention ids_tokens="8" string="Baird" id_sentence="32" />
        <mention ids_tokens="15" string="his" id_sentence="32" />
        <mention ids_tokens="1" string="Baird" id_sentence="33" />
        <mention ids_tokens="13" string="Baird" id_sentence="34" />
        <mention ids_tokens="1" string="We" id_sentence="36" />
        <mention ids_tokens="1-11" string="Baird , who read Goldman's scathing biography of Elvis Presley" id_sentence="38" />
        <mention ids_tokens="1" string="Baird" id_sentence="38" />
        <mention ids_tokens="24" string="Baird" id_sentence="39" />
        <mention ids_tokens="1-2" string="Baird herself" id_sentence="46" />
        <mention ids_tokens="7-9" string="Baird and Lennon" id_sentence="48" />
        <mention ids_tokens="7" string="Baird" id_sentence="48" />
        <mention ids_tokens="23" string="Baird" id_sentence="49" />
      </mentions>
    </coreference>
    <coreference id="7" type="PROPER">
      <referenced ids_tokens="16-17" string="Giuliano 's" id_sentence="2" />
      <mentions>
        <mention ids_tokens="18-35" string="Giuliano , a Beatles aficionado who lives in this Erie Canal city about 25 miles north of Buffalo" id_sentence="6" />
        <mention ids_tokens="18" string="Giuliano" id_sentence="6" />
        <mention ids_tokens="20-35" string="a Beatles aficionado who lives in this Erie Canal city about 25 miles north of Buffalo" id_sentence="6" />
        <mention ids_tokens="1" string="Giuliano" id_sentence="8" />
        <mention ids_tokens="4-21" string="Giuliano , who was in Liverpool to promote his first book , `` The Beatles : A Celebration" id_sentence="24" />
        <mention ids_tokens="4" string="Giuliano" id_sentence="24" />
        <mention ids_tokens="12" string="his" id_sentence="24" />
        <mention ids_tokens="1-7" string="Giuliano , 35 , a self-proclaimed huckster" id_sentence="25" />
        <mention ids_tokens="1" string="Giuliano" id_sentence="25" />
        <mention ids_tokens="1" string="Giuliano" id_sentence="41" />
        <mention ids_tokens="14" string="Giuliano" id_sentence="42" />
        <mention ids_tokens="10" string="Giuliano" id_sentence="51" />
      </mentions>
    </coreference>
    <coreference id="8" type="LIST">
      <referenced ids_tokens="8-9-10-11-12-13-14-15-16-17-18" string="young John Lennon , Baird 's half-brother and Giuliano 's idol" id_sentence="2" />
      <mentions>
        <mention ids_tokens="15-22" string="Lennon , Paul McCartney and George Harrison drop" id_sentence="17" />
      </mentions>
    </coreference>
    <coreference id="9" type="NOMINAL">
      <referenced ids_tokens="2-3-4" string="the neglected urchin" id_sentence="3" />
      <mentions>
        <mention ids_tokens="4" string="my" id_sentence="5" />
        <mention ids_tokens="10" string="my" id_sentence="5" />
        <mention ids_tokens="16" string="my" id_sentence="5" />
      </mentions>
    </coreference>
    <coreference id="10" type="PROPER">
      <referenced ids_tokens="26-27-28" string="big brother John" id_sentence="16" />
      <mentions>
        <mention ids_tokens="13" string="John" id_sentence="4" />
        <mention ids_tokens="16" string="John" id_sentence="4" />
        <mention ids_tokens="2" string="I" id_sentence="12" />
        <mention ids_tokens="8" string="John" id_sentence="12" />
        <mention ids_tokens="2" string="I" id_sentence="13" />
        <mention ids_tokens="18" string="John" id_sentence="13" />
        <mention ids_tokens="1" string="I" id_sentence="14" />
        <mention ids_tokens="3" string="my" id_sentence="14" />
        <mention ids_tokens="25-26" string="John Dykins" id_sentence="15" />
        <mention ids_tokens="40" string="Dykins" id_sentence="18" />
      </mentions>
    </coreference>
    <coreference id="11" type="NOMINAL">
      <referenced ids_tokens="2-3-4" string="That woman laughing" id_sentence="4" />
      <mentions>
        <mention ids_tokens="1" string="That" id_sentence="5" />
      </mentions>
    </coreference>
    <coreference id="14" type="NOMINAL">
      <referenced ids_tokens="4-5-6-7-8-9-10-11-12-13-14-15-16-17-18-19" string="my brother , that 's not my mother , that 's not my children 's grandmother" id_sentence="5" />
      <mentions>
        <mention ids_tokens="12" string="My" id_sentence="6" />
      </mentions>
    </coreference>
    <coreference id="15" type="PROPER">
      <referenced ids_tokens="12-13" string="My Brother" id_sentence="6" />
      <mentions>
        <mention ids_tokens="17-19" string="an aspiring brother" id_sentence="30" />
        <mention ids_tokens="9" string="My" id_sentence="47" />
        <mention ids_tokens="10" string="Brother" id_sentence="47" />
        <mention ids_tokens="18" string="My" id_sentence="47" />
      </mentions>
    </coreference>
    <coreference id="17" type="NOMINAL">
      <referenced ids_tokens="1-2" string="The result" id_sentence="10" />
      <mentions>
        <mention ids_tokens="6" string="it" id_sentence="12" />
      </mentions>
    </coreference>
    <coreference id="22" type="NOMINAL">
      <referenced ids_tokens="15-16-17-18-19-20-21-22-23-24-25-26-27-28" string="an extended family of aunts , uncles , cousins _ and big brother John" id_sentence="16" />
      <mentions>
        <mention ids_tokens="18-20" string="the family's" id_sentence="33" />
      </mentions>
    </coreference>
    <coreference id="23" type="PROPER">
      <referenced ids_tokens="17-18" string="Paul McCartney" id_sentence="17" />
      <mentions>
        <mention ids_tokens="11" string="McCartney" id_sentence="26" />
      </mentions>
    </coreference>
    <coreference id="24" type="NOMINAL">
      <referenced ids_tokens="13-14-15-16-17-18" string="a family album than a biography" id_sentence="28" />
      <mentions>
        <mention ids_tokens="1-2" string="The book" id_sentence="17" />
        <mention ids_tokens="1" string="It" id_sentence="18" />
        <mention ids_tokens="3" string="it" id_sentence="19" />
        <mention ids_tokens="1-2" string="The book" id_sentence="20" />
        <mention ids_tokens="4" string="its" id_sentence="20" />
        <mention ids_tokens="14-15" string="the book" id_sentence="27" />
        <mention ids_tokens="3-4" string="the book" id_sentence="30" />
        <mention ids_tokens="6-19" string="a narrow view of Lennon , a child's recollections of an aspiring brother" id_sentence="30" />
        <mention ids_tokens="4-5" string="the book" id_sentence="32" />
        <mention ids_tokens="8-9" string="the book" id_sentence="39" />
        <mention ids_tokens="16" string="it" id_sentence="40" />
        <mention ids_tokens="5" string="it" id_sentence="42" />
        <mention ids_tokens="1-2" string="The book" id_sentence="50" />
        <mention ids_tokens="3" string="itself" id_sentence="50" />
        <mention ids_tokens="5-17" string="a loving look through rose-colored specs of a brother whose art touched millions" id_sentence="50" />
        <mention ids_tokens="2" string="It" id_sentence="51" />
      </mentions>
    </coreference>
    <coreference id="25" type="NOMINAL">
      <referenced ids_tokens="27-28-29-30-31" string="her brother , the musician" id_sentence="29" />
      <mentions>
        <mention ids_tokens="4-56" string="an alternately thoughtless and protective brother who leaves his step-sisters at the movies for hours while he goes on a date , who between songs at a Beatles concert shouts for security guards to lift the Dykins girls out of the front row , away from screaming fans threatening to storm the stage" id_sentence="18" />
      </mentions>
    </coreference>
    <coreference id="26" type="NOMINAL">
      <referenced ids_tokens="7-8-9" string="an unfit mother" id_sentence="21" />
      <mentions>
        <mention ids_tokens="18-20" string="their mother's" id_sentence="49" />
      </mentions>
    </coreference>
    <coreference id="30" type="NOMINAL">
      <referenced ids_tokens="18-19-20-21-22-23-24-25-26" string="the family 's story of Lennon 's early years" id_sentence="33" />
      <mentions>
        <mention ids_tokens="18-19" string="your story" id_sentence="35" />
      </mentions>
    </coreference>
    <coreference id="31" type="PROPER">
      <referenced ids_tokens="23-24-25-26" string="Lennon 's early years" id_sentence="33" />
      <mentions>
        <mention ids_tokens="2" string="I" id_sentence="34" />
      </mentions>
    </coreference>
    <coreference id="33" type="PRONOMINAL">
      <referenced ids_tokens="18" string="your" id_sentence="35" />
      <mentions>
        <mention ids_tokens="14" string="she" id_sentence="38" />
      </mentions>
    </coreference>
    <coreference id="34" type="PROPER">
      <referenced ids_tokens="6-7" string="Albert Goldman" id_sentence="37" />
      <mentions>
        <mention ids_tokens="2" string="Goldman" id_sentence="43" />
      </mentions>
    </coreference>
    <coreference id="35" type="PROPER">
      <referenced ids_tokens="6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-21-22-23-24-25-26-27-28-29-30" string="Albert Goldman , whose sordid biography , `` The Lives of John Lennon , '' hit bookstores about the same time as Baird 's book" id_sentence="37" />
      <mentions>
        <mention ids_tokens="5-6" string="Goldman's" id_sentence="38" />
        <mention ids_tokens="20-31" string="Goldman , who brands Lennon homosexual , anorexic and possibly a murderer" id_sentence="38" />
        <mention ids_tokens="20" string="Goldman" id_sentence="38" />
        <mention ids_tokens="5" string="Goldman" id_sentence="39" />
        <mention ids_tokens="6" string="Goldman" id_sentence="40" />
        <mention ids_tokens="2" string="Goldman" id_sentence="42" />
      </mentions>
    </coreference>
    <coreference id="36" type="NOMINAL">
      <referenced ids_tokens="29-30-31" string="possibly a murderer" id_sentence="38" />
      <mentions>
        <mention ids_tokens="2" string="I" id_sentence="39" />
        <mention ids_tokens="15" string="I" id_sentence="39" />
      </mentions>
    </coreference>
    <coreference id="38" type="NOMINAL">
      <referenced ids_tokens="7-8-9-10-11-12-13" string="the more sleaze , the more dough" id_sentence="43" />
      <mentions>
        <mention ids_tokens="2" string="herself" id_sentence="46" />
        <mention ids_tokens="1" string="She" id_sentence="47" />
        <mention ids_tokens="17" string="she" id_sentence="48" />
        <mention ids_tokens="19" string="she" id_sentence="48" />
        <mention ids_tokens="14" string="she" id_sentence="49" />
      </mentions>
    </coreference>
    <coreference id="39" type="NOMINAL">
      <referenced ids_tokens="7-8-9" string="the more sleaze" id_sentence="43" />
      <mentions>
        <mention ids_tokens="4" string="his" id_sentence="44" />
        <mention ids_tokens="2" string="he" id_sentence="45" />
        <mention ids_tokens="7" string="he" id_sentence="45" />
      </mentions>
    </coreference>
    <coreference id="40" type="NOMINAL">
      <referenced ids_tokens="11-12-13" string="the more dough" id_sentence="43" />
      <mentions>
        <mention ids_tokens="9-11" string="the most dough" id_sentence="45" />
      </mentions>
    </coreference>
    <coreference id="41" type="PRONOMINAL">
      <referenced ids_tokens="23" string="them" id_sentence="48" />
      <mentions>
        <mention ids_tokens="1" string="They" id_sentence="49" />
      </mentions>
    </coreference>
    <coreference id="44" type="NOMINAL">
      <referenced ids_tokens="5-6-7" string="a little slice" id_sentence="52" />
      <mentions>
        <mention ids_tokens="1" string="It" id_sentence="53" />
        <mention ids_tokens="1" string="It" id_sentence="54" />
        <mention ids_tokens="3-5" string="a sympathetic view" id_sentence="54" />
      </mentions>
    </coreference>
  </coreferences>
</document>
