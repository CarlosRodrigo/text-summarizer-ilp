<?xml version="1.0" encoding="UTF-8"?>
<document id="0" name="LA042789-0068">
  <sentences>
    <sentence id="1" has_coreference="true">
      <content>Lucille Ball, whose death Wednesday morning at the age of 77 will be the most widely and deeply felt show-business loss in recent memory, was the movies&amp;apost; greatest gift to television.</content>
      <tokens>
        <token id="1" string="Lucille" lemma="Lucille" stem="lucil" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="2" string="Ball" lemma="Ball" stem="ball" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="whose" lemma="whose" stem="whose" pos="WP$" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="death" lemma="death" stem="death" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="Wednesday" lemma="Wednesday" stem="wednesdai" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="7" string="morning" lemma="morning" stem="morn" pos="NN" type="Word" isStopWord="false" ner="TIME" is_referenced="false" is_refers="false" />
        <token id="8" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="age" lemma="age" stem="ag" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="77" lemma="77" stem="77" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="13" string="will" lemma="will" stem="will" pos="MD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="be" lemma="be" stem="be" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="most" lemma="most" stem="most" pos="RBS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="widely" lemma="widely" stem="wide" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="deeply" lemma="deeply" stem="deepli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="felt" lemma="feel" stem="felt" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="show-business" lemma="show-business" stem="show-busi" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="loss" lemma="loss" stem="loss" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="recent" lemma="recent" stem="recent" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="memory" lemma="memory" stem="memori" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="29" string="movies" lemma="movie" stem="movi" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="30" string="'" lemma="'" stem="'" pos="POS" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="31" string="greatest" lemma="greatest" stem="greatest" pos="JJS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="gift" lemma="gift" stem="gift" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="television" lemma="television" stem="televis" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Lucille) (NNP Ball)) (, ,) (SBAR (WHNP (NP (WP$ whose) (NN death)) (PP (NP (NNP Wednesday) (NN morning)) (PP (IN at) (NP (NP (DT the) (NN age)) (PP (IN of) (NP (CD 77))))))) (S (VP (VP (MD will) (VP (VB be) (NP (DT the) (RBS most)) (ADVP (RB widely)))) (CC and) (VP (ADVP (RB deeply)) (VBD felt) (NP (JJ show-business) (NN loss)) (PP (IN in) (NP (JJ recent) (NN memory))))))) (, ,)) (VP (VBD was) (NP (NP (DT the) (NNS movies) (POS ')) (JJS greatest) (NN gift)) (PP (TO to) (NP (NN television)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="whose death Wednesday morning at the age of 77 will be the most widely and deeply felt show-business loss in recent memory" type="SBAR">
          <tokens>
            <token id="4" string="whose" />
            <token id="5" string="death" />
            <token id="6" string="Wednesday" />
            <token id="7" string="morning" />
            <token id="8" string="at" />
            <token id="9" string="the" />
            <token id="10" string="age" />
            <token id="11" string="of" />
            <token id="12" string="77" />
            <token id="13" string="will" />
            <token id="14" string="be" />
            <token id="15" string="the" />
            <token id="16" string="most" />
            <token id="17" string="widely" />
            <token id="18" string="and" />
            <token id="19" string="deeply" />
            <token id="20" string="felt" />
            <token id="21" string="show-business" />
            <token id="22" string="loss" />
            <token id="23" string="in" />
            <token id="24" string="recent" />
            <token id="25" string="memory" />
          </tokens>
        </chunking>
        <chunking id="2" string="be the most widely" type="VP">
          <tokens>
            <token id="14" string="be" />
            <token id="15" string="the" />
            <token id="16" string="most" />
            <token id="17" string="widely" />
          </tokens>
        </chunking>
        <chunking id="3" string="Wednesday morning" type="NP">
          <tokens>
            <token id="6" string="Wednesday" />
            <token id="7" string="morning" />
          </tokens>
        </chunking>
        <chunking id="4" string="the age" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="age" />
          </tokens>
        </chunking>
        <chunking id="5" string="show-business loss" type="NP">
          <tokens>
            <token id="21" string="show-business" />
            <token id="22" string="loss" />
          </tokens>
        </chunking>
        <chunking id="6" string="television" type="NP">
          <tokens>
            <token id="34" string="television" />
          </tokens>
        </chunking>
        <chunking id="7" string="will be the most widely" type="VP">
          <tokens>
            <token id="13" string="will" />
            <token id="14" string="be" />
            <token id="15" string="the" />
            <token id="16" string="most" />
            <token id="17" string="widely" />
          </tokens>
        </chunking>
        <chunking id="8" string="deeply felt show-business loss in recent memory" type="VP">
          <tokens>
            <token id="19" string="deeply" />
            <token id="20" string="felt" />
            <token id="21" string="show-business" />
            <token id="22" string="loss" />
            <token id="23" string="in" />
            <token id="24" string="recent" />
            <token id="25" string="memory" />
          </tokens>
        </chunking>
        <chunking id="9" string="the age of 77" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="age" />
            <token id="11" string="of" />
            <token id="12" string="77" />
          </tokens>
        </chunking>
        <chunking id="10" string="the movies '" type="NP">
          <tokens>
            <token id="28" string="the" />
            <token id="29" string="movies" />
            <token id="30" string="'" />
          </tokens>
        </chunking>
        <chunking id="11" string="will be the most widely and deeply felt show-business loss in recent memory" type="VP">
          <tokens>
            <token id="13" string="will" />
            <token id="14" string="be" />
            <token id="15" string="the" />
            <token id="16" string="most" />
            <token id="17" string="widely" />
            <token id="18" string="and" />
            <token id="19" string="deeply" />
            <token id="20" string="felt" />
            <token id="21" string="show-business" />
            <token id="22" string="loss" />
            <token id="23" string="in" />
            <token id="24" string="recent" />
            <token id="25" string="memory" />
          </tokens>
        </chunking>
        <chunking id="12" string="the most" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="most" />
          </tokens>
        </chunking>
        <chunking id="13" string="recent memory" type="NP">
          <tokens>
            <token id="24" string="recent" />
            <token id="25" string="memory" />
          </tokens>
        </chunking>
        <chunking id="14" string="Lucille Ball , whose death Wednesday morning at the age of 77 will be the most widely and deeply felt show-business loss in recent memory ," type="NP">
          <tokens>
            <token id="1" string="Lucille" />
            <token id="2" string="Ball" />
            <token id="3" string="," />
            <token id="4" string="whose" />
            <token id="5" string="death" />
            <token id="6" string="Wednesday" />
            <token id="7" string="morning" />
            <token id="8" string="at" />
            <token id="9" string="the" />
            <token id="10" string="age" />
            <token id="11" string="of" />
            <token id="12" string="77" />
            <token id="13" string="will" />
            <token id="14" string="be" />
            <token id="15" string="the" />
            <token id="16" string="most" />
            <token id="17" string="widely" />
            <token id="18" string="and" />
            <token id="19" string="deeply" />
            <token id="20" string="felt" />
            <token id="21" string="show-business" />
            <token id="22" string="loss" />
            <token id="23" string="in" />
            <token id="24" string="recent" />
            <token id="25" string="memory" />
            <token id="26" string="," />
          </tokens>
        </chunking>
        <chunking id="15" string="was the movies ' greatest gift to television" type="VP">
          <tokens>
            <token id="27" string="was" />
            <token id="28" string="the" />
            <token id="29" string="movies" />
            <token id="30" string="'" />
            <token id="31" string="greatest" />
            <token id="32" string="gift" />
            <token id="33" string="to" />
            <token id="34" string="television" />
          </tokens>
        </chunking>
        <chunking id="16" string="whose death" type="NP">
          <tokens>
            <token id="4" string="whose" />
            <token id="5" string="death" />
          </tokens>
        </chunking>
        <chunking id="17" string="the movies ' greatest gift" type="NP">
          <tokens>
            <token id="28" string="the" />
            <token id="29" string="movies" />
            <token id="30" string="'" />
            <token id="31" string="greatest" />
            <token id="32" string="gift" />
          </tokens>
        </chunking>
        <chunking id="18" string="Lucille Ball" type="NP">
          <tokens>
            <token id="1" string="Lucille" />
            <token id="2" string="Ball" />
          </tokens>
        </chunking>
        <chunking id="19" string="77" type="NP">
          <tokens>
            <token id="12" string="77" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">Ball</governor>
          <dependent id="1">Lucille</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="32">gift</governor>
          <dependent id="2">Ball</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="5">death</governor>
          <dependent id="4">whose</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">the</governor>
          <dependent id="5">death</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">morning</governor>
          <dependent id="6">Wednesday</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">death</governor>
          <dependent id="7">morning</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">age</governor>
          <dependent id="8">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">age</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">morning</governor>
          <dependent id="10">age</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">77</governor>
          <dependent id="11">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">age</governor>
          <dependent id="12">77</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="15">the</governor>
          <dependent id="13">will</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="15">the</governor>
          <dependent id="14">be</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="2">Ball</governor>
          <dependent id="15">the</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="15">the</governor>
          <dependent id="16">most</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="15">the</governor>
          <dependent id="17">widely</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="15">the</governor>
          <dependent id="18">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="20">felt</governor>
          <dependent id="19">deeply</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="15">the</governor>
          <dependent id="20">felt</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="22">loss</governor>
          <dependent id="21">show-business</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="20">felt</governor>
          <dependent id="22">loss</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">memory</governor>
          <dependent id="23">in</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="25">memory</governor>
          <dependent id="24">recent</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">felt</governor>
          <dependent id="25">memory</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="32">gift</governor>
          <dependent id="27">was</dependent>
        </dependency>
        <dependency type="det">
          <governor id="29">movies</governor>
          <dependent id="28">the</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="32">gift</governor>
          <dependent id="29">movies</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">movies</governor>
          <dependent id="30">'</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="32">gift</governor>
          <dependent id="31">greatest</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="32">gift</dependent>
        </dependency>
        <dependency type="case">
          <governor id="34">television</governor>
          <dependent id="33">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="32">gift</governor>
          <dependent id="34">television</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Wednesday" type="DATE" score="0.0">
          <tokens>
            <token id="6" string="Wednesday" />
          </tokens>
        </entity>
        <entity id="2" string="morning" type="TIME" score="0.0">
          <tokens>
            <token id="7" string="morning" />
          </tokens>
        </entity>
        <entity id="3" string="Lucille Ball" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Lucille" />
            <token id="2" string="Ball" />
          </tokens>
        </entity>
        <entity id="4" string="77" type="NUMBER" score="0.0">
          <tokens>
            <token id="12" string="77" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="2" has_coreference="true">
      <content>It turned out to be one of Sam Goldwyn&amp;apost;s finest and most prophetic hours when he signed her as one of his Goldwyn Girls to appear in &amp;quot;Roman Scandals&amp;quot; with Eddie Cantor in 1933.</content>
      <tokens>
        <token id="1" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="turned" lemma="turn" stem="turn" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="out" lemma="out" stem="out" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="be" lemma="be" stem="be" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="one" lemma="one" stem="on" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="7" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Sam" lemma="Sam" stem="sam" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="9" string="Goldwyn" lemma="Goldwyn" stem="goldwyn" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="10" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="finest" lemma="finest" stem="finest" pos="JJS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="most" lemma="most" stem="most" pos="RBS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="prophetic" lemma="prophetic" stem="prophet" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="hours" lemma="hour" stem="hour" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="16" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="signed" lemma="sign" stem="sign" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="her" lemma="she" stem="her" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="20" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="one" lemma="one" stem="on" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="22" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="Goldwyn" lemma="Goldwyn" stem="goldwyn" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="Girls" lemma="girl" stem="girl" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="appear" lemma="appear" stem="appear" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="Roman" lemma="roman" stem="roman" pos="JJ" type="Word" isStopWord="false" ner="MISC" is_referenced="true" is_refers="true" />
        <token id="31" string="Scandals" lemma="scandal" stem="scandal" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="32" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="Eddie" lemma="Eddie" stem="eddie" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="35" string="Cantor" lemma="Cantor" stem="cantor" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="36" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="1933" lemma="1933" stem="1933" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="38" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP It)) (VP (VBD turned) (PRT (RP out)) (S (VP (TO to) (VP (VB be) (NP (NP (CD one)) (PP (IN of) (NP (NP (NP (NNP Sam) (NNP Goldwyn) (POS 's)) (JJS finest)) (CC and) (NP (ADJP (RBS most) (JJ prophetic)) (NNS hours)) (SBAR (WHADVP (WRB when)) (S (NP (PRP he)) (VP (VBD signed) (NP (PRP her)) (PP (IN as) (NP (NP (CD one)) (PP (IN of) (NP (PRP$ his) (NNP Goldwyn) (NNS Girls))))) (S (VP (TO to) (VP (VB appear) (PP (IN in) (`` ``) (NP (JJ Roman) (NNS Scandals)) ('' '')) (PP (IN with) (NP (NP (NNP Eddie) (NNP Cantor)) (PP (IN in) (NP (CD 1933)))))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Sam Goldwyn 's finest" type="NP">
          <tokens>
            <token id="8" string="Sam" />
            <token id="9" string="Goldwyn" />
            <token id="10" string="'s" />
            <token id="11" string="finest" />
          </tokens>
        </chunking>
        <chunking id="2" string="Eddie Cantor in 1933" type="NP">
          <tokens>
            <token id="34" string="Eddie" />
            <token id="35" string="Cantor" />
            <token id="36" string="in" />
            <token id="37" string="1933" />
          </tokens>
        </chunking>
        <chunking id="3" string="one" type="NP">
          <tokens>
            <token id="6" string="one" />
          </tokens>
        </chunking>
        <chunking id="4" string="be one of Sam Goldwyn 's finest and most prophetic hours when he signed her as one of his Goldwyn Girls to appear in `` Roman Scandals '' with Eddie Cantor in 1933" type="VP">
          <tokens>
            <token id="5" string="be" />
            <token id="6" string="one" />
            <token id="7" string="of" />
            <token id="8" string="Sam" />
            <token id="9" string="Goldwyn" />
            <token id="10" string="'s" />
            <token id="11" string="finest" />
            <token id="12" string="and" />
            <token id="13" string="most" />
            <token id="14" string="prophetic" />
            <token id="15" string="hours" />
            <token id="16" string="when" />
            <token id="17" string="he" />
            <token id="18" string="signed" />
            <token id="19" string="her" />
            <token id="20" string="as" />
            <token id="21" string="one" />
            <token id="22" string="of" />
            <token id="23" string="his" />
            <token id="24" string="Goldwyn" />
            <token id="25" string="Girls" />
            <token id="26" string="to" />
            <token id="27" string="appear" />
            <token id="28" string="in" />
            <token id="29" string="&quot;" />
            <token id="30" string="Roman" />
            <token id="31" string="Scandals" />
            <token id="32" string="&quot;" />
            <token id="33" string="with" />
            <token id="34" string="Eddie" />
            <token id="35" string="Cantor" />
            <token id="36" string="in" />
            <token id="37" string="1933" />
          </tokens>
        </chunking>
        <chunking id="5" string="turned out to be one of Sam Goldwyn 's finest and most prophetic hours when he signed her as one of his Goldwyn Girls to appear in `` Roman Scandals '' with Eddie Cantor in 1933" type="VP">
          <tokens>
            <token id="2" string="turned" />
            <token id="3" string="out" />
            <token id="4" string="to" />
            <token id="5" string="be" />
            <token id="6" string="one" />
            <token id="7" string="of" />
            <token id="8" string="Sam" />
            <token id="9" string="Goldwyn" />
            <token id="10" string="'s" />
            <token id="11" string="finest" />
            <token id="12" string="and" />
            <token id="13" string="most" />
            <token id="14" string="prophetic" />
            <token id="15" string="hours" />
            <token id="16" string="when" />
            <token id="17" string="he" />
            <token id="18" string="signed" />
            <token id="19" string="her" />
            <token id="20" string="as" />
            <token id="21" string="one" />
            <token id="22" string="of" />
            <token id="23" string="his" />
            <token id="24" string="Goldwyn" />
            <token id="25" string="Girls" />
            <token id="26" string="to" />
            <token id="27" string="appear" />
            <token id="28" string="in" />
            <token id="29" string="&quot;" />
            <token id="30" string="Roman" />
            <token id="31" string="Scandals" />
            <token id="32" string="&quot;" />
            <token id="33" string="with" />
            <token id="34" string="Eddie" />
            <token id="35" string="Cantor" />
            <token id="36" string="in" />
            <token id="37" string="1933" />
          </tokens>
        </chunking>
        <chunking id="6" string="It" type="NP">
          <tokens>
            <token id="1" string="It" />
          </tokens>
        </chunking>
        <chunking id="7" string="to be one of Sam Goldwyn 's finest and most prophetic hours when he signed her as one of his Goldwyn Girls to appear in `` Roman Scandals '' with Eddie Cantor in 1933" type="VP">
          <tokens>
            <token id="4" string="to" />
            <token id="5" string="be" />
            <token id="6" string="one" />
            <token id="7" string="of" />
            <token id="8" string="Sam" />
            <token id="9" string="Goldwyn" />
            <token id="10" string="'s" />
            <token id="11" string="finest" />
            <token id="12" string="and" />
            <token id="13" string="most" />
            <token id="14" string="prophetic" />
            <token id="15" string="hours" />
            <token id="16" string="when" />
            <token id="17" string="he" />
            <token id="18" string="signed" />
            <token id="19" string="her" />
            <token id="20" string="as" />
            <token id="21" string="one" />
            <token id="22" string="of" />
            <token id="23" string="his" />
            <token id="24" string="Goldwyn" />
            <token id="25" string="Girls" />
            <token id="26" string="to" />
            <token id="27" string="appear" />
            <token id="28" string="in" />
            <token id="29" string="&quot;" />
            <token id="30" string="Roman" />
            <token id="31" string="Scandals" />
            <token id="32" string="&quot;" />
            <token id="33" string="with" />
            <token id="34" string="Eddie" />
            <token id="35" string="Cantor" />
            <token id="36" string="in" />
            <token id="37" string="1933" />
          </tokens>
        </chunking>
        <chunking id="8" string="Sam Goldwyn 's" type="NP">
          <tokens>
            <token id="8" string="Sam" />
            <token id="9" string="Goldwyn" />
            <token id="10" string="'s" />
          </tokens>
        </chunking>
        <chunking id="9" string="Roman Scandals" type="NP">
          <tokens>
            <token id="30" string="Roman" />
            <token id="31" string="Scandals" />
          </tokens>
        </chunking>
        <chunking id="10" string="one of Sam Goldwyn 's finest and most prophetic hours when he signed her as one of his Goldwyn Girls to appear in `` Roman Scandals '' with Eddie Cantor in 1933" type="NP">
          <tokens>
            <token id="6" string="one" />
            <token id="7" string="of" />
            <token id="8" string="Sam" />
            <token id="9" string="Goldwyn" />
            <token id="10" string="'s" />
            <token id="11" string="finest" />
            <token id="12" string="and" />
            <token id="13" string="most" />
            <token id="14" string="prophetic" />
            <token id="15" string="hours" />
            <token id="16" string="when" />
            <token id="17" string="he" />
            <token id="18" string="signed" />
            <token id="19" string="her" />
            <token id="20" string="as" />
            <token id="21" string="one" />
            <token id="22" string="of" />
            <token id="23" string="his" />
            <token id="24" string="Goldwyn" />
            <token id="25" string="Girls" />
            <token id="26" string="to" />
            <token id="27" string="appear" />
            <token id="28" string="in" />
            <token id="29" string="&quot;" />
            <token id="30" string="Roman" />
            <token id="31" string="Scandals" />
            <token id="32" string="&quot;" />
            <token id="33" string="with" />
            <token id="34" string="Eddie" />
            <token id="35" string="Cantor" />
            <token id="36" string="in" />
            <token id="37" string="1933" />
          </tokens>
        </chunking>
        <chunking id="11" string="when" type="WHADVP">
          <tokens>
            <token id="16" string="when" />
          </tokens>
        </chunking>
        <chunking id="12" string="Sam Goldwyn 's finest and most prophetic hours when he signed her as one of his Goldwyn Girls to appear in `` Roman Scandals '' with Eddie Cantor in 1933" type="NP">
          <tokens>
            <token id="8" string="Sam" />
            <token id="9" string="Goldwyn" />
            <token id="10" string="'s" />
            <token id="11" string="finest" />
            <token id="12" string="and" />
            <token id="13" string="most" />
            <token id="14" string="prophetic" />
            <token id="15" string="hours" />
            <token id="16" string="when" />
            <token id="17" string="he" />
            <token id="18" string="signed" />
            <token id="19" string="her" />
            <token id="20" string="as" />
            <token id="21" string="one" />
            <token id="22" string="of" />
            <token id="23" string="his" />
            <token id="24" string="Goldwyn" />
            <token id="25" string="Girls" />
            <token id="26" string="to" />
            <token id="27" string="appear" />
            <token id="28" string="in" />
            <token id="29" string="&quot;" />
            <token id="30" string="Roman" />
            <token id="31" string="Scandals" />
            <token id="32" string="&quot;" />
            <token id="33" string="with" />
            <token id="34" string="Eddie" />
            <token id="35" string="Cantor" />
            <token id="36" string="in" />
            <token id="37" string="1933" />
          </tokens>
        </chunking>
        <chunking id="13" string="1933" type="NP">
          <tokens>
            <token id="37" string="1933" />
          </tokens>
        </chunking>
        <chunking id="14" string="to appear in `` Roman Scandals '' with Eddie Cantor in 1933" type="VP">
          <tokens>
            <token id="26" string="to" />
            <token id="27" string="appear" />
            <token id="28" string="in" />
            <token id="29" string="&quot;" />
            <token id="30" string="Roman" />
            <token id="31" string="Scandals" />
            <token id="32" string="&quot;" />
            <token id="33" string="with" />
            <token id="34" string="Eddie" />
            <token id="35" string="Cantor" />
            <token id="36" string="in" />
            <token id="37" string="1933" />
          </tokens>
        </chunking>
        <chunking id="15" string="Eddie Cantor" type="NP">
          <tokens>
            <token id="34" string="Eddie" />
            <token id="35" string="Cantor" />
          </tokens>
        </chunking>
        <chunking id="16" string="her" type="NP">
          <tokens>
            <token id="19" string="her" />
          </tokens>
        </chunking>
        <chunking id="17" string="one of his Goldwyn Girls" type="NP">
          <tokens>
            <token id="21" string="one" />
            <token id="22" string="of" />
            <token id="23" string="his" />
            <token id="24" string="Goldwyn" />
            <token id="25" string="Girls" />
          </tokens>
        </chunking>
        <chunking id="18" string="when he signed her as one of his Goldwyn Girls to appear in `` Roman Scandals '' with Eddie Cantor in 1933" type="SBAR">
          <tokens>
            <token id="16" string="when" />
            <token id="17" string="he" />
            <token id="18" string="signed" />
            <token id="19" string="her" />
            <token id="20" string="as" />
            <token id="21" string="one" />
            <token id="22" string="of" />
            <token id="23" string="his" />
            <token id="24" string="Goldwyn" />
            <token id="25" string="Girls" />
            <token id="26" string="to" />
            <token id="27" string="appear" />
            <token id="28" string="in" />
            <token id="29" string="&quot;" />
            <token id="30" string="Roman" />
            <token id="31" string="Scandals" />
            <token id="32" string="&quot;" />
            <token id="33" string="with" />
            <token id="34" string="Eddie" />
            <token id="35" string="Cantor" />
            <token id="36" string="in" />
            <token id="37" string="1933" />
          </tokens>
        </chunking>
        <chunking id="19" string="he" type="NP">
          <tokens>
            <token id="17" string="he" />
          </tokens>
        </chunking>
        <chunking id="20" string="most prophetic" type="ADJP">
          <tokens>
            <token id="13" string="most" />
            <token id="14" string="prophetic" />
          </tokens>
        </chunking>
        <chunking id="21" string="his Goldwyn Girls" type="NP">
          <tokens>
            <token id="23" string="his" />
            <token id="24" string="Goldwyn" />
            <token id="25" string="Girls" />
          </tokens>
        </chunking>
        <chunking id="22" string="appear in `` Roman Scandals '' with Eddie Cantor in 1933" type="VP">
          <tokens>
            <token id="27" string="appear" />
            <token id="28" string="in" />
            <token id="29" string="&quot;" />
            <token id="30" string="Roman" />
            <token id="31" string="Scandals" />
            <token id="32" string="&quot;" />
            <token id="33" string="with" />
            <token id="34" string="Eddie" />
            <token id="35" string="Cantor" />
            <token id="36" string="in" />
            <token id="37" string="1933" />
          </tokens>
        </chunking>
        <chunking id="23" string="most prophetic hours" type="NP">
          <tokens>
            <token id="13" string="most" />
            <token id="14" string="prophetic" />
            <token id="15" string="hours" />
          </tokens>
        </chunking>
        <chunking id="24" string="signed her as one of his Goldwyn Girls to appear in `` Roman Scandals '' with Eddie Cantor in 1933" type="VP">
          <tokens>
            <token id="18" string="signed" />
            <token id="19" string="her" />
            <token id="20" string="as" />
            <token id="21" string="one" />
            <token id="22" string="of" />
            <token id="23" string="his" />
            <token id="24" string="Goldwyn" />
            <token id="25" string="Girls" />
            <token id="26" string="to" />
            <token id="27" string="appear" />
            <token id="28" string="in" />
            <token id="29" string="&quot;" />
            <token id="30" string="Roman" />
            <token id="31" string="Scandals" />
            <token id="32" string="&quot;" />
            <token id="33" string="with" />
            <token id="34" string="Eddie" />
            <token id="35" string="Cantor" />
            <token id="36" string="in" />
            <token id="37" string="1933" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">turned</governor>
          <dependent id="1">It</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">turned</dependent>
        </dependency>
        <dependency type="compound:prt">
          <governor id="2">turned</governor>
          <dependent id="3">out</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="6">one</governor>
          <dependent id="4">to</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="6">one</governor>
          <dependent id="5">be</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="2">turned</governor>
          <dependent id="6">one</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">Goldwyn</governor>
          <dependent id="7">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">Goldwyn</governor>
          <dependent id="8">Sam</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">one</governor>
          <dependent id="9">Goldwyn</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">Goldwyn</governor>
          <dependent id="10">'s</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">Goldwyn</governor>
          <dependent id="11">finest</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="9">Goldwyn</governor>
          <dependent id="12">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="14">prophetic</governor>
          <dependent id="13">most</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">hours</governor>
          <dependent id="14">prophetic</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="9">Goldwyn</governor>
          <dependent id="15">hours</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="18">signed</governor>
          <dependent id="16">when</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="18">signed</governor>
          <dependent id="17">he</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="9">Goldwyn</governor>
          <dependent id="18">signed</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="18">signed</governor>
          <dependent id="19">her</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">one</governor>
          <dependent id="20">as</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="18">signed</governor>
          <dependent id="21">one</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">Girls</governor>
          <dependent id="22">of</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="25">Girls</governor>
          <dependent id="23">his</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">Girls</governor>
          <dependent id="24">Goldwyn</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="21">one</governor>
          <dependent id="25">Girls</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="27">appear</governor>
          <dependent id="26">to</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="18">signed</governor>
          <dependent id="27">appear</dependent>
        </dependency>
        <dependency type="case">
          <governor id="31">Scandals</governor>
          <dependent id="28">in</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="31">Scandals</governor>
          <dependent id="30">Roman</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">appear</governor>
          <dependent id="31">Scandals</dependent>
        </dependency>
        <dependency type="case">
          <governor id="35">Cantor</governor>
          <dependent id="33">with</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="35">Cantor</governor>
          <dependent id="34">Eddie</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">appear</governor>
          <dependent id="35">Cantor</dependent>
        </dependency>
        <dependency type="case">
          <governor id="37">1933</governor>
          <dependent id="36">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="35">Cantor</governor>
          <dependent id="37">1933</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="1933" type="DATE" score="0.0">
          <tokens>
            <token id="37" string="1933" />
          </tokens>
        </entity>
        <entity id="2" string="Sam Goldwyn" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Sam" />
            <token id="9" string="Goldwyn" />
          </tokens>
        </entity>
        <entity id="3" string="hours" type="DURATION" score="0.0">
          <tokens>
            <token id="15" string="hours" />
          </tokens>
        </entity>
        <entity id="4" string="Eddie Cantor" type="PERSON" score="0.0">
          <tokens>
            <token id="34" string="Eddie" />
            <token id="35" string="Cantor" />
          </tokens>
        </entity>
        <entity id="5" string="one" type="NUMBER" score="0.0">
          <tokens>
            <token id="6" string="one" />
          </tokens>
        </entity>
        <entity id="6" string="Roman" type="MISC" score="0.0">
          <tokens>
            <token id="30" string="Roman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="3" has_coreference="true">
      <content>She had had bit parts in two earlier films but &amp;quot;Roman Scandals&amp;quot; was the one that gave her a firmer foothold on the slippery slopes of a Hollywood career.</content>
      <tokens>
        <token id="1" string="She" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="had" lemma="have" stem="had" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="bit" lemma="bit" stem="bit" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="parts" lemma="part" stem="part" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="two" lemma="two" stem="two" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="8" string="earlier" lemma="earlier" stem="earlier" pos="JJR" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="films" lemma="film" stem="film" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="Roman" lemma="Roman" stem="roman" pos="NNP" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="13" string="Scandals" lemma="Scandals" stem="scandal" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="17" string="one" lemma="one" stem="on" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="18" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="19" string="gave" lemma="give" stem="gave" pos="VBD" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="20" string="her" lemma="she" stem="her" pos="PRP" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="21" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="22" string="firmer" lemma="firmer" stem="firmer" pos="JJR" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="23" string="foothold" lemma="foothold" stem="foothold" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="24" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="25" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="26" string="slippery" lemma="slippery" stem="slipperi" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="27" string="slopes" lemma="slope" stem="slope" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="28" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="29" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="30" string="Hollywood" lemma="Hollywood" stem="hollywood" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="31" string="career" lemma="career" stem="career" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="32" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP She)) (VP (VBD had) (VP (VBN had) (NP (NN bit) (NNS parts)) (PP (IN in) (NP (CD two) (JJR earlier) (NNS films))) (SBAR (CC but) (S (`` ``) (NP (NNP Roman) (NNP Scandals)) ('' '') (VP (VBD was) (NP (NP (DT the) (CD one)) (SBAR (WHNP (WDT that)) (S (VP (VBD gave) (NP (PRP her)) (NP (NP (DT a) (JJR firmer) (NN foothold)) (PP (IN on) (NP (NP (DT the) (JJ slippery) (NNS slopes)) (PP (IN of) (NP (DT a) (NNP Hollywood) (NN career))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="had had bit parts in two earlier films but `` Roman Scandals '' was the one that gave her a firmer foothold on the slippery slopes of a Hollywood career" type="VP">
          <tokens>
            <token id="2" string="had" />
            <token id="3" string="had" />
            <token id="4" string="bit" />
            <token id="5" string="parts" />
            <token id="6" string="in" />
            <token id="7" string="two" />
            <token id="8" string="earlier" />
            <token id="9" string="films" />
            <token id="10" string="but" />
            <token id="11" string="&quot;" />
            <token id="12" string="Roman" />
            <token id="13" string="Scandals" />
            <token id="14" string="&quot;" />
            <token id="15" string="was" />
            <token id="16" string="the" />
            <token id="17" string="one" />
            <token id="18" string="that" />
            <token id="19" string="gave" />
            <token id="20" string="her" />
            <token id="21" string="a" />
            <token id="22" string="firmer" />
            <token id="23" string="foothold" />
            <token id="24" string="on" />
            <token id="25" string="the" />
            <token id="26" string="slippery" />
            <token id="27" string="slopes" />
            <token id="28" string="of" />
            <token id="29" string="a" />
            <token id="30" string="Hollywood" />
            <token id="31" string="career" />
          </tokens>
        </chunking>
        <chunking id="2" string="the one that gave her a firmer foothold on the slippery slopes of a Hollywood career" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="one" />
            <token id="18" string="that" />
            <token id="19" string="gave" />
            <token id="20" string="her" />
            <token id="21" string="a" />
            <token id="22" string="firmer" />
            <token id="23" string="foothold" />
            <token id="24" string="on" />
            <token id="25" string="the" />
            <token id="26" string="slippery" />
            <token id="27" string="slopes" />
            <token id="28" string="of" />
            <token id="29" string="a" />
            <token id="30" string="Hollywood" />
            <token id="31" string="career" />
          </tokens>
        </chunking>
        <chunking id="3" string="but `` Roman Scandals '' was the one that gave her a firmer foothold on the slippery slopes of a Hollywood career" type="SBAR">
          <tokens>
            <token id="10" string="but" />
            <token id="11" string="&quot;" />
            <token id="12" string="Roman" />
            <token id="13" string="Scandals" />
            <token id="14" string="&quot;" />
            <token id="15" string="was" />
            <token id="16" string="the" />
            <token id="17" string="one" />
            <token id="18" string="that" />
            <token id="19" string="gave" />
            <token id="20" string="her" />
            <token id="21" string="a" />
            <token id="22" string="firmer" />
            <token id="23" string="foothold" />
            <token id="24" string="on" />
            <token id="25" string="the" />
            <token id="26" string="slippery" />
            <token id="27" string="slopes" />
            <token id="28" string="of" />
            <token id="29" string="a" />
            <token id="30" string="Hollywood" />
            <token id="31" string="career" />
          </tokens>
        </chunking>
        <chunking id="4" string="gave her a firmer foothold on the slippery slopes of a Hollywood career" type="VP">
          <tokens>
            <token id="19" string="gave" />
            <token id="20" string="her" />
            <token id="21" string="a" />
            <token id="22" string="firmer" />
            <token id="23" string="foothold" />
            <token id="24" string="on" />
            <token id="25" string="the" />
            <token id="26" string="slippery" />
            <token id="27" string="slopes" />
            <token id="28" string="of" />
            <token id="29" string="a" />
            <token id="30" string="Hollywood" />
            <token id="31" string="career" />
          </tokens>
        </chunking>
        <chunking id="5" string="had bit parts in two earlier films but `` Roman Scandals '' was the one that gave her a firmer foothold on the slippery slopes of a Hollywood career" type="VP">
          <tokens>
            <token id="3" string="had" />
            <token id="4" string="bit" />
            <token id="5" string="parts" />
            <token id="6" string="in" />
            <token id="7" string="two" />
            <token id="8" string="earlier" />
            <token id="9" string="films" />
            <token id="10" string="but" />
            <token id="11" string="&quot;" />
            <token id="12" string="Roman" />
            <token id="13" string="Scandals" />
            <token id="14" string="&quot;" />
            <token id="15" string="was" />
            <token id="16" string="the" />
            <token id="17" string="one" />
            <token id="18" string="that" />
            <token id="19" string="gave" />
            <token id="20" string="her" />
            <token id="21" string="a" />
            <token id="22" string="firmer" />
            <token id="23" string="foothold" />
            <token id="24" string="on" />
            <token id="25" string="the" />
            <token id="26" string="slippery" />
            <token id="27" string="slopes" />
            <token id="28" string="of" />
            <token id="29" string="a" />
            <token id="30" string="Hollywood" />
            <token id="31" string="career" />
          </tokens>
        </chunking>
        <chunking id="6" string="Roman Scandals" type="NP">
          <tokens>
            <token id="12" string="Roman" />
            <token id="13" string="Scandals" />
          </tokens>
        </chunking>
        <chunking id="7" string="She" type="NP">
          <tokens>
            <token id="1" string="She" />
          </tokens>
        </chunking>
        <chunking id="8" string="was the one that gave her a firmer foothold on the slippery slopes of a Hollywood career" type="VP">
          <tokens>
            <token id="15" string="was" />
            <token id="16" string="the" />
            <token id="17" string="one" />
            <token id="18" string="that" />
            <token id="19" string="gave" />
            <token id="20" string="her" />
            <token id="21" string="a" />
            <token id="22" string="firmer" />
            <token id="23" string="foothold" />
            <token id="24" string="on" />
            <token id="25" string="the" />
            <token id="26" string="slippery" />
            <token id="27" string="slopes" />
            <token id="28" string="of" />
            <token id="29" string="a" />
            <token id="30" string="Hollywood" />
            <token id="31" string="career" />
          </tokens>
        </chunking>
        <chunking id="9" string="the slippery slopes of a Hollywood career" type="NP">
          <tokens>
            <token id="25" string="the" />
            <token id="26" string="slippery" />
            <token id="27" string="slopes" />
            <token id="28" string="of" />
            <token id="29" string="a" />
            <token id="30" string="Hollywood" />
            <token id="31" string="career" />
          </tokens>
        </chunking>
        <chunking id="10" string="two earlier films" type="NP">
          <tokens>
            <token id="7" string="two" />
            <token id="8" string="earlier" />
            <token id="9" string="films" />
          </tokens>
        </chunking>
        <chunking id="11" string="her" type="NP">
          <tokens>
            <token id="20" string="her" />
          </tokens>
        </chunking>
        <chunking id="12" string="the slippery slopes" type="NP">
          <tokens>
            <token id="25" string="the" />
            <token id="26" string="slippery" />
            <token id="27" string="slopes" />
          </tokens>
        </chunking>
        <chunking id="13" string="a firmer foothold on the slippery slopes of a Hollywood career" type="NP">
          <tokens>
            <token id="21" string="a" />
            <token id="22" string="firmer" />
            <token id="23" string="foothold" />
            <token id="24" string="on" />
            <token id="25" string="the" />
            <token id="26" string="slippery" />
            <token id="27" string="slopes" />
            <token id="28" string="of" />
            <token id="29" string="a" />
            <token id="30" string="Hollywood" />
            <token id="31" string="career" />
          </tokens>
        </chunking>
        <chunking id="14" string="a Hollywood career" type="NP">
          <tokens>
            <token id="29" string="a" />
            <token id="30" string="Hollywood" />
            <token id="31" string="career" />
          </tokens>
        </chunking>
        <chunking id="15" string="the one" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="one" />
          </tokens>
        </chunking>
        <chunking id="16" string="that gave her a firmer foothold on the slippery slopes of a Hollywood career" type="SBAR">
          <tokens>
            <token id="18" string="that" />
            <token id="19" string="gave" />
            <token id="20" string="her" />
            <token id="21" string="a" />
            <token id="22" string="firmer" />
            <token id="23" string="foothold" />
            <token id="24" string="on" />
            <token id="25" string="the" />
            <token id="26" string="slippery" />
            <token id="27" string="slopes" />
            <token id="28" string="of" />
            <token id="29" string="a" />
            <token id="30" string="Hollywood" />
            <token id="31" string="career" />
          </tokens>
        </chunking>
        <chunking id="17" string="a firmer foothold" type="NP">
          <tokens>
            <token id="21" string="a" />
            <token id="22" string="firmer" />
            <token id="23" string="foothold" />
          </tokens>
        </chunking>
        <chunking id="18" string="bit parts" type="NP">
          <tokens>
            <token id="4" string="bit" />
            <token id="5" string="parts" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">had</governor>
          <dependent id="1">She</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="3">had</governor>
          <dependent id="2">had</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">had</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">parts</governor>
          <dependent id="4">bit</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">had</governor>
          <dependent id="5">parts</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">films</governor>
          <dependent id="6">in</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="9">films</governor>
          <dependent id="7">two</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">films</governor>
          <dependent id="8">earlier</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">had</governor>
          <dependent id="9">films</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="17">one</governor>
          <dependent id="10">but</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="13">Scandals</governor>
          <dependent id="12">Roman</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="17">one</governor>
          <dependent id="13">Scandals</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="17">one</governor>
          <dependent id="15">was</dependent>
        </dependency>
        <dependency type="det">
          <governor id="17">one</governor>
          <dependent id="16">the</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="3">had</governor>
          <dependent id="17">one</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">gave</governor>
          <dependent id="18">that</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="17">one</governor>
          <dependent id="19">gave</dependent>
        </dependency>
        <dependency type="iobj">
          <governor id="19">gave</governor>
          <dependent id="20">her</dependent>
        </dependency>
        <dependency type="det">
          <governor id="23">foothold</governor>
          <dependent id="21">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="23">foothold</governor>
          <dependent id="22">firmer</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="19">gave</governor>
          <dependent id="23">foothold</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">slopes</governor>
          <dependent id="24">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="27">slopes</governor>
          <dependent id="25">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="27">slopes</governor>
          <dependent id="26">slippery</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">foothold</governor>
          <dependent id="27">slopes</dependent>
        </dependency>
        <dependency type="case">
          <governor id="31">career</governor>
          <dependent id="28">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="31">career</governor>
          <dependent id="29">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="31">career</governor>
          <dependent id="30">Hollywood</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">slopes</governor>
          <dependent id="31">career</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Hollywood" type="LOCATION" score="0.0">
          <tokens>
            <token id="30" string="Hollywood" />
          </tokens>
        </entity>
        <entity id="2" string="one" type="NUMBER" score="0.0">
          <tokens>
            <token id="17" string="one" />
          </tokens>
        </entity>
        <entity id="3" string="two" type="NUMBER" score="0.0">
          <tokens>
            <token id="7" string="two" />
          </tokens>
        </entity>
        <entity id="4" string="Roman" type="MISC" score="0.0">
          <tokens>
            <token id="12" string="Roman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="4" has_coreference="true">
      <content>The movies prepared her for television.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="movies" lemma="movie" stem="movi" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="prepared" lemma="prepare" stem="prepar" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="television" lemma="television" stem="televis" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NNS movies)) (VP (VBD prepared) (NP (PRP$ her)) (PP (IN for) (NP (NN television)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="her" type="NP">
          <tokens>
            <token id="4" string="her" />
          </tokens>
        </chunking>
        <chunking id="2" string="television" type="NP">
          <tokens>
            <token id="6" string="television" />
          </tokens>
        </chunking>
        <chunking id="3" string="The movies" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="movies" />
          </tokens>
        </chunking>
        <chunking id="4" string="prepared her for television" type="VP">
          <tokens>
            <token id="3" string="prepared" />
            <token id="4" string="her" />
            <token id="5" string="for" />
            <token id="6" string="television" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">movies</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">prepared</governor>
          <dependent id="2">movies</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">prepared</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">prepared</governor>
          <dependent id="4">her</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">television</governor>
          <dependent id="5">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">prepared</governor>
          <dependent id="6">television</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="5" has_coreference="true">
      <content>She perfected her matchless craft as a comedienne in dozens of films, including three of Fred Astaire&amp;apost;s and one of the Marx Brothers (&amp;quot;Room Service&amp;quot;).</content>
      <tokens>
        <token id="1" string="She" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="perfected" lemma="perfect" stem="perfect" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="4" string="matchless" lemma="matchless" stem="matchless" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="craft" lemma="craft" stem="craft" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="comedienne" lemma="comedienne" stem="comedienn" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="dozens" lemma="dozen" stem="dozen" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="films" lemma="film" stem="film" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="including" lemma="include" stem="includ" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="three" lemma="three" stem="three" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="16" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="Fred" lemma="Fred" stem="fred" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="18" string="Astaire" lemma="Astaire" stem="astair" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="19" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="one" lemma="one" stem="on" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="22" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="Marx" lemma="Marx" stem="marx" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="25" string="Brothers" lemma="brothers" stem="brother" pos="NN" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="26" string="(" lemma="-lrb-" stem="(" pos="-LRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="Room" lemma="Room" stem="room" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="Service" lemma="Service" stem="servic" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string=")" lemma="-rrb-" stem=")" pos="-RRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP She)) (VP (VBD perfected) (NP (PRP$ her) (JJ matchless) (NN craft)) (PP (IN as) (NP (NP (DT a) (NN comedienne)) (PP (IN in) (NP (NP (NNS dozens)) (PP (IN of) (NP (NNS films))) (, ,) (VP (VBG including) (NP (NP (NP (CD three)) (PP (IN of) (NP (NP (NNP Fred) (NNP Astaire) (POS 's)) (CC and) (NP (CD one))))) (PP (IN of)) (SBAR (WHNP (WHADVP (DT the)) (NNP Marx) (NN Brothers))))))) (PRN (-LRB- -LRB-) (`` ``) (NP (NNP Room) (NNP Service)) ('' '') (-RRB- -RRB-))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="perfected her matchless craft as a comedienne in dozens of films , including three of Fred Astaire 's and one of the Marx Brothers -LRB- `` Room Service '' -RRB-" type="VP">
          <tokens>
            <token id="2" string="perfected" />
            <token id="3" string="her" />
            <token id="4" string="matchless" />
            <token id="5" string="craft" />
            <token id="6" string="as" />
            <token id="7" string="a" />
            <token id="8" string="comedienne" />
            <token id="9" string="in" />
            <token id="10" string="dozens" />
            <token id="11" string="of" />
            <token id="12" string="films" />
            <token id="13" string="," />
            <token id="14" string="including" />
            <token id="15" string="three" />
            <token id="16" string="of" />
            <token id="17" string="Fred" />
            <token id="18" string="Astaire" />
            <token id="19" string="'s" />
            <token id="20" string="and" />
            <token id="21" string="one" />
            <token id="22" string="of" />
            <token id="23" string="the" />
            <token id="24" string="Marx" />
            <token id="25" string="Brothers" />
            <token id="26" string="(" />
            <token id="27" string="&quot;" />
            <token id="28" string="Room" />
            <token id="29" string="Service" />
            <token id="30" string="&quot;" />
            <token id="31" string=")" />
          </tokens>
        </chunking>
        <chunking id="2" string="films" type="NP">
          <tokens>
            <token id="12" string="films" />
          </tokens>
        </chunking>
        <chunking id="3" string="Fred Astaire 's and one" type="NP">
          <tokens>
            <token id="17" string="Fred" />
            <token id="18" string="Astaire" />
            <token id="19" string="'s" />
            <token id="20" string="and" />
            <token id="21" string="one" />
          </tokens>
        </chunking>
        <chunking id="4" string="Room Service" type="NP">
          <tokens>
            <token id="28" string="Room" />
            <token id="29" string="Service" />
          </tokens>
        </chunking>
        <chunking id="5" string="dozens" type="NP">
          <tokens>
            <token id="10" string="dozens" />
          </tokens>
        </chunking>
        <chunking id="6" string="including three of Fred Astaire 's and one of the Marx Brothers" type="VP">
          <tokens>
            <token id="14" string="including" />
            <token id="15" string="three" />
            <token id="16" string="of" />
            <token id="17" string="Fred" />
            <token id="18" string="Astaire" />
            <token id="19" string="'s" />
            <token id="20" string="and" />
            <token id="21" string="one" />
            <token id="22" string="of" />
            <token id="23" string="the" />
            <token id="24" string="Marx" />
            <token id="25" string="Brothers" />
          </tokens>
        </chunking>
        <chunking id="7" string="one" type="NP">
          <tokens>
            <token id="21" string="one" />
          </tokens>
        </chunking>
        <chunking id="8" string="dozens of films , including three of Fred Astaire 's and one of the Marx Brothers" type="NP">
          <tokens>
            <token id="10" string="dozens" />
            <token id="11" string="of" />
            <token id="12" string="films" />
            <token id="13" string="," />
            <token id="14" string="including" />
            <token id="15" string="three" />
            <token id="16" string="of" />
            <token id="17" string="Fred" />
            <token id="18" string="Astaire" />
            <token id="19" string="'s" />
            <token id="20" string="and" />
            <token id="21" string="one" />
            <token id="22" string="of" />
            <token id="23" string="the" />
            <token id="24" string="Marx" />
            <token id="25" string="Brothers" />
          </tokens>
        </chunking>
        <chunking id="9" string="the Marx Brothers" type="SBAR">
          <tokens>
            <token id="23" string="the" />
            <token id="24" string="Marx" />
            <token id="25" string="Brothers" />
          </tokens>
        </chunking>
        <chunking id="10" string="a comedienne in dozens of films , including three of Fred Astaire 's and one of the Marx Brothers -LRB- `` Room Service '' -RRB-" type="NP">
          <tokens>
            <token id="7" string="a" />
            <token id="8" string="comedienne" />
            <token id="9" string="in" />
            <token id="10" string="dozens" />
            <token id="11" string="of" />
            <token id="12" string="films" />
            <token id="13" string="," />
            <token id="14" string="including" />
            <token id="15" string="three" />
            <token id="16" string="of" />
            <token id="17" string="Fred" />
            <token id="18" string="Astaire" />
            <token id="19" string="'s" />
            <token id="20" string="and" />
            <token id="21" string="one" />
            <token id="22" string="of" />
            <token id="23" string="the" />
            <token id="24" string="Marx" />
            <token id="25" string="Brothers" />
            <token id="26" string="(" />
            <token id="27" string="&quot;" />
            <token id="28" string="Room" />
            <token id="29" string="Service" />
            <token id="30" string="&quot;" />
            <token id="31" string=")" />
          </tokens>
        </chunking>
        <chunking id="11" string="a comedienne" type="NP">
          <tokens>
            <token id="7" string="a" />
            <token id="8" string="comedienne" />
          </tokens>
        </chunking>
        <chunking id="12" string="She" type="NP">
          <tokens>
            <token id="1" string="She" />
          </tokens>
        </chunking>
        <chunking id="13" string="three" type="NP">
          <tokens>
            <token id="15" string="three" />
          </tokens>
        </chunking>
        <chunking id="14" string="the" type="WHADVP">
          <tokens>
            <token id="23" string="the" />
          </tokens>
        </chunking>
        <chunking id="15" string="three of Fred Astaire 's and one of the Marx Brothers" type="NP">
          <tokens>
            <token id="15" string="three" />
            <token id="16" string="of" />
            <token id="17" string="Fred" />
            <token id="18" string="Astaire" />
            <token id="19" string="'s" />
            <token id="20" string="and" />
            <token id="21" string="one" />
            <token id="22" string="of" />
            <token id="23" string="the" />
            <token id="24" string="Marx" />
            <token id="25" string="Brothers" />
          </tokens>
        </chunking>
        <chunking id="16" string="three of Fred Astaire 's and one" type="NP">
          <tokens>
            <token id="15" string="three" />
            <token id="16" string="of" />
            <token id="17" string="Fred" />
            <token id="18" string="Astaire" />
            <token id="19" string="'s" />
            <token id="20" string="and" />
            <token id="21" string="one" />
          </tokens>
        </chunking>
        <chunking id="17" string="Fred Astaire 's" type="NP">
          <tokens>
            <token id="17" string="Fred" />
            <token id="18" string="Astaire" />
            <token id="19" string="'s" />
          </tokens>
        </chunking>
        <chunking id="18" string="her matchless craft" type="NP">
          <tokens>
            <token id="3" string="her" />
            <token id="4" string="matchless" />
            <token id="5" string="craft" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">perfected</governor>
          <dependent id="1">She</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">perfected</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="5">craft</governor>
          <dependent id="3">her</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">craft</governor>
          <dependent id="4">matchless</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">perfected</governor>
          <dependent id="5">craft</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">comedienne</governor>
          <dependent id="6">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">comedienne</governor>
          <dependent id="7">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">perfected</governor>
          <dependent id="8">comedienne</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">dozens</governor>
          <dependent id="9">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">comedienne</governor>
          <dependent id="10">dozens</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">films</governor>
          <dependent id="11">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">dozens</governor>
          <dependent id="12">films</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="10">dozens</governor>
          <dependent id="14">including</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="14">including</governor>
          <dependent id="15">three</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">Astaire</governor>
          <dependent id="16">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="18">Astaire</governor>
          <dependent id="17">Fred</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">three</governor>
          <dependent id="18">Astaire</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">Astaire</governor>
          <dependent id="19">'s</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="18">Astaire</governor>
          <dependent id="20">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="18">Astaire</governor>
          <dependent id="21">one</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="15">three</governor>
          <dependent id="22">of</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="25">Brothers</governor>
          <dependent id="23">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">Brothers</governor>
          <dependent id="24">Marx</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="15">three</governor>
          <dependent id="25">Brothers</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="29">Service</governor>
          <dependent id="28">Room</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="8">comedienne</governor>
          <dependent id="29">Service</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="one" type="NUMBER" score="0.0">
          <tokens>
            <token id="21" string="one" />
          </tokens>
        </entity>
        <entity id="2" string="Marx Brothers" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="24" string="Marx" />
            <token id="25" string="Brothers" />
          </tokens>
        </entity>
        <entity id="3" string="three" type="NUMBER" score="0.0">
          <tokens>
            <token id="15" string="three" />
          </tokens>
        </entity>
        <entity id="4" string="Fred Astaire" type="PERSON" score="0.0">
          <tokens>
            <token id="17" string="Fred" />
            <token id="18" string="Astaire" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="6" has_coreference="true">
      <content>A lot of the other films caused few ripples on the wide surface of film history.</content>
      <tokens>
        <token id="1" string="A" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="lot" lemma="lot" stem="lot" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="other" lemma="other" stem="other" pos="JJ" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="6" string="films" lemma="film" stem="film" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="caused" lemma="cause" stem="caus" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="few" lemma="few" stem="few" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="ripples" lemma="ripple" stem="rippl" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="wide" lemma="wide" stem="wide" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="surface" lemma="surface" stem="surfac" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="film" lemma="film" stem="film" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="history" lemma="history" stem="histori" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT A) (NN lot)) (PP (IN of) (NP (DT the) (JJ other) (NNS films)))) (VP (VBD caused) (NP (JJ few) (NNS ripples)) (PP (IN on) (NP (NP (DT the) (JJ wide) (NN surface)) (PP (IN of) (NP (NN film) (NN history)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the wide surface of film history" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="wide" />
            <token id="13" string="surface" />
            <token id="14" string="of" />
            <token id="15" string="film" />
            <token id="16" string="history" />
          </tokens>
        </chunking>
        <chunking id="2" string="the wide surface" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="wide" />
            <token id="13" string="surface" />
          </tokens>
        </chunking>
        <chunking id="3" string="A lot" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="lot" />
          </tokens>
        </chunking>
        <chunking id="4" string="A lot of the other films" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="lot" />
            <token id="3" string="of" />
            <token id="4" string="the" />
            <token id="5" string="other" />
            <token id="6" string="films" />
          </tokens>
        </chunking>
        <chunking id="5" string="caused few ripples on the wide surface of film history" type="VP">
          <tokens>
            <token id="7" string="caused" />
            <token id="8" string="few" />
            <token id="9" string="ripples" />
            <token id="10" string="on" />
            <token id="11" string="the" />
            <token id="12" string="wide" />
            <token id="13" string="surface" />
            <token id="14" string="of" />
            <token id="15" string="film" />
            <token id="16" string="history" />
          </tokens>
        </chunking>
        <chunking id="6" string="few ripples" type="NP">
          <tokens>
            <token id="8" string="few" />
            <token id="9" string="ripples" />
          </tokens>
        </chunking>
        <chunking id="7" string="the other films" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="other" />
            <token id="6" string="films" />
          </tokens>
        </chunking>
        <chunking id="8" string="film history" type="NP">
          <tokens>
            <token id="15" string="film" />
            <token id="16" string="history" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">lot</governor>
          <dependent id="1">A</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">caused</governor>
          <dependent id="2">lot</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">films</governor>
          <dependent id="3">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">films</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">films</governor>
          <dependent id="5">other</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">lot</governor>
          <dependent id="6">films</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">caused</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">ripples</governor>
          <dependent id="8">few</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">caused</governor>
          <dependent id="9">ripples</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">surface</governor>
          <dependent id="10">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">surface</governor>
          <dependent id="11">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">surface</governor>
          <dependent id="12">wide</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">caused</governor>
          <dependent id="13">surface</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">history</governor>
          <dependent id="14">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="16">history</governor>
          <dependent id="15">film</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">surface</governor>
          <dependent id="16">history</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="7" has_coreference="true">
      <content>But in those years Lucy was refining her enduring, endearing public image as a perky, saucy woman who was likelier (at least in the early days) to end up with the hero&amp;apost;s funny pal than with the hero himself.</content>
      <tokens>
        <token id="1" string="But" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="those" lemma="those" stem="those" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="5" string="Lucy" lemma="Lucy" stem="luci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="6" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="refining" lemma="refine" stem="refin" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="enduring" lemma="enduring" stem="endur" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="endearing" lemma="endearing" stem="endear" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="public" lemma="public" stem="public" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="image" lemma="image" stem="imag" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="perky" lemma="perky" stem="perki" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="saucy" lemma="saucy" stem="sauci" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="woman" lemma="woman" stem="woman" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="likelier" lemma="likelier" stem="likeli" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="(" lemma="-lrb-" stem="(" pos="-LRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="least" lemma="least" stem="least" pos="JJS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="28" string="early" lemma="early" stem="earli" pos="JJ" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="29" string="days" lemma="day" stem="dai" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="30" string=")" lemma="-rrb-" stem=")" pos="-RRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="end" lemma="end" stem="end" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="up" lemma="up" stem="up" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="36" string="hero" lemma="hero" stem="hero" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="37" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="38" string="funny" lemma="funny" stem="funni" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="39" string="pal" lemma="pal" stem="pal" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="40" string="than" lemma="than" stem="than" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="41" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="42" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="43" string="hero" lemma="hero" stem="hero" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="44" string="himself" lemma="himself" stem="himself" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="45" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (CC But) (PP (IN in) (NP (DT those) (NNS years))) (NP (NNP Lucy)) (VP (VBD was) (VP (VBG refining) (NP (PRP$ her) (ADJP (JJ enduring) (, ,) (JJ endearing)) (JJ public) (NN image)) (PP (IN as) (NP (NP (DT a) (JJ perky) (, ,) (JJ saucy) (NN woman)) (SBAR (WHNP (WP who)) (S (VP (VBD was) (ADJP (JJ likelier) (PRN (-LRB- -LRB-) (PP (ADVP (IN at) (JJS least)) (IN in) (NP (DT the) (JJ early) (NNS days))) (-RRB- -RRB-)) (S (VP (TO to) (VP (VB end) (PRT (RP up)) (PP (IN with) (NP (NP (DT the) (NN hero) (POS 's)) (JJ funny) (NN pal))) (PP (IN than) (PP (IN with) (NP (DT the) (NN hero) (PRP himself))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="who was likelier -LRB- at least in the early days -RRB- to end up with the hero 's funny pal than with the hero himself" type="SBAR">
          <tokens>
            <token id="20" string="who" />
            <token id="21" string="was" />
            <token id="22" string="likelier" />
            <token id="23" string="(" />
            <token id="24" string="at" />
            <token id="25" string="least" />
            <token id="26" string="in" />
            <token id="27" string="the" />
            <token id="28" string="early" />
            <token id="29" string="days" />
            <token id="30" string=")" />
            <token id="31" string="to" />
            <token id="32" string="end" />
            <token id="33" string="up" />
            <token id="34" string="with" />
            <token id="35" string="the" />
            <token id="36" string="hero" />
            <token id="37" string="'s" />
            <token id="38" string="funny" />
            <token id="39" string="pal" />
            <token id="40" string="than" />
            <token id="41" string="with" />
            <token id="42" string="the" />
            <token id="43" string="hero" />
            <token id="44" string="himself" />
          </tokens>
        </chunking>
        <chunking id="2" string="a perky , saucy woman who was likelier -LRB- at least in the early days -RRB- to end up with the hero 's funny pal than with the hero himself" type="NP">
          <tokens>
            <token id="15" string="a" />
            <token id="16" string="perky" />
            <token id="17" string="," />
            <token id="18" string="saucy" />
            <token id="19" string="woman" />
            <token id="20" string="who" />
            <token id="21" string="was" />
            <token id="22" string="likelier" />
            <token id="23" string="(" />
            <token id="24" string="at" />
            <token id="25" string="least" />
            <token id="26" string="in" />
            <token id="27" string="the" />
            <token id="28" string="early" />
            <token id="29" string="days" />
            <token id="30" string=")" />
            <token id="31" string="to" />
            <token id="32" string="end" />
            <token id="33" string="up" />
            <token id="34" string="with" />
            <token id="35" string="the" />
            <token id="36" string="hero" />
            <token id="37" string="'s" />
            <token id="38" string="funny" />
            <token id="39" string="pal" />
            <token id="40" string="than" />
            <token id="41" string="with" />
            <token id="42" string="the" />
            <token id="43" string="hero" />
            <token id="44" string="himself" />
          </tokens>
        </chunking>
        <chunking id="3" string="the early days" type="NP">
          <tokens>
            <token id="27" string="the" />
            <token id="28" string="early" />
            <token id="29" string="days" />
          </tokens>
        </chunking>
        <chunking id="4" string="likelier -LRB- at least in the early days -RRB- to end up with the hero 's funny pal than with the hero himself" type="ADJP">
          <tokens>
            <token id="22" string="likelier" />
            <token id="23" string="(" />
            <token id="24" string="at" />
            <token id="25" string="least" />
            <token id="26" string="in" />
            <token id="27" string="the" />
            <token id="28" string="early" />
            <token id="29" string="days" />
            <token id="30" string=")" />
            <token id="31" string="to" />
            <token id="32" string="end" />
            <token id="33" string="up" />
            <token id="34" string="with" />
            <token id="35" string="the" />
            <token id="36" string="hero" />
            <token id="37" string="'s" />
            <token id="38" string="funny" />
            <token id="39" string="pal" />
            <token id="40" string="than" />
            <token id="41" string="with" />
            <token id="42" string="the" />
            <token id="43" string="hero" />
            <token id="44" string="himself" />
          </tokens>
        </chunking>
        <chunking id="5" string="was refining her enduring , endearing public image as a perky , saucy woman who was likelier -LRB- at least in the early days -RRB- to end up with the hero 's funny pal than with the hero himself" type="VP">
          <tokens>
            <token id="6" string="was" />
            <token id="7" string="refining" />
            <token id="8" string="her" />
            <token id="9" string="enduring" />
            <token id="10" string="," />
            <token id="11" string="endearing" />
            <token id="12" string="public" />
            <token id="13" string="image" />
            <token id="14" string="as" />
            <token id="15" string="a" />
            <token id="16" string="perky" />
            <token id="17" string="," />
            <token id="18" string="saucy" />
            <token id="19" string="woman" />
            <token id="20" string="who" />
            <token id="21" string="was" />
            <token id="22" string="likelier" />
            <token id="23" string="(" />
            <token id="24" string="at" />
            <token id="25" string="least" />
            <token id="26" string="in" />
            <token id="27" string="the" />
            <token id="28" string="early" />
            <token id="29" string="days" />
            <token id="30" string=")" />
            <token id="31" string="to" />
            <token id="32" string="end" />
            <token id="33" string="up" />
            <token id="34" string="with" />
            <token id="35" string="the" />
            <token id="36" string="hero" />
            <token id="37" string="'s" />
            <token id="38" string="funny" />
            <token id="39" string="pal" />
            <token id="40" string="than" />
            <token id="41" string="with" />
            <token id="42" string="the" />
            <token id="43" string="hero" />
            <token id="44" string="himself" />
          </tokens>
        </chunking>
        <chunking id="6" string="a perky , saucy woman" type="NP">
          <tokens>
            <token id="15" string="a" />
            <token id="16" string="perky" />
            <token id="17" string="," />
            <token id="18" string="saucy" />
            <token id="19" string="woman" />
          </tokens>
        </chunking>
        <chunking id="7" string="those years" type="NP">
          <tokens>
            <token id="3" string="those" />
            <token id="4" string="years" />
          </tokens>
        </chunking>
        <chunking id="8" string="enduring , endearing" type="ADJP">
          <tokens>
            <token id="9" string="enduring" />
            <token id="10" string="," />
            <token id="11" string="endearing" />
          </tokens>
        </chunking>
        <chunking id="9" string="the hero 's funny pal" type="NP">
          <tokens>
            <token id="35" string="the" />
            <token id="36" string="hero" />
            <token id="37" string="'s" />
            <token id="38" string="funny" />
            <token id="39" string="pal" />
          </tokens>
        </chunking>
        <chunking id="10" string="her enduring , endearing public image" type="NP">
          <tokens>
            <token id="8" string="her" />
            <token id="9" string="enduring" />
            <token id="10" string="," />
            <token id="11" string="endearing" />
            <token id="12" string="public" />
            <token id="13" string="image" />
          </tokens>
        </chunking>
        <chunking id="11" string="the hero himself" type="NP">
          <tokens>
            <token id="42" string="the" />
            <token id="43" string="hero" />
            <token id="44" string="himself" />
          </tokens>
        </chunking>
        <chunking id="12" string="was likelier -LRB- at least in the early days -RRB- to end up with the hero 's funny pal than with the hero himself" type="VP">
          <tokens>
            <token id="21" string="was" />
            <token id="22" string="likelier" />
            <token id="23" string="(" />
            <token id="24" string="at" />
            <token id="25" string="least" />
            <token id="26" string="in" />
            <token id="27" string="the" />
            <token id="28" string="early" />
            <token id="29" string="days" />
            <token id="30" string=")" />
            <token id="31" string="to" />
            <token id="32" string="end" />
            <token id="33" string="up" />
            <token id="34" string="with" />
            <token id="35" string="the" />
            <token id="36" string="hero" />
            <token id="37" string="'s" />
            <token id="38" string="funny" />
            <token id="39" string="pal" />
            <token id="40" string="than" />
            <token id="41" string="with" />
            <token id="42" string="the" />
            <token id="43" string="hero" />
            <token id="44" string="himself" />
          </tokens>
        </chunking>
        <chunking id="13" string="the hero 's" type="NP">
          <tokens>
            <token id="35" string="the" />
            <token id="36" string="hero" />
            <token id="37" string="'s" />
          </tokens>
        </chunking>
        <chunking id="14" string="to end up with the hero 's funny pal than with the hero himself" type="VP">
          <tokens>
            <token id="31" string="to" />
            <token id="32" string="end" />
            <token id="33" string="up" />
            <token id="34" string="with" />
            <token id="35" string="the" />
            <token id="36" string="hero" />
            <token id="37" string="'s" />
            <token id="38" string="funny" />
            <token id="39" string="pal" />
            <token id="40" string="than" />
            <token id="41" string="with" />
            <token id="42" string="the" />
            <token id="43" string="hero" />
            <token id="44" string="himself" />
          </tokens>
        </chunking>
        <chunking id="15" string="Lucy" type="NP">
          <tokens>
            <token id="5" string="Lucy" />
          </tokens>
        </chunking>
        <chunking id="16" string="end up with the hero 's funny pal than with the hero himself" type="VP">
          <tokens>
            <token id="32" string="end" />
            <token id="33" string="up" />
            <token id="34" string="with" />
            <token id="35" string="the" />
            <token id="36" string="hero" />
            <token id="37" string="'s" />
            <token id="38" string="funny" />
            <token id="39" string="pal" />
            <token id="40" string="than" />
            <token id="41" string="with" />
            <token id="42" string="the" />
            <token id="43" string="hero" />
            <token id="44" string="himself" />
          </tokens>
        </chunking>
        <chunking id="17" string="refining her enduring , endearing public image as a perky , saucy woman who was likelier -LRB- at least in the early days -RRB- to end up with the hero 's funny pal than with the hero himself" type="VP">
          <tokens>
            <token id="7" string="refining" />
            <token id="8" string="her" />
            <token id="9" string="enduring" />
            <token id="10" string="," />
            <token id="11" string="endearing" />
            <token id="12" string="public" />
            <token id="13" string="image" />
            <token id="14" string="as" />
            <token id="15" string="a" />
            <token id="16" string="perky" />
            <token id="17" string="," />
            <token id="18" string="saucy" />
            <token id="19" string="woman" />
            <token id="20" string="who" />
            <token id="21" string="was" />
            <token id="22" string="likelier" />
            <token id="23" string="(" />
            <token id="24" string="at" />
            <token id="25" string="least" />
            <token id="26" string="in" />
            <token id="27" string="the" />
            <token id="28" string="early" />
            <token id="29" string="days" />
            <token id="30" string=")" />
            <token id="31" string="to" />
            <token id="32" string="end" />
            <token id="33" string="up" />
            <token id="34" string="with" />
            <token id="35" string="the" />
            <token id="36" string="hero" />
            <token id="37" string="'s" />
            <token id="38" string="funny" />
            <token id="39" string="pal" />
            <token id="40" string="than" />
            <token id="41" string="with" />
            <token id="42" string="the" />
            <token id="43" string="hero" />
            <token id="44" string="himself" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="cc">
          <governor id="7">refining</governor>
          <dependent id="1">But</dependent>
        </dependency>
        <dependency type="case">
          <governor id="4">years</governor>
          <dependent id="2">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">years</governor>
          <dependent id="3">those</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">refining</governor>
          <dependent id="4">years</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">refining</governor>
          <dependent id="5">Lucy</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="7">refining</governor>
          <dependent id="6">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">refining</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="13">image</governor>
          <dependent id="8">her</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">endearing</governor>
          <dependent id="9">enduring</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">image</governor>
          <dependent id="11">endearing</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">image</governor>
          <dependent id="12">public</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">refining</governor>
          <dependent id="13">image</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">woman</governor>
          <dependent id="14">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">woman</governor>
          <dependent id="15">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">woman</governor>
          <dependent id="16">perky</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">woman</governor>
          <dependent id="18">saucy</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">refining</governor>
          <dependent id="19">woman</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">likelier</governor>
          <dependent id="20">who</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="22">likelier</governor>
          <dependent id="21">was</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="19">woman</governor>
          <dependent id="22">likelier</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="29">days</governor>
          <dependent id="24">at</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="24">at</governor>
          <dependent id="25">least</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">days</governor>
          <dependent id="26">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="29">days</governor>
          <dependent id="27">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="29">days</governor>
          <dependent id="28">early</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="22">likelier</governor>
          <dependent id="29">days</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="32">end</governor>
          <dependent id="31">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="22">likelier</governor>
          <dependent id="32">end</dependent>
        </dependency>
        <dependency type="compound:prt">
          <governor id="32">end</governor>
          <dependent id="33">up</dependent>
        </dependency>
        <dependency type="case">
          <governor id="39">pal</governor>
          <dependent id="34">with</dependent>
        </dependency>
        <dependency type="det">
          <governor id="36">hero</governor>
          <dependent id="35">the</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="39">pal</governor>
          <dependent id="36">hero</dependent>
        </dependency>
        <dependency type="case">
          <governor id="36">hero</governor>
          <dependent id="37">'s</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="39">pal</governor>
          <dependent id="38">funny</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="32">end</governor>
          <dependent id="39">pal</dependent>
        </dependency>
        <dependency type="case">
          <governor id="43">hero</governor>
          <dependent id="40">than</dependent>
        </dependency>
        <dependency type="case">
          <governor id="43">hero</governor>
          <dependent id="41">with</dependent>
        </dependency>
        <dependency type="det">
          <governor id="43">hero</governor>
          <dependent id="42">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="32">end</governor>
          <dependent id="43">hero</dependent>
        </dependency>
        <dependency type="nmod:npmod">
          <governor id="43">hero</governor>
          <dependent id="44">himself</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="the early days" type="DURATION" score="0.0">
          <tokens>
            <token id="27" string="the" />
            <token id="28" string="early" />
            <token id="29" string="days" />
          </tokens>
        </entity>
        <entity id="2" string="Lucy" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="Lucy" />
          </tokens>
        </entity>
        <entity id="3" string="years" type="DURATION" score="0.0">
          <tokens>
            <token id="4" string="years" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="8" has_coreference="true">
      <content>Yet as often happened with supporting performers, her verve, an engagingly sharp tongue and a rather salty view of the world made her more interesting than the square-cut principals, and led to leading roles.</content>
      <tokens>
        <token id="1" string="Yet" lemma="yet" stem="yet" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="as" lemma="as" stem="a" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="often" lemma="often" stem="often" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="happened" lemma="happen" stem="happen" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="supporting" lemma="support" stem="support" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="performers" lemma="performer" stem="perform" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="verve" lemma="verve" stem="verv" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="engagingly" lemma="engagingly" stem="engagingli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="sharp" lemma="sharp" stem="sharp" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="tongue" lemma="tongue" stem="tongu" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="rather" lemma="rather" stem="rather" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="salty" lemma="salty" stem="salti" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="view" lemma="view" stem="view" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="23" string="world" lemma="world" stem="world" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="24" string="made" lemma="make" stem="made" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="26" string="more" lemma="more" stem="more" pos="JJR" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="interesting" lemma="interesting" stem="interest" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="than" lemma="than" stem="than" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="square-cut" lemma="square-cut" stem="square-cut" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="principals" lemma="principal" stem="princip" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="led" lemma="lead" stem="led" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="leading" lemma="lead" stem="lead" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="37" string="roles" lemma="role" stem="role" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="38" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (RB Yet) (SBAR (RB as) (S (ADVP (RB often)) (VP (VBN happened) (PP (IN with) (S (VP (VBG supporting) (NP (NNS performers)))))))) (, ,) (NP (NP (PRP$ her) (NN verve)) (, ,) (NP (NP (DT an) (ADJP (RB engagingly) (JJ sharp)) (NN tongue)) (CC and) (NP (NP (DT a) (ADJP (RB rather) (JJ salty)) (NN view)) (PP (IN of) (NP (DT the) (NN world)))))) (VP (VP (VBD made) (S (NP (PRP$ her) (JJR more)) (ADJP (JJ interesting) (PP (IN than) (NP (DT the) (JJ square-cut) (NNS principals)))))) (, ,) (CC and) (VP (VBD led) (PP (TO to) (NP (VBG leading) (NNS roles))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="rather salty" type="ADJP">
          <tokens>
            <token id="18" string="rather" />
            <token id="19" string="salty" />
          </tokens>
        </chunking>
        <chunking id="2" string="performers" type="NP">
          <tokens>
            <token id="7" string="performers" />
          </tokens>
        </chunking>
        <chunking id="3" string="as often happened with supporting performers" type="SBAR">
          <tokens>
            <token id="2" string="as" />
            <token id="3" string="often" />
            <token id="4" string="happened" />
            <token id="5" string="with" />
            <token id="6" string="supporting" />
            <token id="7" string="performers" />
          </tokens>
        </chunking>
        <chunking id="4" string="a rather salty view of the world" type="NP">
          <tokens>
            <token id="17" string="a" />
            <token id="18" string="rather" />
            <token id="19" string="salty" />
            <token id="20" string="view" />
            <token id="21" string="of" />
            <token id="22" string="the" />
            <token id="23" string="world" />
          </tokens>
        </chunking>
        <chunking id="5" string="a rather salty view" type="NP">
          <tokens>
            <token id="17" string="a" />
            <token id="18" string="rather" />
            <token id="19" string="salty" />
            <token id="20" string="view" />
          </tokens>
        </chunking>
        <chunking id="6" string="the square-cut principals" type="NP">
          <tokens>
            <token id="29" string="the" />
            <token id="30" string="square-cut" />
            <token id="31" string="principals" />
          </tokens>
        </chunking>
        <chunking id="7" string="the world" type="NP">
          <tokens>
            <token id="22" string="the" />
            <token id="23" string="world" />
          </tokens>
        </chunking>
        <chunking id="8" string="an engagingly sharp tongue" type="NP">
          <tokens>
            <token id="12" string="an" />
            <token id="13" string="engagingly" />
            <token id="14" string="sharp" />
            <token id="15" string="tongue" />
          </tokens>
        </chunking>
        <chunking id="9" string="made her more interesting than the square-cut principals" type="VP">
          <tokens>
            <token id="24" string="made" />
            <token id="25" string="her" />
            <token id="26" string="more" />
            <token id="27" string="interesting" />
            <token id="28" string="than" />
            <token id="29" string="the" />
            <token id="30" string="square-cut" />
            <token id="31" string="principals" />
          </tokens>
        </chunking>
        <chunking id="10" string="engagingly sharp" type="ADJP">
          <tokens>
            <token id="13" string="engagingly" />
            <token id="14" string="sharp" />
          </tokens>
        </chunking>
        <chunking id="11" string="made her more interesting than the square-cut principals , and led to leading roles" type="VP">
          <tokens>
            <token id="24" string="made" />
            <token id="25" string="her" />
            <token id="26" string="more" />
            <token id="27" string="interesting" />
            <token id="28" string="than" />
            <token id="29" string="the" />
            <token id="30" string="square-cut" />
            <token id="31" string="principals" />
            <token id="32" string="," />
            <token id="33" string="and" />
            <token id="34" string="led" />
            <token id="35" string="to" />
            <token id="36" string="leading" />
            <token id="37" string="roles" />
          </tokens>
        </chunking>
        <chunking id="12" string="her more" type="NP">
          <tokens>
            <token id="25" string="her" />
            <token id="26" string="more" />
          </tokens>
        </chunking>
        <chunking id="13" string="her verve" type="NP">
          <tokens>
            <token id="9" string="her" />
            <token id="10" string="verve" />
          </tokens>
        </chunking>
        <chunking id="14" string="happened with supporting performers" type="VP">
          <tokens>
            <token id="4" string="happened" />
            <token id="5" string="with" />
            <token id="6" string="supporting" />
            <token id="7" string="performers" />
          </tokens>
        </chunking>
        <chunking id="15" string="her verve , an engagingly sharp tongue and a rather salty view of the world" type="NP">
          <tokens>
            <token id="9" string="her" />
            <token id="10" string="verve" />
            <token id="11" string="," />
            <token id="12" string="an" />
            <token id="13" string="engagingly" />
            <token id="14" string="sharp" />
            <token id="15" string="tongue" />
            <token id="16" string="and" />
            <token id="17" string="a" />
            <token id="18" string="rather" />
            <token id="19" string="salty" />
            <token id="20" string="view" />
            <token id="21" string="of" />
            <token id="22" string="the" />
            <token id="23" string="world" />
          </tokens>
        </chunking>
        <chunking id="16" string="led to leading roles" type="VP">
          <tokens>
            <token id="34" string="led" />
            <token id="35" string="to" />
            <token id="36" string="leading" />
            <token id="37" string="roles" />
          </tokens>
        </chunking>
        <chunking id="17" string="an engagingly sharp tongue and a rather salty view of the world" type="NP">
          <tokens>
            <token id="12" string="an" />
            <token id="13" string="engagingly" />
            <token id="14" string="sharp" />
            <token id="15" string="tongue" />
            <token id="16" string="and" />
            <token id="17" string="a" />
            <token id="18" string="rather" />
            <token id="19" string="salty" />
            <token id="20" string="view" />
            <token id="21" string="of" />
            <token id="22" string="the" />
            <token id="23" string="world" />
          </tokens>
        </chunking>
        <chunking id="18" string="supporting performers" type="VP">
          <tokens>
            <token id="6" string="supporting" />
            <token id="7" string="performers" />
          </tokens>
        </chunking>
        <chunking id="19" string="interesting than the square-cut principals" type="ADJP">
          <tokens>
            <token id="27" string="interesting" />
            <token id="28" string="than" />
            <token id="29" string="the" />
            <token id="30" string="square-cut" />
            <token id="31" string="principals" />
          </tokens>
        </chunking>
        <chunking id="20" string="leading roles" type="NP">
          <tokens>
            <token id="36" string="leading" />
            <token id="37" string="roles" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="24">made</governor>
          <dependent id="1">Yet</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">happened</governor>
          <dependent id="2">as</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">happened</governor>
          <dependent id="3">often</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="24">made</governor>
          <dependent id="4">happened</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="6">supporting</governor>
          <dependent id="5">with</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="4">happened</governor>
          <dependent id="6">supporting</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">supporting</governor>
          <dependent id="7">performers</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="10">verve</governor>
          <dependent id="9">her</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="24">made</governor>
          <dependent id="10">verve</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">tongue</governor>
          <dependent id="12">an</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="14">sharp</governor>
          <dependent id="13">engagingly</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">tongue</governor>
          <dependent id="14">sharp</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="10">verve</governor>
          <dependent id="15">tongue</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="15">tongue</governor>
          <dependent id="16">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="20">view</governor>
          <dependent id="17">a</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="19">salty</governor>
          <dependent id="18">rather</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="20">view</governor>
          <dependent id="19">salty</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="15">tongue</governor>
          <dependent id="20">view</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">world</governor>
          <dependent id="21">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="23">world</governor>
          <dependent id="22">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">view</governor>
          <dependent id="23">world</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="24">made</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="26">more</governor>
          <dependent id="25">her</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="27">interesting</governor>
          <dependent id="26">more</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="24">made</governor>
          <dependent id="27">interesting</dependent>
        </dependency>
        <dependency type="case">
          <governor id="31">principals</governor>
          <dependent id="28">than</dependent>
        </dependency>
        <dependency type="det">
          <governor id="31">principals</governor>
          <dependent id="29">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="31">principals</governor>
          <dependent id="30">square-cut</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">interesting</governor>
          <dependent id="31">principals</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="24">made</governor>
          <dependent id="33">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="24">made</governor>
          <dependent id="34">led</dependent>
        </dependency>
        <dependency type="case">
          <governor id="37">roles</governor>
          <dependent id="35">to</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="37">roles</governor>
          <dependent id="36">leading</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="34">led</governor>
          <dependent id="37">roles</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="9" has_coreference="true">
      <content>To look at the &amp;quot;I Love Lucy&amp;quot; reruns today, as everybody does, again and again, is to realize how superbly professional they are.</content>
      <tokens>
        <token id="1" string="To" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="look" lemma="look" stem="look" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="Love" lemma="Love" stem="love" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="Lucy" lemma="Lucy" stem="luci" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="9" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="reruns" lemma="rerun" stem="rerun" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="11" string="today" lemma="today" stem="todai" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="everybody" lemma="everybody" stem="everybodi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="15" string="does" lemma="do" stem="doe" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="again" lemma="again" stem="again" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="again" lemma="again" stem="again" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="realize" lemma="realize" stem="realiz" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="how" lemma="how" stem="how" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="superbly" lemma="superbly" stem="superbli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="professional" lemma="professional" stem="profession" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="are" lemma="be" stem="ar" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (VP (TO To) (VP (VB look) (ADVP (IN at) (DT the)) (NP (`` ``) (NP (NP (PRP I)) (NP (NNP Love) (NNP Lucy))) ('' '') (SBAR (S (NP (NP (NNS reruns) (NN today)) (, ,) (PP (IN as) (NP (NN everybody)))) (VP (VBZ does)))))))) (, ,) (ADVP (RB again) (CC and) (RB again)) (, ,) (VP (VBZ is) (S (VP (TO to) (VP (VB realize) (SBAR (WHADVP (WRB how)) (FRAG (ADVP (RB superbly)) (NP (NP (JJ professional)) (SBAR (S (NP (PRP they)) (VP (VBP are))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="everybody" type="NP">
          <tokens>
            <token id="14" string="everybody" />
          </tokens>
        </chunking>
        <chunking id="2" string="how superbly professional they are" type="SBAR">
          <tokens>
            <token id="24" string="how" />
            <token id="25" string="superbly" />
            <token id="26" string="professional" />
            <token id="27" string="they" />
            <token id="28" string="are" />
          </tokens>
        </chunking>
        <chunking id="3" string="reruns today , as everybody does" type="SBAR">
          <tokens>
            <token id="10" string="reruns" />
            <token id="11" string="today" />
            <token id="12" string="," />
            <token id="13" string="as" />
            <token id="14" string="everybody" />
            <token id="15" string="does" />
          </tokens>
        </chunking>
        <chunking id="4" string="realize how superbly professional they are" type="VP">
          <tokens>
            <token id="23" string="realize" />
            <token id="24" string="how" />
            <token id="25" string="superbly" />
            <token id="26" string="professional" />
            <token id="27" string="they" />
            <token id="28" string="are" />
          </tokens>
        </chunking>
        <chunking id="5" string="I" type="NP">
          <tokens>
            <token id="6" string="I" />
          </tokens>
        </chunking>
        <chunking id="6" string="to realize how superbly professional they are" type="VP">
          <tokens>
            <token id="22" string="to" />
            <token id="23" string="realize" />
            <token id="24" string="how" />
            <token id="25" string="superbly" />
            <token id="26" string="professional" />
            <token id="27" string="they" />
            <token id="28" string="are" />
          </tokens>
        </chunking>
        <chunking id="7" string="reruns today" type="NP">
          <tokens>
            <token id="10" string="reruns" />
            <token id="11" string="today" />
          </tokens>
        </chunking>
        <chunking id="8" string="is to realize how superbly professional they are" type="VP">
          <tokens>
            <token id="21" string="is" />
            <token id="22" string="to" />
            <token id="23" string="realize" />
            <token id="24" string="how" />
            <token id="25" string="superbly" />
            <token id="26" string="professional" />
            <token id="27" string="they" />
            <token id="28" string="are" />
          </tokens>
        </chunking>
        <chunking id="9" string="To look at the `` I Love Lucy '' reruns today , as everybody does" type="VP">
          <tokens>
            <token id="1" string="To" />
            <token id="2" string="look" />
            <token id="3" string="at" />
            <token id="4" string="the" />
            <token id="5" string="&quot;" />
            <token id="6" string="I" />
            <token id="7" string="Love" />
            <token id="8" string="Lucy" />
            <token id="9" string="&quot;" />
            <token id="10" string="reruns" />
            <token id="11" string="today" />
            <token id="12" string="," />
            <token id="13" string="as" />
            <token id="14" string="everybody" />
            <token id="15" string="does" />
          </tokens>
        </chunking>
        <chunking id="10" string="I Love Lucy" type="NP">
          <tokens>
            <token id="6" string="I" />
            <token id="7" string="Love" />
            <token id="8" string="Lucy" />
          </tokens>
        </chunking>
        <chunking id="11" string="how" type="WHADVP">
          <tokens>
            <token id="24" string="how" />
          </tokens>
        </chunking>
        <chunking id="12" string="professional" type="NP">
          <tokens>
            <token id="26" string="professional" />
          </tokens>
        </chunking>
        <chunking id="13" string="they" type="NP">
          <tokens>
            <token id="27" string="they" />
          </tokens>
        </chunking>
        <chunking id="14" string="are" type="VP">
          <tokens>
            <token id="28" string="are" />
          </tokens>
        </chunking>
        <chunking id="15" string="`` I Love Lucy '' reruns today , as everybody does" type="NP">
          <tokens>
            <token id="5" string="&quot;" />
            <token id="6" string="I" />
            <token id="7" string="Love" />
            <token id="8" string="Lucy" />
            <token id="9" string="&quot;" />
            <token id="10" string="reruns" />
            <token id="11" string="today" />
            <token id="12" string="," />
            <token id="13" string="as" />
            <token id="14" string="everybody" />
            <token id="15" string="does" />
          </tokens>
        </chunking>
        <chunking id="16" string="look at the `` I Love Lucy '' reruns today , as everybody does" type="VP">
          <tokens>
            <token id="2" string="look" />
            <token id="3" string="at" />
            <token id="4" string="the" />
            <token id="5" string="&quot;" />
            <token id="6" string="I" />
            <token id="7" string="Love" />
            <token id="8" string="Lucy" />
            <token id="9" string="&quot;" />
            <token id="10" string="reruns" />
            <token id="11" string="today" />
            <token id="12" string="," />
            <token id="13" string="as" />
            <token id="14" string="everybody" />
            <token id="15" string="does" />
          </tokens>
        </chunking>
        <chunking id="17" string="does" type="VP">
          <tokens>
            <token id="15" string="does" />
          </tokens>
        </chunking>
        <chunking id="18" string="they are" type="SBAR">
          <tokens>
            <token id="27" string="they" />
            <token id="28" string="are" />
          </tokens>
        </chunking>
        <chunking id="19" string="reruns today , as everybody" type="NP">
          <tokens>
            <token id="10" string="reruns" />
            <token id="11" string="today" />
            <token id="12" string="," />
            <token id="13" string="as" />
            <token id="14" string="everybody" />
          </tokens>
        </chunking>
        <chunking id="20" string="professional they are" type="NP">
          <tokens>
            <token id="26" string="professional" />
            <token id="27" string="they" />
            <token id="28" string="are" />
          </tokens>
        </chunking>
        <chunking id="21" string="Love Lucy" type="NP">
          <tokens>
            <token id="7" string="Love" />
            <token id="8" string="Lucy" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="mark">
          <governor id="2">look</governor>
          <dependent id="1">To</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="21">is</governor>
          <dependent id="2">look</dependent>
        </dependency>
        <dependency type="case">
          <governor id="4">the</governor>
          <dependent id="3">at</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="2">look</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">look</governor>
          <dependent id="6">I</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">Lucy</governor>
          <dependent id="7">Love</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="6">I</governor>
          <dependent id="8">Lucy</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">today</governor>
          <dependent id="10">reruns</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">does</governor>
          <dependent id="11">today</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">everybody</governor>
          <dependent id="13">as</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">today</governor>
          <dependent id="14">everybody</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="6">I</governor>
          <dependent id="15">does</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="21">is</governor>
          <dependent id="17">again</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="17">again</governor>
          <dependent id="18">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="17">again</governor>
          <dependent id="19">again</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="21">is</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="23">realize</governor>
          <dependent id="22">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="21">is</governor>
          <dependent id="23">realize</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="26">professional</governor>
          <dependent id="24">how</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="26">professional</governor>
          <dependent id="25">superbly</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="23">realize</governor>
          <dependent id="26">professional</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="28">are</governor>
          <dependent id="27">they</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="26">professional</governor>
          <dependent id="28">are</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="today" type="DATE" score="0.0">
          <tokens>
            <token id="11" string="today" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="10" has_coreference="true">
      <content>Today many sitcoms still use the three-camera technique, shooting in front of live audiences, that Desi Arnaz pioneered.</content>
      <tokens>
        <token id="1" string="Today" lemma="today" stem="todai" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="many" lemma="many" stem="mani" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="sitcoms" lemma="sitcom" stem="sitcom" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="still" lemma="still" stem="still" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="use" lemma="use" stem="us" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="three-camera" lemma="three-camera" stem="three-camera" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="technique" lemma="technique" stem="techniqu" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="shooting" lemma="shooting" stem="shoot" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="front" lemma="front" stem="front" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="live" lemma="live" stem="live" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="audiences" lemma="audience" stem="audienc" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="Desi" lemma="Desi" stem="desi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="19" string="Arnaz" lemma="Arnaz" stem="arnaz" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="20" string="pioneered" lemma="pioneer" stem="pioneer" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP-TMP (NN Today)) (NP (JJ many) (NNS sitcoms)) (VP (ADVP (RB still)) (VBP use) (NP (NP (DT the) (JJ three-camera) (NN technique)) (, ,) (NP (NP (NN shooting)) (PP (IN in) (NP (NP (NN front)) (PP (IN of) (NP (JJ live) (NNS audiences)))))) (, ,) (SBAR (IN that) (S (NP (NNP Desi) (NNP Arnaz)) (VP (VBD pioneered)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="shooting in front of live audiences" type="NP">
          <tokens>
            <token id="10" string="shooting" />
            <token id="11" string="in" />
            <token id="12" string="front" />
            <token id="13" string="of" />
            <token id="14" string="live" />
            <token id="15" string="audiences" />
          </tokens>
        </chunking>
        <chunking id="2" string="the three-camera technique" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="three-camera" />
            <token id="8" string="technique" />
          </tokens>
        </chunking>
        <chunking id="3" string="still use the three-camera technique , shooting in front of live audiences , that Desi Arnaz pioneered" type="VP">
          <tokens>
            <token id="4" string="still" />
            <token id="5" string="use" />
            <token id="6" string="the" />
            <token id="7" string="three-camera" />
            <token id="8" string="technique" />
            <token id="9" string="," />
            <token id="10" string="shooting" />
            <token id="11" string="in" />
            <token id="12" string="front" />
            <token id="13" string="of" />
            <token id="14" string="live" />
            <token id="15" string="audiences" />
            <token id="16" string="," />
            <token id="17" string="that" />
            <token id="18" string="Desi" />
            <token id="19" string="Arnaz" />
            <token id="20" string="pioneered" />
          </tokens>
        </chunking>
        <chunking id="4" string="live audiences" type="NP">
          <tokens>
            <token id="14" string="live" />
            <token id="15" string="audiences" />
          </tokens>
        </chunking>
        <chunking id="5" string="the three-camera technique , shooting in front of live audiences , that Desi Arnaz pioneered" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="three-camera" />
            <token id="8" string="technique" />
            <token id="9" string="," />
            <token id="10" string="shooting" />
            <token id="11" string="in" />
            <token id="12" string="front" />
            <token id="13" string="of" />
            <token id="14" string="live" />
            <token id="15" string="audiences" />
            <token id="16" string="," />
            <token id="17" string="that" />
            <token id="18" string="Desi" />
            <token id="19" string="Arnaz" />
            <token id="20" string="pioneered" />
          </tokens>
        </chunking>
        <chunking id="6" string="Desi Arnaz" type="NP">
          <tokens>
            <token id="18" string="Desi" />
            <token id="19" string="Arnaz" />
          </tokens>
        </chunking>
        <chunking id="7" string="shooting" type="NP">
          <tokens>
            <token id="10" string="shooting" />
          </tokens>
        </chunking>
        <chunking id="8" string="front" type="NP">
          <tokens>
            <token id="12" string="front" />
          </tokens>
        </chunking>
        <chunking id="9" string="pioneered" type="VP">
          <tokens>
            <token id="20" string="pioneered" />
          </tokens>
        </chunking>
        <chunking id="10" string="many sitcoms" type="NP">
          <tokens>
            <token id="2" string="many" />
            <token id="3" string="sitcoms" />
          </tokens>
        </chunking>
        <chunking id="11" string="front of live audiences" type="NP">
          <tokens>
            <token id="12" string="front" />
            <token id="13" string="of" />
            <token id="14" string="live" />
            <token id="15" string="audiences" />
          </tokens>
        </chunking>
        <chunking id="12" string="that Desi Arnaz pioneered" type="SBAR">
          <tokens>
            <token id="17" string="that" />
            <token id="18" string="Desi" />
            <token id="19" string="Arnaz" />
            <token id="20" string="pioneered" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:tmod">
          <governor id="5">use</governor>
          <dependent id="1">Today</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">sitcoms</governor>
          <dependent id="2">many</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">use</governor>
          <dependent id="3">sitcoms</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">use</governor>
          <dependent id="4">still</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">use</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">technique</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">technique</governor>
          <dependent id="7">three-camera</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">use</governor>
          <dependent id="8">technique</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="8">technique</governor>
          <dependent id="10">shooting</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">audiences</governor>
          <dependent id="11">in</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="11">in</governor>
          <dependent id="12">front</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="11">in</governor>
          <dependent id="13">of</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">audiences</governor>
          <dependent id="14">live</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">shooting</governor>
          <dependent id="15">audiences</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="20">pioneered</governor>
          <dependent id="17">that</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">Arnaz</governor>
          <dependent id="18">Desi</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="20">pioneered</governor>
          <dependent id="19">Arnaz</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="8">technique</governor>
          <dependent id="20">pioneered</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Today" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Today" />
          </tokens>
        </entity>
        <entity id="2" string="Desi Arnaz" type="PERSON" score="0.0">
          <tokens>
            <token id="18" string="Desi" />
            <token id="19" string="Arnaz" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="11" has_coreference="true">
      <content>The idea now seems as inevitable as the wheel or pre-sliced muffins, and yet you wonder if it would have worked nearly as well or caught on at all without Lucy&amp;apost;s sheer expertise as a physical as well as a verbal comedienne.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="idea" lemma="idea" stem="idea" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="now" lemma="now" stem="now" pos="RB" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="seems" lemma="seem" stem="seem" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="inevitable" lemma="inevitable" stem="inevit" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="wheel" lemma="wheel" stem="wheel" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="or" lemma="or" stem="or" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="pre-sliced" lemma="pre-sliced" stem="pre-slic" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="muffins" lemma="muffin" stem="muffin" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="yet" lemma="yet" stem="yet" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="you" lemma="you" stem="you" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="wonder" lemma="wonder" stem="wonder" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="if" lemma="if" stem="if" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="would" lemma="would" stem="would" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="have" lemma="have" stem="have" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="worked" lemma="work" stem="work" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="nearly" lemma="nearly" stem="nearli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="as" lemma="as" stem="a" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="well" lemma="well" stem="well" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="or" lemma="or" stem="or" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="caught" lemma="catch" stem="caught" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="without" lemma="without" stem="without" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="Lucy" lemma="Lucy" stem="luci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="33" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="34" string="sheer" lemma="sheer" stem="sheer" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="expertise" lemma="expertise" stem="expertis" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="36" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="physical" lemma="physical" stem="physic" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="as" lemma="as" stem="a" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="40" string="well" lemma="well" stem="well" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="41" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="42" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="43" string="verbal" lemma="verbal" stem="verbal" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="44" string="comedienne" lemma="comedienne" stem="comedienn" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="45" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (DT The) (NN idea)) (ADVP (RB now)) (VP (VBZ seems) (PP (IN as) (ADJP (JJ inevitable))) (PP (IN as) (NP (NP (DT the) (NN wheel)) (CC or) (NP (JJ pre-sliced) (NNS muffins)))))) (, ,) (CC and) (S (ADVP (RB yet)) (NP (PRP you)) (VP (VBP wonder) (SBAR (IN if) (S (NP (PRP it)) (VP (MD would) (VP (VB have) (VP (VP (VBN worked) (ADVP (RB nearly) (RB as) (RB well))) (CC or) (VP (VBN caught) (PP (IN on) (IN at) (NP (NP (DT all)) (PP (IN without) (NP (NP (NNP Lucy) (POS 's)) (JJ sheer) (NN expertise))))) (PP (IN as) (NP (NP (DT a) (JJ physical)) (CONJP (RB as) (RB well) (IN as)) (NP (DT a) (JJ verbal) (NN comedienne)))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="inevitable" type="ADJP">
          <tokens>
            <token id="6" string="inevitable" />
          </tokens>
        </chunking>
        <chunking id="2" string="the wheel" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="wheel" />
          </tokens>
        </chunking>
        <chunking id="3" string="all" type="NP">
          <tokens>
            <token id="30" string="all" />
          </tokens>
        </chunking>
        <chunking id="4" string="pre-sliced muffins" type="NP">
          <tokens>
            <token id="11" string="pre-sliced" />
            <token id="12" string="muffins" />
          </tokens>
        </chunking>
        <chunking id="5" string="would have worked nearly as well or caught on at all without Lucy 's sheer expertise as a physical as well as a verbal comedienne" type="VP">
          <tokens>
            <token id="20" string="would" />
            <token id="21" string="have" />
            <token id="22" string="worked" />
            <token id="23" string="nearly" />
            <token id="24" string="as" />
            <token id="25" string="well" />
            <token id="26" string="or" />
            <token id="27" string="caught" />
            <token id="28" string="on" />
            <token id="29" string="at" />
            <token id="30" string="all" />
            <token id="31" string="without" />
            <token id="32" string="Lucy" />
            <token id="33" string="'s" />
            <token id="34" string="sheer" />
            <token id="35" string="expertise" />
            <token id="36" string="as" />
            <token id="37" string="a" />
            <token id="38" string="physical" />
            <token id="39" string="as" />
            <token id="40" string="well" />
            <token id="41" string="as" />
            <token id="42" string="a" />
            <token id="43" string="verbal" />
            <token id="44" string="comedienne" />
          </tokens>
        </chunking>
        <chunking id="6" string="if it would have worked nearly as well or caught on at all without Lucy 's sheer expertise as a physical as well as a verbal comedienne" type="SBAR">
          <tokens>
            <token id="18" string="if" />
            <token id="19" string="it" />
            <token id="20" string="would" />
            <token id="21" string="have" />
            <token id="22" string="worked" />
            <token id="23" string="nearly" />
            <token id="24" string="as" />
            <token id="25" string="well" />
            <token id="26" string="or" />
            <token id="27" string="caught" />
            <token id="28" string="on" />
            <token id="29" string="at" />
            <token id="30" string="all" />
            <token id="31" string="without" />
            <token id="32" string="Lucy" />
            <token id="33" string="'s" />
            <token id="34" string="sheer" />
            <token id="35" string="expertise" />
            <token id="36" string="as" />
            <token id="37" string="a" />
            <token id="38" string="physical" />
            <token id="39" string="as" />
            <token id="40" string="well" />
            <token id="41" string="as" />
            <token id="42" string="a" />
            <token id="43" string="verbal" />
            <token id="44" string="comedienne" />
          </tokens>
        </chunking>
        <chunking id="7" string="Lucy 's" type="NP">
          <tokens>
            <token id="32" string="Lucy" />
            <token id="33" string="'s" />
          </tokens>
        </chunking>
        <chunking id="8" string="it" type="NP">
          <tokens>
            <token id="19" string="it" />
          </tokens>
        </chunking>
        <chunking id="9" string="all without Lucy 's sheer expertise" type="NP">
          <tokens>
            <token id="30" string="all" />
            <token id="31" string="without" />
            <token id="32" string="Lucy" />
            <token id="33" string="'s" />
            <token id="34" string="sheer" />
            <token id="35" string="expertise" />
          </tokens>
        </chunking>
        <chunking id="10" string="the wheel or pre-sliced muffins" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="wheel" />
            <token id="10" string="or" />
            <token id="11" string="pre-sliced" />
            <token id="12" string="muffins" />
          </tokens>
        </chunking>
        <chunking id="11" string="a verbal comedienne" type="NP">
          <tokens>
            <token id="42" string="a" />
            <token id="43" string="verbal" />
            <token id="44" string="comedienne" />
          </tokens>
        </chunking>
        <chunking id="12" string="a physical as well as a verbal comedienne" type="NP">
          <tokens>
            <token id="37" string="a" />
            <token id="38" string="physical" />
            <token id="39" string="as" />
            <token id="40" string="well" />
            <token id="41" string="as" />
            <token id="42" string="a" />
            <token id="43" string="verbal" />
            <token id="44" string="comedienne" />
          </tokens>
        </chunking>
        <chunking id="13" string="worked nearly as well or caught on at all without Lucy 's sheer expertise as a physical as well as a verbal comedienne" type="VP">
          <tokens>
            <token id="22" string="worked" />
            <token id="23" string="nearly" />
            <token id="24" string="as" />
            <token id="25" string="well" />
            <token id="26" string="or" />
            <token id="27" string="caught" />
            <token id="28" string="on" />
            <token id="29" string="at" />
            <token id="30" string="all" />
            <token id="31" string="without" />
            <token id="32" string="Lucy" />
            <token id="33" string="'s" />
            <token id="34" string="sheer" />
            <token id="35" string="expertise" />
            <token id="36" string="as" />
            <token id="37" string="a" />
            <token id="38" string="physical" />
            <token id="39" string="as" />
            <token id="40" string="well" />
            <token id="41" string="as" />
            <token id="42" string="a" />
            <token id="43" string="verbal" />
            <token id="44" string="comedienne" />
          </tokens>
        </chunking>
        <chunking id="14" string="caught on at all without Lucy 's sheer expertise as a physical as well as a verbal comedienne" type="VP">
          <tokens>
            <token id="27" string="caught" />
            <token id="28" string="on" />
            <token id="29" string="at" />
            <token id="30" string="all" />
            <token id="31" string="without" />
            <token id="32" string="Lucy" />
            <token id="33" string="'s" />
            <token id="34" string="sheer" />
            <token id="35" string="expertise" />
            <token id="36" string="as" />
            <token id="37" string="a" />
            <token id="38" string="physical" />
            <token id="39" string="as" />
            <token id="40" string="well" />
            <token id="41" string="as" />
            <token id="42" string="a" />
            <token id="43" string="verbal" />
            <token id="44" string="comedienne" />
          </tokens>
        </chunking>
        <chunking id="15" string="wonder if it would have worked nearly as well or caught on at all without Lucy 's sheer expertise as a physical as well as a verbal comedienne" type="VP">
          <tokens>
            <token id="17" string="wonder" />
            <token id="18" string="if" />
            <token id="19" string="it" />
            <token id="20" string="would" />
            <token id="21" string="have" />
            <token id="22" string="worked" />
            <token id="23" string="nearly" />
            <token id="24" string="as" />
            <token id="25" string="well" />
            <token id="26" string="or" />
            <token id="27" string="caught" />
            <token id="28" string="on" />
            <token id="29" string="at" />
            <token id="30" string="all" />
            <token id="31" string="without" />
            <token id="32" string="Lucy" />
            <token id="33" string="'s" />
            <token id="34" string="sheer" />
            <token id="35" string="expertise" />
            <token id="36" string="as" />
            <token id="37" string="a" />
            <token id="38" string="physical" />
            <token id="39" string="as" />
            <token id="40" string="well" />
            <token id="41" string="as" />
            <token id="42" string="a" />
            <token id="43" string="verbal" />
            <token id="44" string="comedienne" />
          </tokens>
        </chunking>
        <chunking id="16" string="seems as inevitable as the wheel or pre-sliced muffins" type="VP">
          <tokens>
            <token id="4" string="seems" />
            <token id="5" string="as" />
            <token id="6" string="inevitable" />
            <token id="7" string="as" />
            <token id="8" string="the" />
            <token id="9" string="wheel" />
            <token id="10" string="or" />
            <token id="11" string="pre-sliced" />
            <token id="12" string="muffins" />
          </tokens>
        </chunking>
        <chunking id="17" string="Lucy 's sheer expertise" type="NP">
          <tokens>
            <token id="32" string="Lucy" />
            <token id="33" string="'s" />
            <token id="34" string="sheer" />
            <token id="35" string="expertise" />
          </tokens>
        </chunking>
        <chunking id="18" string="a physical" type="NP">
          <tokens>
            <token id="37" string="a" />
            <token id="38" string="physical" />
          </tokens>
        </chunking>
        <chunking id="19" string="The idea" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="idea" />
          </tokens>
        </chunking>
        <chunking id="20" string="you" type="NP">
          <tokens>
            <token id="16" string="you" />
          </tokens>
        </chunking>
        <chunking id="21" string="have worked nearly as well or caught on at all without Lucy 's sheer expertise as a physical as well as a verbal comedienne" type="VP">
          <tokens>
            <token id="21" string="have" />
            <token id="22" string="worked" />
            <token id="23" string="nearly" />
            <token id="24" string="as" />
            <token id="25" string="well" />
            <token id="26" string="or" />
            <token id="27" string="caught" />
            <token id="28" string="on" />
            <token id="29" string="at" />
            <token id="30" string="all" />
            <token id="31" string="without" />
            <token id="32" string="Lucy" />
            <token id="33" string="'s" />
            <token id="34" string="sheer" />
            <token id="35" string="expertise" />
            <token id="36" string="as" />
            <token id="37" string="a" />
            <token id="38" string="physical" />
            <token id="39" string="as" />
            <token id="40" string="well" />
            <token id="41" string="as" />
            <token id="42" string="a" />
            <token id="43" string="verbal" />
            <token id="44" string="comedienne" />
          </tokens>
        </chunking>
        <chunking id="22" string="worked nearly as well" type="VP">
          <tokens>
            <token id="22" string="worked" />
            <token id="23" string="nearly" />
            <token id="24" string="as" />
            <token id="25" string="well" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">idea</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">seems</governor>
          <dependent id="2">idea</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">seems</governor>
          <dependent id="3">now</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">seems</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">inevitable</governor>
          <dependent id="5">as</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="4">seems</governor>
          <dependent id="6">inevitable</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">wheel</governor>
          <dependent id="7">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">wheel</governor>
          <dependent id="8">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">seems</governor>
          <dependent id="9">wheel</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="9">wheel</governor>
          <dependent id="10">or</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">muffins</governor>
          <dependent id="11">pre-sliced</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="9">wheel</governor>
          <dependent id="12">muffins</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">seems</governor>
          <dependent id="14">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="17">wonder</governor>
          <dependent id="15">yet</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="17">wonder</governor>
          <dependent id="16">you</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">seems</governor>
          <dependent id="17">wonder</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="22">worked</governor>
          <dependent id="18">if</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">worked</governor>
          <dependent id="19">it</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="22">worked</governor>
          <dependent id="20">would</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="22">worked</governor>
          <dependent id="21">have</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="17">wonder</governor>
          <dependent id="22">worked</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="25">well</governor>
          <dependent id="23">nearly</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="25">well</governor>
          <dependent id="24">as</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="22">worked</governor>
          <dependent id="25">well</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="22">worked</governor>
          <dependent id="26">or</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="22">worked</governor>
          <dependent id="27">caught</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">all</governor>
          <dependent id="28">on</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">all</governor>
          <dependent id="29">at</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">caught</governor>
          <dependent id="30">all</dependent>
        </dependency>
        <dependency type="case">
          <governor id="35">expertise</governor>
          <dependent id="31">without</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="35">expertise</governor>
          <dependent id="32">Lucy</dependent>
        </dependency>
        <dependency type="case">
          <governor id="32">Lucy</governor>
          <dependent id="33">'s</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="35">expertise</governor>
          <dependent id="34">sheer</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="30">all</governor>
          <dependent id="35">expertise</dependent>
        </dependency>
        <dependency type="case">
          <governor id="38">physical</governor>
          <dependent id="36">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="38">physical</governor>
          <dependent id="37">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">caught</governor>
          <dependent id="38">physical</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="38">physical</governor>
          <dependent id="39">as</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="39">as</governor>
          <dependent id="40">well</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="39">as</governor>
          <dependent id="41">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="44">comedienne</governor>
          <dependent id="42">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="44">comedienne</governor>
          <dependent id="43">verbal</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="38">physical</governor>
          <dependent id="44">comedienne</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="now" type="DATE" score="0.0">
          <tokens>
            <token id="3" string="now" />
          </tokens>
        </entity>
        <entity id="2" string="Lucy" type="PERSON" score="0.0">
          <tokens>
            <token id="32" string="Lucy" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="12" has_coreference="true">
      <content>She made it all -- even the wildest, craziest capers -- look as if it were happening now -- spontaneous, unrehearsed, accidental, the inevitable consequences of the screen Lucy&amp;apost;s scatterbrained and lovable personality.</content>
      <tokens>
        <token id="1" string="She" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="made" lemma="make" stem="made" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="even" lemma="even" stem="even" pos="RB" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="wildest" lemma="wildest" stem="wildest" pos="JJS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="craziest" lemma="craziest" stem="craziest" pos="JJS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="capers" lemma="caper" stem="caper" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="look" lemma="look" stem="look" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="if" lemma="if" stem="if" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="happening" lemma="happen" stem="happen" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="now" lemma="now" stem="now" pos="RB" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="20" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="spontaneous" lemma="spontaneous" stem="spontan" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="unrehearsed" lemma="unrehearsed" stem="unrehears" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="accidental" lemma="accidental" stem="accident" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="inevitable" lemma="inevitable" stem="inevit" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="consequences" lemma="consequence" stem="consequ" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="screen" lemma="screen" stem="screen" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="Lucy" lemma="Lucy" stem="luci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="34" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="35" string="scatterbrained" lemma="scatterbrained" stem="scatterbrain" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="36" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="lovable" lemma="lovable" stem="lovabl" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="38" string="personality" lemma="personality" stem="person" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP She)) (VP (VBD made) (NP (PRP it)) (SBAR (S (NP (NP (DT all)) (PRN (: --) (NP (NP (RB even) (DT the) (JJS wildest)) (, ,) (NP (JJS craziest) (NNS capers))) (: --))) (VP (VBP look) (SBAR (IN as) (IN if) (S (NP (PRP it)) (VP (VBD were) (VP (VBG happening) (ADVP (RB now)) (: --) (S (NP (NP (JJ spontaneous) (, ,) (ADJP (JJ unrehearsed) (, ,) (JJ accidental)) (, ,) (ADJP (DT the) (JJ inevitable)) (NNS consequences)) (PP (IN of) (NP (NP (DT the) (NN screen)) (NP (NP (NNP Lucy) (POS 's)) (JJ scatterbrained) (CC and) (JJ lovable) (NN personality)))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="all" type="NP">
          <tokens>
            <token id="4" string="all" />
          </tokens>
        </chunking>
        <chunking id="2" string="the screen Lucy 's scatterbrained and lovable personality" type="NP">
          <tokens>
            <token id="31" string="the" />
            <token id="32" string="screen" />
            <token id="33" string="Lucy" />
            <token id="34" string="'s" />
            <token id="35" string="scatterbrained" />
            <token id="36" string="and" />
            <token id="37" string="lovable" />
            <token id="38" string="personality" />
          </tokens>
        </chunking>
        <chunking id="3" string="the screen" type="NP">
          <tokens>
            <token id="31" string="the" />
            <token id="32" string="screen" />
          </tokens>
        </chunking>
        <chunking id="4" string="even the wildest" type="NP">
          <tokens>
            <token id="6" string="even" />
            <token id="7" string="the" />
            <token id="8" string="wildest" />
          </tokens>
        </chunking>
        <chunking id="5" string="Lucy 's" type="NP">
          <tokens>
            <token id="33" string="Lucy" />
            <token id="34" string="'s" />
          </tokens>
        </chunking>
        <chunking id="6" string="even the wildest , craziest capers" type="NP">
          <tokens>
            <token id="6" string="even" />
            <token id="7" string="the" />
            <token id="8" string="wildest" />
            <token id="9" string="," />
            <token id="10" string="craziest" />
            <token id="11" string="capers" />
          </tokens>
        </chunking>
        <chunking id="7" string="the inevitable" type="ADJP">
          <tokens>
            <token id="27" string="the" />
            <token id="28" string="inevitable" />
          </tokens>
        </chunking>
        <chunking id="8" string="it" type="NP">
          <tokens>
            <token id="3" string="it" />
          </tokens>
        </chunking>
        <chunking id="9" string="unrehearsed , accidental" type="ADJP">
          <tokens>
            <token id="23" string="unrehearsed" />
            <token id="24" string="," />
            <token id="25" string="accidental" />
          </tokens>
        </chunking>
        <chunking id="10" string="look as if it were happening now -- spontaneous , unrehearsed , accidental , the inevitable consequences of the screen Lucy 's scatterbrained and lovable personality" type="VP">
          <tokens>
            <token id="13" string="look" />
            <token id="14" string="as" />
            <token id="15" string="if" />
            <token id="16" string="it" />
            <token id="17" string="were" />
            <token id="18" string="happening" />
            <token id="19" string="now" />
            <token id="20" string="--" />
            <token id="21" string="spontaneous" />
            <token id="22" string="," />
            <token id="23" string="unrehearsed" />
            <token id="24" string="," />
            <token id="25" string="accidental" />
            <token id="26" string="," />
            <token id="27" string="the" />
            <token id="28" string="inevitable" />
            <token id="29" string="consequences" />
            <token id="30" string="of" />
            <token id="31" string="the" />
            <token id="32" string="screen" />
            <token id="33" string="Lucy" />
            <token id="34" string="'s" />
            <token id="35" string="scatterbrained" />
            <token id="36" string="and" />
            <token id="37" string="lovable" />
            <token id="38" string="personality" />
          </tokens>
        </chunking>
        <chunking id="11" string="She" type="NP">
          <tokens>
            <token id="1" string="She" />
          </tokens>
        </chunking>
        <chunking id="12" string="spontaneous , unrehearsed , accidental , the inevitable consequences of the screen Lucy 's scatterbrained and lovable personality" type="NP">
          <tokens>
            <token id="21" string="spontaneous" />
            <token id="22" string="," />
            <token id="23" string="unrehearsed" />
            <token id="24" string="," />
            <token id="25" string="accidental" />
            <token id="26" string="," />
            <token id="27" string="the" />
            <token id="28" string="inevitable" />
            <token id="29" string="consequences" />
            <token id="30" string="of" />
            <token id="31" string="the" />
            <token id="32" string="screen" />
            <token id="33" string="Lucy" />
            <token id="34" string="'s" />
            <token id="35" string="scatterbrained" />
            <token id="36" string="and" />
            <token id="37" string="lovable" />
            <token id="38" string="personality" />
          </tokens>
        </chunking>
        <chunking id="13" string="happening now -- spontaneous , unrehearsed , accidental , the inevitable consequences of the screen Lucy 's scatterbrained and lovable personality" type="VP">
          <tokens>
            <token id="18" string="happening" />
            <token id="19" string="now" />
            <token id="20" string="--" />
            <token id="21" string="spontaneous" />
            <token id="22" string="," />
            <token id="23" string="unrehearsed" />
            <token id="24" string="," />
            <token id="25" string="accidental" />
            <token id="26" string="," />
            <token id="27" string="the" />
            <token id="28" string="inevitable" />
            <token id="29" string="consequences" />
            <token id="30" string="of" />
            <token id="31" string="the" />
            <token id="32" string="screen" />
            <token id="33" string="Lucy" />
            <token id="34" string="'s" />
            <token id="35" string="scatterbrained" />
            <token id="36" string="and" />
            <token id="37" string="lovable" />
            <token id="38" string="personality" />
          </tokens>
        </chunking>
        <chunking id="14" string="all -- even the wildest , craziest capers -- look as if it were happening now -- spontaneous , unrehearsed , accidental , the inevitable consequences of the screen Lucy 's scatterbrained and lovable personality" type="SBAR">
          <tokens>
            <token id="4" string="all" />
            <token id="5" string="--" />
            <token id="6" string="even" />
            <token id="7" string="the" />
            <token id="8" string="wildest" />
            <token id="9" string="," />
            <token id="10" string="craziest" />
            <token id="11" string="capers" />
            <token id="12" string="--" />
            <token id="13" string="look" />
            <token id="14" string="as" />
            <token id="15" string="if" />
            <token id="16" string="it" />
            <token id="17" string="were" />
            <token id="18" string="happening" />
            <token id="19" string="now" />
            <token id="20" string="--" />
            <token id="21" string="spontaneous" />
            <token id="22" string="," />
            <token id="23" string="unrehearsed" />
            <token id="24" string="," />
            <token id="25" string="accidental" />
            <token id="26" string="," />
            <token id="27" string="the" />
            <token id="28" string="inevitable" />
            <token id="29" string="consequences" />
            <token id="30" string="of" />
            <token id="31" string="the" />
            <token id="32" string="screen" />
            <token id="33" string="Lucy" />
            <token id="34" string="'s" />
            <token id="35" string="scatterbrained" />
            <token id="36" string="and" />
            <token id="37" string="lovable" />
            <token id="38" string="personality" />
          </tokens>
        </chunking>
        <chunking id="15" string="craziest capers" type="NP">
          <tokens>
            <token id="10" string="craziest" />
            <token id="11" string="capers" />
          </tokens>
        </chunking>
        <chunking id="16" string="were happening now -- spontaneous , unrehearsed , accidental , the inevitable consequences of the screen Lucy 's scatterbrained and lovable personality" type="VP">
          <tokens>
            <token id="17" string="were" />
            <token id="18" string="happening" />
            <token id="19" string="now" />
            <token id="20" string="--" />
            <token id="21" string="spontaneous" />
            <token id="22" string="," />
            <token id="23" string="unrehearsed" />
            <token id="24" string="," />
            <token id="25" string="accidental" />
            <token id="26" string="," />
            <token id="27" string="the" />
            <token id="28" string="inevitable" />
            <token id="29" string="consequences" />
            <token id="30" string="of" />
            <token id="31" string="the" />
            <token id="32" string="screen" />
            <token id="33" string="Lucy" />
            <token id="34" string="'s" />
            <token id="35" string="scatterbrained" />
            <token id="36" string="and" />
            <token id="37" string="lovable" />
            <token id="38" string="personality" />
          </tokens>
        </chunking>
        <chunking id="17" string="as if it were happening now -- spontaneous , unrehearsed , accidental , the inevitable consequences of the screen Lucy 's scatterbrained and lovable personality" type="SBAR">
          <tokens>
            <token id="14" string="as" />
            <token id="15" string="if" />
            <token id="16" string="it" />
            <token id="17" string="were" />
            <token id="18" string="happening" />
            <token id="19" string="now" />
            <token id="20" string="--" />
            <token id="21" string="spontaneous" />
            <token id="22" string="," />
            <token id="23" string="unrehearsed" />
            <token id="24" string="," />
            <token id="25" string="accidental" />
            <token id="26" string="," />
            <token id="27" string="the" />
            <token id="28" string="inevitable" />
            <token id="29" string="consequences" />
            <token id="30" string="of" />
            <token id="31" string="the" />
            <token id="32" string="screen" />
            <token id="33" string="Lucy" />
            <token id="34" string="'s" />
            <token id="35" string="scatterbrained" />
            <token id="36" string="and" />
            <token id="37" string="lovable" />
            <token id="38" string="personality" />
          </tokens>
        </chunking>
        <chunking id="18" string="Lucy 's scatterbrained and lovable personality" type="NP">
          <tokens>
            <token id="33" string="Lucy" />
            <token id="34" string="'s" />
            <token id="35" string="scatterbrained" />
            <token id="36" string="and" />
            <token id="37" string="lovable" />
            <token id="38" string="personality" />
          </tokens>
        </chunking>
        <chunking id="19" string="made it all -- even the wildest , craziest capers -- look as if it were happening now -- spontaneous , unrehearsed , accidental , the inevitable consequences of the screen Lucy 's scatterbrained and lovable personality" type="VP">
          <tokens>
            <token id="2" string="made" />
            <token id="3" string="it" />
            <token id="4" string="all" />
            <token id="5" string="--" />
            <token id="6" string="even" />
            <token id="7" string="the" />
            <token id="8" string="wildest" />
            <token id="9" string="," />
            <token id="10" string="craziest" />
            <token id="11" string="capers" />
            <token id="12" string="--" />
            <token id="13" string="look" />
            <token id="14" string="as" />
            <token id="15" string="if" />
            <token id="16" string="it" />
            <token id="17" string="were" />
            <token id="18" string="happening" />
            <token id="19" string="now" />
            <token id="20" string="--" />
            <token id="21" string="spontaneous" />
            <token id="22" string="," />
            <token id="23" string="unrehearsed" />
            <token id="24" string="," />
            <token id="25" string="accidental" />
            <token id="26" string="," />
            <token id="27" string="the" />
            <token id="28" string="inevitable" />
            <token id="29" string="consequences" />
            <token id="30" string="of" />
            <token id="31" string="the" />
            <token id="32" string="screen" />
            <token id="33" string="Lucy" />
            <token id="34" string="'s" />
            <token id="35" string="scatterbrained" />
            <token id="36" string="and" />
            <token id="37" string="lovable" />
            <token id="38" string="personality" />
          </tokens>
        </chunking>
        <chunking id="20" string="spontaneous , unrehearsed , accidental , the inevitable consequences" type="NP">
          <tokens>
            <token id="21" string="spontaneous" />
            <token id="22" string="," />
            <token id="23" string="unrehearsed" />
            <token id="24" string="," />
            <token id="25" string="accidental" />
            <token id="26" string="," />
            <token id="27" string="the" />
            <token id="28" string="inevitable" />
            <token id="29" string="consequences" />
          </tokens>
        </chunking>
        <chunking id="21" string="all -- even the wildest , craziest capers --" type="NP">
          <tokens>
            <token id="4" string="all" />
            <token id="5" string="--" />
            <token id="6" string="even" />
            <token id="7" string="the" />
            <token id="8" string="wildest" />
            <token id="9" string="," />
            <token id="10" string="craziest" />
            <token id="11" string="capers" />
            <token id="12" string="--" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">made</governor>
          <dependent id="1">She</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">made</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">made</governor>
          <dependent id="3">it</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="13">look</governor>
          <dependent id="4">all</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="8">wildest</governor>
          <dependent id="6">even</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">wildest</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="4">all</governor>
          <dependent id="8">wildest</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">capers</governor>
          <dependent id="10">craziest</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="8">wildest</governor>
          <dependent id="11">capers</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="2">made</governor>
          <dependent id="13">look</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="18">happening</governor>
          <dependent id="14">as</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="14">as</governor>
          <dependent id="15">if</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="18">happening</governor>
          <dependent id="16">it</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="18">happening</governor>
          <dependent id="17">were</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="13">look</governor>
          <dependent id="18">happening</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="18">happening</governor>
          <dependent id="19">now</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="29">consequences</governor>
          <dependent id="21">spontaneous</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="25">accidental</governor>
          <dependent id="23">unrehearsed</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="29">consequences</governor>
          <dependent id="25">accidental</dependent>
        </dependency>
        <dependency type="det">
          <governor id="28">inevitable</governor>
          <dependent id="27">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="29">consequences</governor>
          <dependent id="28">inevitable</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="18">happening</governor>
          <dependent id="29">consequences</dependent>
        </dependency>
        <dependency type="case">
          <governor id="32">screen</governor>
          <dependent id="30">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="32">screen</governor>
          <dependent id="31">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="29">consequences</governor>
          <dependent id="32">screen</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="35">scatterbrained</governor>
          <dependent id="33">Lucy</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">Lucy</governor>
          <dependent id="34">'s</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="32">screen</governor>
          <dependent id="35">scatterbrained</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="35">scatterbrained</governor>
          <dependent id="36">and</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="38">personality</governor>
          <dependent id="37">lovable</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="35">scatterbrained</governor>
          <dependent id="38">personality</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="now" type="DATE" score="0.0">
          <tokens>
            <token id="19" string="now" />
          </tokens>
        </entity>
        <entity id="2" string="Lucy" type="PERSON" score="0.0">
          <tokens>
            <token id="33" string="Lucy" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="13" has_coreference="true">
      <content>Her foils -- Desi himself, Vivian Vance and William Frawley -- were superb farceurs themselves, but Lucy was the dynamo that made it all work.</content>
      <tokens>
        <token id="1" string="Her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="foils" lemma="foil" stem="foil" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="Desi" lemma="desi" stem="desi" pos="JJ" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="5" string="himself" lemma="himself" stem="himself" pos="PRP" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="Vivian" lemma="Vivian" stem="vivian" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="8" string="Vance" lemma="Vance" stem="vanc" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="9" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="William" lemma="William" stem="william" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="11" string="Frawley" lemma="Frawley" stem="frawlei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="12" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="superb" lemma="superb" stem="superb" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="farceurs" lemma="farceur" stem="farceur" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="themselves" lemma="themselves" stem="themselv" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="Lucy" lemma="Lucy" stem="luci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="20" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="dynamo" lemma="dynamo" stem="dynamo" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="made" lemma="make" stem="made" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="26" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="work" lemma="work" stem="work" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (NP (PRP$ Her) (NNS foils)) (PRN (: --) (NP (NP (JJ Desi)) (NP (NP (PRP himself)) (, ,) (NP (NNP Vivian) (NNP Vance)) (CC and) (NP (NNP William) (NNP Frawley)))) (: --))) (VP (VBD were) (ADJP (JJ superb) (NP (NP (NNS farceurs)) (ADVP (PRP themselves)))))) (, ,) (CC but) (S (NP (NNP Lucy)) (VP (VBD was) (NP (NP (DT the) (NN dynamo)) (SBAR (WHNP (WDT that)) (S (VP (VBD made) (S (NP (PRP it)) (NP (DT all) (NN work))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the dynamo that made it all work" type="NP">
          <tokens>
            <token id="21" string="the" />
            <token id="22" string="dynamo" />
            <token id="23" string="that" />
            <token id="24" string="made" />
            <token id="25" string="it" />
            <token id="26" string="all" />
            <token id="27" string="work" />
          </tokens>
        </chunking>
        <chunking id="2" string="farceurs" type="NP">
          <tokens>
            <token id="15" string="farceurs" />
          </tokens>
        </chunking>
        <chunking id="3" string="that made it all work" type="SBAR">
          <tokens>
            <token id="23" string="that" />
            <token id="24" string="made" />
            <token id="25" string="it" />
            <token id="26" string="all" />
            <token id="27" string="work" />
          </tokens>
        </chunking>
        <chunking id="4" string="himself , Vivian Vance and William Frawley" type="NP">
          <tokens>
            <token id="5" string="himself" />
            <token id="6" string="," />
            <token id="7" string="Vivian" />
            <token id="8" string="Vance" />
            <token id="9" string="and" />
            <token id="10" string="William" />
            <token id="11" string="Frawley" />
          </tokens>
        </chunking>
        <chunking id="5" string="was the dynamo that made it all work" type="VP">
          <tokens>
            <token id="20" string="was" />
            <token id="21" string="the" />
            <token id="22" string="dynamo" />
            <token id="23" string="that" />
            <token id="24" string="made" />
            <token id="25" string="it" />
            <token id="26" string="all" />
            <token id="27" string="work" />
          </tokens>
        </chunking>
        <chunking id="6" string="it" type="NP">
          <tokens>
            <token id="25" string="it" />
          </tokens>
        </chunking>
        <chunking id="7" string="the dynamo" type="NP">
          <tokens>
            <token id="21" string="the" />
            <token id="22" string="dynamo" />
          </tokens>
        </chunking>
        <chunking id="8" string="were superb farceurs themselves" type="VP">
          <tokens>
            <token id="13" string="were" />
            <token id="14" string="superb" />
            <token id="15" string="farceurs" />
            <token id="16" string="themselves" />
          </tokens>
        </chunking>
        <chunking id="9" string="superb farceurs themselves" type="ADJP">
          <tokens>
            <token id="14" string="superb" />
            <token id="15" string="farceurs" />
            <token id="16" string="themselves" />
          </tokens>
        </chunking>
        <chunking id="10" string="Her foils" type="NP">
          <tokens>
            <token id="1" string="Her" />
            <token id="2" string="foils" />
          </tokens>
        </chunking>
        <chunking id="11" string="Desi himself , Vivian Vance and William Frawley" type="NP">
          <tokens>
            <token id="4" string="Desi" />
            <token id="5" string="himself" />
            <token id="6" string="," />
            <token id="7" string="Vivian" />
            <token id="8" string="Vance" />
            <token id="9" string="and" />
            <token id="10" string="William" />
            <token id="11" string="Frawley" />
          </tokens>
        </chunking>
        <chunking id="12" string="Desi" type="NP">
          <tokens>
            <token id="4" string="Desi" />
          </tokens>
        </chunking>
        <chunking id="13" string="William Frawley" type="NP">
          <tokens>
            <token id="10" string="William" />
            <token id="11" string="Frawley" />
          </tokens>
        </chunking>
        <chunking id="14" string="all work" type="NP">
          <tokens>
            <token id="26" string="all" />
            <token id="27" string="work" />
          </tokens>
        </chunking>
        <chunking id="15" string="Lucy" type="NP">
          <tokens>
            <token id="19" string="Lucy" />
          </tokens>
        </chunking>
        <chunking id="16" string="made it all work" type="VP">
          <tokens>
            <token id="24" string="made" />
            <token id="25" string="it" />
            <token id="26" string="all" />
            <token id="27" string="work" />
          </tokens>
        </chunking>
        <chunking id="17" string="farceurs themselves" type="NP">
          <tokens>
            <token id="15" string="farceurs" />
            <token id="16" string="themselves" />
          </tokens>
        </chunking>
        <chunking id="18" string="himself" type="NP">
          <tokens>
            <token id="5" string="himself" />
          </tokens>
        </chunking>
        <chunking id="19" string="Vivian Vance" type="NP">
          <tokens>
            <token id="7" string="Vivian" />
            <token id="8" string="Vance" />
          </tokens>
        </chunking>
        <chunking id="20" string="Her foils -- Desi himself , Vivian Vance and William Frawley --" type="NP">
          <tokens>
            <token id="1" string="Her" />
            <token id="2" string="foils" />
            <token id="3" string="--" />
            <token id="4" string="Desi" />
            <token id="5" string="himself" />
            <token id="6" string="," />
            <token id="7" string="Vivian" />
            <token id="8" string="Vance" />
            <token id="9" string="and" />
            <token id="10" string="William" />
            <token id="11" string="Frawley" />
            <token id="12" string="--" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="2">foils</governor>
          <dependent id="1">Her</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">superb</governor>
          <dependent id="2">foils</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="2">foils</governor>
          <dependent id="4">Desi</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="4">Desi</governor>
          <dependent id="5">himself</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">Vance</governor>
          <dependent id="7">Vivian</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="5">himself</governor>
          <dependent id="8">Vance</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="5">himself</governor>
          <dependent id="9">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">Frawley</governor>
          <dependent id="10">William</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="5">himself</governor>
          <dependent id="11">Frawley</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="14">superb</governor>
          <dependent id="13">were</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="14">superb</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="14">superb</governor>
          <dependent id="15">farceurs</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="15">farceurs</governor>
          <dependent id="16">themselves</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="14">superb</governor>
          <dependent id="18">but</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">dynamo</governor>
          <dependent id="19">Lucy</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="22">dynamo</governor>
          <dependent id="20">was</dependent>
        </dependency>
        <dependency type="det">
          <governor id="22">dynamo</governor>
          <dependent id="21">the</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">superb</governor>
          <dependent id="22">dynamo</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="24">made</governor>
          <dependent id="23">that</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="22">dynamo</governor>
          <dependent id="24">made</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="27">work</governor>
          <dependent id="25">it</dependent>
        </dependency>
        <dependency type="det">
          <governor id="27">work</governor>
          <dependent id="26">all</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="24">made</governor>
          <dependent id="27">work</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Desi" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="Desi" />
          </tokens>
        </entity>
        <entity id="2" string="William Frawley" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="William" />
            <token id="11" string="Frawley" />
          </tokens>
        </entity>
        <entity id="3" string="Lucy" type="PERSON" score="0.0">
          <tokens>
            <token id="19" string="Lucy" />
          </tokens>
        </entity>
        <entity id="4" string="Vivian Vance" type="PERSON" score="0.0">
          <tokens>
            <token id="7" string="Vivian" />
            <token id="8" string="Vance" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="14" has_coreference="true">
      <content>The craft was invisible, the skills so perfected they concealed themselves totally.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="craft" lemma="craft" stem="craft" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="invisible" lemma="invisible" stem="invis" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="skills" lemma="skill" stem="skill" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="so" lemma="so" stem="so" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="perfected" lemma="perfect" stem="perfect" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="concealed" lemma="conceal" stem="conceal" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="themselves" lemma="themselves" stem="themselv" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="totally" lemma="totally" stem="total" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (DT The) (NN craft)) (VP (VBD was) (ADJP (JJ invisible)))) (, ,) (NP (DT the) (NNS skills)) (ADVP (IN so)) (VP (VBN perfected) (S (NP (PRP they)) (VP (VBD concealed) (NP (PRP themselves)) (ADVP (RB totally))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="they" type="NP">
          <tokens>
            <token id="10" string="they" />
          </tokens>
        </chunking>
        <chunking id="2" string="concealed themselves totally" type="VP">
          <tokens>
            <token id="11" string="concealed" />
            <token id="12" string="themselves" />
            <token id="13" string="totally" />
          </tokens>
        </chunking>
        <chunking id="3" string="was invisible" type="VP">
          <tokens>
            <token id="3" string="was" />
            <token id="4" string="invisible" />
          </tokens>
        </chunking>
        <chunking id="4" string="the skills" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="skills" />
          </tokens>
        </chunking>
        <chunking id="5" string="themselves" type="NP">
          <tokens>
            <token id="12" string="themselves" />
          </tokens>
        </chunking>
        <chunking id="6" string="perfected they concealed themselves totally" type="VP">
          <tokens>
            <token id="9" string="perfected" />
            <token id="10" string="they" />
            <token id="11" string="concealed" />
            <token id="12" string="themselves" />
            <token id="13" string="totally" />
          </tokens>
        </chunking>
        <chunking id="7" string="The craft" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="craft" />
          </tokens>
        </chunking>
        <chunking id="8" string="invisible" type="ADJP">
          <tokens>
            <token id="4" string="invisible" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">craft</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">invisible</governor>
          <dependent id="2">craft</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="4">invisible</governor>
          <dependent id="3">was</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="9">perfected</governor>
          <dependent id="4">invisible</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">skills</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">perfected</governor>
          <dependent id="7">skills</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="9">perfected</governor>
          <dependent id="8">so</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="9">perfected</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">concealed</governor>
          <dependent id="10">they</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="9">perfected</governor>
          <dependent id="11">concealed</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">concealed</governor>
          <dependent id="12">themselves</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">concealed</governor>
          <dependent id="13">totally</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="15" has_coreference="true">
      <content>It was a brilliant illusion, generating a charm that hid the hard work and the artful writing and editing as well as the performing.</content>
      <tokens>
        <token id="1" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="brilliant" lemma="brilliant" stem="brilliant" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="illusion" lemma="illusion" stem="illus" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="generating" lemma="generate" stem="gener" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="charm" lemma="charm" stem="charm" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="hid" lemma="hide" stem="hid" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="hard" lemma="hard" stem="hard" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="work" lemma="work" stem="work" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="artful" lemma="artful" stem="art" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="writing" lemma="writing" stem="write" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="editing" lemma="editing" stem="edit" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="as" lemma="as" stem="a" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="well" lemma="well" stem="well" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="25" string="performing" lemma="perform" stem="perform" pos="VBG" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="26" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP It)) (VP (VBD was) (NP (DT a) (JJ brilliant) (NN illusion)) (, ,) (S (VP (VBG generating) (NP (NP (DT a) (NN charm)) (SBAR (WHNP (WDT that)) (S (VP (VBD hid) (NP (NP (NP (DT the) (JJ hard) (NN work)) (CC and) (NP (DT the) (JJ artful) (NN writing) (CC and) (NN editing))) (CONJP (RB as) (RB well) (IN as)) (NP (NP (DT the)) (VP (VBG performing))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the hard work" type="NP">
          <tokens>
            <token id="12" string="the" />
            <token id="13" string="hard" />
            <token id="14" string="work" />
          </tokens>
        </chunking>
        <chunking id="2" string="the performing" type="NP">
          <tokens>
            <token id="24" string="the" />
            <token id="25" string="performing" />
          </tokens>
        </chunking>
        <chunking id="3" string="the artful writing and editing" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="artful" />
            <token id="18" string="writing" />
            <token id="19" string="and" />
            <token id="20" string="editing" />
          </tokens>
        </chunking>
        <chunking id="4" string="performing" type="VP">
          <tokens>
            <token id="25" string="performing" />
          </tokens>
        </chunking>
        <chunking id="5" string="the hard work and the artful writing and editing as well as the performing" type="NP">
          <tokens>
            <token id="12" string="the" />
            <token id="13" string="hard" />
            <token id="14" string="work" />
            <token id="15" string="and" />
            <token id="16" string="the" />
            <token id="17" string="artful" />
            <token id="18" string="writing" />
            <token id="19" string="and" />
            <token id="20" string="editing" />
            <token id="21" string="as" />
            <token id="22" string="well" />
            <token id="23" string="as" />
            <token id="24" string="the" />
            <token id="25" string="performing" />
          </tokens>
        </chunking>
        <chunking id="6" string="It" type="NP">
          <tokens>
            <token id="1" string="It" />
          </tokens>
        </chunking>
        <chunking id="7" string="a charm that hid the hard work and the artful writing and editing as well as the performing" type="NP">
          <tokens>
            <token id="8" string="a" />
            <token id="9" string="charm" />
            <token id="10" string="that" />
            <token id="11" string="hid" />
            <token id="12" string="the" />
            <token id="13" string="hard" />
            <token id="14" string="work" />
            <token id="15" string="and" />
            <token id="16" string="the" />
            <token id="17" string="artful" />
            <token id="18" string="writing" />
            <token id="19" string="and" />
            <token id="20" string="editing" />
            <token id="21" string="as" />
            <token id="22" string="well" />
            <token id="23" string="as" />
            <token id="24" string="the" />
            <token id="25" string="performing" />
          </tokens>
        </chunking>
        <chunking id="8" string="a charm" type="NP">
          <tokens>
            <token id="8" string="a" />
            <token id="9" string="charm" />
          </tokens>
        </chunking>
        <chunking id="9" string="that hid the hard work and the artful writing and editing as well as the performing" type="SBAR">
          <tokens>
            <token id="10" string="that" />
            <token id="11" string="hid" />
            <token id="12" string="the" />
            <token id="13" string="hard" />
            <token id="14" string="work" />
            <token id="15" string="and" />
            <token id="16" string="the" />
            <token id="17" string="artful" />
            <token id="18" string="writing" />
            <token id="19" string="and" />
            <token id="20" string="editing" />
            <token id="21" string="as" />
            <token id="22" string="well" />
            <token id="23" string="as" />
            <token id="24" string="the" />
            <token id="25" string="performing" />
          </tokens>
        </chunking>
        <chunking id="10" string="was a brilliant illusion , generating a charm that hid the hard work and the artful writing and editing as well as the performing" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="a" />
            <token id="4" string="brilliant" />
            <token id="5" string="illusion" />
            <token id="6" string="," />
            <token id="7" string="generating" />
            <token id="8" string="a" />
            <token id="9" string="charm" />
            <token id="10" string="that" />
            <token id="11" string="hid" />
            <token id="12" string="the" />
            <token id="13" string="hard" />
            <token id="14" string="work" />
            <token id="15" string="and" />
            <token id="16" string="the" />
            <token id="17" string="artful" />
            <token id="18" string="writing" />
            <token id="19" string="and" />
            <token id="20" string="editing" />
            <token id="21" string="as" />
            <token id="22" string="well" />
            <token id="23" string="as" />
            <token id="24" string="the" />
            <token id="25" string="performing" />
          </tokens>
        </chunking>
        <chunking id="11" string="a brilliant illusion" type="NP">
          <tokens>
            <token id="3" string="a" />
            <token id="4" string="brilliant" />
            <token id="5" string="illusion" />
          </tokens>
        </chunking>
        <chunking id="12" string="the" type="NP">
          <tokens>
            <token id="24" string="the" />
          </tokens>
        </chunking>
        <chunking id="13" string="the hard work and the artful writing and editing" type="NP">
          <tokens>
            <token id="12" string="the" />
            <token id="13" string="hard" />
            <token id="14" string="work" />
            <token id="15" string="and" />
            <token id="16" string="the" />
            <token id="17" string="artful" />
            <token id="18" string="writing" />
            <token id="19" string="and" />
            <token id="20" string="editing" />
          </tokens>
        </chunking>
        <chunking id="14" string="hid the hard work and the artful writing and editing as well as the performing" type="VP">
          <tokens>
            <token id="11" string="hid" />
            <token id="12" string="the" />
            <token id="13" string="hard" />
            <token id="14" string="work" />
            <token id="15" string="and" />
            <token id="16" string="the" />
            <token id="17" string="artful" />
            <token id="18" string="writing" />
            <token id="19" string="and" />
            <token id="20" string="editing" />
            <token id="21" string="as" />
            <token id="22" string="well" />
            <token id="23" string="as" />
            <token id="24" string="the" />
            <token id="25" string="performing" />
          </tokens>
        </chunking>
        <chunking id="15" string="generating a charm that hid the hard work and the artful writing and editing as well as the performing" type="VP">
          <tokens>
            <token id="7" string="generating" />
            <token id="8" string="a" />
            <token id="9" string="charm" />
            <token id="10" string="that" />
            <token id="11" string="hid" />
            <token id="12" string="the" />
            <token id="13" string="hard" />
            <token id="14" string="work" />
            <token id="15" string="and" />
            <token id="16" string="the" />
            <token id="17" string="artful" />
            <token id="18" string="writing" />
            <token id="19" string="and" />
            <token id="20" string="editing" />
            <token id="21" string="as" />
            <token id="22" string="well" />
            <token id="23" string="as" />
            <token id="24" string="the" />
            <token id="25" string="performing" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="5">illusion</governor>
          <dependent id="1">It</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="5">illusion</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">illusion</governor>
          <dependent id="3">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">illusion</governor>
          <dependent id="4">brilliant</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">illusion</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="5">illusion</governor>
          <dependent id="7">generating</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">charm</governor>
          <dependent id="8">a</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">generating</governor>
          <dependent id="9">charm</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">hid</governor>
          <dependent id="10">that</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="9">charm</governor>
          <dependent id="11">hid</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">work</governor>
          <dependent id="12">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="14">work</governor>
          <dependent id="13">hard</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">hid</governor>
          <dependent id="14">work</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="14">work</governor>
          <dependent id="15">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">writing</governor>
          <dependent id="16">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="18">writing</governor>
          <dependent id="17">artful</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">work</governor>
          <dependent id="18">writing</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="18">writing</governor>
          <dependent id="19">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="18">writing</governor>
          <dependent id="20">editing</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="14">work</governor>
          <dependent id="21">as</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="21">as</governor>
          <dependent id="22">well</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="21">as</governor>
          <dependent id="23">as</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">work</governor>
          <dependent id="24">the</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="24">the</governor>
          <dependent id="25">performing</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="16" has_coreference="false">
      <content>The result was probably the finest and certainly the most durable single series in television&amp;apost;s history thus far.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="result" lemma="result" stem="result" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="probably" lemma="probably" stem="probabl" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="finest" lemma="finest" stem="finest" pos="JJS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="certainly" lemma="certainly" stem="certainli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="most" lemma="most" stem="most" pos="RBS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="durable" lemma="durable" stem="durabl" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="single" lemma="single" stem="singl" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="series" lemma="series" stem="seri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="television" lemma="television" stem="televis" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="history" lemma="history" stem="histori" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="thus" lemma="thus" stem="thu" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="far" lemma="far" stem="far" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN result)) (VP (VBD was) (ADVP (RB probably)) (NP (NP (DT the) (JJS finest)) (CC and) (NP (NP (ADVP (RB certainly)) (DT the) (ADJP (RBS most) (JJ durable)) (JJ single) (NN series)) (PP (IN in) (NP (NP (NN television) (POS 's)) (NN history))))) (ADVP (RB thus) (RB far))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="most durable" type="ADJP">
          <tokens>
            <token id="10" string="most" />
            <token id="11" string="durable" />
          </tokens>
        </chunking>
        <chunking id="2" string="television 's" type="NP">
          <tokens>
            <token id="15" string="television" />
            <token id="16" string="'s" />
          </tokens>
        </chunking>
        <chunking id="3" string="the finest" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="finest" />
          </tokens>
        </chunking>
        <chunking id="4" string="television 's history" type="NP">
          <tokens>
            <token id="15" string="television" />
            <token id="16" string="'s" />
            <token id="17" string="history" />
          </tokens>
        </chunking>
        <chunking id="5" string="certainly the most durable single series in television 's history" type="NP">
          <tokens>
            <token id="8" string="certainly" />
            <token id="9" string="the" />
            <token id="10" string="most" />
            <token id="11" string="durable" />
            <token id="12" string="single" />
            <token id="13" string="series" />
            <token id="14" string="in" />
            <token id="15" string="television" />
            <token id="16" string="'s" />
            <token id="17" string="history" />
          </tokens>
        </chunking>
        <chunking id="6" string="the finest and certainly the most durable single series in television 's history" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="finest" />
            <token id="7" string="and" />
            <token id="8" string="certainly" />
            <token id="9" string="the" />
            <token id="10" string="most" />
            <token id="11" string="durable" />
            <token id="12" string="single" />
            <token id="13" string="series" />
            <token id="14" string="in" />
            <token id="15" string="television" />
            <token id="16" string="'s" />
            <token id="17" string="history" />
          </tokens>
        </chunking>
        <chunking id="7" string="The result" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="result" />
          </tokens>
        </chunking>
        <chunking id="8" string="was probably the finest and certainly the most durable single series in television 's history thus far" type="VP">
          <tokens>
            <token id="3" string="was" />
            <token id="4" string="probably" />
            <token id="5" string="the" />
            <token id="6" string="finest" />
            <token id="7" string="and" />
            <token id="8" string="certainly" />
            <token id="9" string="the" />
            <token id="10" string="most" />
            <token id="11" string="durable" />
            <token id="12" string="single" />
            <token id="13" string="series" />
            <token id="14" string="in" />
            <token id="15" string="television" />
            <token id="16" string="'s" />
            <token id="17" string="history" />
            <token id="18" string="thus" />
            <token id="19" string="far" />
          </tokens>
        </chunking>
        <chunking id="9" string="certainly the most durable single series" type="NP">
          <tokens>
            <token id="8" string="certainly" />
            <token id="9" string="the" />
            <token id="10" string="most" />
            <token id="11" string="durable" />
            <token id="12" string="single" />
            <token id="13" string="series" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">result</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">finest</governor>
          <dependent id="2">result</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="6">finest</governor>
          <dependent id="3">was</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="6">finest</governor>
          <dependent id="4">probably</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">finest</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">finest</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="6">finest</governor>
          <dependent id="7">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="13">series</governor>
          <dependent id="8">certainly</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">series</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">durable</governor>
          <dependent id="10">most</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">series</governor>
          <dependent id="11">durable</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">series</governor>
          <dependent id="12">single</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="6">finest</governor>
          <dependent id="13">series</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">history</governor>
          <dependent id="14">in</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="17">history</governor>
          <dependent id="15">television</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">television</governor>
          <dependent id="16">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">series</governor>
          <dependent id="17">history</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="19">far</governor>
          <dependent id="18">thus</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="6">finest</governor>
          <dependent id="19">far</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="17" has_coreference="true">
      <content>The appeal of the Lucy shows was also the most universal that American television has yet provided.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="appeal" lemma="appeal" stem="appeal" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="Lucy" lemma="Lucy" stem="luci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="6" string="shows" lemma="show" stem="show" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="most" lemma="most" stem="most" pos="RBS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="universal" lemma="universal" stem="univers" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="American" lemma="american" stem="american" pos="JJ" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="false" is_refers="false" />
        <token id="14" string="television" lemma="television" stem="televis" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="yet" lemma="yet" stem="yet" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="provided" lemma="provide" stem="provid" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT The) (NN appeal)) (PP (IN of) (NP (DT the) (NNP Lucy) (NNS shows)))) (VP (VBD was) (ADVP (RB also)) (NP (DT the) (RBS most) (JJ universal)) (SBAR (IN that) (S (NP (JJ American) (NN television)) (VP (VBZ has) (ADVP (RB yet)) (VP (VBN provided)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="was also the most universal that American television has yet provided" type="VP">
          <tokens>
            <token id="7" string="was" />
            <token id="8" string="also" />
            <token id="9" string="the" />
            <token id="10" string="most" />
            <token id="11" string="universal" />
            <token id="12" string="that" />
            <token id="13" string="American" />
            <token id="14" string="television" />
            <token id="15" string="has" />
            <token id="16" string="yet" />
            <token id="17" string="provided" />
          </tokens>
        </chunking>
        <chunking id="2" string="the Lucy shows" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="Lucy" />
            <token id="6" string="shows" />
          </tokens>
        </chunking>
        <chunking id="3" string="has yet provided" type="VP">
          <tokens>
            <token id="15" string="has" />
            <token id="16" string="yet" />
            <token id="17" string="provided" />
          </tokens>
        </chunking>
        <chunking id="4" string="American television" type="NP">
          <tokens>
            <token id="13" string="American" />
            <token id="14" string="television" />
          </tokens>
        </chunking>
        <chunking id="5" string="The appeal" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="appeal" />
          </tokens>
        </chunking>
        <chunking id="6" string="the most universal" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="most" />
            <token id="11" string="universal" />
          </tokens>
        </chunking>
        <chunking id="7" string="provided" type="VP">
          <tokens>
            <token id="17" string="provided" />
          </tokens>
        </chunking>
        <chunking id="8" string="that American television has yet provided" type="SBAR">
          <tokens>
            <token id="12" string="that" />
            <token id="13" string="American" />
            <token id="14" string="television" />
            <token id="15" string="has" />
            <token id="16" string="yet" />
            <token id="17" string="provided" />
          </tokens>
        </chunking>
        <chunking id="9" string="The appeal of the Lucy shows" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="appeal" />
            <token id="3" string="of" />
            <token id="4" string="the" />
            <token id="5" string="Lucy" />
            <token id="6" string="shows" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">appeal</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">universal</governor>
          <dependent id="2">appeal</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">shows</governor>
          <dependent id="3">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">shows</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">shows</governor>
          <dependent id="5">Lucy</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">appeal</governor>
          <dependent id="6">shows</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="11">universal</governor>
          <dependent id="7">was</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">universal</governor>
          <dependent id="8">also</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">universal</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">universal</governor>
          <dependent id="10">most</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="11">universal</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="17">provided</governor>
          <dependent id="12">that</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="14">television</governor>
          <dependent id="13">American</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="17">provided</governor>
          <dependent id="14">television</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="17">provided</governor>
          <dependent id="15">has</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="17">provided</governor>
          <dependent id="16">yet</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="11">universal</governor>
          <dependent id="17">provided</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Lucy" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="Lucy" />
          </tokens>
        </entity>
        <entity id="2" string="American" type="NATIONALITY" score="0.0">
          <tokens>
            <token id="13" string="American" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="18" has_coreference="true">
      <content>They were wholly, prototypically American, and that is central to their popularity abroad.</content>
      <tokens>
        <token id="1" string="They" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="wholly" lemma="wholly" stem="wholli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="prototypically" lemma="prototypically" stem="prototyp" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="American" lemma="american" stem="american" pos="JJ" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="false" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="that" lemma="that" stem="that" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="central" lemma="central" stem="central" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="popularity" lemma="popularity" stem="popular" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="abroad" lemma="abroad" stem="abroad" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (PRP They)) (VP (VBD were) (ADJP (RB wholly) (, ,) (RB prototypically) (JJ American)))) (, ,) (CC and) (S (NP (DT that)) (VP (VBZ is) (ADJP (JJ central) (PP (TO to) (NP (PRP$ their) (NN popularity)))) (ADVP (RB abroad)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="They" type="NP">
          <tokens>
            <token id="1" string="They" />
          </tokens>
        </chunking>
        <chunking id="2" string="that" type="NP">
          <tokens>
            <token id="9" string="that" />
          </tokens>
        </chunking>
        <chunking id="3" string="were wholly , prototypically American" type="VP">
          <tokens>
            <token id="2" string="were" />
            <token id="3" string="wholly" />
            <token id="4" string="," />
            <token id="5" string="prototypically" />
            <token id="6" string="American" />
          </tokens>
        </chunking>
        <chunking id="4" string="central to their popularity" type="ADJP">
          <tokens>
            <token id="11" string="central" />
            <token id="12" string="to" />
            <token id="13" string="their" />
            <token id="14" string="popularity" />
          </tokens>
        </chunking>
        <chunking id="5" string="their popularity" type="NP">
          <tokens>
            <token id="13" string="their" />
            <token id="14" string="popularity" />
          </tokens>
        </chunking>
        <chunking id="6" string="wholly , prototypically American" type="ADJP">
          <tokens>
            <token id="3" string="wholly" />
            <token id="4" string="," />
            <token id="5" string="prototypically" />
            <token id="6" string="American" />
          </tokens>
        </chunking>
        <chunking id="7" string="is central to their popularity abroad" type="VP">
          <tokens>
            <token id="10" string="is" />
            <token id="11" string="central" />
            <token id="12" string="to" />
            <token id="13" string="their" />
            <token id="14" string="popularity" />
            <token id="15" string="abroad" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="6">American</governor>
          <dependent id="1">They</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="6">American</governor>
          <dependent id="2">were</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="6">American</governor>
          <dependent id="3">wholly</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="6">American</governor>
          <dependent id="5">prototypically</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">American</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="6">American</governor>
          <dependent id="8">and</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">central</governor>
          <dependent id="9">that</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="11">central</governor>
          <dependent id="10">is</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="6">American</governor>
          <dependent id="11">central</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">popularity</governor>
          <dependent id="12">to</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="14">popularity</governor>
          <dependent id="13">their</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">central</governor>
          <dependent id="14">popularity</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">central</governor>
          <dependent id="15">abroad</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="American" type="NATIONALITY" score="0.0">
          <tokens>
            <token id="6" string="American" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="19" has_coreference="true">
      <content>Yet they have been embraced as the kind of middle-class goings-on that might possibly arise almost anywhere there is a middle class.</content>
      <tokens>
        <token id="1" string="Yet" lemma="yet" stem="yet" pos="CC" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="have" lemma="have" stem="have" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="embraced" lemma="embrace" stem="embrac" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="kind" lemma="kind" stem="kind" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="middle-class" lemma="middle-class" stem="middle-class" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="goings-on" lemma="goings-on" stem="goings-on" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="might" lemma="might" stem="might" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="possibly" lemma="possibly" stem="possibli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="arise" lemma="arise" stem="aris" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="almost" lemma="almost" stem="almost" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="anywhere" lemma="anywhere" stem="anywher" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="there" lemma="there" stem="there" pos="EX" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="middle" lemma="middle" stem="middl" pos="JJ" type="Word" isStopWord="false" ner="IDEOLOGY" is_referenced="false" is_refers="false" />
        <token id="22" string="class" lemma="class" stem="class" pos="NN" type="Word" isStopWord="false" ner="IDEOLOGY" is_referenced="false" is_refers="false" />
        <token id="23" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (CC Yet) (NP (PRP they)) (VP (VBP have) (VP (VBN been) (VP (VBN embraced) (PP (IN as) (NP (NP (DT the) (NN kind)) (PP (IN of) (NP (NN middle-class) (NNS goings-on))) (SBAR (WHNP (WDT that)) (S (VP (MD might) (ADVP (RB possibly)) (VP (VB arise) (NP (RB almost) (RB anywhere) (SBAR (S (NP (EX there)) (VP (VBZ is) (NP (DT a) (JJ middle) (NN class))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="embraced as the kind of middle-class goings-on that might possibly arise almost anywhere there is a middle class" type="VP">
          <tokens>
            <token id="5" string="embraced" />
            <token id="6" string="as" />
            <token id="7" string="the" />
            <token id="8" string="kind" />
            <token id="9" string="of" />
            <token id="10" string="middle-class" />
            <token id="11" string="goings-on" />
            <token id="12" string="that" />
            <token id="13" string="might" />
            <token id="14" string="possibly" />
            <token id="15" string="arise" />
            <token id="16" string="almost" />
            <token id="17" string="anywhere" />
            <token id="18" string="there" />
            <token id="19" string="is" />
            <token id="20" string="a" />
            <token id="21" string="middle" />
            <token id="22" string="class" />
          </tokens>
        </chunking>
        <chunking id="2" string="there is a middle class" type="SBAR">
          <tokens>
            <token id="18" string="there" />
            <token id="19" string="is" />
            <token id="20" string="a" />
            <token id="21" string="middle" />
            <token id="22" string="class" />
          </tokens>
        </chunking>
        <chunking id="3" string="almost anywhere there is a middle class" type="NP">
          <tokens>
            <token id="16" string="almost" />
            <token id="17" string="anywhere" />
            <token id="18" string="there" />
            <token id="19" string="is" />
            <token id="20" string="a" />
            <token id="21" string="middle" />
            <token id="22" string="class" />
          </tokens>
        </chunking>
        <chunking id="4" string="middle-class goings-on" type="NP">
          <tokens>
            <token id="10" string="middle-class" />
            <token id="11" string="goings-on" />
          </tokens>
        </chunking>
        <chunking id="5" string="there" type="NP">
          <tokens>
            <token id="18" string="there" />
          </tokens>
        </chunking>
        <chunking id="6" string="a middle class" type="NP">
          <tokens>
            <token id="20" string="a" />
            <token id="21" string="middle" />
            <token id="22" string="class" />
          </tokens>
        </chunking>
        <chunking id="7" string="they" type="NP">
          <tokens>
            <token id="2" string="they" />
          </tokens>
        </chunking>
        <chunking id="8" string="have been embraced as the kind of middle-class goings-on that might possibly arise almost anywhere there is a middle class" type="VP">
          <tokens>
            <token id="3" string="have" />
            <token id="4" string="been" />
            <token id="5" string="embraced" />
            <token id="6" string="as" />
            <token id="7" string="the" />
            <token id="8" string="kind" />
            <token id="9" string="of" />
            <token id="10" string="middle-class" />
            <token id="11" string="goings-on" />
            <token id="12" string="that" />
            <token id="13" string="might" />
            <token id="14" string="possibly" />
            <token id="15" string="arise" />
            <token id="16" string="almost" />
            <token id="17" string="anywhere" />
            <token id="18" string="there" />
            <token id="19" string="is" />
            <token id="20" string="a" />
            <token id="21" string="middle" />
            <token id="22" string="class" />
          </tokens>
        </chunking>
        <chunking id="9" string="the kind of middle-class goings-on that might possibly arise almost anywhere there is a middle class" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="kind" />
            <token id="9" string="of" />
            <token id="10" string="middle-class" />
            <token id="11" string="goings-on" />
            <token id="12" string="that" />
            <token id="13" string="might" />
            <token id="14" string="possibly" />
            <token id="15" string="arise" />
            <token id="16" string="almost" />
            <token id="17" string="anywhere" />
            <token id="18" string="there" />
            <token id="19" string="is" />
            <token id="20" string="a" />
            <token id="21" string="middle" />
            <token id="22" string="class" />
          </tokens>
        </chunking>
        <chunking id="10" string="been embraced as the kind of middle-class goings-on that might possibly arise almost anywhere there is a middle class" type="VP">
          <tokens>
            <token id="4" string="been" />
            <token id="5" string="embraced" />
            <token id="6" string="as" />
            <token id="7" string="the" />
            <token id="8" string="kind" />
            <token id="9" string="of" />
            <token id="10" string="middle-class" />
            <token id="11" string="goings-on" />
            <token id="12" string="that" />
            <token id="13" string="might" />
            <token id="14" string="possibly" />
            <token id="15" string="arise" />
            <token id="16" string="almost" />
            <token id="17" string="anywhere" />
            <token id="18" string="there" />
            <token id="19" string="is" />
            <token id="20" string="a" />
            <token id="21" string="middle" />
            <token id="22" string="class" />
          </tokens>
        </chunking>
        <chunking id="11" string="might possibly arise almost anywhere there is a middle class" type="VP">
          <tokens>
            <token id="13" string="might" />
            <token id="14" string="possibly" />
            <token id="15" string="arise" />
            <token id="16" string="almost" />
            <token id="17" string="anywhere" />
            <token id="18" string="there" />
            <token id="19" string="is" />
            <token id="20" string="a" />
            <token id="21" string="middle" />
            <token id="22" string="class" />
          </tokens>
        </chunking>
        <chunking id="12" string="is a middle class" type="VP">
          <tokens>
            <token id="19" string="is" />
            <token id="20" string="a" />
            <token id="21" string="middle" />
            <token id="22" string="class" />
          </tokens>
        </chunking>
        <chunking id="13" string="the kind" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="kind" />
          </tokens>
        </chunking>
        <chunking id="14" string="that might possibly arise almost anywhere there is a middle class" type="SBAR">
          <tokens>
            <token id="12" string="that" />
            <token id="13" string="might" />
            <token id="14" string="possibly" />
            <token id="15" string="arise" />
            <token id="16" string="almost" />
            <token id="17" string="anywhere" />
            <token id="18" string="there" />
            <token id="19" string="is" />
            <token id="20" string="a" />
            <token id="21" string="middle" />
            <token id="22" string="class" />
          </tokens>
        </chunking>
        <chunking id="15" string="arise almost anywhere there is a middle class" type="VP">
          <tokens>
            <token id="15" string="arise" />
            <token id="16" string="almost" />
            <token id="17" string="anywhere" />
            <token id="18" string="there" />
            <token id="19" string="is" />
            <token id="20" string="a" />
            <token id="21" string="middle" />
            <token id="22" string="class" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="cc">
          <governor id="5">embraced</governor>
          <dependent id="1">Yet</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="5">embraced</governor>
          <dependent id="2">they</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="5">embraced</governor>
          <dependent id="3">have</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="5">embraced</governor>
          <dependent id="4">been</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">embraced</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">kind</governor>
          <dependent id="6">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">kind</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">embraced</governor>
          <dependent id="8">kind</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">goings-on</governor>
          <dependent id="9">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">goings-on</governor>
          <dependent id="10">middle-class</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">kind</governor>
          <dependent id="11">goings-on</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">arise</governor>
          <dependent id="12">that</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="15">arise</governor>
          <dependent id="13">might</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="15">arise</governor>
          <dependent id="14">possibly</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="8">kind</governor>
          <dependent id="15">arise</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="17">anywhere</governor>
          <dependent id="16">almost</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">arise</governor>
          <dependent id="17">anywhere</dependent>
        </dependency>
        <dependency type="expl">
          <governor id="19">is</governor>
          <dependent id="18">there</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="17">anywhere</governor>
          <dependent id="19">is</dependent>
        </dependency>
        <dependency type="det">
          <governor id="22">class</governor>
          <dependent id="20">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="22">class</governor>
          <dependent id="21">middle</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">is</governor>
          <dependent id="22">class</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="middle class" type="IDEOLOGY" score="0.0">
          <tokens>
            <token id="21" string="middle" />
            <token id="22" string="class" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="20" has_coreference="true">
      <content>Lucy was all-American but, equally, a citizen of the world.</content>
      <tokens>
        <token id="1" string="Lucy" lemma="Lucy" stem="luci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="all-American" lemma="all-american" stem="all-american" pos="JJ" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="4" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="equally" lemma="equally" stem="equal" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="citizen" lemma="citizen" stem="citizen" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="world" lemma="world" stem="world" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Lucy)) (VP (VBD was) (ADJP (JJ all-American)) (PRN (CC but) (, ,) (ADVP (RB equally)) (, ,) (NP (NP (DT a) (NN citizen)) (PP (IN of) (NP (DT the) (NN world)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a citizen" type="NP">
          <tokens>
            <token id="8" string="a" />
            <token id="9" string="citizen" />
          </tokens>
        </chunking>
        <chunking id="2" string="all-American" type="ADJP">
          <tokens>
            <token id="3" string="all-American" />
          </tokens>
        </chunking>
        <chunking id="3" string="Lucy" type="NP">
          <tokens>
            <token id="1" string="Lucy" />
          </tokens>
        </chunking>
        <chunking id="4" string="was all-American but , equally , a citizen of the world" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="all-American" />
            <token id="4" string="but" />
            <token id="5" string="," />
            <token id="6" string="equally" />
            <token id="7" string="," />
            <token id="8" string="a" />
            <token id="9" string="citizen" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="world" />
          </tokens>
        </chunking>
        <chunking id="5" string="the world" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="world" />
          </tokens>
        </chunking>
        <chunking id="6" string="a citizen of the world" type="NP">
          <tokens>
            <token id="8" string="a" />
            <token id="9" string="citizen" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="world" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">all-American</governor>
          <dependent id="1">Lucy</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="3">all-American</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">all-American</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="9">citizen</governor>
          <dependent id="4">but</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="9">citizen</governor>
          <dependent id="6">equally</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">citizen</governor>
          <dependent id="8">a</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="3">all-American</governor>
          <dependent id="9">citizen</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">world</governor>
          <dependent id="10">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">world</governor>
          <dependent id="11">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">citizen</governor>
          <dependent id="12">world</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="all-American" type="MISC" score="0.0">
          <tokens>
            <token id="3" string="all-American" />
          </tokens>
        </entity>
        <entity id="2" string="Lucy" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Lucy" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="21" has_coreference="true">
      <content>Her successful career in films, in which she ultimately earned star billing if not superstar status, gave her an invaluable identification when &amp;quot;I Love Lucy&amp;quot; began.</content>
      <tokens>
        <token id="1" string="Her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="successful" lemma="successful" stem="success" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="career" lemma="career" stem="career" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="films" lemma="film" stem="film" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="which" lemma="which" stem="which" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="ultimately" lemma="ultimately" stem="ultim" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="earned" lemma="earn" stem="earn" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="star" lemma="star" stem="star" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="billing" lemma="billing" stem="bill" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="if" lemma="if" stem="if" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="superstar" lemma="superstar" stem="superstar" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="status" lemma="status" stem="statu" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="gave" lemma="give" stem="gave" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="21" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="invaluable" lemma="invaluable" stem="invalu" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="identification" lemma="identification" stem="identif" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="27" string="Love" lemma="Love" stem="love" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="28" string="Lucy" lemma="Lucy" stem="luci" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="29" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="began" lemma="begin" stem="began" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (PRP$ Her) (JJ successful) (NN career)) (PP (IN in) (NP (NNS films))) (, ,) (SBAR (WHPP (IN in) (WHNP (WDT which))) (S (NP (PRP she)) (ADVP (RB ultimately)) (VP (VBD earned) (NP (NP (NN star) (NN billing)) (CONJP (IN if) (RB not)) (NP (NN superstar) (NN status)))))) (, ,)) (VP (VBD gave) (NP (PRP$ her)) (NP (NP (DT an) (JJ invaluable) (NN identification)) (SBAR (WHADVP (WRB when)) (S (`` ``) (NP (NP (PRP I)) (NP (NNP Love) (NNP Lucy))) ('' '') (VP (VBD began)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="films" type="NP">
          <tokens>
            <token id="5" string="films" />
          </tokens>
        </chunking>
        <chunking id="2" string="Her successful career" type="NP">
          <tokens>
            <token id="1" string="Her" />
            <token id="2" string="successful" />
            <token id="3" string="career" />
          </tokens>
        </chunking>
        <chunking id="3" string="star billing if not superstar status" type="NP">
          <tokens>
            <token id="12" string="star" />
            <token id="13" string="billing" />
            <token id="14" string="if" />
            <token id="15" string="not" />
            <token id="16" string="superstar" />
            <token id="17" string="status" />
          </tokens>
        </chunking>
        <chunking id="4" string="began" type="VP">
          <tokens>
            <token id="30" string="began" />
          </tokens>
        </chunking>
        <chunking id="5" string="when `` I Love Lucy '' began" type="SBAR">
          <tokens>
            <token id="24" string="when" />
            <token id="25" string="&quot;" />
            <token id="26" string="I" />
            <token id="27" string="Love" />
            <token id="28" string="Lucy" />
            <token id="29" string="&quot;" />
            <token id="30" string="began" />
          </tokens>
        </chunking>
        <chunking id="6" string="I" type="NP">
          <tokens>
            <token id="26" string="I" />
          </tokens>
        </chunking>
        <chunking id="7" string="earned star billing if not superstar status" type="VP">
          <tokens>
            <token id="11" string="earned" />
            <token id="12" string="star" />
            <token id="13" string="billing" />
            <token id="14" string="if" />
            <token id="15" string="not" />
            <token id="16" string="superstar" />
            <token id="17" string="status" />
          </tokens>
        </chunking>
        <chunking id="8" string="in which she ultimately earned star billing if not superstar status" type="SBAR">
          <tokens>
            <token id="7" string="in" />
            <token id="8" string="which" />
            <token id="9" string="she" />
            <token id="10" string="ultimately" />
            <token id="11" string="earned" />
            <token id="12" string="star" />
            <token id="13" string="billing" />
            <token id="14" string="if" />
            <token id="15" string="not" />
            <token id="16" string="superstar" />
            <token id="17" string="status" />
          </tokens>
        </chunking>
        <chunking id="9" string="I Love Lucy" type="NP">
          <tokens>
            <token id="26" string="I" />
            <token id="27" string="Love" />
            <token id="28" string="Lucy" />
          </tokens>
        </chunking>
        <chunking id="10" string="she" type="NP">
          <tokens>
            <token id="9" string="she" />
          </tokens>
        </chunking>
        <chunking id="11" string="when" type="WHADVP">
          <tokens>
            <token id="24" string="when" />
          </tokens>
        </chunking>
        <chunking id="12" string="gave her an invaluable identification when `` I Love Lucy '' began" type="VP">
          <tokens>
            <token id="19" string="gave" />
            <token id="20" string="her" />
            <token id="21" string="an" />
            <token id="22" string="invaluable" />
            <token id="23" string="identification" />
            <token id="24" string="when" />
            <token id="25" string="&quot;" />
            <token id="26" string="I" />
            <token id="27" string="Love" />
            <token id="28" string="Lucy" />
            <token id="29" string="&quot;" />
            <token id="30" string="began" />
          </tokens>
        </chunking>
        <chunking id="13" string="superstar status" type="NP">
          <tokens>
            <token id="16" string="superstar" />
            <token id="17" string="status" />
          </tokens>
        </chunking>
        <chunking id="14" string="her" type="NP">
          <tokens>
            <token id="20" string="her" />
          </tokens>
        </chunking>
        <chunking id="15" string="Her successful career in films , in which she ultimately earned star billing if not superstar status ," type="NP">
          <tokens>
            <token id="1" string="Her" />
            <token id="2" string="successful" />
            <token id="3" string="career" />
            <token id="4" string="in" />
            <token id="5" string="films" />
            <token id="6" string="," />
            <token id="7" string="in" />
            <token id="8" string="which" />
            <token id="9" string="she" />
            <token id="10" string="ultimately" />
            <token id="11" string="earned" />
            <token id="12" string="star" />
            <token id="13" string="billing" />
            <token id="14" string="if" />
            <token id="15" string="not" />
            <token id="16" string="superstar" />
            <token id="17" string="status" />
            <token id="18" string="," />
          </tokens>
        </chunking>
        <chunking id="16" string="star billing" type="NP">
          <tokens>
            <token id="12" string="star" />
            <token id="13" string="billing" />
          </tokens>
        </chunking>
        <chunking id="17" string="an invaluable identification" type="NP">
          <tokens>
            <token id="21" string="an" />
            <token id="22" string="invaluable" />
            <token id="23" string="identification" />
          </tokens>
        </chunking>
        <chunking id="18" string="an invaluable identification when `` I Love Lucy '' began" type="NP">
          <tokens>
            <token id="21" string="an" />
            <token id="22" string="invaluable" />
            <token id="23" string="identification" />
            <token id="24" string="when" />
            <token id="25" string="&quot;" />
            <token id="26" string="I" />
            <token id="27" string="Love" />
            <token id="28" string="Lucy" />
            <token id="29" string="&quot;" />
            <token id="30" string="began" />
          </tokens>
        </chunking>
        <chunking id="19" string="Love Lucy" type="NP">
          <tokens>
            <token id="27" string="Love" />
            <token id="28" string="Lucy" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="3">career</governor>
          <dependent id="1">Her</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">career</governor>
          <dependent id="2">successful</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">gave</governor>
          <dependent id="3">career</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">films</governor>
          <dependent id="4">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">career</governor>
          <dependent id="5">films</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">which</governor>
          <dependent id="7">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">earned</governor>
          <dependent id="8">which</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">earned</governor>
          <dependent id="9">she</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">earned</governor>
          <dependent id="10">ultimately</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="3">career</governor>
          <dependent id="11">earned</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="13">billing</governor>
          <dependent id="12">star</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">earned</governor>
          <dependent id="13">billing</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="15">not</governor>
          <dependent id="14">if</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="13">billing</governor>
          <dependent id="15">not</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="17">status</governor>
          <dependent id="16">superstar</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="13">billing</governor>
          <dependent id="17">status</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="19">gave</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="19">gave</governor>
          <dependent id="20">her</dependent>
        </dependency>
        <dependency type="det">
          <governor id="23">identification</governor>
          <dependent id="21">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="23">identification</governor>
          <dependent id="22">invaluable</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="19">gave</governor>
          <dependent id="23">identification</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="30">began</governor>
          <dependent id="24">when</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="30">began</governor>
          <dependent id="26">I</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="28">Lucy</governor>
          <dependent id="27">Love</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="26">I</governor>
          <dependent id="28">Lucy</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="23">identification</governor>
          <dependent id="30">began</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="22" has_coreference="true">
      <content>The irony is that, created by the movies, she was to achieve fame and fortune in television on a scale that had eluded her in the movies.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="irony" lemma="irony" stem="ironi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="created" lemma="create" stem="creat" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="movies" lemma="movie" stem="movi" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="achieve" lemma="achieve" stem="achiev" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="fame" lemma="fame" stem="fame" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="fortune" lemma="fortune" stem="fortun" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="television" lemma="television" stem="televis" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="scale" lemma="scale" stem="scale" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="eluded" lemma="elude" stem="elud" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="27" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="29" string="movies" lemma="movie" stem="movi" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="30" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN irony)) (VP (VBZ is) (SBAR (IN that) (PRN (, ,) (S (VP (VBN created) (PP (IN by) (NP (DT the) (NNS movies))))) (, ,)) (S (NP (PRP she)) (VP (VBD was) (S (VP (TO to) (VP (VB achieve) (NP (NP (NN fame) (CC and) (NN fortune)) (PP (IN in) (NP (NN television)))) (PP (IN on) (NP (NP (DT a) (NN scale)) (SBAR (WHNP (WDT that)) (S (VP (VBD had) (VP (VBN eluded) (NP (PRP$ her)) (PP (IN in) (NP (DT the) (NNS movies)))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="fame and fortune in television" type="NP">
          <tokens>
            <token id="15" string="fame" />
            <token id="16" string="and" />
            <token id="17" string="fortune" />
            <token id="18" string="in" />
            <token id="19" string="television" />
          </tokens>
        </chunking>
        <chunking id="2" string="had eluded her in the movies" type="VP">
          <tokens>
            <token id="24" string="had" />
            <token id="25" string="eluded" />
            <token id="26" string="her" />
            <token id="27" string="in" />
            <token id="28" string="the" />
            <token id="29" string="movies" />
          </tokens>
        </chunking>
        <chunking id="3" string="created by the movies" type="VP">
          <tokens>
            <token id="6" string="created" />
            <token id="7" string="by" />
            <token id="8" string="the" />
            <token id="9" string="movies" />
          </tokens>
        </chunking>
        <chunking id="4" string="television" type="NP">
          <tokens>
            <token id="19" string="television" />
          </tokens>
        </chunking>
        <chunking id="5" string="achieve fame and fortune in television on a scale that had eluded her in the movies" type="VP">
          <tokens>
            <token id="14" string="achieve" />
            <token id="15" string="fame" />
            <token id="16" string="and" />
            <token id="17" string="fortune" />
            <token id="18" string="in" />
            <token id="19" string="television" />
            <token id="20" string="on" />
            <token id="21" string="a" />
            <token id="22" string="scale" />
            <token id="23" string="that" />
            <token id="24" string="had" />
            <token id="25" string="eluded" />
            <token id="26" string="her" />
            <token id="27" string="in" />
            <token id="28" string="the" />
            <token id="29" string="movies" />
          </tokens>
        </chunking>
        <chunking id="6" string="she" type="NP">
          <tokens>
            <token id="11" string="she" />
          </tokens>
        </chunking>
        <chunking id="7" string="a scale that had eluded her in the movies" type="NP">
          <tokens>
            <token id="21" string="a" />
            <token id="22" string="scale" />
            <token id="23" string="that" />
            <token id="24" string="had" />
            <token id="25" string="eluded" />
            <token id="26" string="her" />
            <token id="27" string="in" />
            <token id="28" string="the" />
            <token id="29" string="movies" />
          </tokens>
        </chunking>
        <chunking id="8" string="eluded her in the movies" type="VP">
          <tokens>
            <token id="25" string="eluded" />
            <token id="26" string="her" />
            <token id="27" string="in" />
            <token id="28" string="the" />
            <token id="29" string="movies" />
          </tokens>
        </chunking>
        <chunking id="9" string="a scale" type="NP">
          <tokens>
            <token id="21" string="a" />
            <token id="22" string="scale" />
          </tokens>
        </chunking>
        <chunking id="10" string="to achieve fame and fortune in television on a scale that had eluded her in the movies" type="VP">
          <tokens>
            <token id="13" string="to" />
            <token id="14" string="achieve" />
            <token id="15" string="fame" />
            <token id="16" string="and" />
            <token id="17" string="fortune" />
            <token id="18" string="in" />
            <token id="19" string="television" />
            <token id="20" string="on" />
            <token id="21" string="a" />
            <token id="22" string="scale" />
            <token id="23" string="that" />
            <token id="24" string="had" />
            <token id="25" string="eluded" />
            <token id="26" string="her" />
            <token id="27" string="in" />
            <token id="28" string="the" />
            <token id="29" string="movies" />
          </tokens>
        </chunking>
        <chunking id="11" string="her" type="NP">
          <tokens>
            <token id="26" string="her" />
          </tokens>
        </chunking>
        <chunking id="12" string="that had eluded her in the movies" type="SBAR">
          <tokens>
            <token id="23" string="that" />
            <token id="24" string="had" />
            <token id="25" string="eluded" />
            <token id="26" string="her" />
            <token id="27" string="in" />
            <token id="28" string="the" />
            <token id="29" string="movies" />
          </tokens>
        </chunking>
        <chunking id="13" string="the movies" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="movies" />
          </tokens>
        </chunking>
        <chunking id="14" string="that , created by the movies , she was to achieve fame and fortune in television on a scale that had eluded her in the movies" type="SBAR">
          <tokens>
            <token id="4" string="that" />
            <token id="5" string="," />
            <token id="6" string="created" />
            <token id="7" string="by" />
            <token id="8" string="the" />
            <token id="9" string="movies" />
            <token id="10" string="," />
            <token id="11" string="she" />
            <token id="12" string="was" />
            <token id="13" string="to" />
            <token id="14" string="achieve" />
            <token id="15" string="fame" />
            <token id="16" string="and" />
            <token id="17" string="fortune" />
            <token id="18" string="in" />
            <token id="19" string="television" />
            <token id="20" string="on" />
            <token id="21" string="a" />
            <token id="22" string="scale" />
            <token id="23" string="that" />
            <token id="24" string="had" />
            <token id="25" string="eluded" />
            <token id="26" string="her" />
            <token id="27" string="in" />
            <token id="28" string="the" />
            <token id="29" string="movies" />
          </tokens>
        </chunking>
        <chunking id="15" string="fame and fortune" type="NP">
          <tokens>
            <token id="15" string="fame" />
            <token id="16" string="and" />
            <token id="17" string="fortune" />
          </tokens>
        </chunking>
        <chunking id="16" string="is that , created by the movies , she was to achieve fame and fortune in television on a scale that had eluded her in the movies" type="VP">
          <tokens>
            <token id="3" string="is" />
            <token id="4" string="that" />
            <token id="5" string="," />
            <token id="6" string="created" />
            <token id="7" string="by" />
            <token id="8" string="the" />
            <token id="9" string="movies" />
            <token id="10" string="," />
            <token id="11" string="she" />
            <token id="12" string="was" />
            <token id="13" string="to" />
            <token id="14" string="achieve" />
            <token id="15" string="fame" />
            <token id="16" string="and" />
            <token id="17" string="fortune" />
            <token id="18" string="in" />
            <token id="19" string="television" />
            <token id="20" string="on" />
            <token id="21" string="a" />
            <token id="22" string="scale" />
            <token id="23" string="that" />
            <token id="24" string="had" />
            <token id="25" string="eluded" />
            <token id="26" string="her" />
            <token id="27" string="in" />
            <token id="28" string="the" />
            <token id="29" string="movies" />
          </tokens>
        </chunking>
        <chunking id="17" string="was to achieve fame and fortune in television on a scale that had eluded her in the movies" type="VP">
          <tokens>
            <token id="12" string="was" />
            <token id="13" string="to" />
            <token id="14" string="achieve" />
            <token id="15" string="fame" />
            <token id="16" string="and" />
            <token id="17" string="fortune" />
            <token id="18" string="in" />
            <token id="19" string="television" />
            <token id="20" string="on" />
            <token id="21" string="a" />
            <token id="22" string="scale" />
            <token id="23" string="that" />
            <token id="24" string="had" />
            <token id="25" string="eluded" />
            <token id="26" string="her" />
            <token id="27" string="in" />
            <token id="28" string="the" />
            <token id="29" string="movies" />
          </tokens>
        </chunking>
        <chunking id="18" string="The irony" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="irony" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">irony</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">is</governor>
          <dependent id="2">irony</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">is</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="12">was</governor>
          <dependent id="4">that</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="12">was</governor>
          <dependent id="6">created</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">movies</governor>
          <dependent id="7">by</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">movies</governor>
          <dependent id="8">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">created</governor>
          <dependent id="9">movies</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">was</governor>
          <dependent id="11">she</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">is</governor>
          <dependent id="12">was</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="14">achieve</governor>
          <dependent id="13">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="12">was</governor>
          <dependent id="14">achieve</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="14">achieve</governor>
          <dependent id="15">fame</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="15">fame</governor>
          <dependent id="16">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="15">fame</governor>
          <dependent id="17">fortune</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">television</governor>
          <dependent id="18">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">fame</governor>
          <dependent id="19">television</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">scale</governor>
          <dependent id="20">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="22">scale</governor>
          <dependent id="21">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">achieve</governor>
          <dependent id="22">scale</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="25">eluded</governor>
          <dependent id="23">that</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="25">eluded</governor>
          <dependent id="24">had</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="22">scale</governor>
          <dependent id="25">eluded</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="25">eluded</governor>
          <dependent id="26">her</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">movies</governor>
          <dependent id="27">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="29">movies</governor>
          <dependent id="28">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="25">eluded</governor>
          <dependent id="29">movies</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="23" has_coreference="true">
      <content>Indeed, when Lucy began to commandeer our hearts on &amp;quot;I Love Lucy&amp;quot; in 1951, the movies were already in considerable trouble, their audiences staying home in large numbers to watch the likes of Lucy in their living rooms.</content>
      <tokens>
        <token id="1" string="Indeed" lemma="indeed" stem="indeed" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="Lucy" lemma="Lucy" stem="luci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="5" string="began" lemma="begin" stem="began" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="commandeer" lemma="commandeer" stem="command" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="our" lemma="we" stem="our" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="hearts" lemma="heart" stem="heart" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="Love" lemma="Love" stem="love" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="14" string="Lucy" lemma="Lucy" stem="luci" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="15" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="1951" lemma="1951" stem="1951" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="18" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="20" string="movies" lemma="movie" stem="movi" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="21" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="already" lemma="already" stem="alreadi" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="considerable" lemma="considerable" stem="consider" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="trouble" lemma="trouble" stem="troubl" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="audiences" lemma="audience" stem="audienc" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="29" string="staying" lemma="stay" stem="stai" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="home" lemma="home" stem="home" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="large" lemma="large" stem="larg" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="numbers" lemma="number" stem="number" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="watch" lemma="watch" stem="watch" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="36" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="likes" lemma="likes" stem="like" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="38" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="39" string="Lucy" lemma="Lucy" stem="luci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="40" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="41" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="42" string="living" lemma="living" stem="live" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="43" string="rooms" lemma="room" stem="room" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="44" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (ADVP (RB Indeed)) (, ,) (SBAR (WHADVP (WRB when)) (S (NP (NNP Lucy)) (VP (VBD began) (S (VP (TO to) (VP (VB commandeer) (NP (PRP$ our) (NNS hearts)) (PP (IN on) (`` ``) (NP (NP (PRP I)) (NP (NNP Love) (NNP Lucy))) ('' '')) (PP (IN in) (NP (CD 1951))))))))) (, ,) (NP (DT the) (NNS movies)) (VP (VBD were) (ADVP (RB already)) (PP (IN in) (NP (NP (JJ considerable) (NN trouble)) (, ,) (NP (PRP$ their) (NNS audiences)))) (S (VP (VBG staying) (NP (NN home)) (PP (IN in) (NP (JJ large) (NNS numbers))) (S (VP (TO to) (VP (VB watch) (NP (NP (DT the) (NN likes)) (PP (IN of) (NP (NNP Lucy)))) (PP (IN in) (NP (PRP$ their) (NN living) (NNS rooms))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="commandeer our hearts on `` I Love Lucy '' in 1951" type="VP">
          <tokens>
            <token id="7" string="commandeer" />
            <token id="8" string="our" />
            <token id="9" string="hearts" />
            <token id="10" string="on" />
            <token id="11" string="&quot;" />
            <token id="12" string="I" />
            <token id="13" string="Love" />
            <token id="14" string="Lucy" />
            <token id="15" string="&quot;" />
            <token id="16" string="in" />
            <token id="17" string="1951" />
          </tokens>
        </chunking>
        <chunking id="2" string="our hearts" type="NP">
          <tokens>
            <token id="8" string="our" />
            <token id="9" string="hearts" />
          </tokens>
        </chunking>
        <chunking id="3" string="staying home in large numbers to watch the likes of Lucy in their living rooms" type="VP">
          <tokens>
            <token id="29" string="staying" />
            <token id="30" string="home" />
            <token id="31" string="in" />
            <token id="32" string="large" />
            <token id="33" string="numbers" />
            <token id="34" string="to" />
            <token id="35" string="watch" />
            <token id="36" string="the" />
            <token id="37" string="likes" />
            <token id="38" string="of" />
            <token id="39" string="Lucy" />
            <token id="40" string="in" />
            <token id="41" string="their" />
            <token id="42" string="living" />
            <token id="43" string="rooms" />
          </tokens>
        </chunking>
        <chunking id="4" string="watch the likes of Lucy in their living rooms" type="VP">
          <tokens>
            <token id="35" string="watch" />
            <token id="36" string="the" />
            <token id="37" string="likes" />
            <token id="38" string="of" />
            <token id="39" string="Lucy" />
            <token id="40" string="in" />
            <token id="41" string="their" />
            <token id="42" string="living" />
            <token id="43" string="rooms" />
          </tokens>
        </chunking>
        <chunking id="5" string="when Lucy began to commandeer our hearts on `` I Love Lucy '' in 1951" type="SBAR">
          <tokens>
            <token id="3" string="when" />
            <token id="4" string="Lucy" />
            <token id="5" string="began" />
            <token id="6" string="to" />
            <token id="7" string="commandeer" />
            <token id="8" string="our" />
            <token id="9" string="hearts" />
            <token id="10" string="on" />
            <token id="11" string="&quot;" />
            <token id="12" string="I" />
            <token id="13" string="Love" />
            <token id="14" string="Lucy" />
            <token id="15" string="&quot;" />
            <token id="16" string="in" />
            <token id="17" string="1951" />
          </tokens>
        </chunking>
        <chunking id="6" string="to commandeer our hearts on `` I Love Lucy '' in 1951" type="VP">
          <tokens>
            <token id="6" string="to" />
            <token id="7" string="commandeer" />
            <token id="8" string="our" />
            <token id="9" string="hearts" />
            <token id="10" string="on" />
            <token id="11" string="&quot;" />
            <token id="12" string="I" />
            <token id="13" string="Love" />
            <token id="14" string="Lucy" />
            <token id="15" string="&quot;" />
            <token id="16" string="in" />
            <token id="17" string="1951" />
          </tokens>
        </chunking>
        <chunking id="7" string="large numbers" type="NP">
          <tokens>
            <token id="32" string="large" />
            <token id="33" string="numbers" />
          </tokens>
        </chunking>
        <chunking id="8" string="I" type="NP">
          <tokens>
            <token id="12" string="I" />
          </tokens>
        </chunking>
        <chunking id="9" string="their living rooms" type="NP">
          <tokens>
            <token id="41" string="their" />
            <token id="42" string="living" />
            <token id="43" string="rooms" />
          </tokens>
        </chunking>
        <chunking id="10" string="began to commandeer our hearts on `` I Love Lucy '' in 1951" type="VP">
          <tokens>
            <token id="5" string="began" />
            <token id="6" string="to" />
            <token id="7" string="commandeer" />
            <token id="8" string="our" />
            <token id="9" string="hearts" />
            <token id="10" string="on" />
            <token id="11" string="&quot;" />
            <token id="12" string="I" />
            <token id="13" string="Love" />
            <token id="14" string="Lucy" />
            <token id="15" string="&quot;" />
            <token id="16" string="in" />
            <token id="17" string="1951" />
          </tokens>
        </chunking>
        <chunking id="11" string="considerable trouble" type="NP">
          <tokens>
            <token id="24" string="considerable" />
            <token id="25" string="trouble" />
          </tokens>
        </chunking>
        <chunking id="12" string="considerable trouble , their audiences" type="NP">
          <tokens>
            <token id="24" string="considerable" />
            <token id="25" string="trouble" />
            <token id="26" string="," />
            <token id="27" string="their" />
            <token id="28" string="audiences" />
          </tokens>
        </chunking>
        <chunking id="13" string="I Love Lucy" type="NP">
          <tokens>
            <token id="12" string="I" />
            <token id="13" string="Love" />
            <token id="14" string="Lucy" />
          </tokens>
        </chunking>
        <chunking id="14" string="when" type="WHADVP">
          <tokens>
            <token id="3" string="when" />
          </tokens>
        </chunking>
        <chunking id="15" string="home" type="NP">
          <tokens>
            <token id="30" string="home" />
          </tokens>
        </chunking>
        <chunking id="16" string="were already in considerable trouble , their audiences staying home in large numbers to watch the likes of Lucy in their living rooms" type="VP">
          <tokens>
            <token id="21" string="were" />
            <token id="22" string="already" />
            <token id="23" string="in" />
            <token id="24" string="considerable" />
            <token id="25" string="trouble" />
            <token id="26" string="," />
            <token id="27" string="their" />
            <token id="28" string="audiences" />
            <token id="29" string="staying" />
            <token id="30" string="home" />
            <token id="31" string="in" />
            <token id="32" string="large" />
            <token id="33" string="numbers" />
            <token id="34" string="to" />
            <token id="35" string="watch" />
            <token id="36" string="the" />
            <token id="37" string="likes" />
            <token id="38" string="of" />
            <token id="39" string="Lucy" />
            <token id="40" string="in" />
            <token id="41" string="their" />
            <token id="42" string="living" />
            <token id="43" string="rooms" />
          </tokens>
        </chunking>
        <chunking id="17" string="the likes of Lucy" type="NP">
          <tokens>
            <token id="36" string="the" />
            <token id="37" string="likes" />
            <token id="38" string="of" />
            <token id="39" string="Lucy" />
          </tokens>
        </chunking>
        <chunking id="18" string="1951" type="NP">
          <tokens>
            <token id="17" string="1951" />
          </tokens>
        </chunking>
        <chunking id="19" string="the movies" type="NP">
          <tokens>
            <token id="19" string="the" />
            <token id="20" string="movies" />
          </tokens>
        </chunking>
        <chunking id="20" string="their audiences" type="NP">
          <tokens>
            <token id="27" string="their" />
            <token id="28" string="audiences" />
          </tokens>
        </chunking>
        <chunking id="21" string="Lucy" type="NP">
          <tokens>
            <token id="4" string="Lucy" />
          </tokens>
        </chunking>
        <chunking id="22" string="the likes" type="NP">
          <tokens>
            <token id="36" string="the" />
            <token id="37" string="likes" />
          </tokens>
        </chunking>
        <chunking id="23" string="Love Lucy" type="NP">
          <tokens>
            <token id="13" string="Love" />
            <token id="14" string="Lucy" />
          </tokens>
        </chunking>
        <chunking id="24" string="to watch the likes of Lucy in their living rooms" type="VP">
          <tokens>
            <token id="34" string="to" />
            <token id="35" string="watch" />
            <token id="36" string="the" />
            <token id="37" string="likes" />
            <token id="38" string="of" />
            <token id="39" string="Lucy" />
            <token id="40" string="in" />
            <token id="41" string="their" />
            <token id="42" string="living" />
            <token id="43" string="rooms" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="25">trouble</governor>
          <dependent id="1">Indeed</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">began</governor>
          <dependent id="3">when</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">began</governor>
          <dependent id="4">Lucy</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="25">trouble</governor>
          <dependent id="5">began</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="7">commandeer</governor>
          <dependent id="6">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="5">began</governor>
          <dependent id="7">commandeer</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="9">hearts</governor>
          <dependent id="8">our</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">commandeer</governor>
          <dependent id="9">hearts</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">I</governor>
          <dependent id="10">on</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">commandeer</governor>
          <dependent id="12">I</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">Lucy</governor>
          <dependent id="13">Love</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="12">I</governor>
          <dependent id="14">Lucy</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">1951</governor>
          <dependent id="16">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">commandeer</governor>
          <dependent id="17">1951</dependent>
        </dependency>
        <dependency type="det">
          <governor id="20">movies</governor>
          <dependent id="19">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="25">trouble</governor>
          <dependent id="20">movies</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="25">trouble</governor>
          <dependent id="21">were</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="25">trouble</governor>
          <dependent id="22">already</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">trouble</governor>
          <dependent id="23">in</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="25">trouble</governor>
          <dependent id="24">considerable</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="25">trouble</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="28">audiences</governor>
          <dependent id="27">their</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="25">trouble</governor>
          <dependent id="28">audiences</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="25">trouble</governor>
          <dependent id="29">staying</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="29">staying</governor>
          <dependent id="30">home</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">numbers</governor>
          <dependent id="31">in</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="33">numbers</governor>
          <dependent id="32">large</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="29">staying</governor>
          <dependent id="33">numbers</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="35">watch</governor>
          <dependent id="34">to</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="29">staying</governor>
          <dependent id="35">watch</dependent>
        </dependency>
        <dependency type="det">
          <governor id="37">likes</governor>
          <dependent id="36">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="35">watch</governor>
          <dependent id="37">likes</dependent>
        </dependency>
        <dependency type="case">
          <governor id="39">Lucy</governor>
          <dependent id="38">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="37">likes</governor>
          <dependent id="39">Lucy</dependent>
        </dependency>
        <dependency type="case">
          <governor id="43">rooms</governor>
          <dependent id="40">in</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="43">rooms</governor>
          <dependent id="41">their</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="43">rooms</governor>
          <dependent id="42">living</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="35">watch</governor>
          <dependent id="43">rooms</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="1951" type="DATE" score="0.0">
          <tokens>
            <token id="17" string="1951" />
          </tokens>
        </entity>
        <entity id="2" string="Lucy" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="Lucy" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="24" has_coreference="true">
      <content>As always, her timing was impeccable.</content>
      <tokens>
        <token id="1" string="As" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="always" lemma="always" stem="alwai" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="timing" lemma="timing" stem="time" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="impeccable" lemma="impeccable" stem="impecc" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (IN As) (ADVP (RB always))) (, ,) (NP (PRP$ her) (NN timing)) (VP (VBD was) (ADJP (JJ impeccable))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="her timing" type="NP">
          <tokens>
            <token id="4" string="her" />
            <token id="5" string="timing" />
          </tokens>
        </chunking>
        <chunking id="2" string="was impeccable" type="VP">
          <tokens>
            <token id="6" string="was" />
            <token id="7" string="impeccable" />
          </tokens>
        </chunking>
        <chunking id="3" string="impeccable" type="ADJP">
          <tokens>
            <token id="7" string="impeccable" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="case">
          <governor id="2">always</governor>
          <dependent id="1">As</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="7">impeccable</governor>
          <dependent id="2">always</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="5">timing</governor>
          <dependent id="4">her</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">impeccable</governor>
          <dependent id="5">timing</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="7">impeccable</governor>
          <dependent id="6">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">impeccable</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="25" has_coreference="true">
      <content>Lucille Ball was not Lucy, of course.</content>
      <tokens>
        <token id="1" string="Lucille" lemma="Lucille" stem="lucil" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="2" string="Ball" lemma="Ball" stem="ball" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="3" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="Lucy" lemma="Lucy" stem="luci" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="course" lemma="course" stem="cours" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="9" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Lucille) (NNP Ball)) (VP (VBD was) (RB not) (NP (NP (NNP Lucy)) (, ,) (PP (IN of) (NP (NN course))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Lucy , of course" type="NP">
          <tokens>
            <token id="5" string="Lucy" />
            <token id="6" string="," />
            <token id="7" string="of" />
            <token id="8" string="course" />
          </tokens>
        </chunking>
        <chunking id="2" string="course" type="NP">
          <tokens>
            <token id="8" string="course" />
          </tokens>
        </chunking>
        <chunking id="3" string="Lucy" type="NP">
          <tokens>
            <token id="5" string="Lucy" />
          </tokens>
        </chunking>
        <chunking id="4" string="was not Lucy , of course" type="VP">
          <tokens>
            <token id="3" string="was" />
            <token id="4" string="not" />
            <token id="5" string="Lucy" />
            <token id="6" string="," />
            <token id="7" string="of" />
            <token id="8" string="course" />
          </tokens>
        </chunking>
        <chunking id="5" string="Lucille Ball" type="NP">
          <tokens>
            <token id="1" string="Lucille" />
            <token id="2" string="Ball" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">Ball</governor>
          <dependent id="1">Lucille</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">Lucy</governor>
          <dependent id="2">Ball</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="5">Lucy</governor>
          <dependent id="3">was</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="5">Lucy</governor>
          <dependent id="4">not</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">Lucy</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">course</governor>
          <dependent id="7">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">Lucy</governor>
          <dependent id="8">course</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Lucy" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="Lucy" />
          </tokens>
        </entity>
        <entity id="2" string="Lucille Ball" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Lucille" />
            <token id="2" string="Ball" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="26" has_coreference="true">
      <content>She had come a hard road from Jamestown, N.Y., in a line of work in which you did not survive by being scatterbrained.</content>
      <tokens>
        <token id="1" string="She" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="come" lemma="come" stem="come" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="hard" lemma="hard" stem="hard" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="road" lemma="road" stem="road" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Jamestown" lemma="Jamestown" stem="jamestown" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="N.Y." lemma="N.Y." stem="n.y." pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="line" lemma="line" stem="line" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="work" lemma="work" stem="work" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="which" lemma="which" stem="which" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="you" lemma="you" stem="you" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="did" lemma="do" stem="did" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="survive" lemma="survive" stem="surviv" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="being" lemma="be" stem="be" pos="VBG" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="scatterbrained" lemma="scatterbrain" stem="scatterbrain" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP She)) (VP (VBD had) (VP (VBN come) (NP (DT a) (JJ hard) (NN road)) (PP (IN from) (NP (NNP Jamestown) (, ,) (NNP N.Y.))) (, ,) (PP (IN in) (NP (NP (DT a) (NN line)) (PP (IN of) (NP (NN work))) (SBAR (WHPP (IN in) (WHNP (WDT which))) (S (NP (PRP you)) (VP (VBD did) (RB not) (VP (VB survive) (PP (IN by) (S (VP (VBG being) (VP (VBN scatterbrained))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="work" type="NP">
          <tokens>
            <token id="16" string="work" />
          </tokens>
        </chunking>
        <chunking id="2" string="did not survive by being scatterbrained" type="VP">
          <tokens>
            <token id="20" string="did" />
            <token id="21" string="not" />
            <token id="22" string="survive" />
            <token id="23" string="by" />
            <token id="24" string="being" />
            <token id="25" string="scatterbrained" />
          </tokens>
        </chunking>
        <chunking id="3" string="scatterbrained" type="VP">
          <tokens>
            <token id="25" string="scatterbrained" />
          </tokens>
        </chunking>
        <chunking id="4" string="a line" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="line" />
          </tokens>
        </chunking>
        <chunking id="5" string="being scatterbrained" type="VP">
          <tokens>
            <token id="24" string="being" />
            <token id="25" string="scatterbrained" />
          </tokens>
        </chunking>
        <chunking id="6" string="a hard road" type="NP">
          <tokens>
            <token id="4" string="a" />
            <token id="5" string="hard" />
            <token id="6" string="road" />
          </tokens>
        </chunking>
        <chunking id="7" string="She" type="NP">
          <tokens>
            <token id="1" string="She" />
          </tokens>
        </chunking>
        <chunking id="8" string="survive by being scatterbrained" type="VP">
          <tokens>
            <token id="22" string="survive" />
            <token id="23" string="by" />
            <token id="24" string="being" />
            <token id="25" string="scatterbrained" />
          </tokens>
        </chunking>
        <chunking id="9" string="come a hard road from Jamestown , N.Y. , in a line of work in which you did not survive by being scatterbrained" type="VP">
          <tokens>
            <token id="3" string="come" />
            <token id="4" string="a" />
            <token id="5" string="hard" />
            <token id="6" string="road" />
            <token id="7" string="from" />
            <token id="8" string="Jamestown" />
            <token id="9" string="," />
            <token id="10" string="N.Y." />
            <token id="11" string="," />
            <token id="12" string="in" />
            <token id="13" string="a" />
            <token id="14" string="line" />
            <token id="15" string="of" />
            <token id="16" string="work" />
            <token id="17" string="in" />
            <token id="18" string="which" />
            <token id="19" string="you" />
            <token id="20" string="did" />
            <token id="21" string="not" />
            <token id="22" string="survive" />
            <token id="23" string="by" />
            <token id="24" string="being" />
            <token id="25" string="scatterbrained" />
          </tokens>
        </chunking>
        <chunking id="10" string="had come a hard road from Jamestown , N.Y. , in a line of work in which you did not survive by being scatterbrained" type="VP">
          <tokens>
            <token id="2" string="had" />
            <token id="3" string="come" />
            <token id="4" string="a" />
            <token id="5" string="hard" />
            <token id="6" string="road" />
            <token id="7" string="from" />
            <token id="8" string="Jamestown" />
            <token id="9" string="," />
            <token id="10" string="N.Y." />
            <token id="11" string="," />
            <token id="12" string="in" />
            <token id="13" string="a" />
            <token id="14" string="line" />
            <token id="15" string="of" />
            <token id="16" string="work" />
            <token id="17" string="in" />
            <token id="18" string="which" />
            <token id="19" string="you" />
            <token id="20" string="did" />
            <token id="21" string="not" />
            <token id="22" string="survive" />
            <token id="23" string="by" />
            <token id="24" string="being" />
            <token id="25" string="scatterbrained" />
          </tokens>
        </chunking>
        <chunking id="11" string="Jamestown , N.Y." type="NP">
          <tokens>
            <token id="8" string="Jamestown" />
            <token id="9" string="," />
            <token id="10" string="N.Y." />
          </tokens>
        </chunking>
        <chunking id="12" string="a line of work in which you did not survive by being scatterbrained" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="line" />
            <token id="15" string="of" />
            <token id="16" string="work" />
            <token id="17" string="in" />
            <token id="18" string="which" />
            <token id="19" string="you" />
            <token id="20" string="did" />
            <token id="21" string="not" />
            <token id="22" string="survive" />
            <token id="23" string="by" />
            <token id="24" string="being" />
            <token id="25" string="scatterbrained" />
          </tokens>
        </chunking>
        <chunking id="13" string="in which you did not survive by being scatterbrained" type="SBAR">
          <tokens>
            <token id="17" string="in" />
            <token id="18" string="which" />
            <token id="19" string="you" />
            <token id="20" string="did" />
            <token id="21" string="not" />
            <token id="22" string="survive" />
            <token id="23" string="by" />
            <token id="24" string="being" />
            <token id="25" string="scatterbrained" />
          </tokens>
        </chunking>
        <chunking id="14" string="you" type="NP">
          <tokens>
            <token id="19" string="you" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">come</governor>
          <dependent id="1">She</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="3">come</governor>
          <dependent id="2">had</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">come</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">road</governor>
          <dependent id="4">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">road</governor>
          <dependent id="5">hard</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">come</governor>
          <dependent id="6">road</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">N.Y.</governor>
          <dependent id="7">from</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">N.Y.</governor>
          <dependent id="8">Jamestown</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">come</governor>
          <dependent id="10">N.Y.</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">line</governor>
          <dependent id="12">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">line</governor>
          <dependent id="13">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">come</governor>
          <dependent id="14">line</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">work</governor>
          <dependent id="15">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">line</governor>
          <dependent id="16">work</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">which</governor>
          <dependent id="17">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="22">survive</governor>
          <dependent id="18">which</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">survive</governor>
          <dependent id="19">you</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="22">survive</governor>
          <dependent id="20">did</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="22">survive</governor>
          <dependent id="21">not</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="14">line</governor>
          <dependent id="22">survive</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="25">scatterbrained</governor>
          <dependent id="23">by</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="25">scatterbrained</governor>
          <dependent id="24">being</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="22">survive</governor>
          <dependent id="25">scatterbrained</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Jamestown" type="LOCATION" score="0.0">
          <tokens>
            <token id="8" string="Jamestown" />
          </tokens>
        </entity>
        <entity id="2" string="N.Y." type="LOCATION" score="0.0">
          <tokens>
            <token id="10" string="N.Y." />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="27" has_coreference="true">
      <content>She was smart, hard-working and a perfectionist who knew that whatever else it was, comedy was not accidental.</content>
      <tokens>
        <token id="1" string="She" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="smart" lemma="smart" stem="smart" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="hard-working" lemma="hard-working" stem="hard-work" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="perfectionist" lemma="perfectionist" stem="perfectionist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="knew" lemma="know" stem="knew" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="whatever" lemma="whatever" stem="whatev" pos="WDT" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="else" lemma="else" stem="els" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="14" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="comedy" lemma="comedy" stem="comedi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="accidental" lemma="accidental" stem="accident" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP She)) (VP (VBD was) (UCP (ADJP (JJ smart) (, ,) (JJ hard-working)) (CC and) (NP (NP (DT a) (NN perfectionist)) (SBAR (WHNP (WP who)) (S (VP (VBD knew) (SBAR (IN that) (S (SBAR (WHNP (WDT whatever)) (S (NP (RB else) (PRP it)) (VP (VBD was)))) (, ,) (NP (NN comedy)) (VP (VBD was) (RB not) (ADJP (JJ accidental))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="who knew that whatever else it was , comedy was not accidental" type="SBAR">
          <tokens>
            <token id="9" string="who" />
            <token id="10" string="knew" />
            <token id="11" string="that" />
            <token id="12" string="whatever" />
            <token id="13" string="else" />
            <token id="14" string="it" />
            <token id="15" string="was" />
            <token id="16" string="," />
            <token id="17" string="comedy" />
            <token id="18" string="was" />
            <token id="19" string="not" />
            <token id="20" string="accidental" />
          </tokens>
        </chunking>
        <chunking id="2" string="comedy" type="NP">
          <tokens>
            <token id="17" string="comedy" />
          </tokens>
        </chunking>
        <chunking id="3" string="was" type="VP">
          <tokens>
            <token id="15" string="was" />
          </tokens>
        </chunking>
        <chunking id="4" string="knew that whatever else it was , comedy was not accidental" type="VP">
          <tokens>
            <token id="10" string="knew" />
            <token id="11" string="that" />
            <token id="12" string="whatever" />
            <token id="13" string="else" />
            <token id="14" string="it" />
            <token id="15" string="was" />
            <token id="16" string="," />
            <token id="17" string="comedy" />
            <token id="18" string="was" />
            <token id="19" string="not" />
            <token id="20" string="accidental" />
          </tokens>
        </chunking>
        <chunking id="5" string="a perfectionist" type="NP">
          <tokens>
            <token id="7" string="a" />
            <token id="8" string="perfectionist" />
          </tokens>
        </chunking>
        <chunking id="6" string="that whatever else it was , comedy was not accidental" type="SBAR">
          <tokens>
            <token id="11" string="that" />
            <token id="12" string="whatever" />
            <token id="13" string="else" />
            <token id="14" string="it" />
            <token id="15" string="was" />
            <token id="16" string="," />
            <token id="17" string="comedy" />
            <token id="18" string="was" />
            <token id="19" string="not" />
            <token id="20" string="accidental" />
          </tokens>
        </chunking>
        <chunking id="7" string="accidental" type="ADJP">
          <tokens>
            <token id="20" string="accidental" />
          </tokens>
        </chunking>
        <chunking id="8" string="She" type="NP">
          <tokens>
            <token id="1" string="She" />
          </tokens>
        </chunking>
        <chunking id="9" string="was smart , hard-working and a perfectionist who knew that whatever else it was , comedy was not accidental" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="smart" />
            <token id="4" string="," />
            <token id="5" string="hard-working" />
            <token id="6" string="and" />
            <token id="7" string="a" />
            <token id="8" string="perfectionist" />
            <token id="9" string="who" />
            <token id="10" string="knew" />
            <token id="11" string="that" />
            <token id="12" string="whatever" />
            <token id="13" string="else" />
            <token id="14" string="it" />
            <token id="15" string="was" />
            <token id="16" string="," />
            <token id="17" string="comedy" />
            <token id="18" string="was" />
            <token id="19" string="not" />
            <token id="20" string="accidental" />
          </tokens>
        </chunking>
        <chunking id="10" string="else it" type="NP">
          <tokens>
            <token id="13" string="else" />
            <token id="14" string="it" />
          </tokens>
        </chunking>
        <chunking id="11" string="was not accidental" type="VP">
          <tokens>
            <token id="18" string="was" />
            <token id="19" string="not" />
            <token id="20" string="accidental" />
          </tokens>
        </chunking>
        <chunking id="12" string="smart , hard-working" type="ADJP">
          <tokens>
            <token id="3" string="smart" />
            <token id="4" string="," />
            <token id="5" string="hard-working" />
          </tokens>
        </chunking>
        <chunking id="13" string="a perfectionist who knew that whatever else it was , comedy was not accidental" type="NP">
          <tokens>
            <token id="7" string="a" />
            <token id="8" string="perfectionist" />
            <token id="9" string="who" />
            <token id="10" string="knew" />
            <token id="11" string="that" />
            <token id="12" string="whatever" />
            <token id="13" string="else" />
            <token id="14" string="it" />
            <token id="15" string="was" />
            <token id="16" string="," />
            <token id="17" string="comedy" />
            <token id="18" string="was" />
            <token id="19" string="not" />
            <token id="20" string="accidental" />
          </tokens>
        </chunking>
        <chunking id="14" string="whatever else it was" type="SBAR">
          <tokens>
            <token id="12" string="whatever" />
            <token id="13" string="else" />
            <token id="14" string="it" />
            <token id="15" string="was" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="5">hard-working</governor>
          <dependent id="1">She</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="5">hard-working</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">hard-working</governor>
          <dependent id="3">smart</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">hard-working</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="5">hard-working</governor>
          <dependent id="6">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">perfectionist</governor>
          <dependent id="7">a</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="5">hard-working</governor>
          <dependent id="8">perfectionist</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">knew</governor>
          <dependent id="9">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="8">perfectionist</governor>
          <dependent id="10">knew</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="20">accidental</governor>
          <dependent id="11">that</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">was</governor>
          <dependent id="12">whatever</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="14">it</governor>
          <dependent id="13">else</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">was</governor>
          <dependent id="14">it</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="20">accidental</governor>
          <dependent id="15">was</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="20">accidental</governor>
          <dependent id="17">comedy</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="20">accidental</governor>
          <dependent id="18">was</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="20">accidental</governor>
          <dependent id="19">not</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="10">knew</governor>
          <dependent id="20">accidental</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="28" has_coreference="true">
      <content>Then again, the cameras, television or film, have X-ray properties, and the millions who saw and loved her were not wrong to think they perceived a generous and loving woman, as well as a glorious clown who knew the world was better off laughing.</content>
      <tokens>
        <token id="1" string="Then" lemma="then" stem="then" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="again" lemma="again" stem="again" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="cameras" lemma="camera" stem="camera" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="television" lemma="television" stem="televis" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="or" lemma="or" stem="or" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="film" lemma="film" stem="film" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="have" lemma="have" stem="have" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="X-ray" lemma="x-ray" stem="x-rai" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="properties" lemma="property" stem="properti" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="17" string="millions" lemma="million" stem="million" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="18" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="19" string="saw" lemma="see" stem="saw" pos="VBD" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="20" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="21" string="loved" lemma="love" stem="love" pos="VBD" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="22" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="23" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="wrong" lemma="wrong" stem="wrong" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="think" lemma="think" stem="think" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="perceived" lemma="perceive" stem="perceiv" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="generous" lemma="generous" stem="gener" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="loving" lemma="loving" stem="love" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="woman" lemma="woman" stem="woman" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="as" lemma="as" stem="a" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="well" lemma="well" stem="well" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="38" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="39" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="40" string="glorious" lemma="glorious" stem="gloriou" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="41" string="clown" lemma="clown" stem="clown" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="42" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="43" string="knew" lemma="know" stem="knew" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="44" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="45" string="world" lemma="world" stem="world" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="46" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="47" string="better" lemma="better" stem="better" pos="JJR" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="48" string="off" lemma="off" stem="off" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="49" string="laughing" lemma="laughing" stem="laugh" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="50" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (RB Then) (S (ADVP (RB again)) (, ,) (NP (NP (DT the) (NNS cameras)) (, ,) (NP (NN television) (CC or) (NN film)) (, ,)) (VP (VBP have) (NP (NN X-ray) (NNS properties)))) (, ,) (CC and) (S (NP (NP (DT the) (NNS millions)) (SBAR (WHNP (WP who)) (S (VP (VP (VBD saw)) (CC and) (VP (VBD loved) (NP (PRP$ her))))))) (VP (VBD were) (RB not) (ADJP (JJ wrong) (S (VP (TO to) (VP (VB think) (NP (NP (NP (PRP they)) (VP (VBN perceived) (NP (DT a) (ADJP (JJ generous) (CC and) (JJ loving)) (NN woman)))) (, ,) (CONJP (RB as) (RB well) (IN as)) (NP (NP (DT a) (JJ glorious) (NN clown)) (SBAR (WHNP (WP who)) (S (VP (VBD knew) (SBAR (S (NP (DT the) (NN world)) (VP (VBD was) (ADJP (JJR better)) (PP (RP off) (NP (NN laughing))))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="they perceived a generous and loving woman , as well as a glorious clown who knew the world was better off laughing" type="NP">
          <tokens>
            <token id="28" string="they" />
            <token id="29" string="perceived" />
            <token id="30" string="a" />
            <token id="31" string="generous" />
            <token id="32" string="and" />
            <token id="33" string="loving" />
            <token id="34" string="woman" />
            <token id="35" string="," />
            <token id="36" string="as" />
            <token id="37" string="well" />
            <token id="38" string="as" />
            <token id="39" string="a" />
            <token id="40" string="glorious" />
            <token id="41" string="clown" />
            <token id="42" string="who" />
            <token id="43" string="knew" />
            <token id="44" string="the" />
            <token id="45" string="world" />
            <token id="46" string="was" />
            <token id="47" string="better" />
            <token id="48" string="off" />
            <token id="49" string="laughing" />
          </tokens>
        </chunking>
        <chunking id="2" string="knew the world was better off laughing" type="VP">
          <tokens>
            <token id="43" string="knew" />
            <token id="44" string="the" />
            <token id="45" string="world" />
            <token id="46" string="was" />
            <token id="47" string="better" />
            <token id="48" string="off" />
            <token id="49" string="laughing" />
          </tokens>
        </chunking>
        <chunking id="3" string="a glorious clown who knew the world was better off laughing" type="NP">
          <tokens>
            <token id="39" string="a" />
            <token id="40" string="glorious" />
            <token id="41" string="clown" />
            <token id="42" string="who" />
            <token id="43" string="knew" />
            <token id="44" string="the" />
            <token id="45" string="world" />
            <token id="46" string="was" />
            <token id="47" string="better" />
            <token id="48" string="off" />
            <token id="49" string="laughing" />
          </tokens>
        </chunking>
        <chunking id="4" string="wrong to think they perceived a generous and loving woman , as well as a glorious clown who knew the world was better off laughing" type="ADJP">
          <tokens>
            <token id="25" string="wrong" />
            <token id="26" string="to" />
            <token id="27" string="think" />
            <token id="28" string="they" />
            <token id="29" string="perceived" />
            <token id="30" string="a" />
            <token id="31" string="generous" />
            <token id="32" string="and" />
            <token id="33" string="loving" />
            <token id="34" string="woman" />
            <token id="35" string="," />
            <token id="36" string="as" />
            <token id="37" string="well" />
            <token id="38" string="as" />
            <token id="39" string="a" />
            <token id="40" string="glorious" />
            <token id="41" string="clown" />
            <token id="42" string="who" />
            <token id="43" string="knew" />
            <token id="44" string="the" />
            <token id="45" string="world" />
            <token id="46" string="was" />
            <token id="47" string="better" />
            <token id="48" string="off" />
            <token id="49" string="laughing" />
          </tokens>
        </chunking>
        <chunking id="5" string="to think they perceived a generous and loving woman , as well as a glorious clown who knew the world was better off laughing" type="VP">
          <tokens>
            <token id="26" string="to" />
            <token id="27" string="think" />
            <token id="28" string="they" />
            <token id="29" string="perceived" />
            <token id="30" string="a" />
            <token id="31" string="generous" />
            <token id="32" string="and" />
            <token id="33" string="loving" />
            <token id="34" string="woman" />
            <token id="35" string="," />
            <token id="36" string="as" />
            <token id="37" string="well" />
            <token id="38" string="as" />
            <token id="39" string="a" />
            <token id="40" string="glorious" />
            <token id="41" string="clown" />
            <token id="42" string="who" />
            <token id="43" string="knew" />
            <token id="44" string="the" />
            <token id="45" string="world" />
            <token id="46" string="was" />
            <token id="47" string="better" />
            <token id="48" string="off" />
            <token id="49" string="laughing" />
          </tokens>
        </chunking>
        <chunking id="6" string="the cameras" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="cameras" />
          </tokens>
        </chunking>
        <chunking id="7" string="was better off laughing" type="VP">
          <tokens>
            <token id="46" string="was" />
            <token id="47" string="better" />
            <token id="48" string="off" />
            <token id="49" string="laughing" />
          </tokens>
        </chunking>
        <chunking id="8" string="generous and loving" type="ADJP">
          <tokens>
            <token id="31" string="generous" />
            <token id="32" string="and" />
            <token id="33" string="loving" />
          </tokens>
        </chunking>
        <chunking id="9" string="saw" type="VP">
          <tokens>
            <token id="19" string="saw" />
          </tokens>
        </chunking>
        <chunking id="10" string="a generous and loving woman" type="NP">
          <tokens>
            <token id="30" string="a" />
            <token id="31" string="generous" />
            <token id="32" string="and" />
            <token id="33" string="loving" />
            <token id="34" string="woman" />
          </tokens>
        </chunking>
        <chunking id="11" string="loved her" type="VP">
          <tokens>
            <token id="21" string="loved" />
            <token id="22" string="her" />
          </tokens>
        </chunking>
        <chunking id="12" string="the cameras , television or film ," type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="cameras" />
            <token id="6" string="," />
            <token id="7" string="television" />
            <token id="8" string="or" />
            <token id="9" string="film" />
            <token id="10" string="," />
          </tokens>
        </chunking>
        <chunking id="13" string="better" type="ADJP">
          <tokens>
            <token id="47" string="better" />
          </tokens>
        </chunking>
        <chunking id="14" string="a glorious clown" type="NP">
          <tokens>
            <token id="39" string="a" />
            <token id="40" string="glorious" />
            <token id="41" string="clown" />
          </tokens>
        </chunking>
        <chunking id="15" string="saw and loved her" type="VP">
          <tokens>
            <token id="19" string="saw" />
            <token id="20" string="and" />
            <token id="21" string="loved" />
            <token id="22" string="her" />
          </tokens>
        </chunking>
        <chunking id="16" string="have X-ray properties" type="VP">
          <tokens>
            <token id="11" string="have" />
            <token id="12" string="X-ray" />
            <token id="13" string="properties" />
          </tokens>
        </chunking>
        <chunking id="17" string="the millions" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="millions" />
          </tokens>
        </chunking>
        <chunking id="18" string="laughing" type="NP">
          <tokens>
            <token id="49" string="laughing" />
          </tokens>
        </chunking>
        <chunking id="19" string="who knew the world was better off laughing" type="SBAR">
          <tokens>
            <token id="42" string="who" />
            <token id="43" string="knew" />
            <token id="44" string="the" />
            <token id="45" string="world" />
            <token id="46" string="was" />
            <token id="47" string="better" />
            <token id="48" string="off" />
            <token id="49" string="laughing" />
          </tokens>
        </chunking>
        <chunking id="20" string="they perceived a generous and loving woman" type="NP">
          <tokens>
            <token id="28" string="they" />
            <token id="29" string="perceived" />
            <token id="30" string="a" />
            <token id="31" string="generous" />
            <token id="32" string="and" />
            <token id="33" string="loving" />
            <token id="34" string="woman" />
          </tokens>
        </chunking>
        <chunking id="21" string="X-ray properties" type="NP">
          <tokens>
            <token id="12" string="X-ray" />
            <token id="13" string="properties" />
          </tokens>
        </chunking>
        <chunking id="22" string="the world" type="NP">
          <tokens>
            <token id="44" string="the" />
            <token id="45" string="world" />
          </tokens>
        </chunking>
        <chunking id="23" string="perceived a generous and loving woman" type="VP">
          <tokens>
            <token id="29" string="perceived" />
            <token id="30" string="a" />
            <token id="31" string="generous" />
            <token id="32" string="and" />
            <token id="33" string="loving" />
            <token id="34" string="woman" />
          </tokens>
        </chunking>
        <chunking id="24" string="who saw and loved her" type="SBAR">
          <tokens>
            <token id="18" string="who" />
            <token id="19" string="saw" />
            <token id="20" string="and" />
            <token id="21" string="loved" />
            <token id="22" string="her" />
          </tokens>
        </chunking>
        <chunking id="25" string="think they perceived a generous and loving woman , as well as a glorious clown who knew the world was better off laughing" type="VP">
          <tokens>
            <token id="27" string="think" />
            <token id="28" string="they" />
            <token id="29" string="perceived" />
            <token id="30" string="a" />
            <token id="31" string="generous" />
            <token id="32" string="and" />
            <token id="33" string="loving" />
            <token id="34" string="woman" />
            <token id="35" string="," />
            <token id="36" string="as" />
            <token id="37" string="well" />
            <token id="38" string="as" />
            <token id="39" string="a" />
            <token id="40" string="glorious" />
            <token id="41" string="clown" />
            <token id="42" string="who" />
            <token id="43" string="knew" />
            <token id="44" string="the" />
            <token id="45" string="world" />
            <token id="46" string="was" />
            <token id="47" string="better" />
            <token id="48" string="off" />
            <token id="49" string="laughing" />
          </tokens>
        </chunking>
        <chunking id="26" string="they" type="NP">
          <tokens>
            <token id="28" string="they" />
          </tokens>
        </chunking>
        <chunking id="27" string="television or film" type="NP">
          <tokens>
            <token id="7" string="television" />
            <token id="8" string="or" />
            <token id="9" string="film" />
          </tokens>
        </chunking>
        <chunking id="28" string="were not wrong to think they perceived a generous and loving woman , as well as a glorious clown who knew the world was better off laughing" type="VP">
          <tokens>
            <token id="23" string="were" />
            <token id="24" string="not" />
            <token id="25" string="wrong" />
            <token id="26" string="to" />
            <token id="27" string="think" />
            <token id="28" string="they" />
            <token id="29" string="perceived" />
            <token id="30" string="a" />
            <token id="31" string="generous" />
            <token id="32" string="and" />
            <token id="33" string="loving" />
            <token id="34" string="woman" />
            <token id="35" string="," />
            <token id="36" string="as" />
            <token id="37" string="well" />
            <token id="38" string="as" />
            <token id="39" string="a" />
            <token id="40" string="glorious" />
            <token id="41" string="clown" />
            <token id="42" string="who" />
            <token id="43" string="knew" />
            <token id="44" string="the" />
            <token id="45" string="world" />
            <token id="46" string="was" />
            <token id="47" string="better" />
            <token id="48" string="off" />
            <token id="49" string="laughing" />
          </tokens>
        </chunking>
        <chunking id="29" string="her" type="NP">
          <tokens>
            <token id="22" string="her" />
          </tokens>
        </chunking>
        <chunking id="30" string="the world was better off laughing" type="SBAR">
          <tokens>
            <token id="44" string="the" />
            <token id="45" string="world" />
            <token id="46" string="was" />
            <token id="47" string="better" />
            <token id="48" string="off" />
            <token id="49" string="laughing" />
          </tokens>
        </chunking>
        <chunking id="31" string="the millions who saw and loved her" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="millions" />
            <token id="18" string="who" />
            <token id="19" string="saw" />
            <token id="20" string="and" />
            <token id="21" string="loved" />
            <token id="22" string="her" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="11">have</governor>
          <dependent id="1">Then</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">have</governor>
          <dependent id="2">again</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">cameras</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">have</governor>
          <dependent id="5">cameras</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="5">cameras</governor>
          <dependent id="7">television</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">television</governor>
          <dependent id="8">or</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">television</governor>
          <dependent id="9">film</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="11">have</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="13">properties</governor>
          <dependent id="12">X-ray</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">have</governor>
          <dependent id="13">properties</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="11">have</governor>
          <dependent id="15">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="17">millions</governor>
          <dependent id="16">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="25">wrong</governor>
          <dependent id="17">millions</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">saw</governor>
          <dependent id="18">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="17">millions</governor>
          <dependent id="19">saw</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="19">saw</governor>
          <dependent id="20">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="19">saw</governor>
          <dependent id="21">loved</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="21">loved</governor>
          <dependent id="22">her</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="25">wrong</governor>
          <dependent id="23">were</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="25">wrong</governor>
          <dependent id="24">not</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="11">have</governor>
          <dependent id="25">wrong</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="27">think</governor>
          <dependent id="26">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="25">wrong</governor>
          <dependent id="27">think</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="27">think</governor>
          <dependent id="28">they</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="28">they</governor>
          <dependent id="29">perceived</dependent>
        </dependency>
        <dependency type="det">
          <governor id="34">woman</governor>
          <dependent id="30">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="34">woman</governor>
          <dependent id="31">generous</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="31">generous</governor>
          <dependent id="32">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="31">generous</governor>
          <dependent id="33">loving</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="29">perceived</governor>
          <dependent id="34">woman</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="28">they</governor>
          <dependent id="36">as</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="36">as</governor>
          <dependent id="37">well</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="36">as</governor>
          <dependent id="38">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="41">clown</governor>
          <dependent id="39">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="41">clown</governor>
          <dependent id="40">glorious</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="28">they</governor>
          <dependent id="41">clown</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="43">knew</governor>
          <dependent id="42">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="41">clown</governor>
          <dependent id="43">knew</dependent>
        </dependency>
        <dependency type="det">
          <governor id="45">world</governor>
          <dependent id="44">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="47">better</governor>
          <dependent id="45">world</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="47">better</governor>
          <dependent id="46">was</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="43">knew</governor>
          <dependent id="47">better</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="49">laughing</governor>
          <dependent id="48">off</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="47">better</governor>
          <dependent id="49">laughing</dependent>
        </dependency>
      </dependencies>
    </sentence>
  </sentences>
  <coreferences>
    <coreference id="1" type="PROPER">
      <referenced ids_tokens="1-2" string="Lucille Ball" id_sentence="1" />
      <mentions>
        <mention ids_tokens="19" string="her" id_sentence="2" />
        <mention ids_tokens="1" string="She" id_sentence="3" />
        <mention ids_tokens="20" string="her" id_sentence="3" />
        <mention ids_tokens="4" string="her" id_sentence="4" />
        <mention ids_tokens="1" string="She" id_sentence="5" />
        <mention ids_tokens="3" string="her" id_sentence="5" />
        <mention ids_tokens="5" string="Lucy" id_sentence="7" />
        <mention ids_tokens="8" string="her" id_sentence="7" />
        <mention ids_tokens="9" string="her" id_sentence="8" />
        <mention ids_tokens="25" string="her" id_sentence="8" />
        <mention ids_tokens="5-15" string="&quot; I Love Lucy &quot; reruns today , as everybody does" id_sentence="9" />
        <mention ids_tokens="7-8" string="Love Lucy" id_sentence="9" />
        <mention ids_tokens="32-33" string="Lucy's" id_sentence="11" />
        <mention ids_tokens="1" string="She" id_sentence="12" />
        <mention ids_tokens="33-34" string="Lucy's" id_sentence="12" />
        <mention ids_tokens="1" string="Her" id_sentence="13" />
        <mention ids_tokens="19" string="Lucy" id_sentence="13" />
        <mention ids_tokens="5" string="Lucy" id_sentence="17" />
        <mention ids_tokens="1" string="Lucy" id_sentence="20" />
        <mention ids_tokens="1" string="Her" id_sentence="21" />
        <mention ids_tokens="9" string="she" id_sentence="21" />
        <mention ids_tokens="20" string="her" id_sentence="21" />
        <mention ids_tokens="26-28" string="I Love Lucy" id_sentence="21" />
        <mention ids_tokens="27-28" string="Love Lucy" id_sentence="21" />
        <mention ids_tokens="11" string="she" id_sentence="22" />
        <mention ids_tokens="26" string="her" id_sentence="22" />
        <mention ids_tokens="4" string="Lucy" id_sentence="23" />
        <mention ids_tokens="12-14" string="I Love Lucy" id_sentence="23" />
        <mention ids_tokens="13-14" string="Love Lucy" id_sentence="23" />
        <mention ids_tokens="39" string="Lucy" id_sentence="23" />
        <mention ids_tokens="4" string="her" id_sentence="24" />
        <mention ids_tokens="5-8" string="Lucy , of course" id_sentence="25" />
        <mention ids_tokens="5" string="Lucy" id_sentence="25" />
        <mention ids_tokens="1" string="She" id_sentence="26" />
        <mention ids_tokens="1" string="She" id_sentence="27" />
        <mention ids_tokens="22" string="her" id_sentence="28" />
      </mentions>
    </coreference>
    <coreference id="2" type="NOMINAL">
      <referenced ids_tokens="28-29-30" string="the movies '" id_sentence="1" />
      <mentions>
        <mention ids_tokens="1-2" string="The movies" id_sentence="4" />
        <mention ids_tokens="8-9" string="the movies" id_sentence="22" />
        <mention ids_tokens="28-29" string="the movies" id_sentence="22" />
        <mention ids_tokens="19-20" string="the movies" id_sentence="23" />
      </mentions>
    </coreference>
    <coreference id="4" type="PROPER">
      <referenced ids_tokens="16-17-18-19-20-21-22-23-24-25-26-27-28-29-30-31" string="the one that gave her a firmer foothold on the slippery slopes of a Hollywood career" id_sentence="3" />
      <mentions>
        <mention ids_tokens="30-31" string="Roman Scandals" id_sentence="2" />
      </mentions>
    </coreference>
    <coreference id="5" type="NOMINAL">
      <referenced ids_tokens="3-4-5" string="her matchless craft" id_sentence="5" />
      <mentions>
        <mention ids_tokens="1-2" string="The craft" id_sentence="14" />
        <mention ids_tokens="1" string="It" id_sentence="15" />
        <mention ids_tokens="3-5" string="a brilliant illusion" id_sentence="15" />
      </mentions>
    </coreference>
    <coreference id="6" type="NOMINAL">
      <referenced ids_tokens="4-5-6" string="the other films" id_sentence="6" />
      <mentions>
        <mention ids_tokens="5" string="films" id_sentence="21" />
      </mentions>
    </coreference>
    <coreference id="9" type="NOMINAL">
      <referenced ids_tokens="7" string="performers" id_sentence="8" />
      <mentions>
        <mention ids_tokens="27" string="they" id_sentence="9" />
      </mentions>
    </coreference>
    <coreference id="11" type="PROPER">
      <referenced ids_tokens="18-19" string="Desi Arnaz" id_sentence="10" />
      <mentions>
        <mention ids_tokens="5" string="himself" id_sentence="13" />
      </mentions>
    </coreference>
    <coreference id="13" type="NOMINAL">
      <referenced ids_tokens="14-15" string="live audiences" id_sentence="10" />
      <mentions>
        <mention ids_tokens="27-28" string="their audiences" id_sentence="23" />
        <mention ids_tokens="41" string="their" id_sentence="23" />
      </mentions>
    </coreference>
    <coreference id="14" type="NOMINAL">
      <referenced ids_tokens="1-2" string="The idea" id_sentence="11" />
      <mentions>
        <mention ids_tokens="3" string="it" id_sentence="12" />
        <mention ids_tokens="25" string="it" id_sentence="13" />
      </mentions>
    </coreference>
    <coreference id="17" type="LIST">
      <referenced ids_tokens="4-5-6-7-8-9-10-11" string="Desi himself , Vivian Vance and William Frawley" id_sentence="13" />
      <mentions>
        <mention ids_tokens="10" string="they" id_sentence="14" />
        <mention ids_tokens="12" string="themselves" id_sentence="14" />
      </mentions>
    </coreference>
    <coreference id="18" type="NOMINAL">
      <referenced ids_tokens="24-25" string="the performing" id_sentence="15" />
      <mentions>
        <mention ids_tokens="1" string="They" id_sentence="18" />
        <mention ids_tokens="9" string="that" id_sentence="18" />
        <mention ids_tokens="13" string="their" id_sentence="18" />
        <mention ids_tokens="2" string="they" id_sentence="19" />
      </mentions>
    </coreference>
    <coreference id="21" type="NOMINAL">
      <referenced ids_tokens="4-5-6" string="a hard road" id_sentence="26" />
      <mentions>
        <mention ids_tokens="13-14" string="else it" id_sentence="27" />
      </mentions>
    </coreference>
  </coreferences>
</document>
