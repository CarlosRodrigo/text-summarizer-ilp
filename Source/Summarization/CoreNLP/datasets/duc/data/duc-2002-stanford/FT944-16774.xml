<?xml version="1.0" encoding="UTF-8"?>
<document id="0" name="FT944-16774">
  <sentences>
    <sentence id="1" has_coreference="true">
      <content>After hours of heated delib-eration - under a system described as &amp;apost;mad&amp;apost; by one judge - the Pounds 20,000 Booker Prize for fiction last night went to Mr James Kelman for How Late It Was, How Late, published by Secker and Warburg.</content>
      <tokens>
        <token id="1" string="After" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="hours" lemma="hour" stem="hour" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="3" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="4" string="heated" lemma="heated" stem="heat" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="delib-eration" lemma="delib-eration" stem="delib-er" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="-" lemma="-" stem="-" pos=":" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="under" lemma="under" stem="under" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="system" lemma="system" stem="system" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="described" lemma="describe" stem="describ" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="12" string="'" lemma="`" stem="'" pos="``" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="mad" lemma="mad" stem="mad" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="16" string="one" lemma="one" stem="on" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="17" string="judge" lemma="judge" stem="judg" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="18" string="-" lemma="-" stem="-" pos=":" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="19" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="Pounds" lemma="Pounds" stem="pound" pos="NNPS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="20,000" lemma="20,000" stem="20,000" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="22" string="Booker" lemma="Booker" stem="booker" pos="NNP" type="Word" isStopWord="false" ner="MISC" is_referenced="true" is_refers="false" />
        <token id="23" string="Prize" lemma="Prize" stem="prize" pos="NNP" type="Word" isStopWord="false" ner="MISC" is_referenced="true" is_refers="false" />
        <token id="24" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="25" string="fiction" lemma="fiction" stem="fiction" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="26" string="last" lemma="last" stem="last" pos="JJ" type="Word" isStopWord="false" ner="TIME" is_referenced="false" is_refers="false" />
        <token id="27" string="night" lemma="night" stem="night" pos="NN" type="Word" isStopWord="false" ner="TIME" is_referenced="false" is_refers="false" />
        <token id="28" string="went" lemma="go" stem="went" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="Mr" lemma="Mr" stem="mr" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="31" string="James" lemma="James" stem="jame" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="32" string="Kelman" lemma="Kelman" stem="kelman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="33" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="34" string="How" lemma="how" stem="how" pos="WRB" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="35" string="Late" lemma="late" stem="late" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="36" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="37" string="Was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="38" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="39" string="How" lemma="how" stem="how" pos="WRB" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="40" string="Late" lemma="late" stem="late" pos="RB" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="41" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="42" string="published" lemma="publish" stem="publish" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="43" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="44" string="Secker" lemma="Secker" stem="secker" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="45" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="46" string="Warburg" lemma="Warburg" stem="warburg" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="47" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (IN After) (NP (NP (NP (NNS hours)) (PP (IN of) (NP (JJ heated) (NN delib-eration)))) (: -) (PP (IN under) (NP (NP (DT a) (NN system)) (VP (VBN described) (PP (IN as) (`` `) (ADJP (JJ mad)) ('' ')) (PP (IN by) (NP (CD one) (NN judge)))))) (: -))) (NP (NP (DT the) (NNPS Pounds)) (NP (CD 20,000)) (NP (NP (NNP Booker) (NNP Prize)) (PP (IN for) (NP (NN fiction))))) (NP-TMP (JJ last) (NN night)) (VP (VBD went) (PP (TO to) (NP (NP (NP (NNP Mr) (NNP James) (NNP Kelman)) (SBAR (WHPP (IN for) (WHNP (WRB How) (JJ Late))) (S (NP (PRP It)) (VP (VBD Was)))) (PRN (, ,) (FRAG (WHADVP (WRB How)) (ADJP (RB Late))) (, ,))) (VP (VBN published) (PP (IN by) (NP (NNP Secker) (CC and) (NNP Warburg))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Secker and Warburg" type="NP">
          <tokens>
            <token id="44" string="Secker" />
            <token id="45" string="and" />
            <token id="46" string="Warburg" />
          </tokens>
        </chunking>
        <chunking id="2" string="Late" type="ADJP">
          <tokens>
            <token id="40" string="Late" />
          </tokens>
        </chunking>
        <chunking id="3" string="hours" type="NP">
          <tokens>
            <token id="2" string="hours" />
          </tokens>
        </chunking>
        <chunking id="4" string="heated delib-eration" type="NP">
          <tokens>
            <token id="4" string="heated" />
            <token id="5" string="delib-eration" />
          </tokens>
        </chunking>
        <chunking id="5" string="Mr James Kelman for How Late It Was , How Late ," type="NP">
          <tokens>
            <token id="30" string="Mr" />
            <token id="31" string="James" />
            <token id="32" string="Kelman" />
            <token id="33" string="for" />
            <token id="34" string="How" />
            <token id="35" string="Late" />
            <token id="36" string="It" />
            <token id="37" string="Was" />
            <token id="38" string="," />
            <token id="39" string="How" />
            <token id="40" string="Late" />
            <token id="41" string="," />
          </tokens>
        </chunking>
        <chunking id="6" string="How" type="WHADVP">
          <tokens>
            <token id="39" string="How" />
          </tokens>
        </chunking>
        <chunking id="7" string="Booker Prize for fiction" type="NP">
          <tokens>
            <token id="22" string="Booker" />
            <token id="23" string="Prize" />
            <token id="24" string="for" />
            <token id="25" string="fiction" />
          </tokens>
        </chunking>
        <chunking id="8" string="one judge" type="NP">
          <tokens>
            <token id="16" string="one" />
            <token id="17" string="judge" />
          </tokens>
        </chunking>
        <chunking id="9" string="a system" type="NP">
          <tokens>
            <token id="8" string="a" />
            <token id="9" string="system" />
          </tokens>
        </chunking>
        <chunking id="10" string="Booker Prize" type="NP">
          <tokens>
            <token id="22" string="Booker" />
            <token id="23" string="Prize" />
          </tokens>
        </chunking>
        <chunking id="11" string="Mr James Kelman for How Late It Was , How Late , published by Secker and Warburg" type="NP">
          <tokens>
            <token id="30" string="Mr" />
            <token id="31" string="James" />
            <token id="32" string="Kelman" />
            <token id="33" string="for" />
            <token id="34" string="How" />
            <token id="35" string="Late" />
            <token id="36" string="It" />
            <token id="37" string="Was" />
            <token id="38" string="," />
            <token id="39" string="How" />
            <token id="40" string="Late" />
            <token id="41" string="," />
            <token id="42" string="published" />
            <token id="43" string="by" />
            <token id="44" string="Secker" />
            <token id="45" string="and" />
            <token id="46" string="Warburg" />
          </tokens>
        </chunking>
        <chunking id="12" string="hours of heated delib-eration" type="NP">
          <tokens>
            <token id="2" string="hours" />
            <token id="3" string="of" />
            <token id="4" string="heated" />
            <token id="5" string="delib-eration" />
          </tokens>
        </chunking>
        <chunking id="13" string="fiction" type="NP">
          <tokens>
            <token id="25" string="fiction" />
          </tokens>
        </chunking>
        <chunking id="14" string="published by Secker and Warburg" type="VP">
          <tokens>
            <token id="42" string="published" />
            <token id="43" string="by" />
            <token id="44" string="Secker" />
            <token id="45" string="and" />
            <token id="46" string="Warburg" />
          </tokens>
        </chunking>
        <chunking id="15" string="described as ` mad ' by one judge" type="VP">
          <tokens>
            <token id="10" string="described" />
            <token id="11" string="as" />
            <token id="12" string="'" />
            <token id="13" string="mad" />
            <token id="14" string="'" />
            <token id="15" string="by" />
            <token id="16" string="one" />
            <token id="17" string="judge" />
          </tokens>
        </chunking>
        <chunking id="16" string="Mr James Kelman" type="NP">
          <tokens>
            <token id="30" string="Mr" />
            <token id="31" string="James" />
            <token id="32" string="Kelman" />
          </tokens>
        </chunking>
        <chunking id="17" string="Was" type="VP">
          <tokens>
            <token id="37" string="Was" />
          </tokens>
        </chunking>
        <chunking id="18" string="20,000" type="NP">
          <tokens>
            <token id="21" string="20,000" />
          </tokens>
        </chunking>
        <chunking id="19" string="It" type="NP">
          <tokens>
            <token id="36" string="It" />
          </tokens>
        </chunking>
        <chunking id="20" string="mad" type="ADJP">
          <tokens>
            <token id="13" string="mad" />
          </tokens>
        </chunking>
        <chunking id="21" string="hours of heated delib-eration - under a system described as ` mad ' by one judge -" type="NP">
          <tokens>
            <token id="2" string="hours" />
            <token id="3" string="of" />
            <token id="4" string="heated" />
            <token id="5" string="delib-eration" />
            <token id="6" string="-" />
            <token id="7" string="under" />
            <token id="8" string="a" />
            <token id="9" string="system" />
            <token id="10" string="described" />
            <token id="11" string="as" />
            <token id="12" string="'" />
            <token id="13" string="mad" />
            <token id="14" string="'" />
            <token id="15" string="by" />
            <token id="16" string="one" />
            <token id="17" string="judge" />
            <token id="18" string="-" />
          </tokens>
        </chunking>
        <chunking id="22" string="went to Mr James Kelman for How Late It Was , How Late , published by Secker and Warburg" type="VP">
          <tokens>
            <token id="28" string="went" />
            <token id="29" string="to" />
            <token id="30" string="Mr" />
            <token id="31" string="James" />
            <token id="32" string="Kelman" />
            <token id="33" string="for" />
            <token id="34" string="How" />
            <token id="35" string="Late" />
            <token id="36" string="It" />
            <token id="37" string="Was" />
            <token id="38" string="," />
            <token id="39" string="How" />
            <token id="40" string="Late" />
            <token id="41" string="," />
            <token id="42" string="published" />
            <token id="43" string="by" />
            <token id="44" string="Secker" />
            <token id="45" string="and" />
            <token id="46" string="Warburg" />
          </tokens>
        </chunking>
        <chunking id="23" string="a system described as ` mad ' by one judge" type="NP">
          <tokens>
            <token id="8" string="a" />
            <token id="9" string="system" />
            <token id="10" string="described" />
            <token id="11" string="as" />
            <token id="12" string="'" />
            <token id="13" string="mad" />
            <token id="14" string="'" />
            <token id="15" string="by" />
            <token id="16" string="one" />
            <token id="17" string="judge" />
          </tokens>
        </chunking>
        <chunking id="24" string="for How Late It Was" type="SBAR">
          <tokens>
            <token id="33" string="for" />
            <token id="34" string="How" />
            <token id="35" string="Late" />
            <token id="36" string="It" />
            <token id="37" string="Was" />
          </tokens>
        </chunking>
        <chunking id="25" string="the Pounds" type="NP">
          <tokens>
            <token id="19" string="the" />
            <token id="20" string="Pounds" />
          </tokens>
        </chunking>
        <chunking id="26" string="the Pounds 20,000 Booker Prize for fiction" type="NP">
          <tokens>
            <token id="19" string="the" />
            <token id="20" string="Pounds" />
            <token id="21" string="20,000" />
            <token id="22" string="Booker" />
            <token id="23" string="Prize" />
            <token id="24" string="for" />
            <token id="25" string="fiction" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="case">
          <governor id="2">hours</governor>
          <dependent id="1">After</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="28">went</governor>
          <dependent id="2">hours</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">delib-eration</governor>
          <dependent id="3">of</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">delib-eration</governor>
          <dependent id="4">heated</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">hours</governor>
          <dependent id="5">delib-eration</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">system</governor>
          <dependent id="7">under</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">system</governor>
          <dependent id="8">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">hours</governor>
          <dependent id="9">system</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="9">system</governor>
          <dependent id="10">described</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">mad</governor>
          <dependent id="11">as</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="10">described</governor>
          <dependent id="13">mad</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">judge</governor>
          <dependent id="15">by</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="17">judge</governor>
          <dependent id="16">one</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">described</governor>
          <dependent id="17">judge</dependent>
        </dependency>
        <dependency type="det">
          <governor id="20">Pounds</governor>
          <dependent id="19">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="28">went</governor>
          <dependent id="20">Pounds</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="20">Pounds</governor>
          <dependent id="21">20,000</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="23">Prize</governor>
          <dependent id="22">Booker</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="20">Pounds</governor>
          <dependent id="23">Prize</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">fiction</governor>
          <dependent id="24">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">Prize</governor>
          <dependent id="25">fiction</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="27">night</governor>
          <dependent id="26">last</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="28">went</governor>
          <dependent id="27">night</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="28">went</dependent>
        </dependency>
        <dependency type="case">
          <governor id="32">Kelman</governor>
          <dependent id="29">to</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="32">Kelman</governor>
          <dependent id="30">Mr</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="32">Kelman</governor>
          <dependent id="31">James</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="28">went</governor>
          <dependent id="32">Kelman</dependent>
        </dependency>
        <dependency type="case">
          <governor id="35">Late</governor>
          <dependent id="33">for</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="35">Late</governor>
          <dependent id="34">How</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="37">Was</governor>
          <dependent id="35">Late</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="37">Was</governor>
          <dependent id="36">It</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="32">Kelman</governor>
          <dependent id="37">Was</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="40">Late</governor>
          <dependent id="39">How</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="32">Kelman</governor>
          <dependent id="40">Late</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="32">Kelman</governor>
          <dependent id="42">published</dependent>
        </dependency>
        <dependency type="case">
          <governor id="44">Secker</governor>
          <dependent id="43">by</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="42">published</governor>
          <dependent id="44">Secker</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="44">Secker</governor>
          <dependent id="45">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="44">Secker</governor>
          <dependent id="46">Warburg</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="James Kelman" type="PERSON" score="0.0">
          <tokens>
            <token id="31" string="James" />
            <token id="32" string="Kelman" />
          </tokens>
        </entity>
        <entity id="2" string="hours" type="DURATION" score="0.0">
          <tokens>
            <token id="2" string="hours" />
          </tokens>
        </entity>
        <entity id="3" string="last night" type="TIME" score="0.0">
          <tokens>
            <token id="26" string="last" />
            <token id="27" string="night" />
          </tokens>
        </entity>
        <entity id="4" string="one" type="NUMBER" score="0.0">
          <tokens>
            <token id="16" string="one" />
          </tokens>
        </entity>
        <entity id="5" string="Secker" type="PERSON" score="0.0">
          <tokens>
            <token id="44" string="Secker" />
          </tokens>
        </entity>
        <entity id="6" string="Warburg" type="PERSON" score="0.0">
          <tokens>
            <token id="46" string="Warburg" />
          </tokens>
        </entity>
        <entity id="7" string="20,000" type="NUMBER" score="0.0">
          <tokens>
            <token id="21" string="20,000" />
          </tokens>
        </entity>
        <entity id="8" string="Booker Prize" type="MISC" score="0.0">
          <tokens>
            <token id="22" string="Booker" />
            <token id="23" string="Prize" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="2" has_coreference="true">
      <content>The novel, a story of Scottish low-life narrated largely in Glaswegian dialect, is unlikely to prove a popular choice with booksellers, who have damned all six books shortlisted for the prize as boring, elitist and - worst of all - unsaleable.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="novel" lemma="novel" stem="novel" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="story" lemma="story" stem="stori" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="Scottish" lemma="scottish" stem="scottish" pos="JJ" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="8" string="low-life" lemma="low-life" stem="low-lif" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="narrated" lemma="narrate" stem="narrat" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="largely" lemma="largely" stem="larg" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="Glaswegian" lemma="glaswegian" stem="glaswegian" pos="JJ" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="13" string="dialect" lemma="dialect" stem="dialect" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="unlikely" lemma="unlikely" stem="unlik" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="prove" lemma="prove" stem="prove" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="popular" lemma="popular" stem="popular" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="choice" lemma="choice" stem="choic" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="booksellers" lemma="bookseller" stem="booksel" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="24" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="have" lemma="have" stem="have" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="damned" lemma="damned" stem="damn" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="six" lemma="six" stem="six" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="30" string="books" lemma="book" stem="book" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="shortlisted" lemma="shortlist" stem="shortlist" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="34" string="prize" lemma="prize" stem="prize" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="35" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="boring" lemma="boring" stem="bore" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="37" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="elitist" lemma="elitist" stem="elitist" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="40" string="-" lemma="-" stem="-" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="41" string="worst" lemma="worst" stem="worst" pos="JJS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="42" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="43" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="44" string="-" lemma="-" stem="-" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="45" string="unsaleable" lemma="unsaleable" stem="unsal" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="46" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT The) (JJ novel)) (, ,) (NP (NP (DT a) (NN story)) (PP (IN of) (NP (JJ Scottish) (JJ low-life))) (VP (VBN narrated) (ADVP (RB largely)) (PP (IN in) (NP (JJ Glaswegian) (NN dialect))))) (, ,)) (VP (VBZ is) (ADJP (JJ unlikely) (S (VP (TO to) (VP (VB prove) (NP (DT a) (JJ popular) (NN choice)) (PP (IN with) (NP (NP (NNS booksellers)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBP have) (ADJP (RB damned) (SBAR (S (NP (DT all) (CD six) (NNS books)) (VP (VBD shortlisted) (PP (IN for) (NP (DT the) (NN prize))) (PP (IN as) (ADJP (JJ boring))) (, ,) (ADJP (JJ elitist) (CC and) (PRN (: -) (NP (NP (JJS worst)) (PP (IN of) (NP (DT all)))) (: -)) (JJ unsaleable)))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="unlikely to prove a popular choice with booksellers , who have damned all six books shortlisted for the prize as boring , elitist and - worst of all - unsaleable" type="ADJP">
          <tokens>
            <token id="16" string="unlikely" />
            <token id="17" string="to" />
            <token id="18" string="prove" />
            <token id="19" string="a" />
            <token id="20" string="popular" />
            <token id="21" string="choice" />
            <token id="22" string="with" />
            <token id="23" string="booksellers" />
            <token id="24" string="," />
            <token id="25" string="who" />
            <token id="26" string="have" />
            <token id="27" string="damned" />
            <token id="28" string="all" />
            <token id="29" string="six" />
            <token id="30" string="books" />
            <token id="31" string="shortlisted" />
            <token id="32" string="for" />
            <token id="33" string="the" />
            <token id="34" string="prize" />
            <token id="35" string="as" />
            <token id="36" string="boring" />
            <token id="37" string="," />
            <token id="38" string="elitist" />
            <token id="39" string="and" />
            <token id="40" string="-" />
            <token id="41" string="worst" />
            <token id="42" string="of" />
            <token id="43" string="all" />
            <token id="44" string="-" />
            <token id="45" string="unsaleable" />
          </tokens>
        </chunking>
        <chunking id="2" string="all" type="NP">
          <tokens>
            <token id="43" string="all" />
          </tokens>
        </chunking>
        <chunking id="3" string="is unlikely to prove a popular choice with booksellers , who have damned all six books shortlisted for the prize as boring , elitist and - worst of all - unsaleable" type="VP">
          <tokens>
            <token id="15" string="is" />
            <token id="16" string="unlikely" />
            <token id="17" string="to" />
            <token id="18" string="prove" />
            <token id="19" string="a" />
            <token id="20" string="popular" />
            <token id="21" string="choice" />
            <token id="22" string="with" />
            <token id="23" string="booksellers" />
            <token id="24" string="," />
            <token id="25" string="who" />
            <token id="26" string="have" />
            <token id="27" string="damned" />
            <token id="28" string="all" />
            <token id="29" string="six" />
            <token id="30" string="books" />
            <token id="31" string="shortlisted" />
            <token id="32" string="for" />
            <token id="33" string="the" />
            <token id="34" string="prize" />
            <token id="35" string="as" />
            <token id="36" string="boring" />
            <token id="37" string="," />
            <token id="38" string="elitist" />
            <token id="39" string="and" />
            <token id="40" string="-" />
            <token id="41" string="worst" />
            <token id="42" string="of" />
            <token id="43" string="all" />
            <token id="44" string="-" />
            <token id="45" string="unsaleable" />
          </tokens>
        </chunking>
        <chunking id="4" string="all six books shortlisted for the prize as boring , elitist and - worst of all - unsaleable" type="SBAR">
          <tokens>
            <token id="28" string="all" />
            <token id="29" string="six" />
            <token id="30" string="books" />
            <token id="31" string="shortlisted" />
            <token id="32" string="for" />
            <token id="33" string="the" />
            <token id="34" string="prize" />
            <token id="35" string="as" />
            <token id="36" string="boring" />
            <token id="37" string="," />
            <token id="38" string="elitist" />
            <token id="39" string="and" />
            <token id="40" string="-" />
            <token id="41" string="worst" />
            <token id="42" string="of" />
            <token id="43" string="all" />
            <token id="44" string="-" />
            <token id="45" string="unsaleable" />
          </tokens>
        </chunking>
        <chunking id="5" string="the prize" type="NP">
          <tokens>
            <token id="33" string="the" />
            <token id="34" string="prize" />
          </tokens>
        </chunking>
        <chunking id="6" string="boring" type="ADJP">
          <tokens>
            <token id="36" string="boring" />
          </tokens>
        </chunking>
        <chunking id="7" string="a popular choice" type="NP">
          <tokens>
            <token id="19" string="a" />
            <token id="20" string="popular" />
            <token id="21" string="choice" />
          </tokens>
        </chunking>
        <chunking id="8" string="worst" type="NP">
          <tokens>
            <token id="41" string="worst" />
          </tokens>
        </chunking>
        <chunking id="9" string="The novel , a story of Scottish low-life narrated largely in Glaswegian dialect ," type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="novel" />
            <token id="3" string="," />
            <token id="4" string="a" />
            <token id="5" string="story" />
            <token id="6" string="of" />
            <token id="7" string="Scottish" />
            <token id="8" string="low-life" />
            <token id="9" string="narrated" />
            <token id="10" string="largely" />
            <token id="11" string="in" />
            <token id="12" string="Glaswegian" />
            <token id="13" string="dialect" />
            <token id="14" string="," />
          </tokens>
        </chunking>
        <chunking id="10" string="a story of Scottish low-life narrated largely in Glaswegian dialect" type="NP">
          <tokens>
            <token id="4" string="a" />
            <token id="5" string="story" />
            <token id="6" string="of" />
            <token id="7" string="Scottish" />
            <token id="8" string="low-life" />
            <token id="9" string="narrated" />
            <token id="10" string="largely" />
            <token id="11" string="in" />
            <token id="12" string="Glaswegian" />
            <token id="13" string="dialect" />
          </tokens>
        </chunking>
        <chunking id="11" string="Scottish low-life" type="NP">
          <tokens>
            <token id="7" string="Scottish" />
            <token id="8" string="low-life" />
          </tokens>
        </chunking>
        <chunking id="12" string="shortlisted for the prize as boring , elitist and - worst of all - unsaleable" type="VP">
          <tokens>
            <token id="31" string="shortlisted" />
            <token id="32" string="for" />
            <token id="33" string="the" />
            <token id="34" string="prize" />
            <token id="35" string="as" />
            <token id="36" string="boring" />
            <token id="37" string="," />
            <token id="38" string="elitist" />
            <token id="39" string="and" />
            <token id="40" string="-" />
            <token id="41" string="worst" />
            <token id="42" string="of" />
            <token id="43" string="all" />
            <token id="44" string="-" />
            <token id="45" string="unsaleable" />
          </tokens>
        </chunking>
        <chunking id="13" string="narrated largely in Glaswegian dialect" type="VP">
          <tokens>
            <token id="9" string="narrated" />
            <token id="10" string="largely" />
            <token id="11" string="in" />
            <token id="12" string="Glaswegian" />
            <token id="13" string="dialect" />
          </tokens>
        </chunking>
        <chunking id="14" string="worst of all" type="NP">
          <tokens>
            <token id="41" string="worst" />
            <token id="42" string="of" />
            <token id="43" string="all" />
          </tokens>
        </chunking>
        <chunking id="15" string="The novel" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="novel" />
          </tokens>
        </chunking>
        <chunking id="16" string="prove a popular choice with booksellers , who have damned all six books shortlisted for the prize as boring , elitist and - worst of all - unsaleable" type="VP">
          <tokens>
            <token id="18" string="prove" />
            <token id="19" string="a" />
            <token id="20" string="popular" />
            <token id="21" string="choice" />
            <token id="22" string="with" />
            <token id="23" string="booksellers" />
            <token id="24" string="," />
            <token id="25" string="who" />
            <token id="26" string="have" />
            <token id="27" string="damned" />
            <token id="28" string="all" />
            <token id="29" string="six" />
            <token id="30" string="books" />
            <token id="31" string="shortlisted" />
            <token id="32" string="for" />
            <token id="33" string="the" />
            <token id="34" string="prize" />
            <token id="35" string="as" />
            <token id="36" string="boring" />
            <token id="37" string="," />
            <token id="38" string="elitist" />
            <token id="39" string="and" />
            <token id="40" string="-" />
            <token id="41" string="worst" />
            <token id="42" string="of" />
            <token id="43" string="all" />
            <token id="44" string="-" />
            <token id="45" string="unsaleable" />
          </tokens>
        </chunking>
        <chunking id="17" string="booksellers , who have damned all six books shortlisted for the prize as boring , elitist and - worst of all - unsaleable" type="NP">
          <tokens>
            <token id="23" string="booksellers" />
            <token id="24" string="," />
            <token id="25" string="who" />
            <token id="26" string="have" />
            <token id="27" string="damned" />
            <token id="28" string="all" />
            <token id="29" string="six" />
            <token id="30" string="books" />
            <token id="31" string="shortlisted" />
            <token id="32" string="for" />
            <token id="33" string="the" />
            <token id="34" string="prize" />
            <token id="35" string="as" />
            <token id="36" string="boring" />
            <token id="37" string="," />
            <token id="38" string="elitist" />
            <token id="39" string="and" />
            <token id="40" string="-" />
            <token id="41" string="worst" />
            <token id="42" string="of" />
            <token id="43" string="all" />
            <token id="44" string="-" />
            <token id="45" string="unsaleable" />
          </tokens>
        </chunking>
        <chunking id="18" string="to prove a popular choice with booksellers , who have damned all six books shortlisted for the prize as boring , elitist and - worst of all - unsaleable" type="VP">
          <tokens>
            <token id="17" string="to" />
            <token id="18" string="prove" />
            <token id="19" string="a" />
            <token id="20" string="popular" />
            <token id="21" string="choice" />
            <token id="22" string="with" />
            <token id="23" string="booksellers" />
            <token id="24" string="," />
            <token id="25" string="who" />
            <token id="26" string="have" />
            <token id="27" string="damned" />
            <token id="28" string="all" />
            <token id="29" string="six" />
            <token id="30" string="books" />
            <token id="31" string="shortlisted" />
            <token id="32" string="for" />
            <token id="33" string="the" />
            <token id="34" string="prize" />
            <token id="35" string="as" />
            <token id="36" string="boring" />
            <token id="37" string="," />
            <token id="38" string="elitist" />
            <token id="39" string="and" />
            <token id="40" string="-" />
            <token id="41" string="worst" />
            <token id="42" string="of" />
            <token id="43" string="all" />
            <token id="44" string="-" />
            <token id="45" string="unsaleable" />
          </tokens>
        </chunking>
        <chunking id="19" string="booksellers" type="NP">
          <tokens>
            <token id="23" string="booksellers" />
          </tokens>
        </chunking>
        <chunking id="20" string="Glaswegian dialect" type="NP">
          <tokens>
            <token id="12" string="Glaswegian" />
            <token id="13" string="dialect" />
          </tokens>
        </chunking>
        <chunking id="21" string="elitist and - worst of all - unsaleable" type="ADJP">
          <tokens>
            <token id="38" string="elitist" />
            <token id="39" string="and" />
            <token id="40" string="-" />
            <token id="41" string="worst" />
            <token id="42" string="of" />
            <token id="43" string="all" />
            <token id="44" string="-" />
            <token id="45" string="unsaleable" />
          </tokens>
        </chunking>
        <chunking id="22" string="all six books" type="NP">
          <tokens>
            <token id="28" string="all" />
            <token id="29" string="six" />
            <token id="30" string="books" />
          </tokens>
        </chunking>
        <chunking id="23" string="a story" type="NP">
          <tokens>
            <token id="4" string="a" />
            <token id="5" string="story" />
          </tokens>
        </chunking>
        <chunking id="24" string="who have damned all six books shortlisted for the prize as boring , elitist and - worst of all - unsaleable" type="SBAR">
          <tokens>
            <token id="25" string="who" />
            <token id="26" string="have" />
            <token id="27" string="damned" />
            <token id="28" string="all" />
            <token id="29" string="six" />
            <token id="30" string="books" />
            <token id="31" string="shortlisted" />
            <token id="32" string="for" />
            <token id="33" string="the" />
            <token id="34" string="prize" />
            <token id="35" string="as" />
            <token id="36" string="boring" />
            <token id="37" string="," />
            <token id="38" string="elitist" />
            <token id="39" string="and" />
            <token id="40" string="-" />
            <token id="41" string="worst" />
            <token id="42" string="of" />
            <token id="43" string="all" />
            <token id="44" string="-" />
            <token id="45" string="unsaleable" />
          </tokens>
        </chunking>
        <chunking id="25" string="damned all six books shortlisted for the prize as boring , elitist and - worst of all - unsaleable" type="ADJP">
          <tokens>
            <token id="27" string="damned" />
            <token id="28" string="all" />
            <token id="29" string="six" />
            <token id="30" string="books" />
            <token id="31" string="shortlisted" />
            <token id="32" string="for" />
            <token id="33" string="the" />
            <token id="34" string="prize" />
            <token id="35" string="as" />
            <token id="36" string="boring" />
            <token id="37" string="," />
            <token id="38" string="elitist" />
            <token id="39" string="and" />
            <token id="40" string="-" />
            <token id="41" string="worst" />
            <token id="42" string="of" />
            <token id="43" string="all" />
            <token id="44" string="-" />
            <token id="45" string="unsaleable" />
          </tokens>
        </chunking>
        <chunking id="26" string="have damned all six books shortlisted for the prize as boring , elitist and - worst of all - unsaleable" type="VP">
          <tokens>
            <token id="26" string="have" />
            <token id="27" string="damned" />
            <token id="28" string="all" />
            <token id="29" string="six" />
            <token id="30" string="books" />
            <token id="31" string="shortlisted" />
            <token id="32" string="for" />
            <token id="33" string="the" />
            <token id="34" string="prize" />
            <token id="35" string="as" />
            <token id="36" string="boring" />
            <token id="37" string="," />
            <token id="38" string="elitist" />
            <token id="39" string="and" />
            <token id="40" string="-" />
            <token id="41" string="worst" />
            <token id="42" string="of" />
            <token id="43" string="all" />
            <token id="44" string="-" />
            <token id="45" string="unsaleable" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">novel</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">unlikely</governor>
          <dependent id="2">novel</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">story</governor>
          <dependent id="4">a</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="2">novel</governor>
          <dependent id="5">story</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">low-life</governor>
          <dependent id="6">of</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">low-life</governor>
          <dependent id="7">Scottish</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">story</governor>
          <dependent id="8">low-life</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="5">story</governor>
          <dependent id="9">narrated</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="9">narrated</governor>
          <dependent id="10">largely</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">dialect</governor>
          <dependent id="11">in</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">dialect</governor>
          <dependent id="12">Glaswegian</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">narrated</governor>
          <dependent id="13">dialect</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="16">unlikely</governor>
          <dependent id="15">is</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="16">unlikely</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="18">prove</governor>
          <dependent id="17">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="16">unlikely</governor>
          <dependent id="18">prove</dependent>
        </dependency>
        <dependency type="det">
          <governor id="21">choice</governor>
          <dependent id="19">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="21">choice</governor>
          <dependent id="20">popular</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="18">prove</governor>
          <dependent id="21">choice</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">booksellers</governor>
          <dependent id="22">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="18">prove</governor>
          <dependent id="23">booksellers</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="26">have</governor>
          <dependent id="25">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="23">booksellers</governor>
          <dependent id="26">have</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="26">have</governor>
          <dependent id="27">damned</dependent>
        </dependency>
        <dependency type="det">
          <governor id="30">books</governor>
          <dependent id="28">all</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="30">books</governor>
          <dependent id="29">six</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="31">shortlisted</governor>
          <dependent id="30">books</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="27">damned</governor>
          <dependent id="31">shortlisted</dependent>
        </dependency>
        <dependency type="case">
          <governor id="34">prize</governor>
          <dependent id="32">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="34">prize</governor>
          <dependent id="33">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="31">shortlisted</governor>
          <dependent id="34">prize</dependent>
        </dependency>
        <dependency type="case">
          <governor id="36">boring</governor>
          <dependent id="35">as</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="31">shortlisted</governor>
          <dependent id="36">boring</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="45">unsaleable</governor>
          <dependent id="38">elitist</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="45">unsaleable</governor>
          <dependent id="39">and</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="45">unsaleable</governor>
          <dependent id="41">worst</dependent>
        </dependency>
        <dependency type="case">
          <governor id="43">all</governor>
          <dependent id="42">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="41">worst</governor>
          <dependent id="43">all</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="31">shortlisted</governor>
          <dependent id="45">unsaleable</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="six" type="NUMBER" score="0.0">
          <tokens>
            <token id="29" string="six" />
          </tokens>
        </entity>
        <entity id="2" string="Scottish" type="MISC" score="0.0">
          <tokens>
            <token id="7" string="Scottish" />
          </tokens>
        </entity>
        <entity id="3" string="Glaswegian" type="MISC" score="0.0">
          <tokens>
            <token id="12" string="Glaswegian" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="3" has_coreference="true">
      <content>Professor John Bayley, chairman of the Booker judges, acknowledged that Mr Kelman&amp;apost;s book in particular might be deemed inaccessible.</content>
      <tokens>
        <token id="1" string="Professor" lemma="Professor" stem="professor" pos="NNP" type="Word" isStopWord="false" ner="TITLE" is_referenced="false" is_refers="false" />
        <token id="2" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="3" string="Bayley" lemma="Bayley" stem="baylei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="chairman" lemma="chairman" stem="chairman" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Booker" lemma="Booker" stem="booker" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="9" string="judges" lemma="judge" stem="judg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="acknowledged" lemma="acknowledge" stem="acknowledg" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="Mr" lemma="Mr" stem="mr" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="14" string="Kelman" lemma="Kelman" stem="kelman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="15" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="16" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="17" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="18" string="particular" lemma="particular" stem="particular" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="19" string="might" lemma="might" stem="might" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="be" lemma="be" stem="be" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="deemed" lemma="deem" stem="deem" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="inaccessible" lemma="inaccessible" stem="inaccess" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Professor) (NNP John) (NNP Bayley)) (, ,) (NP (NP (NN chairman)) (PP (IN of) (NP (DT the) (NNP Booker) (NNS judges)))) (, ,)) (VP (VBD acknowledged) (SBAR (IN that) (S (NP (NP (NP (NNP Mr) (NNP Kelman) (POS 's)) (NN book)) (PP (IN in) (NP (JJ particular)))) (VP (MD might) (VP (VB be) (VP (VBN deemed) (S (ADJP (JJ inaccessible))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="acknowledged that Mr Kelman 's book in particular might be deemed inaccessible" type="VP">
          <tokens>
            <token id="11" string="acknowledged" />
            <token id="12" string="that" />
            <token id="13" string="Mr" />
            <token id="14" string="Kelman" />
            <token id="15" string="'s" />
            <token id="16" string="book" />
            <token id="17" string="in" />
            <token id="18" string="particular" />
            <token id="19" string="might" />
            <token id="20" string="be" />
            <token id="21" string="deemed" />
            <token id="22" string="inaccessible" />
          </tokens>
        </chunking>
        <chunking id="2" string="that Mr Kelman 's book in particular might be deemed inaccessible" type="SBAR">
          <tokens>
            <token id="12" string="that" />
            <token id="13" string="Mr" />
            <token id="14" string="Kelman" />
            <token id="15" string="'s" />
            <token id="16" string="book" />
            <token id="17" string="in" />
            <token id="18" string="particular" />
            <token id="19" string="might" />
            <token id="20" string="be" />
            <token id="21" string="deemed" />
            <token id="22" string="inaccessible" />
          </tokens>
        </chunking>
        <chunking id="3" string="Professor John Bayley" type="NP">
          <tokens>
            <token id="1" string="Professor" />
            <token id="2" string="John" />
            <token id="3" string="Bayley" />
          </tokens>
        </chunking>
        <chunking id="4" string="inaccessible" type="ADJP">
          <tokens>
            <token id="22" string="inaccessible" />
          </tokens>
        </chunking>
        <chunking id="5" string="Mr Kelman 's book" type="NP">
          <tokens>
            <token id="13" string="Mr" />
            <token id="14" string="Kelman" />
            <token id="15" string="'s" />
            <token id="16" string="book" />
          </tokens>
        </chunking>
        <chunking id="6" string="chairman of the Booker judges" type="NP">
          <tokens>
            <token id="5" string="chairman" />
            <token id="6" string="of" />
            <token id="7" string="the" />
            <token id="8" string="Booker" />
            <token id="9" string="judges" />
          </tokens>
        </chunking>
        <chunking id="7" string="Mr Kelman 's" type="NP">
          <tokens>
            <token id="13" string="Mr" />
            <token id="14" string="Kelman" />
            <token id="15" string="'s" />
          </tokens>
        </chunking>
        <chunking id="8" string="chairman" type="NP">
          <tokens>
            <token id="5" string="chairman" />
          </tokens>
        </chunking>
        <chunking id="9" string="the Booker judges" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="Booker" />
            <token id="9" string="judges" />
          </tokens>
        </chunking>
        <chunking id="10" string="particular" type="NP">
          <tokens>
            <token id="18" string="particular" />
          </tokens>
        </chunking>
        <chunking id="11" string="Mr Kelman 's book in particular" type="NP">
          <tokens>
            <token id="13" string="Mr" />
            <token id="14" string="Kelman" />
            <token id="15" string="'s" />
            <token id="16" string="book" />
            <token id="17" string="in" />
            <token id="18" string="particular" />
          </tokens>
        </chunking>
        <chunking id="12" string="might be deemed inaccessible" type="VP">
          <tokens>
            <token id="19" string="might" />
            <token id="20" string="be" />
            <token id="21" string="deemed" />
            <token id="22" string="inaccessible" />
          </tokens>
        </chunking>
        <chunking id="13" string="deemed inaccessible" type="VP">
          <tokens>
            <token id="21" string="deemed" />
            <token id="22" string="inaccessible" />
          </tokens>
        </chunking>
        <chunking id="14" string="be deemed inaccessible" type="VP">
          <tokens>
            <token id="20" string="be" />
            <token id="21" string="deemed" />
            <token id="22" string="inaccessible" />
          </tokens>
        </chunking>
        <chunking id="15" string="Professor John Bayley , chairman of the Booker judges ," type="NP">
          <tokens>
            <token id="1" string="Professor" />
            <token id="2" string="John" />
            <token id="3" string="Bayley" />
            <token id="4" string="," />
            <token id="5" string="chairman" />
            <token id="6" string="of" />
            <token id="7" string="the" />
            <token id="8" string="Booker" />
            <token id="9" string="judges" />
            <token id="10" string="," />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="3">Bayley</governor>
          <dependent id="1">Professor</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">Bayley</governor>
          <dependent id="2">John</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">acknowledged</governor>
          <dependent id="3">Bayley</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="3">Bayley</governor>
          <dependent id="5">chairman</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">judges</governor>
          <dependent id="6">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">judges</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">judges</governor>
          <dependent id="8">Booker</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">chairman</governor>
          <dependent id="9">judges</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="11">acknowledged</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="21">deemed</governor>
          <dependent id="12">that</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">Kelman</governor>
          <dependent id="13">Mr</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="16">book</governor>
          <dependent id="14">Kelman</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">Kelman</governor>
          <dependent id="15">'s</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="21">deemed</governor>
          <dependent id="16">book</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">particular</governor>
          <dependent id="17">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">book</governor>
          <dependent id="18">particular</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="21">deemed</governor>
          <dependent id="19">might</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="21">deemed</governor>
          <dependent id="20">be</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="11">acknowledged</governor>
          <dependent id="21">deemed</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="21">deemed</governor>
          <dependent id="22">inaccessible</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Professor" type="TITLE" score="0.0">
          <tokens>
            <token id="1" string="Professor" />
          </tokens>
        </entity>
        <entity id="2" string="John Bayley" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="John" />
            <token id="3" string="Bayley" />
          </tokens>
        </entity>
        <entity id="3" string="Booker" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Booker" />
          </tokens>
        </entity>
        <entity id="4" string="Kelman" type="PERSON" score="0.0">
          <tokens>
            <token id="14" string="Kelman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="4" has_coreference="true">
      <content>&amp;apost;We had an extremely difficult final meeting and were very divided,&amp;apost; he said after three hours of debate.</content>
      <tokens>
        <token id="1" string="'" lemma="`" stem="'" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="We" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="extremely" lemma="extremely" stem="extrem" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="difficult" lemma="difficult" stem="difficult" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="final" lemma="final" stem="final" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="meeting" lemma="meeting" stem="meet" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="very" lemma="very" stem="veri" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="divided" lemma="divide" stem="divid" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="16" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="after" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="three" lemma="three" stem="three" pos="CD" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="19" string="hours" lemma="hour" stem="hour" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="20" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="debate" lemma="debate" stem="debat" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` `) (S (NP (PRP We)) (VP (VP (VBD had) (NP (DT an) (ADJP (RB extremely) (JJ difficult)) (JJ final) (NN meeting))) (CC and) (VP (VBD were) (ADJP (RB very) (VBN divided))))) (, ,) ('' ') (NP (PRP he)) (VP (VBD said) (PP (IN after) (NP (NP (CD three) (NNS hours)) (PP (IN of) (NP (NN debate)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="debate" type="NP">
          <tokens>
            <token id="21" string="debate" />
          </tokens>
        </chunking>
        <chunking id="2" string="had an extremely difficult final meeting and were very divided" type="VP">
          <tokens>
            <token id="3" string="had" />
            <token id="4" string="an" />
            <token id="5" string="extremely" />
            <token id="6" string="difficult" />
            <token id="7" string="final" />
            <token id="8" string="meeting" />
            <token id="9" string="and" />
            <token id="10" string="were" />
            <token id="11" string="very" />
            <token id="12" string="divided" />
          </tokens>
        </chunking>
        <chunking id="3" string="had an extremely difficult final meeting" type="VP">
          <tokens>
            <token id="3" string="had" />
            <token id="4" string="an" />
            <token id="5" string="extremely" />
            <token id="6" string="difficult" />
            <token id="7" string="final" />
            <token id="8" string="meeting" />
          </tokens>
        </chunking>
        <chunking id="4" string="three hours of debate" type="NP">
          <tokens>
            <token id="18" string="three" />
            <token id="19" string="hours" />
            <token id="20" string="of" />
            <token id="21" string="debate" />
          </tokens>
        </chunking>
        <chunking id="5" string="three hours" type="NP">
          <tokens>
            <token id="18" string="three" />
            <token id="19" string="hours" />
          </tokens>
        </chunking>
        <chunking id="6" string="were very divided" type="VP">
          <tokens>
            <token id="10" string="were" />
            <token id="11" string="very" />
            <token id="12" string="divided" />
          </tokens>
        </chunking>
        <chunking id="7" string="very divided" type="ADJP">
          <tokens>
            <token id="11" string="very" />
            <token id="12" string="divided" />
          </tokens>
        </chunking>
        <chunking id="8" string="he" type="NP">
          <tokens>
            <token id="15" string="he" />
          </tokens>
        </chunking>
        <chunking id="9" string="We" type="NP">
          <tokens>
            <token id="2" string="We" />
          </tokens>
        </chunking>
        <chunking id="10" string="an extremely difficult final meeting" type="NP">
          <tokens>
            <token id="4" string="an" />
            <token id="5" string="extremely" />
            <token id="6" string="difficult" />
            <token id="7" string="final" />
            <token id="8" string="meeting" />
          </tokens>
        </chunking>
        <chunking id="11" string="extremely difficult" type="ADJP">
          <tokens>
            <token id="5" string="extremely" />
            <token id="6" string="difficult" />
          </tokens>
        </chunking>
        <chunking id="12" string="said after three hours of debate" type="VP">
          <tokens>
            <token id="16" string="said" />
            <token id="17" string="after" />
            <token id="18" string="three" />
            <token id="19" string="hours" />
            <token id="20" string="of" />
            <token id="21" string="debate" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">had</governor>
          <dependent id="2">We</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="16">said</governor>
          <dependent id="3">had</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">meeting</governor>
          <dependent id="4">an</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="6">difficult</governor>
          <dependent id="5">extremely</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">meeting</governor>
          <dependent id="6">difficult</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">meeting</governor>
          <dependent id="7">final</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">had</governor>
          <dependent id="8">meeting</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="3">had</governor>
          <dependent id="9">and</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="12">divided</governor>
          <dependent id="10">were</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="12">divided</governor>
          <dependent id="11">very</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="3">had</governor>
          <dependent id="12">divided</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">said</governor>
          <dependent id="15">he</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="16">said</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">hours</governor>
          <dependent id="17">after</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="19">hours</governor>
          <dependent id="18">three</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">said</governor>
          <dependent id="19">hours</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">debate</governor>
          <dependent id="20">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">hours</governor>
          <dependent id="21">debate</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="three hours" type="DURATION" score="0.0">
          <tokens>
            <token id="18" string="three" />
            <token id="19" string="hours" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="5" has_coreference="true">
      <content>Rabbi Julia Neuberger, the only woman on the judging panel, said she was made extremely cross by the decision.</content>
      <tokens>
        <token id="1" string="Rabbi" lemma="Rabbi" stem="rabbi" pos="NNP" type="Word" isStopWord="false" ner="TITLE" is_referenced="false" is_refers="false" />
        <token id="2" string="Julia" lemma="Julia" stem="julia" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="3" string="Neuberger" lemma="Neuberger" stem="neuberg" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="only" lemma="only" stem="onli" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="woman" lemma="woman" stem="woman" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="judging" lemma="judge" stem="judg" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="11" string="panel" lemma="panel" stem="panel" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="made" lemma="make" stem="made" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="extremely" lemma="extremely" stem="extrem" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="cross" lemma="cross" stem="cross" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="decision" lemma="decision" stem="decis" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Rabbi) (NNP Julia) (NNP Neuberger)) (, ,) (NP (NP (DT the) (JJ only) (NN woman)) (PP (IN on) (NP (DT the) (VBG judging) (NN panel)))) (, ,)) (VP (VBD said) (SBAR (S (NP (PRP she)) (VP (VBD was) (VP (VBN made) (ADJP (RB extremely) (JJ cross)) (PP (IN by) (NP (DT the) (NN decision)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Rabbi Julia Neuberger" type="NP">
          <tokens>
            <token id="1" string="Rabbi" />
            <token id="2" string="Julia" />
            <token id="3" string="Neuberger" />
          </tokens>
        </chunking>
        <chunking id="2" string="made extremely cross by the decision" type="VP">
          <tokens>
            <token id="16" string="made" />
            <token id="17" string="extremely" />
            <token id="18" string="cross" />
            <token id="19" string="by" />
            <token id="20" string="the" />
            <token id="21" string="decision" />
          </tokens>
        </chunking>
        <chunking id="3" string="Rabbi Julia Neuberger , the only woman on the judging panel ," type="NP">
          <tokens>
            <token id="1" string="Rabbi" />
            <token id="2" string="Julia" />
            <token id="3" string="Neuberger" />
            <token id="4" string="," />
            <token id="5" string="the" />
            <token id="6" string="only" />
            <token id="7" string="woman" />
            <token id="8" string="on" />
            <token id="9" string="the" />
            <token id="10" string="judging" />
            <token id="11" string="panel" />
            <token id="12" string="," />
          </tokens>
        </chunking>
        <chunking id="4" string="said she was made extremely cross by the decision" type="VP">
          <tokens>
            <token id="13" string="said" />
            <token id="14" string="she" />
            <token id="15" string="was" />
            <token id="16" string="made" />
            <token id="17" string="extremely" />
            <token id="18" string="cross" />
            <token id="19" string="by" />
            <token id="20" string="the" />
            <token id="21" string="decision" />
          </tokens>
        </chunking>
        <chunking id="5" string="she was made extremely cross by the decision" type="SBAR">
          <tokens>
            <token id="14" string="she" />
            <token id="15" string="was" />
            <token id="16" string="made" />
            <token id="17" string="extremely" />
            <token id="18" string="cross" />
            <token id="19" string="by" />
            <token id="20" string="the" />
            <token id="21" string="decision" />
          </tokens>
        </chunking>
        <chunking id="6" string="the judging panel" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="judging" />
            <token id="11" string="panel" />
          </tokens>
        </chunking>
        <chunking id="7" string="was made extremely cross by the decision" type="VP">
          <tokens>
            <token id="15" string="was" />
            <token id="16" string="made" />
            <token id="17" string="extremely" />
            <token id="18" string="cross" />
            <token id="19" string="by" />
            <token id="20" string="the" />
            <token id="21" string="decision" />
          </tokens>
        </chunking>
        <chunking id="8" string="the only woman on the judging panel" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="only" />
            <token id="7" string="woman" />
            <token id="8" string="on" />
            <token id="9" string="the" />
            <token id="10" string="judging" />
            <token id="11" string="panel" />
          </tokens>
        </chunking>
        <chunking id="9" string="the decision" type="NP">
          <tokens>
            <token id="20" string="the" />
            <token id="21" string="decision" />
          </tokens>
        </chunking>
        <chunking id="10" string="she" type="NP">
          <tokens>
            <token id="14" string="she" />
          </tokens>
        </chunking>
        <chunking id="11" string="extremely cross" type="ADJP">
          <tokens>
            <token id="17" string="extremely" />
            <token id="18" string="cross" />
          </tokens>
        </chunking>
        <chunking id="12" string="the only woman" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="only" />
            <token id="7" string="woman" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="3">Neuberger</governor>
          <dependent id="1">Rabbi</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">Neuberger</governor>
          <dependent id="2">Julia</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="13">said</governor>
          <dependent id="3">Neuberger</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">woman</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">woman</governor>
          <dependent id="6">only</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="3">Neuberger</governor>
          <dependent id="7">woman</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">panel</governor>
          <dependent id="8">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">panel</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">panel</governor>
          <dependent id="10">judging</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">woman</governor>
          <dependent id="11">panel</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="13">said</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="16">made</governor>
          <dependent id="14">she</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="16">made</governor>
          <dependent id="15">was</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="13">said</governor>
          <dependent id="16">made</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="18">cross</governor>
          <dependent id="17">extremely</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="16">made</governor>
          <dependent id="18">cross</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">decision</governor>
          <dependent id="19">by</dependent>
        </dependency>
        <dependency type="det">
          <governor id="21">decision</governor>
          <dependent id="20">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">made</governor>
          <dependent id="21">decision</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Julia Neuberger" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Julia" />
            <token id="3" string="Neuberger" />
          </tokens>
        </entity>
        <entity id="2" string="Rabbi" type="TITLE" score="0.0">
          <tokens>
            <token id="1" string="Rabbi" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="6" has_coreference="true">
      <content>&amp;apost;It&amp;apost;s just a drunken Scotsman railing against bureaucracy,&amp;apost; she said.</content>
      <tokens>
        <token id="1" string="'" lemma="`" stem="'" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="just" lemma="just" stem="just" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="6" string="drunken" lemma="drunken" stem="drunken" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="Scotsman" lemma="scotsman" stem="scotsman" pos="NN" type="Word" isStopWord="false" ner="MISC" is_referenced="true" is_refers="false" />
        <token id="8" string="railing" lemma="railing" stem="rail" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="against" lemma="against" stem="against" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="bureaucracy" lemma="bureaucracy" stem="bureaucraci" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` `) (S (NP (PRP It)) (VP (VBZ 's) (ADVP (RB just)) (NP (NP (DT a) (JJ drunken) (NN Scotsman) (NN railing)) (PP (IN against) (NP (NN bureaucracy)))))) (, ,) ('' ') (NP (PRP she)) (VP (VBD said)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a drunken Scotsman railing against bureaucracy" type="NP">
          <tokens>
            <token id="5" string="a" />
            <token id="6" string="drunken" />
            <token id="7" string="Scotsman" />
            <token id="8" string="railing" />
            <token id="9" string="against" />
            <token id="10" string="bureaucracy" />
          </tokens>
        </chunking>
        <chunking id="2" string="bureaucracy" type="NP">
          <tokens>
            <token id="10" string="bureaucracy" />
          </tokens>
        </chunking>
        <chunking id="3" string="'s just a drunken Scotsman railing against bureaucracy" type="VP">
          <tokens>
            <token id="3" string="'s" />
            <token id="4" string="just" />
            <token id="5" string="a" />
            <token id="6" string="drunken" />
            <token id="7" string="Scotsman" />
            <token id="8" string="railing" />
            <token id="9" string="against" />
            <token id="10" string="bureaucracy" />
          </tokens>
        </chunking>
        <chunking id="4" string="It" type="NP">
          <tokens>
            <token id="2" string="It" />
          </tokens>
        </chunking>
        <chunking id="5" string="a drunken Scotsman railing" type="NP">
          <tokens>
            <token id="5" string="a" />
            <token id="6" string="drunken" />
            <token id="7" string="Scotsman" />
            <token id="8" string="railing" />
          </tokens>
        </chunking>
        <chunking id="6" string="said" type="VP">
          <tokens>
            <token id="14" string="said" />
          </tokens>
        </chunking>
        <chunking id="7" string="she" type="NP">
          <tokens>
            <token id="13" string="she" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="8">railing</governor>
          <dependent id="2">It</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="8">railing</governor>
          <dependent id="3">'s</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="8">railing</governor>
          <dependent id="4">just</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">railing</governor>
          <dependent id="5">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">railing</governor>
          <dependent id="6">drunken</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">railing</governor>
          <dependent id="7">Scotsman</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="14">said</governor>
          <dependent id="8">railing</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">bureaucracy</governor>
          <dependent id="9">against</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">railing</governor>
          <dependent id="10">bureaucracy</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">said</governor>
          <dependent id="13">she</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="14">said</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Scotsman" type="MISC" score="0.0">
          <tokens>
            <token id="7" string="Scotsman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="7" has_coreference="false">
      <content>&amp;apost;My eyes are stretched in disbelief.</content>
      <tokens>
        <token id="1" string="'" lemma="`" stem="'" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="My" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="3" string="eyes" lemma="eye" stem="ey" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="are" lemma="be" stem="ar" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="stretched" lemma="stretch" stem="stretch" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="disbelief" lemma="disbelief" stem="disbelief" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` `) (NP (PRP$ My) (NNS eyes)) (VP (VBP are) (VP (VBN stretched) (PP (IN in) (NP (NN disbelief))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="stretched in disbelief" type="VP">
          <tokens>
            <token id="5" string="stretched" />
            <token id="6" string="in" />
            <token id="7" string="disbelief" />
          </tokens>
        </chunking>
        <chunking id="2" string="are stretched in disbelief" type="VP">
          <tokens>
            <token id="4" string="are" />
            <token id="5" string="stretched" />
            <token id="6" string="in" />
            <token id="7" string="disbelief" />
          </tokens>
        </chunking>
        <chunking id="3" string="My eyes" type="NP">
          <tokens>
            <token id="2" string="My" />
            <token id="3" string="eyes" />
          </tokens>
        </chunking>
        <chunking id="4" string="disbelief" type="NP">
          <tokens>
            <token id="7" string="disbelief" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="3">eyes</governor>
          <dependent id="2">My</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="5">stretched</governor>
          <dependent id="3">eyes</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="5">stretched</governor>
          <dependent id="4">are</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">stretched</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">disbelief</governor>
          <dependent id="6">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">stretched</governor>
          <dependent id="7">disbelief</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="8" has_coreference="false">
      <content>The voting system is completely mad.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="voting" lemma="voting" stem="vote" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="system" lemma="system" stem="system" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="completely" lemma="completely" stem="complet" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="mad" lemma="mad" stem="mad" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN voting) (NN system)) (VP (VBZ is) (ADJP (RB completely) (JJ mad))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="The voting system" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="voting" />
            <token id="3" string="system" />
          </tokens>
        </chunking>
        <chunking id="2" string="completely mad" type="ADJP">
          <tokens>
            <token id="5" string="completely" />
            <token id="6" string="mad" />
          </tokens>
        </chunking>
        <chunking id="3" string="is completely mad" type="VP">
          <tokens>
            <token id="4" string="is" />
            <token id="5" string="completely" />
            <token id="6" string="mad" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">system</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">system</governor>
          <dependent id="2">voting</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">mad</governor>
          <dependent id="3">system</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="6">mad</governor>
          <dependent id="4">is</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="6">mad</governor>
          <dependent id="5">completely</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">mad</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="9" has_coreference="true">
      <content>Kelman was the least favourite of the three front runners.</content>
      <tokens>
        <token id="1" string="Kelman" lemma="Kelman" stem="kelman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="least" lemma="least" stem="least" pos="JJS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="favourite" lemma="favourite" stem="favourit" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="three" lemma="three" stem="three" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="9" string="front" lemma="front" stem="front" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="runners" lemma="runner" stem="runner" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Kelman)) (VP (VBD was) (NP (NP (DT the) (JJS least) (NN favourite)) (PP (IN of) (NP (DT the) (CD three) (JJ front) (NNS runners))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the least favourite" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="least" />
            <token id="5" string="favourite" />
          </tokens>
        </chunking>
        <chunking id="2" string="was the least favourite of the three front runners" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="the" />
            <token id="4" string="least" />
            <token id="5" string="favourite" />
            <token id="6" string="of" />
            <token id="7" string="the" />
            <token id="8" string="three" />
            <token id="9" string="front" />
            <token id="10" string="runners" />
          </tokens>
        </chunking>
        <chunking id="3" string="the least favourite of the three front runners" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="least" />
            <token id="5" string="favourite" />
            <token id="6" string="of" />
            <token id="7" string="the" />
            <token id="8" string="three" />
            <token id="9" string="front" />
            <token id="10" string="runners" />
          </tokens>
        </chunking>
        <chunking id="4" string="the three front runners" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="three" />
            <token id="9" string="front" />
            <token id="10" string="runners" />
          </tokens>
        </chunking>
        <chunking id="5" string="Kelman" type="NP">
          <tokens>
            <token id="1" string="Kelman" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="5">favourite</governor>
          <dependent id="1">Kelman</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="5">favourite</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">favourite</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">favourite</governor>
          <dependent id="4">least</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">favourite</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">runners</governor>
          <dependent id="6">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">runners</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="10">runners</governor>
          <dependent id="8">three</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="10">runners</governor>
          <dependent id="9">front</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">favourite</governor>
          <dependent id="10">runners</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Kelman" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Kelman" />
          </tokens>
        </entity>
        <entity id="2" string="three" type="NUMBER" score="0.0">
          <tokens>
            <token id="8" string="three" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="10" has_coreference="true">
      <content>If it had been a short story it would have been fine.&amp;apost;</content>
      <tokens>
        <token id="1" string="If" lemma="if" stem="if" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="short" lemma="short" stem="short" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="story" lemma="story" stem="stori" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="would" lemma="would" stem="would" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="have" lemma="have" stem="have" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="fine" lemma="fine" stem="fine" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (SBAR (IN If) (S (NP (PRP it)) (VP (VBD had) (VP (VBN been) (NP (DT a) (JJ short) (NN story)))))) (NP (PRP it)) (VP (MD would) (VP (VB have) (VP (VBN been) (ADJP (JJ fine))))) (. .) ('' ')))</syntactictree>
      <chunkings>
        <chunking id="1" string="have been fine" type="VP">
          <tokens>
            <token id="10" string="have" />
            <token id="11" string="been" />
            <token id="12" string="fine" />
          </tokens>
        </chunking>
        <chunking id="2" string="a short story" type="NP">
          <tokens>
            <token id="5" string="a" />
            <token id="6" string="short" />
            <token id="7" string="story" />
          </tokens>
        </chunking>
        <chunking id="3" string="fine" type="ADJP">
          <tokens>
            <token id="12" string="fine" />
          </tokens>
        </chunking>
        <chunking id="4" string="would have been fine" type="VP">
          <tokens>
            <token id="9" string="would" />
            <token id="10" string="have" />
            <token id="11" string="been" />
            <token id="12" string="fine" />
          </tokens>
        </chunking>
        <chunking id="5" string="it" type="NP">
          <tokens>
            <token id="2" string="it" />
          </tokens>
        </chunking>
        <chunking id="6" string="been fine" type="VP">
          <tokens>
            <token id="11" string="been" />
            <token id="12" string="fine" />
          </tokens>
        </chunking>
        <chunking id="7" string="If it had been a short story" type="SBAR">
          <tokens>
            <token id="1" string="If" />
            <token id="2" string="it" />
            <token id="3" string="had" />
            <token id="4" string="been" />
            <token id="5" string="a" />
            <token id="6" string="short" />
            <token id="7" string="story" />
          </tokens>
        </chunking>
        <chunking id="8" string="had been a short story" type="VP">
          <tokens>
            <token id="3" string="had" />
            <token id="4" string="been" />
            <token id="5" string="a" />
            <token id="6" string="short" />
            <token id="7" string="story" />
          </tokens>
        </chunking>
        <chunking id="9" string="been a short story" type="VP">
          <tokens>
            <token id="4" string="been" />
            <token id="5" string="a" />
            <token id="6" string="short" />
            <token id="7" string="story" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="mark">
          <governor id="7">story</governor>
          <dependent id="1">If</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">story</governor>
          <dependent id="2">it</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="7">story</governor>
          <dependent id="3">had</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="7">story</governor>
          <dependent id="4">been</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">story</governor>
          <dependent id="5">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">story</governor>
          <dependent id="6">short</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="12">fine</governor>
          <dependent id="7">story</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">fine</governor>
          <dependent id="8">it</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="12">fine</governor>
          <dependent id="9">would</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="12">fine</governor>
          <dependent id="10">have</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="12">fine</governor>
          <dependent id="11">been</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="12">fine</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="11" has_coreference="true">
      <content>Mr Kelman&amp;apost;s book is, according to some observers, the most difficult work to have won the prize.</content>
      <tokens>
        <token id="1" string="Mr" lemma="Mr" stem="mr" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="2" string="Kelman" lemma="Kelman" stem="kelman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="according" lemma="accord" stem="accord" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="some" lemma="some" stem="some" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="observers" lemma="observer" stem="observ" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="most" lemma="most" stem="most" pos="RBS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="difficult" lemma="difficult" stem="difficult" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="work" lemma="work" stem="work" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="have" lemma="have" stem="have" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="won" lemma="win" stem="won" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="20" string="prize" lemma="prize" stem="prize" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="21" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Mr) (NNP Kelman) (POS 's)) (NN book)) (VP (VP (VBZ is)) (PRN (, ,) (PP (VBG according) (PP (TO to) (NP (DT some) (NNS observers)))) (, ,)) (NP (DT the) (ADJP (RBS most) (JJ difficult)) (NN work) (S (VP (TO to) (VP (VB have) (VP (VBN won) (NP (DT the) (NN prize)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="some observers" type="NP">
          <tokens>
            <token id="9" string="some" />
            <token id="10" string="observers" />
          </tokens>
        </chunking>
        <chunking id="2" string="the most difficult work to have won the prize" type="NP">
          <tokens>
            <token id="12" string="the" />
            <token id="13" string="most" />
            <token id="14" string="difficult" />
            <token id="15" string="work" />
            <token id="16" string="to" />
            <token id="17" string="have" />
            <token id="18" string="won" />
            <token id="19" string="the" />
            <token id="20" string="prize" />
          </tokens>
        </chunking>
        <chunking id="3" string="most difficult" type="ADJP">
          <tokens>
            <token id="13" string="most" />
            <token id="14" string="difficult" />
          </tokens>
        </chunking>
        <chunking id="4" string="Mr Kelman 's book" type="NP">
          <tokens>
            <token id="1" string="Mr" />
            <token id="2" string="Kelman" />
            <token id="3" string="'s" />
            <token id="4" string="book" />
          </tokens>
        </chunking>
        <chunking id="5" string="have won the prize" type="VP">
          <tokens>
            <token id="17" string="have" />
            <token id="18" string="won" />
            <token id="19" string="the" />
            <token id="20" string="prize" />
          </tokens>
        </chunking>
        <chunking id="6" string="won the prize" type="VP">
          <tokens>
            <token id="18" string="won" />
            <token id="19" string="the" />
            <token id="20" string="prize" />
          </tokens>
        </chunking>
        <chunking id="7" string="to have won the prize" type="VP">
          <tokens>
            <token id="16" string="to" />
            <token id="17" string="have" />
            <token id="18" string="won" />
            <token id="19" string="the" />
            <token id="20" string="prize" />
          </tokens>
        </chunking>
        <chunking id="8" string="is" type="VP">
          <tokens>
            <token id="5" string="is" />
          </tokens>
        </chunking>
        <chunking id="9" string="Mr Kelman 's" type="NP">
          <tokens>
            <token id="1" string="Mr" />
            <token id="2" string="Kelman" />
            <token id="3" string="'s" />
          </tokens>
        </chunking>
        <chunking id="10" string="is , according to some observers , the most difficult work to have won the prize" type="VP">
          <tokens>
            <token id="5" string="is" />
            <token id="6" string="," />
            <token id="7" string="according" />
            <token id="8" string="to" />
            <token id="9" string="some" />
            <token id="10" string="observers" />
            <token id="11" string="," />
            <token id="12" string="the" />
            <token id="13" string="most" />
            <token id="14" string="difficult" />
            <token id="15" string="work" />
            <token id="16" string="to" />
            <token id="17" string="have" />
            <token id="18" string="won" />
            <token id="19" string="the" />
            <token id="20" string="prize" />
          </tokens>
        </chunking>
        <chunking id="11" string="the prize" type="NP">
          <tokens>
            <token id="19" string="the" />
            <token id="20" string="prize" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">Kelman</governor>
          <dependent id="1">Mr</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="4">book</governor>
          <dependent id="2">Kelman</dependent>
        </dependency>
        <dependency type="case">
          <governor id="2">Kelman</governor>
          <dependent id="3">'s</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">is</governor>
          <dependent id="4">book</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">is</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">observers</governor>
          <dependent id="7">according</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="7">according</governor>
          <dependent id="8">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">observers</governor>
          <dependent id="9">some</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="5">is</governor>
          <dependent id="10">observers</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">work</governor>
          <dependent id="12">the</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="14">difficult</governor>
          <dependent id="13">most</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">work</governor>
          <dependent id="14">difficult</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">is</governor>
          <dependent id="15">work</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="18">won</governor>
          <dependent id="16">to</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="18">won</governor>
          <dependent id="17">have</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="15">work</governor>
          <dependent id="18">won</dependent>
        </dependency>
        <dependency type="det">
          <governor id="20">prize</governor>
          <dependent id="19">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="18">won</governor>
          <dependent id="20">prize</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Kelman" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Kelman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="12" has_coreference="true">
      <content>In his acceptance speech, Mr Kelman, perceived by some as a Scottish isolationist, anticipated the criticism, by declaring: &amp;apost;My culture and my language have the right to exist.&amp;apost;</content>
      <tokens>
        <token id="1" string="In" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="acceptance" lemma="acceptance" stem="accept" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="speech" lemma="speech" stem="speech" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Mr" lemma="Mr" stem="mr" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="Kelman" lemma="Kelman" stem="kelman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="perceived" lemma="perceive" stem="perceiv" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="10" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="some" lemma="some" stem="some" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="Scottish" lemma="scottish" stem="scottish" pos="JJ" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="true" />
        <token id="15" string="isolationist" lemma="isolationist" stem="isolationist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="anticipated" lemma="anticipate" stem="anticip" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="criticism" lemma="criticism" stem="critic" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="declaring" lemma="declare" stem="declar" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="'" lemma="`" stem="'" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="My" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="culture" lemma="culture" stem="cultur" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="my" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="language" lemma="language" stem="languag" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="have" lemma="have" stem="have" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="right" lemma="right" stem="right" pos="NN" type="Word" isStopWord="false" ner="IDEOLOGY" is_referenced="false" is_refers="false" />
        <token id="33" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="exist" lemma="exist" stem="exist" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (IN In) (NP (PRP$ his) (NN acceptance) (NN speech))) (, ,) (NP (NP (NNP Mr) (NNP Kelman)) (, ,) (VP (VBN perceived) (PP (IN by) (NP (NP (DT some)) (PP (IN as) (NP (DT a) (JJ Scottish) (NN isolationist)))))) (, ,)) (VP (VBD anticipated) (NP (DT the) (NN criticism)) (, ,) (PP (IN by) (S (VP (VBG declaring) (: :) (`` `) (S (NP (NP (PRP$ My) (NN culture)) (CC and) (NP (PRP$ my) (NN language))) (VP (VBP have) (NP (DT the) (NN right) (S (VP (TO to) (VP (VB exist))))))))))) (. .) ('' ')))</syntactictree>
      <chunkings>
        <chunking id="1" string="anticipated the criticism , by declaring : ` My culture and my language have the right to exist" type="VP">
          <tokens>
            <token id="17" string="anticipated" />
            <token id="18" string="the" />
            <token id="19" string="criticism" />
            <token id="20" string="," />
            <token id="21" string="by" />
            <token id="22" string="declaring" />
            <token id="23" string=":" />
            <token id="24" string="'" />
            <token id="25" string="My" />
            <token id="26" string="culture" />
            <token id="27" string="and" />
            <token id="28" string="my" />
            <token id="29" string="language" />
            <token id="30" string="have" />
            <token id="31" string="the" />
            <token id="32" string="right" />
            <token id="33" string="to" />
            <token id="34" string="exist" />
          </tokens>
        </chunking>
        <chunking id="2" string="Mr Kelman" type="NP">
          <tokens>
            <token id="6" string="Mr" />
            <token id="7" string="Kelman" />
          </tokens>
        </chunking>
        <chunking id="3" string="his acceptance speech" type="NP">
          <tokens>
            <token id="2" string="his" />
            <token id="3" string="acceptance" />
            <token id="4" string="speech" />
          </tokens>
        </chunking>
        <chunking id="4" string="some as a Scottish isolationist" type="NP">
          <tokens>
            <token id="11" string="some" />
            <token id="12" string="as" />
            <token id="13" string="a" />
            <token id="14" string="Scottish" />
            <token id="15" string="isolationist" />
          </tokens>
        </chunking>
        <chunking id="5" string="some" type="NP">
          <tokens>
            <token id="11" string="some" />
          </tokens>
        </chunking>
        <chunking id="6" string="a Scottish isolationist" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="Scottish" />
            <token id="15" string="isolationist" />
          </tokens>
        </chunking>
        <chunking id="7" string="declaring : ` My culture and my language have the right to exist" type="VP">
          <tokens>
            <token id="22" string="declaring" />
            <token id="23" string=":" />
            <token id="24" string="'" />
            <token id="25" string="My" />
            <token id="26" string="culture" />
            <token id="27" string="and" />
            <token id="28" string="my" />
            <token id="29" string="language" />
            <token id="30" string="have" />
            <token id="31" string="the" />
            <token id="32" string="right" />
            <token id="33" string="to" />
            <token id="34" string="exist" />
          </tokens>
        </chunking>
        <chunking id="8" string="the right to exist" type="NP">
          <tokens>
            <token id="31" string="the" />
            <token id="32" string="right" />
            <token id="33" string="to" />
            <token id="34" string="exist" />
          </tokens>
        </chunking>
        <chunking id="9" string="to exist" type="VP">
          <tokens>
            <token id="33" string="to" />
            <token id="34" string="exist" />
          </tokens>
        </chunking>
        <chunking id="10" string="Mr Kelman , perceived by some as a Scottish isolationist ," type="NP">
          <tokens>
            <token id="6" string="Mr" />
            <token id="7" string="Kelman" />
            <token id="8" string="," />
            <token id="9" string="perceived" />
            <token id="10" string="by" />
            <token id="11" string="some" />
            <token id="12" string="as" />
            <token id="13" string="a" />
            <token id="14" string="Scottish" />
            <token id="15" string="isolationist" />
            <token id="16" string="," />
          </tokens>
        </chunking>
        <chunking id="11" string="My culture and my language" type="NP">
          <tokens>
            <token id="25" string="My" />
            <token id="26" string="culture" />
            <token id="27" string="and" />
            <token id="28" string="my" />
            <token id="29" string="language" />
          </tokens>
        </chunking>
        <chunking id="12" string="exist" type="VP">
          <tokens>
            <token id="34" string="exist" />
          </tokens>
        </chunking>
        <chunking id="13" string="perceived by some as a Scottish isolationist" type="VP">
          <tokens>
            <token id="9" string="perceived" />
            <token id="10" string="by" />
            <token id="11" string="some" />
            <token id="12" string="as" />
            <token id="13" string="a" />
            <token id="14" string="Scottish" />
            <token id="15" string="isolationist" />
          </tokens>
        </chunking>
        <chunking id="14" string="My culture" type="NP">
          <tokens>
            <token id="25" string="My" />
            <token id="26" string="culture" />
          </tokens>
        </chunking>
        <chunking id="15" string="the criticism" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="criticism" />
          </tokens>
        </chunking>
        <chunking id="16" string="have the right to exist" type="VP">
          <tokens>
            <token id="30" string="have" />
            <token id="31" string="the" />
            <token id="32" string="right" />
            <token id="33" string="to" />
            <token id="34" string="exist" />
          </tokens>
        </chunking>
        <chunking id="17" string="my language" type="NP">
          <tokens>
            <token id="28" string="my" />
            <token id="29" string="language" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="case">
          <governor id="4">speech</governor>
          <dependent id="1">In</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="4">speech</governor>
          <dependent id="2">his</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">speech</governor>
          <dependent id="3">acceptance</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">anticipated</governor>
          <dependent id="4">speech</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">Kelman</governor>
          <dependent id="6">Mr</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="17">anticipated</governor>
          <dependent id="7">Kelman</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="7">Kelman</governor>
          <dependent id="9">perceived</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">some</governor>
          <dependent id="10">by</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">perceived</governor>
          <dependent id="11">some</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">isolationist</governor>
          <dependent id="12">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">isolationist</governor>
          <dependent id="13">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">isolationist</governor>
          <dependent id="14">Scottish</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">some</governor>
          <dependent id="15">isolationist</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="17">anticipated</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">criticism</governor>
          <dependent id="18">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="17">anticipated</governor>
          <dependent id="19">criticism</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="22">declaring</governor>
          <dependent id="21">by</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="17">anticipated</governor>
          <dependent id="22">declaring</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="26">culture</governor>
          <dependent id="25">My</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="30">have</governor>
          <dependent id="26">culture</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="26">culture</governor>
          <dependent id="27">and</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="29">language</governor>
          <dependent id="28">my</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="26">culture</governor>
          <dependent id="29">language</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="22">declaring</governor>
          <dependent id="30">have</dependent>
        </dependency>
        <dependency type="det">
          <governor id="32">right</governor>
          <dependent id="31">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="30">have</governor>
          <dependent id="32">right</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="34">exist</governor>
          <dependent id="33">to</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="32">right</governor>
          <dependent id="34">exist</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Scottish" type="MISC" score="0.0">
          <tokens>
            <token id="14" string="Scottish" />
          </tokens>
        </entity>
        <entity id="2" string="Kelman" type="PERSON" score="0.0">
          <tokens>
            <token id="7" string="Kelman" />
          </tokens>
        </entity>
        <entity id="3" string="right" type="IDEOLOGY" score="0.0">
          <tokens>
            <token id="32" string="right" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="13" has_coreference="true">
      <content>One guest at the prize-giving dinner at London&amp;apost;s Guildhall, Mr Salman Rushdie, who won the prize in 1981 with Midnight&amp;apost;s Children, remarked: &amp;apost;James Kelman is good, but not as good as he thinks he is.&amp;apost;</content>
      <tokens>
        <token id="1" string="One" lemma="one" stem="one" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="2" string="guest" lemma="guest" stem="guest" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="prize-giving" lemma="prize-giving" stem="prize-giv" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="dinner" lemma="dinner" stem="dinner" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="London" lemma="London" stem="london" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="9" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="Guildhall" lemma="Guildhall" stem="guildhal" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="Mr" lemma="Mr" stem="mr" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="Salman" lemma="Salman" stem="salman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="14" string="Rushdie" lemma="Rushdie" stem="rushdi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="15" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="won" lemma="win" stem="won" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="prize" lemma="prize" stem="prize" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="20" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="1981" lemma="1981" stem="1981" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="22" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="Midnight" lemma="midnight" stem="midnight" pos="NN" type="Word" isStopWord="false" ner="TIME" is_referenced="false" is_refers="false" />
        <token id="24" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="Children" lemma="Children" stem="children" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="remarked" lemma="remark" stem="remark" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="'" lemma="`" stem="'" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="James" lemma="James" stem="jame" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="31" string="Kelman" lemma="Kelman" stem="kelman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="32" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="good" lemma="good" stem="good" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="as" lemma="as" stem="a" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="good" lemma="good" stem="good" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="40" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="41" string="thinks" lemma="think" stem="think" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="42" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="43" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="44" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="45" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (CD One) (NN guest)) (PP (IN at) (NP (NP (DT the) (JJ prize-giving) (NN dinner)) (PP (IN at) (NP (NP (NP (NNP London) (POS 's)) (NNP Guildhall)) (, ,) (NP (NNP Mr) (NNP Salman) (NNP Rushdie)))))) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBD won) (NP (DT the) (NN prize)) (PP (IN in) (NP (CD 1981))) (PP (IN with) (NP (NP (NN Midnight) (POS 's)) (NNP Children)))))) (, ,)) (VP (VBD remarked) (: :) (`` `) (S (NP (NNP James) (NNP Kelman)) (VP (VBZ is) (ADJP (ADJP (JJ good)) (, ,) (CC but) (ADJP (RB not) (RB as) (JJ good))) (SBAR (IN as) (S (NP (PRP he)) (VP (VBZ thinks) (SBAR (S (NP (PRP he)) (VP (VBZ is)))))))))) (. .) ('' ')))</syntactictree>
      <chunkings>
        <chunking id="1" string="is good , but not as good as he thinks he is" type="VP">
          <tokens>
            <token id="32" string="is" />
            <token id="33" string="good" />
            <token id="34" string="," />
            <token id="35" string="but" />
            <token id="36" string="not" />
            <token id="37" string="as" />
            <token id="38" string="good" />
            <token id="39" string="as" />
            <token id="40" string="he" />
            <token id="41" string="thinks" />
            <token id="42" string="he" />
            <token id="43" string="is" />
          </tokens>
        </chunking>
        <chunking id="2" string="is" type="VP">
          <tokens>
            <token id="43" string="is" />
          </tokens>
        </chunking>
        <chunking id="3" string="not as good" type="ADJP">
          <tokens>
            <token id="36" string="not" />
            <token id="37" string="as" />
            <token id="38" string="good" />
          </tokens>
        </chunking>
        <chunking id="4" string="London 's Guildhall , Mr Salman Rushdie" type="NP">
          <tokens>
            <token id="8" string="London" />
            <token id="9" string="'s" />
            <token id="10" string="Guildhall" />
            <token id="11" string="," />
            <token id="12" string="Mr" />
            <token id="13" string="Salman" />
            <token id="14" string="Rushdie" />
          </tokens>
        </chunking>
        <chunking id="5" string="the prize" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="prize" />
          </tokens>
        </chunking>
        <chunking id="6" string="thinks he is" type="VP">
          <tokens>
            <token id="41" string="thinks" />
            <token id="42" string="he" />
            <token id="43" string="is" />
          </tokens>
        </chunking>
        <chunking id="7" string="as he thinks he is" type="SBAR">
          <tokens>
            <token id="39" string="as" />
            <token id="40" string="he" />
            <token id="41" string="thinks" />
            <token id="42" string="he" />
            <token id="43" string="is" />
          </tokens>
        </chunking>
        <chunking id="8" string="won the prize in 1981 with Midnight 's Children" type="VP">
          <tokens>
            <token id="17" string="won" />
            <token id="18" string="the" />
            <token id="19" string="prize" />
            <token id="20" string="in" />
            <token id="21" string="1981" />
            <token id="22" string="with" />
            <token id="23" string="Midnight" />
            <token id="24" string="'s" />
            <token id="25" string="Children" />
          </tokens>
        </chunking>
        <chunking id="9" string="1981" type="NP">
          <tokens>
            <token id="21" string="1981" />
          </tokens>
        </chunking>
        <chunking id="10" string="the prize-giving dinner at London 's Guildhall , Mr Salman Rushdie" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="prize-giving" />
            <token id="6" string="dinner" />
            <token id="7" string="at" />
            <token id="8" string="London" />
            <token id="9" string="'s" />
            <token id="10" string="Guildhall" />
            <token id="11" string="," />
            <token id="12" string="Mr" />
            <token id="13" string="Salman" />
            <token id="14" string="Rushdie" />
          </tokens>
        </chunking>
        <chunking id="11" string="the prize-giving dinner" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="prize-giving" />
            <token id="6" string="dinner" />
          </tokens>
        </chunking>
        <chunking id="12" string="he" type="NP">
          <tokens>
            <token id="40" string="he" />
          </tokens>
        </chunking>
        <chunking id="13" string="good , but not as good" type="ADJP">
          <tokens>
            <token id="33" string="good" />
            <token id="34" string="," />
            <token id="35" string="but" />
            <token id="36" string="not" />
            <token id="37" string="as" />
            <token id="38" string="good" />
          </tokens>
        </chunking>
        <chunking id="14" string="London 's" type="NP">
          <tokens>
            <token id="8" string="London" />
            <token id="9" string="'s" />
          </tokens>
        </chunking>
        <chunking id="15" string="he is" type="SBAR">
          <tokens>
            <token id="42" string="he" />
            <token id="43" string="is" />
          </tokens>
        </chunking>
        <chunking id="16" string="good" type="ADJP">
          <tokens>
            <token id="33" string="good" />
          </tokens>
        </chunking>
        <chunking id="17" string="One guest" type="NP">
          <tokens>
            <token id="1" string="One" />
            <token id="2" string="guest" />
          </tokens>
        </chunking>
        <chunking id="18" string="James Kelman" type="NP">
          <tokens>
            <token id="30" string="James" />
            <token id="31" string="Kelman" />
          </tokens>
        </chunking>
        <chunking id="19" string="who won the prize in 1981 with Midnight 's Children" type="SBAR">
          <tokens>
            <token id="16" string="who" />
            <token id="17" string="won" />
            <token id="18" string="the" />
            <token id="19" string="prize" />
            <token id="20" string="in" />
            <token id="21" string="1981" />
            <token id="22" string="with" />
            <token id="23" string="Midnight" />
            <token id="24" string="'s" />
            <token id="25" string="Children" />
          </tokens>
        </chunking>
        <chunking id="20" string="London 's Guildhall" type="NP">
          <tokens>
            <token id="8" string="London" />
            <token id="9" string="'s" />
            <token id="10" string="Guildhall" />
          </tokens>
        </chunking>
        <chunking id="21" string="Mr Salman Rushdie" type="NP">
          <tokens>
            <token id="12" string="Mr" />
            <token id="13" string="Salman" />
            <token id="14" string="Rushdie" />
          </tokens>
        </chunking>
        <chunking id="22" string="Midnight 's Children" type="NP">
          <tokens>
            <token id="23" string="Midnight" />
            <token id="24" string="'s" />
            <token id="25" string="Children" />
          </tokens>
        </chunking>
        <chunking id="23" string="remarked : ` James Kelman is good , but not as good as he thinks he is" type="VP">
          <tokens>
            <token id="27" string="remarked" />
            <token id="28" string=":" />
            <token id="29" string="'" />
            <token id="30" string="James" />
            <token id="31" string="Kelman" />
            <token id="32" string="is" />
            <token id="33" string="good" />
            <token id="34" string="," />
            <token id="35" string="but" />
            <token id="36" string="not" />
            <token id="37" string="as" />
            <token id="38" string="good" />
            <token id="39" string="as" />
            <token id="40" string="he" />
            <token id="41" string="thinks" />
            <token id="42" string="he" />
            <token id="43" string="is" />
          </tokens>
        </chunking>
        <chunking id="24" string="Midnight 's" type="NP">
          <tokens>
            <token id="23" string="Midnight" />
            <token id="24" string="'s" />
          </tokens>
        </chunking>
        <chunking id="25" string="One guest at the prize-giving dinner at London 's Guildhall , Mr Salman Rushdie , who won the prize in 1981 with Midnight 's Children ," type="NP">
          <tokens>
            <token id="1" string="One" />
            <token id="2" string="guest" />
            <token id="3" string="at" />
            <token id="4" string="the" />
            <token id="5" string="prize-giving" />
            <token id="6" string="dinner" />
            <token id="7" string="at" />
            <token id="8" string="London" />
            <token id="9" string="'s" />
            <token id="10" string="Guildhall" />
            <token id="11" string="," />
            <token id="12" string="Mr" />
            <token id="13" string="Salman" />
            <token id="14" string="Rushdie" />
            <token id="15" string="," />
            <token id="16" string="who" />
            <token id="17" string="won" />
            <token id="18" string="the" />
            <token id="19" string="prize" />
            <token id="20" string="in" />
            <token id="21" string="1981" />
            <token id="22" string="with" />
            <token id="23" string="Midnight" />
            <token id="24" string="'s" />
            <token id="25" string="Children" />
            <token id="26" string="," />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nummod">
          <governor id="2">guest</governor>
          <dependent id="1">One</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="27">remarked</governor>
          <dependent id="2">guest</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">dinner</governor>
          <dependent id="3">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">dinner</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">dinner</governor>
          <dependent id="5">prize-giving</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">guest</governor>
          <dependent id="6">dinner</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">Guildhall</governor>
          <dependent id="7">at</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="10">Guildhall</governor>
          <dependent id="8">London</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">London</governor>
          <dependent id="9">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">dinner</governor>
          <dependent id="10">Guildhall</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">Rushdie</governor>
          <dependent id="12">Mr</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">Rushdie</governor>
          <dependent id="13">Salman</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="10">Guildhall</governor>
          <dependent id="14">Rushdie</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="17">won</governor>
          <dependent id="16">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="2">guest</governor>
          <dependent id="17">won</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">prize</governor>
          <dependent id="18">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="17">won</governor>
          <dependent id="19">prize</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">1981</governor>
          <dependent id="20">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">won</governor>
          <dependent id="21">1981</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">Children</governor>
          <dependent id="22">with</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="25">Children</governor>
          <dependent id="23">Midnight</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">Midnight</governor>
          <dependent id="24">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">won</governor>
          <dependent id="25">Children</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="27">remarked</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="31">Kelman</governor>
          <dependent id="30">James</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="33">good</governor>
          <dependent id="31">Kelman</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="33">good</governor>
          <dependent id="32">is</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="27">remarked</governor>
          <dependent id="33">good</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="33">good</governor>
          <dependent id="35">but</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="38">good</governor>
          <dependent id="36">not</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="38">good</governor>
          <dependent id="37">as</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="33">good</governor>
          <dependent id="38">good</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="41">thinks</governor>
          <dependent id="39">as</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="41">thinks</governor>
          <dependent id="40">he</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="33">good</governor>
          <dependent id="41">thinks</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="43">is</governor>
          <dependent id="42">he</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="41">thinks</governor>
          <dependent id="43">is</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="James Kelman" type="PERSON" score="0.0">
          <tokens>
            <token id="30" string="James" />
            <token id="31" string="Kelman" />
          </tokens>
        </entity>
        <entity id="2" string="Midnight" type="TIME" score="0.0">
          <tokens>
            <token id="23" string="Midnight" />
          </tokens>
        </entity>
        <entity id="3" string="One" type="NUMBER" score="0.0">
          <tokens>
            <token id="1" string="One" />
          </tokens>
        </entity>
        <entity id="4" string="London" type="LOCATION" score="0.0">
          <tokens>
            <token id="8" string="London" />
          </tokens>
        </entity>
        <entity id="5" string="1981" type="DATE" score="0.0">
          <tokens>
            <token id="21" string="1981" />
          </tokens>
        </entity>
        <entity id="6" string="Salman Rushdie" type="PERSON" score="0.0">
          <tokens>
            <token id="13" string="Salman" />
            <token id="14" string="Rushdie" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="14" has_coreference="true">
      <content>The Booker prize has, in its 26-year history, always provoked controversy.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="Booker" lemma="Booker" stem="booker" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="prize" lemma="prize" stem="prize" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="4" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="its" lemma="its" stem="it" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="26-year" lemma="26-year" stem="26-year" pos="JJ" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="9" string="history" lemma="history" stem="histori" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="always" lemma="always" stem="alwai" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="provoked" lemma="provoke" stem="provok" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="controversy" lemma="controversy" stem="controversi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NNP Booker) (NN prize)) (VP (VBZ has) (, ,) (PP (IN in) (NP (PRP$ its) (JJ 26-year) (NN history))) (, ,) (ADVP (RB always)) (VP (VBN provoked) (NP (NN controversy)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="provoked controversy" type="VP">
          <tokens>
            <token id="12" string="provoked" />
            <token id="13" string="controversy" />
          </tokens>
        </chunking>
        <chunking id="2" string="The Booker prize" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Booker" />
            <token id="3" string="prize" />
          </tokens>
        </chunking>
        <chunking id="3" string="its 26-year history" type="NP">
          <tokens>
            <token id="7" string="its" />
            <token id="8" string="26-year" />
            <token id="9" string="history" />
          </tokens>
        </chunking>
        <chunking id="4" string="has , in its 26-year history , always provoked controversy" type="VP">
          <tokens>
            <token id="4" string="has" />
            <token id="5" string="," />
            <token id="6" string="in" />
            <token id="7" string="its" />
            <token id="8" string="26-year" />
            <token id="9" string="history" />
            <token id="10" string="," />
            <token id="11" string="always" />
            <token id="12" string="provoked" />
            <token id="13" string="controversy" />
          </tokens>
        </chunking>
        <chunking id="5" string="controversy" type="NP">
          <tokens>
            <token id="13" string="controversy" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">prize</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">prize</governor>
          <dependent id="2">Booker</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">provoked</governor>
          <dependent id="3">prize</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="12">provoked</governor>
          <dependent id="4">has</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">history</governor>
          <dependent id="6">in</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="9">history</governor>
          <dependent id="7">its</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">history</governor>
          <dependent id="8">26-year</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">provoked</governor>
          <dependent id="9">history</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="12">provoked</governor>
          <dependent id="11">always</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="12">provoked</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="12">provoked</governor>
          <dependent id="13">controversy</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="26-year" type="DURATION" score="0.0">
          <tokens>
            <token id="8" string="26-year" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="15" has_coreference="false">
      <content>But this year&amp;apost;s rows have been of a particularly fine vintage.</content>
      <tokens>
        <token id="1" string="But" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="year" lemma="year" stem="year" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="rows" lemma="row" stem="row" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="have" lemma="have" stem="have" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="particularly" lemma="particularly" stem="particularli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="fine" lemma="fine" stem="fine" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="vintage" lemma="vintage" stem="vintag" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (CC But) (NP (NP (DT this) (NN year) (POS 's)) (NNS rows)) (VP (VBP have) (VP (VBN been) (PP (IN of) (NP (DT a) (ADJP (RB particularly) (JJ fine)) (NN vintage))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="this year 's rows" type="NP">
          <tokens>
            <token id="2" string="this" />
            <token id="3" string="year" />
            <token id="4" string="'s" />
            <token id="5" string="rows" />
          </tokens>
        </chunking>
        <chunking id="2" string="been of a particularly fine vintage" type="VP">
          <tokens>
            <token id="7" string="been" />
            <token id="8" string="of" />
            <token id="9" string="a" />
            <token id="10" string="particularly" />
            <token id="11" string="fine" />
            <token id="12" string="vintage" />
          </tokens>
        </chunking>
        <chunking id="3" string="particularly fine" type="ADJP">
          <tokens>
            <token id="10" string="particularly" />
            <token id="11" string="fine" />
          </tokens>
        </chunking>
        <chunking id="4" string="have been of a particularly fine vintage" type="VP">
          <tokens>
            <token id="6" string="have" />
            <token id="7" string="been" />
            <token id="8" string="of" />
            <token id="9" string="a" />
            <token id="10" string="particularly" />
            <token id="11" string="fine" />
            <token id="12" string="vintage" />
          </tokens>
        </chunking>
        <chunking id="5" string="a particularly fine vintage" type="NP">
          <tokens>
            <token id="9" string="a" />
            <token id="10" string="particularly" />
            <token id="11" string="fine" />
            <token id="12" string="vintage" />
          </tokens>
        </chunking>
        <chunking id="6" string="this year 's" type="NP">
          <tokens>
            <token id="2" string="this" />
            <token id="3" string="year" />
            <token id="4" string="'s" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="cc">
          <governor id="12">vintage</governor>
          <dependent id="1">But</dependent>
        </dependency>
        <dependency type="det">
          <governor id="3">year</governor>
          <dependent id="2">this</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="5">rows</governor>
          <dependent id="3">year</dependent>
        </dependency>
        <dependency type="case">
          <governor id="3">year</governor>
          <dependent id="4">'s</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">vintage</governor>
          <dependent id="5">rows</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="12">vintage</governor>
          <dependent id="6">have</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="12">vintage</governor>
          <dependent id="7">been</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">vintage</governor>
          <dependent id="8">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">vintage</governor>
          <dependent id="9">a</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">fine</governor>
          <dependent id="10">particularly</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">vintage</governor>
          <dependent id="11">fine</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="12">vintage</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="this year" type="DATE" score="0.0">
          <tokens>
            <token id="2" string="this" />
            <token id="3" string="year" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="16" has_coreference="false">
      <content>Prof Bayley went on record with a rousing condemnation of contemporary fiction.</content>
      <tokens>
        <token id="1" string="Prof" lemma="Prof" stem="prof" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="Bayley" lemma="Bayley" stem="baylei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="3" string="went" lemma="go" stem="went" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="record" lemma="record" stem="record" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="rousing" lemma="rousing" stem="rous" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="condemnation" lemma="condemnation" stem="condemn" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="contemporary" lemma="contemporary" stem="contemporari" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="fiction" lemma="fiction" stem="fiction" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Prof) (NNP Bayley)) (VP (VBD went) (PP (IN on) (NP (NN record))) (PP (IN with) (NP (NP (DT a) (JJ rousing) (NN condemnation)) (PP (IN of) (NP (JJ contemporary) (NN fiction)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a rousing condemnation of contemporary fiction" type="NP">
          <tokens>
            <token id="7" string="a" />
            <token id="8" string="rousing" />
            <token id="9" string="condemnation" />
            <token id="10" string="of" />
            <token id="11" string="contemporary" />
            <token id="12" string="fiction" />
          </tokens>
        </chunking>
        <chunking id="2" string="went on record with a rousing condemnation of contemporary fiction" type="VP">
          <tokens>
            <token id="3" string="went" />
            <token id="4" string="on" />
            <token id="5" string="record" />
            <token id="6" string="with" />
            <token id="7" string="a" />
            <token id="8" string="rousing" />
            <token id="9" string="condemnation" />
            <token id="10" string="of" />
            <token id="11" string="contemporary" />
            <token id="12" string="fiction" />
          </tokens>
        </chunking>
        <chunking id="3" string="contemporary fiction" type="NP">
          <tokens>
            <token id="11" string="contemporary" />
            <token id="12" string="fiction" />
          </tokens>
        </chunking>
        <chunking id="4" string="record" type="NP">
          <tokens>
            <token id="5" string="record" />
          </tokens>
        </chunking>
        <chunking id="5" string="Prof Bayley" type="NP">
          <tokens>
            <token id="1" string="Prof" />
            <token id="2" string="Bayley" />
          </tokens>
        </chunking>
        <chunking id="6" string="a rousing condemnation" type="NP">
          <tokens>
            <token id="7" string="a" />
            <token id="8" string="rousing" />
            <token id="9" string="condemnation" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">Bayley</governor>
          <dependent id="1">Prof</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">went</governor>
          <dependent id="2">Bayley</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">went</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">record</governor>
          <dependent id="4">on</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">went</governor>
          <dependent id="5">record</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">condemnation</governor>
          <dependent id="6">with</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">condemnation</governor>
          <dependent id="7">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">condemnation</governor>
          <dependent id="8">rousing</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">went</governor>
          <dependent id="9">condemnation</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">fiction</governor>
          <dependent id="10">of</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">fiction</governor>
          <dependent id="11">contemporary</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">condemnation</governor>
          <dependent id="12">fiction</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Bayley" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Bayley" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="17" has_coreference="false">
      <content>Another judge, Mr James Wood, literary critic of The Guardian, was attacked for failing to reveal that a young novelist being considered for the award, Ms Claire Messud, was in fact Mrs James Wood.</content>
      <tokens>
        <token id="1" string="Another" lemma="another" stem="another" pos="DT" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="judge" lemma="judge" stem="judg" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="Mr" lemma="Mr" stem="mr" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="James" lemma="James" stem="jame" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="6" string="Wood" lemma="Wood" stem="wood" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="literary" lemma="literary" stem="literari" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="critic" lemma="critic" stem="critic" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="Guardian" lemma="Guardian" stem="guardian" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="attacked" lemma="attack" stem="attack" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="failing" lemma="fail" stem="fail" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="reveal" lemma="reveal" stem="reveal" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="young" lemma="young" stem="young" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="novelist" lemma="novelist" stem="novelist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="being" lemma="be" stem="be" pos="VBG" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="considered" lemma="consider" stem="consid" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="award" lemma="award" stem="award" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="Ms" lemma="Ms" stem="m" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="Claire" lemma="Claire" stem="clair" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="32" string="Messud" lemma="Messud" stem="messud" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="33" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="fact" lemma="fact" stem="fact" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="37" string="Mrs" lemma="Mrs" stem="mr" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="38" string="James" lemma="James" stem="jame" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="39" string="Wood" lemma="Wood" stem="wood" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="40" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT Another) (NN judge)) (, ,) (NP (NNP Mr) (NNP James) (NNP Wood)) (, ,) (NP (NP (JJ literary) (NN critic)) (PP (IN of) (NP (DT The) (NNP Guardian)))) (, ,)) (VP (VBD was) (VP (VBN attacked) (PP (IN for) (S (VP (VBG failing) (S (VP (TO to) (VP (VB reveal) (SBAR (IN that) (S (NP (NP (DT a) (JJ young) (NN novelist)) (VP (VBG being) (VP (VBN considered) (PP (IN for) (NP (NP (DT the) (NN award)) (, ,) (NP (NNP Ms) (NNP Claire) (NNP Messud)) (, ,)))))) (VP (VBD was) (PP (IN in) (NP (NN fact) (NNP Mrs) (NNP James) (NNP Wood)))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Mr James Wood" type="NP">
          <tokens>
            <token id="4" string="Mr" />
            <token id="5" string="James" />
            <token id="6" string="Wood" />
          </tokens>
        </chunking>
        <chunking id="2" string="reveal that a young novelist being considered for the award , Ms Claire Messud , was in fact Mrs James Wood" type="VP">
          <tokens>
            <token id="19" string="reveal" />
            <token id="20" string="that" />
            <token id="21" string="a" />
            <token id="22" string="young" />
            <token id="23" string="novelist" />
            <token id="24" string="being" />
            <token id="25" string="considered" />
            <token id="26" string="for" />
            <token id="27" string="the" />
            <token id="28" string="award" />
            <token id="29" string="," />
            <token id="30" string="Ms" />
            <token id="31" string="Claire" />
            <token id="32" string="Messud" />
            <token id="33" string="," />
            <token id="34" string="was" />
            <token id="35" string="in" />
            <token id="36" string="fact" />
            <token id="37" string="Mrs" />
            <token id="38" string="James" />
            <token id="39" string="Wood" />
          </tokens>
        </chunking>
        <chunking id="3" string="considered for the award , Ms Claire Messud ," type="VP">
          <tokens>
            <token id="25" string="considered" />
            <token id="26" string="for" />
            <token id="27" string="the" />
            <token id="28" string="award" />
            <token id="29" string="," />
            <token id="30" string="Ms" />
            <token id="31" string="Claire" />
            <token id="32" string="Messud" />
            <token id="33" string="," />
          </tokens>
        </chunking>
        <chunking id="4" string="Another judge" type="NP">
          <tokens>
            <token id="1" string="Another" />
            <token id="2" string="judge" />
          </tokens>
        </chunking>
        <chunking id="5" string="Ms Claire Messud" type="NP">
          <tokens>
            <token id="30" string="Ms" />
            <token id="31" string="Claire" />
            <token id="32" string="Messud" />
          </tokens>
        </chunking>
        <chunking id="6" string="was in fact Mrs James Wood" type="VP">
          <tokens>
            <token id="34" string="was" />
            <token id="35" string="in" />
            <token id="36" string="fact" />
            <token id="37" string="Mrs" />
            <token id="38" string="James" />
            <token id="39" string="Wood" />
          </tokens>
        </chunking>
        <chunking id="7" string="a young novelist" type="NP">
          <tokens>
            <token id="21" string="a" />
            <token id="22" string="young" />
            <token id="23" string="novelist" />
          </tokens>
        </chunking>
        <chunking id="8" string="was attacked for failing to reveal that a young novelist being considered for the award , Ms Claire Messud , was in fact Mrs James Wood" type="VP">
          <tokens>
            <token id="14" string="was" />
            <token id="15" string="attacked" />
            <token id="16" string="for" />
            <token id="17" string="failing" />
            <token id="18" string="to" />
            <token id="19" string="reveal" />
            <token id="20" string="that" />
            <token id="21" string="a" />
            <token id="22" string="young" />
            <token id="23" string="novelist" />
            <token id="24" string="being" />
            <token id="25" string="considered" />
            <token id="26" string="for" />
            <token id="27" string="the" />
            <token id="28" string="award" />
            <token id="29" string="," />
            <token id="30" string="Ms" />
            <token id="31" string="Claire" />
            <token id="32" string="Messud" />
            <token id="33" string="," />
            <token id="34" string="was" />
            <token id="35" string="in" />
            <token id="36" string="fact" />
            <token id="37" string="Mrs" />
            <token id="38" string="James" />
            <token id="39" string="Wood" />
          </tokens>
        </chunking>
        <chunking id="9" string="that a young novelist being considered for the award , Ms Claire Messud , was in fact Mrs James Wood" type="SBAR">
          <tokens>
            <token id="20" string="that" />
            <token id="21" string="a" />
            <token id="22" string="young" />
            <token id="23" string="novelist" />
            <token id="24" string="being" />
            <token id="25" string="considered" />
            <token id="26" string="for" />
            <token id="27" string="the" />
            <token id="28" string="award" />
            <token id="29" string="," />
            <token id="30" string="Ms" />
            <token id="31" string="Claire" />
            <token id="32" string="Messud" />
            <token id="33" string="," />
            <token id="34" string="was" />
            <token id="35" string="in" />
            <token id="36" string="fact" />
            <token id="37" string="Mrs" />
            <token id="38" string="James" />
            <token id="39" string="Wood" />
          </tokens>
        </chunking>
        <chunking id="10" string="Another judge , Mr James Wood , literary critic of The Guardian ," type="NP">
          <tokens>
            <token id="1" string="Another" />
            <token id="2" string="judge" />
            <token id="3" string="," />
            <token id="4" string="Mr" />
            <token id="5" string="James" />
            <token id="6" string="Wood" />
            <token id="7" string="," />
            <token id="8" string="literary" />
            <token id="9" string="critic" />
            <token id="10" string="of" />
            <token id="11" string="The" />
            <token id="12" string="Guardian" />
            <token id="13" string="," />
          </tokens>
        </chunking>
        <chunking id="11" string="literary critic" type="NP">
          <tokens>
            <token id="8" string="literary" />
            <token id="9" string="critic" />
          </tokens>
        </chunking>
        <chunking id="12" string="a young novelist being considered for the award , Ms Claire Messud ," type="NP">
          <tokens>
            <token id="21" string="a" />
            <token id="22" string="young" />
            <token id="23" string="novelist" />
            <token id="24" string="being" />
            <token id="25" string="considered" />
            <token id="26" string="for" />
            <token id="27" string="the" />
            <token id="28" string="award" />
            <token id="29" string="," />
            <token id="30" string="Ms" />
            <token id="31" string="Claire" />
            <token id="32" string="Messud" />
            <token id="33" string="," />
          </tokens>
        </chunking>
        <chunking id="13" string="fact Mrs James Wood" type="NP">
          <tokens>
            <token id="36" string="fact" />
            <token id="37" string="Mrs" />
            <token id="38" string="James" />
            <token id="39" string="Wood" />
          </tokens>
        </chunking>
        <chunking id="14" string="failing to reveal that a young novelist being considered for the award , Ms Claire Messud , was in fact Mrs James Wood" type="VP">
          <tokens>
            <token id="17" string="failing" />
            <token id="18" string="to" />
            <token id="19" string="reveal" />
            <token id="20" string="that" />
            <token id="21" string="a" />
            <token id="22" string="young" />
            <token id="23" string="novelist" />
            <token id="24" string="being" />
            <token id="25" string="considered" />
            <token id="26" string="for" />
            <token id="27" string="the" />
            <token id="28" string="award" />
            <token id="29" string="," />
            <token id="30" string="Ms" />
            <token id="31" string="Claire" />
            <token id="32" string="Messud" />
            <token id="33" string="," />
            <token id="34" string="was" />
            <token id="35" string="in" />
            <token id="36" string="fact" />
            <token id="37" string="Mrs" />
            <token id="38" string="James" />
            <token id="39" string="Wood" />
          </tokens>
        </chunking>
        <chunking id="15" string="the award" type="NP">
          <tokens>
            <token id="27" string="the" />
            <token id="28" string="award" />
          </tokens>
        </chunking>
        <chunking id="16" string="the award , Ms Claire Messud ," type="NP">
          <tokens>
            <token id="27" string="the" />
            <token id="28" string="award" />
            <token id="29" string="," />
            <token id="30" string="Ms" />
            <token id="31" string="Claire" />
            <token id="32" string="Messud" />
            <token id="33" string="," />
          </tokens>
        </chunking>
        <chunking id="17" string="to reveal that a young novelist being considered for the award , Ms Claire Messud , was in fact Mrs James Wood" type="VP">
          <tokens>
            <token id="18" string="to" />
            <token id="19" string="reveal" />
            <token id="20" string="that" />
            <token id="21" string="a" />
            <token id="22" string="young" />
            <token id="23" string="novelist" />
            <token id="24" string="being" />
            <token id="25" string="considered" />
            <token id="26" string="for" />
            <token id="27" string="the" />
            <token id="28" string="award" />
            <token id="29" string="," />
            <token id="30" string="Ms" />
            <token id="31" string="Claire" />
            <token id="32" string="Messud" />
            <token id="33" string="," />
            <token id="34" string="was" />
            <token id="35" string="in" />
            <token id="36" string="fact" />
            <token id="37" string="Mrs" />
            <token id="38" string="James" />
            <token id="39" string="Wood" />
          </tokens>
        </chunking>
        <chunking id="18" string="attacked for failing to reveal that a young novelist being considered for the award , Ms Claire Messud , was in fact Mrs James Wood" type="VP">
          <tokens>
            <token id="15" string="attacked" />
            <token id="16" string="for" />
            <token id="17" string="failing" />
            <token id="18" string="to" />
            <token id="19" string="reveal" />
            <token id="20" string="that" />
            <token id="21" string="a" />
            <token id="22" string="young" />
            <token id="23" string="novelist" />
            <token id="24" string="being" />
            <token id="25" string="considered" />
            <token id="26" string="for" />
            <token id="27" string="the" />
            <token id="28" string="award" />
            <token id="29" string="," />
            <token id="30" string="Ms" />
            <token id="31" string="Claire" />
            <token id="32" string="Messud" />
            <token id="33" string="," />
            <token id="34" string="was" />
            <token id="35" string="in" />
            <token id="36" string="fact" />
            <token id="37" string="Mrs" />
            <token id="38" string="James" />
            <token id="39" string="Wood" />
          </tokens>
        </chunking>
        <chunking id="19" string="being considered for the award , Ms Claire Messud ," type="VP">
          <tokens>
            <token id="24" string="being" />
            <token id="25" string="considered" />
            <token id="26" string="for" />
            <token id="27" string="the" />
            <token id="28" string="award" />
            <token id="29" string="," />
            <token id="30" string="Ms" />
            <token id="31" string="Claire" />
            <token id="32" string="Messud" />
            <token id="33" string="," />
          </tokens>
        </chunking>
        <chunking id="20" string="The Guardian" type="NP">
          <tokens>
            <token id="11" string="The" />
            <token id="12" string="Guardian" />
          </tokens>
        </chunking>
        <chunking id="21" string="literary critic of The Guardian" type="NP">
          <tokens>
            <token id="8" string="literary" />
            <token id="9" string="critic" />
            <token id="10" string="of" />
            <token id="11" string="The" />
            <token id="12" string="Guardian" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">judge</governor>
          <dependent id="1">Another</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="15">attacked</governor>
          <dependent id="2">judge</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">Wood</governor>
          <dependent id="4">Mr</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">Wood</governor>
          <dependent id="5">James</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="2">judge</governor>
          <dependent id="6">Wood</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">critic</governor>
          <dependent id="8">literary</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="2">judge</governor>
          <dependent id="9">critic</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">Guardian</governor>
          <dependent id="10">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">Guardian</governor>
          <dependent id="11">The</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">critic</governor>
          <dependent id="12">Guardian</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="15">attacked</governor>
          <dependent id="14">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="15">attacked</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="17">failing</governor>
          <dependent id="16">for</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="15">attacked</governor>
          <dependent id="17">failing</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="19">reveal</governor>
          <dependent id="18">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="17">failing</governor>
          <dependent id="19">reveal</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="39">Wood</governor>
          <dependent id="20">that</dependent>
        </dependency>
        <dependency type="det">
          <governor id="23">novelist</governor>
          <dependent id="21">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="23">novelist</governor>
          <dependent id="22">young</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="39">Wood</governor>
          <dependent id="23">novelist</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="25">considered</governor>
          <dependent id="24">being</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="23">novelist</governor>
          <dependent id="25">considered</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">award</governor>
          <dependent id="26">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="28">award</governor>
          <dependent id="27">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="25">considered</governor>
          <dependent id="28">award</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="32">Messud</governor>
          <dependent id="30">Ms</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="32">Messud</governor>
          <dependent id="31">Claire</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="28">award</governor>
          <dependent id="32">Messud</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="39">Wood</governor>
          <dependent id="34">was</dependent>
        </dependency>
        <dependency type="case">
          <governor id="39">Wood</governor>
          <dependent id="35">in</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="39">Wood</governor>
          <dependent id="36">fact</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="39">Wood</governor>
          <dependent id="37">Mrs</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="39">Wood</governor>
          <dependent id="38">James</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="19">reveal</governor>
          <dependent id="39">Wood</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Claire Messud" type="PERSON" score="0.0">
          <tokens>
            <token id="31" string="Claire" />
            <token id="32" string="Messud" />
          </tokens>
        </entity>
        <entity id="2" string="James Wood" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="James" />
            <token id="6" string="Wood" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="18" has_coreference="true">
      <content>Most damaging of all was the verdict of the booksellers, who dismissed the shortlist as narcoleptic.</content>
      <tokens>
        <token id="1" string="Most" lemma="most" stem="most" pos="JJS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="damaging" lemma="damaging" stem="damag" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="4" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="verdict" lemma="verdict" stem="verdict" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="booksellers" lemma="bookseller" stem="booksel" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="dismissed" lemma="dismiss" stem="dismiss" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="shortlist" lemma="shortlist" stem="shortlist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="narcoleptic" lemma="narcoleptic" stem="narcolept" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (JJS Most) (NN damaging)) (PP (IN of) (NP (DT all)))) (VP (VBD was) (NP (NP (DT the) (NN verdict)) (PP (IN of) (NP (NP (DT the) (NNS booksellers)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBD dismissed) (NP (DT the) (NN shortlist)) (PP (IN as) (ADJP (JJ narcoleptic)))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="all" type="NP">
          <tokens>
            <token id="4" string="all" />
          </tokens>
        </chunking>
        <chunking id="2" string="Most damaging" type="NP">
          <tokens>
            <token id="1" string="Most" />
            <token id="2" string="damaging" />
          </tokens>
        </chunking>
        <chunking id="3" string="the verdict" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="verdict" />
          </tokens>
        </chunking>
        <chunking id="4" string="the verdict of the booksellers , who dismissed the shortlist as narcoleptic" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="verdict" />
            <token id="8" string="of" />
            <token id="9" string="the" />
            <token id="10" string="booksellers" />
            <token id="11" string="," />
            <token id="12" string="who" />
            <token id="13" string="dismissed" />
            <token id="14" string="the" />
            <token id="15" string="shortlist" />
            <token id="16" string="as" />
            <token id="17" string="narcoleptic" />
          </tokens>
        </chunking>
        <chunking id="5" string="the booksellers , who dismissed the shortlist as narcoleptic" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="booksellers" />
            <token id="11" string="," />
            <token id="12" string="who" />
            <token id="13" string="dismissed" />
            <token id="14" string="the" />
            <token id="15" string="shortlist" />
            <token id="16" string="as" />
            <token id="17" string="narcoleptic" />
          </tokens>
        </chunking>
        <chunking id="6" string="the shortlist" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="shortlist" />
          </tokens>
        </chunking>
        <chunking id="7" string="was the verdict of the booksellers , who dismissed the shortlist as narcoleptic" type="VP">
          <tokens>
            <token id="5" string="was" />
            <token id="6" string="the" />
            <token id="7" string="verdict" />
            <token id="8" string="of" />
            <token id="9" string="the" />
            <token id="10" string="booksellers" />
            <token id="11" string="," />
            <token id="12" string="who" />
            <token id="13" string="dismissed" />
            <token id="14" string="the" />
            <token id="15" string="shortlist" />
            <token id="16" string="as" />
            <token id="17" string="narcoleptic" />
          </tokens>
        </chunking>
        <chunking id="8" string="narcoleptic" type="ADJP">
          <tokens>
            <token id="17" string="narcoleptic" />
          </tokens>
        </chunking>
        <chunking id="9" string="Most damaging of all" type="NP">
          <tokens>
            <token id="1" string="Most" />
            <token id="2" string="damaging" />
            <token id="3" string="of" />
            <token id="4" string="all" />
          </tokens>
        </chunking>
        <chunking id="10" string="who dismissed the shortlist as narcoleptic" type="SBAR">
          <tokens>
            <token id="12" string="who" />
            <token id="13" string="dismissed" />
            <token id="14" string="the" />
            <token id="15" string="shortlist" />
            <token id="16" string="as" />
            <token id="17" string="narcoleptic" />
          </tokens>
        </chunking>
        <chunking id="11" string="dismissed the shortlist as narcoleptic" type="VP">
          <tokens>
            <token id="13" string="dismissed" />
            <token id="14" string="the" />
            <token id="15" string="shortlist" />
            <token id="16" string="as" />
            <token id="17" string="narcoleptic" />
          </tokens>
        </chunking>
        <chunking id="12" string="the booksellers" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="booksellers" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="amod">
          <governor id="2">damaging</governor>
          <dependent id="1">Most</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">verdict</governor>
          <dependent id="2">damaging</dependent>
        </dependency>
        <dependency type="case">
          <governor id="4">all</governor>
          <dependent id="3">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">damaging</governor>
          <dependent id="4">all</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="7">verdict</governor>
          <dependent id="5">was</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">verdict</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">verdict</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">booksellers</governor>
          <dependent id="8">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">booksellers</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">verdict</governor>
          <dependent id="10">booksellers</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="13">dismissed</governor>
          <dependent id="12">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="10">booksellers</governor>
          <dependent id="13">dismissed</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">shortlist</governor>
          <dependent id="14">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="13">dismissed</governor>
          <dependent id="15">shortlist</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">narcoleptic</governor>
          <dependent id="16">as</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="13">dismissed</governor>
          <dependent id="17">narcoleptic</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="19" has_coreference="false">
      <content>The other contenders were The Folding Star by Alan Hollinghurst, Beside the Ocean of Time by George Mackay Brown, Paradise by Abdulrazak Gurnah, Reef by Romesh Gunesekera and Knowledge of Angels by Jill Paton Walsh.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="other" lemma="other" stem="other" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="contenders" lemma="contender" stem="contend" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Folding" lemma="folding" stem="fold" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="Star" lemma="star" stem="star" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="Alan" lemma="Alan" stem="alan" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="10" string="Hollinghurst" lemma="Hollinghurst" stem="hollinghurst" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="Beside" lemma="beside" stem="besid" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="Ocean" lemma="ocean" stem="ocean" pos="NN" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="15" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="16" string="Time" lemma="Time" stem="time" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="17" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="George" lemma="George" stem="georg" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="19" string="Mackay" lemma="Mackay" stem="mackai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="20" string="Brown" lemma="Brown" stem="brown" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="21" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="Paradise" lemma="Paradise" stem="paradis" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="Abdulrazak" lemma="Abdulrazak" stem="abdulrazak" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="25" string="Gurnah" lemma="Gurnah" stem="gurnah" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="26" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="Reef" lemma="reef" stem="reef" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="Romesh" lemma="Romesh" stem="romesh" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="30" string="Gunesekera" lemma="Gunesekera" stem="gunesekera" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="31" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="Knowledge" lemma="Knowledge" stem="knowledg" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="33" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="34" string="Angels" lemma="Angels" stem="angel" pos="NNPS" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="35" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="Jill" lemma="Jill" stem="jill" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="37" string="Paton" lemma="Paton" stem="paton" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="38" string="Walsh" lemma="Walsh" stem="walsh" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="39" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (JJ other) (NNS contenders)) (VP (VBD were) (NP (NP (DT The) (JJ Folding) (NN Star)) (PP (IN by) (NP (NP (NP (NNP Alan) (NNP Hollinghurst) (PRN (, ,) (PP (IN Beside) (NP (NP (DT the) (NN Ocean)) (PP (IN of) (NP (NNP Time))))) (PP (IN by) (NP (NNP George) (NNP Mackay) (NNP Brown))) (, ,)) (NNP Paradise)) (PP (IN by) (NP (NNP Abdulrazak) (NNP Gurnah)))) (, ,) (NP (NP (NN Reef)) (PP (IN by) (NP (NNP Romesh) (NNP Gunesekera)))) (CC and) (NP (NP (NNP Knowledge)) (PP (IN of) (NP (NNPS Angels))))))) (PP (IN by) (NP (NNP Jill) (NNP Paton) (NNP Walsh)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Abdulrazak Gurnah" type="NP">
          <tokens>
            <token id="24" string="Abdulrazak" />
            <token id="25" string="Gurnah" />
          </tokens>
        </chunking>
        <chunking id="2" string="George Mackay Brown" type="NP">
          <tokens>
            <token id="18" string="George" />
            <token id="19" string="Mackay" />
            <token id="20" string="Brown" />
          </tokens>
        </chunking>
        <chunking id="3" string="The Folding Star by Alan Hollinghurst , Beside the Ocean of Time by George Mackay Brown , Paradise by Abdulrazak Gurnah , Reef by Romesh Gunesekera and Knowledge of Angels" type="NP">
          <tokens>
            <token id="5" string="The" />
            <token id="6" string="Folding" />
            <token id="7" string="Star" />
            <token id="8" string="by" />
            <token id="9" string="Alan" />
            <token id="10" string="Hollinghurst" />
            <token id="11" string="," />
            <token id="12" string="Beside" />
            <token id="13" string="the" />
            <token id="14" string="Ocean" />
            <token id="15" string="of" />
            <token id="16" string="Time" />
            <token id="17" string="by" />
            <token id="18" string="George" />
            <token id="19" string="Mackay" />
            <token id="20" string="Brown" />
            <token id="21" string="," />
            <token id="22" string="Paradise" />
            <token id="23" string="by" />
            <token id="24" string="Abdulrazak" />
            <token id="25" string="Gurnah" />
            <token id="26" string="," />
            <token id="27" string="Reef" />
            <token id="28" string="by" />
            <token id="29" string="Romesh" />
            <token id="30" string="Gunesekera" />
            <token id="31" string="and" />
            <token id="32" string="Knowledge" />
            <token id="33" string="of" />
            <token id="34" string="Angels" />
          </tokens>
        </chunking>
        <chunking id="4" string="the Ocean of Time" type="NP">
          <tokens>
            <token id="13" string="the" />
            <token id="14" string="Ocean" />
            <token id="15" string="of" />
            <token id="16" string="Time" />
          </tokens>
        </chunking>
        <chunking id="5" string="Time" type="NP">
          <tokens>
            <token id="16" string="Time" />
          </tokens>
        </chunking>
        <chunking id="6" string="Reef by Romesh Gunesekera" type="NP">
          <tokens>
            <token id="27" string="Reef" />
            <token id="28" string="by" />
            <token id="29" string="Romesh" />
            <token id="30" string="Gunesekera" />
          </tokens>
        </chunking>
        <chunking id="7" string="Alan Hollinghurst , Beside the Ocean of Time by George Mackay Brown , Paradise" type="NP">
          <tokens>
            <token id="9" string="Alan" />
            <token id="10" string="Hollinghurst" />
            <token id="11" string="," />
            <token id="12" string="Beside" />
            <token id="13" string="the" />
            <token id="14" string="Ocean" />
            <token id="15" string="of" />
            <token id="16" string="Time" />
            <token id="17" string="by" />
            <token id="18" string="George" />
            <token id="19" string="Mackay" />
            <token id="20" string="Brown" />
            <token id="21" string="," />
            <token id="22" string="Paradise" />
          </tokens>
        </chunking>
        <chunking id="8" string="Alan Hollinghurst , Beside the Ocean of Time by George Mackay Brown , Paradise by Abdulrazak Gurnah" type="NP">
          <tokens>
            <token id="9" string="Alan" />
            <token id="10" string="Hollinghurst" />
            <token id="11" string="," />
            <token id="12" string="Beside" />
            <token id="13" string="the" />
            <token id="14" string="Ocean" />
            <token id="15" string="of" />
            <token id="16" string="Time" />
            <token id="17" string="by" />
            <token id="18" string="George" />
            <token id="19" string="Mackay" />
            <token id="20" string="Brown" />
            <token id="21" string="," />
            <token id="22" string="Paradise" />
            <token id="23" string="by" />
            <token id="24" string="Abdulrazak" />
            <token id="25" string="Gurnah" />
          </tokens>
        </chunking>
        <chunking id="9" string="Knowledge" type="NP">
          <tokens>
            <token id="32" string="Knowledge" />
          </tokens>
        </chunking>
        <chunking id="10" string="Jill Paton Walsh" type="NP">
          <tokens>
            <token id="36" string="Jill" />
            <token id="37" string="Paton" />
            <token id="38" string="Walsh" />
          </tokens>
        </chunking>
        <chunking id="11" string="were The Folding Star by Alan Hollinghurst , Beside the Ocean of Time by George Mackay Brown , Paradise by Abdulrazak Gurnah , Reef by Romesh Gunesekera and Knowledge of Angels by Jill Paton Walsh" type="VP">
          <tokens>
            <token id="4" string="were" />
            <token id="5" string="The" />
            <token id="6" string="Folding" />
            <token id="7" string="Star" />
            <token id="8" string="by" />
            <token id="9" string="Alan" />
            <token id="10" string="Hollinghurst" />
            <token id="11" string="," />
            <token id="12" string="Beside" />
            <token id="13" string="the" />
            <token id="14" string="Ocean" />
            <token id="15" string="of" />
            <token id="16" string="Time" />
            <token id="17" string="by" />
            <token id="18" string="George" />
            <token id="19" string="Mackay" />
            <token id="20" string="Brown" />
            <token id="21" string="," />
            <token id="22" string="Paradise" />
            <token id="23" string="by" />
            <token id="24" string="Abdulrazak" />
            <token id="25" string="Gurnah" />
            <token id="26" string="," />
            <token id="27" string="Reef" />
            <token id="28" string="by" />
            <token id="29" string="Romesh" />
            <token id="30" string="Gunesekera" />
            <token id="31" string="and" />
            <token id="32" string="Knowledge" />
            <token id="33" string="of" />
            <token id="34" string="Angels" />
            <token id="35" string="by" />
            <token id="36" string="Jill" />
            <token id="37" string="Paton" />
            <token id="38" string="Walsh" />
          </tokens>
        </chunking>
        <chunking id="12" string="The Folding Star" type="NP">
          <tokens>
            <token id="5" string="The" />
            <token id="6" string="Folding" />
            <token id="7" string="Star" />
          </tokens>
        </chunking>
        <chunking id="13" string="Reef" type="NP">
          <tokens>
            <token id="27" string="Reef" />
          </tokens>
        </chunking>
        <chunking id="14" string="Romesh Gunesekera" type="NP">
          <tokens>
            <token id="29" string="Romesh" />
            <token id="30" string="Gunesekera" />
          </tokens>
        </chunking>
        <chunking id="15" string="Angels" type="NP">
          <tokens>
            <token id="34" string="Angels" />
          </tokens>
        </chunking>
        <chunking id="16" string="The other contenders" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="other" />
            <token id="3" string="contenders" />
          </tokens>
        </chunking>
        <chunking id="17" string="Alan Hollinghurst , Beside the Ocean of Time by George Mackay Brown , Paradise by Abdulrazak Gurnah , Reef by Romesh Gunesekera and Knowledge of Angels" type="NP">
          <tokens>
            <token id="9" string="Alan" />
            <token id="10" string="Hollinghurst" />
            <token id="11" string="," />
            <token id="12" string="Beside" />
            <token id="13" string="the" />
            <token id="14" string="Ocean" />
            <token id="15" string="of" />
            <token id="16" string="Time" />
            <token id="17" string="by" />
            <token id="18" string="George" />
            <token id="19" string="Mackay" />
            <token id="20" string="Brown" />
            <token id="21" string="," />
            <token id="22" string="Paradise" />
            <token id="23" string="by" />
            <token id="24" string="Abdulrazak" />
            <token id="25" string="Gurnah" />
            <token id="26" string="," />
            <token id="27" string="Reef" />
            <token id="28" string="by" />
            <token id="29" string="Romesh" />
            <token id="30" string="Gunesekera" />
            <token id="31" string="and" />
            <token id="32" string="Knowledge" />
            <token id="33" string="of" />
            <token id="34" string="Angels" />
          </tokens>
        </chunking>
        <chunking id="18" string="the Ocean" type="NP">
          <tokens>
            <token id="13" string="the" />
            <token id="14" string="Ocean" />
          </tokens>
        </chunking>
        <chunking id="19" string="Knowledge of Angels" type="NP">
          <tokens>
            <token id="32" string="Knowledge" />
            <token id="33" string="of" />
            <token id="34" string="Angels" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">contenders</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">contenders</governor>
          <dependent id="2">other</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">Star</governor>
          <dependent id="3">contenders</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="7">Star</governor>
          <dependent id="4">were</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">Star</governor>
          <dependent id="5">The</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">Star</governor>
          <dependent id="6">Folding</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">Star</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">Paradise</governor>
          <dependent id="8">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="22">Paradise</governor>
          <dependent id="9">Alan</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="22">Paradise</governor>
          <dependent id="10">Hollinghurst</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">Ocean</governor>
          <dependent id="12">Beside</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">Ocean</governor>
          <dependent id="13">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="22">Paradise</governor>
          <dependent id="14">Ocean</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">Time</governor>
          <dependent id="15">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">Ocean</governor>
          <dependent id="16">Time</dependent>
        </dependency>
        <dependency type="case">
          <governor id="20">Brown</governor>
          <dependent id="17">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="20">Brown</governor>
          <dependent id="18">George</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="20">Brown</governor>
          <dependent id="19">Mackay</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">Ocean</governor>
          <dependent id="20">Brown</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">Star</governor>
          <dependent id="22">Paradise</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">Gurnah</governor>
          <dependent id="23">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">Gurnah</governor>
          <dependent id="24">Abdulrazak</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="22">Paradise</governor>
          <dependent id="25">Gurnah</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="22">Paradise</governor>
          <dependent id="27">Reef</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">Gunesekera</governor>
          <dependent id="28">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="30">Gunesekera</governor>
          <dependent id="29">Romesh</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">Reef</governor>
          <dependent id="30">Gunesekera</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="22">Paradise</governor>
          <dependent id="31">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="22">Paradise</governor>
          <dependent id="32">Knowledge</dependent>
        </dependency>
        <dependency type="case">
          <governor id="34">Angels</governor>
          <dependent id="33">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="32">Knowledge</governor>
          <dependent id="34">Angels</dependent>
        </dependency>
        <dependency type="case">
          <governor id="38">Walsh</governor>
          <dependent id="35">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="38">Walsh</governor>
          <dependent id="36">Jill</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="38">Walsh</governor>
          <dependent id="37">Paton</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">Star</governor>
          <dependent id="38">Walsh</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Alan Hollinghurst" type="PERSON" score="0.0">
          <tokens>
            <token id="9" string="Alan" />
            <token id="10" string="Hollinghurst" />
          </tokens>
        </entity>
        <entity id="2" string="Jill Paton Walsh" type="PERSON" score="0.0">
          <tokens>
            <token id="36" string="Jill" />
            <token id="37" string="Paton" />
            <token id="38" string="Walsh" />
          </tokens>
        </entity>
        <entity id="3" string="Abdulrazak Gurnah" type="PERSON" score="0.0">
          <tokens>
            <token id="24" string="Abdulrazak" />
            <token id="25" string="Gurnah" />
          </tokens>
        </entity>
        <entity id="4" string="George Mackay Brown" type="PERSON" score="0.0">
          <tokens>
            <token id="18" string="George" />
            <token id="19" string="Mackay" />
            <token id="20" string="Brown" />
          </tokens>
        </entity>
        <entity id="5" string="Romesh Gunesekera" type="PERSON" score="0.0">
          <tokens>
            <token id="29" string="Romesh" />
            <token id="30" string="Gunesekera" />
          </tokens>
        </entity>
        <entity id="6" string="Ocean of Time" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="14" string="Ocean" />
            <token id="15" string="of" />
            <token id="16" string="Time" />
          </tokens>
        </entity>
        <entity id="7" string="Knowledge of Angels" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="32" string="Knowledge" />
            <token id="33" string="of" />
            <token id="34" string="Angels" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="20" has_coreference="false">
      <content>In spite of heavy publicity, the novels nominated have been selling 50 per cent less than last year&amp;apost;s shortlist.</content>
      <tokens>
        <token id="1" string="In" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="spite" lemma="spite" stem="spite" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="heavy" lemma="heavy" stem="heavi" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="publicity" lemma="publicity" stem="public" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="novels" lemma="novel" stem="novel" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="nominated" lemma="nominate" stem="nomin" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="have" lemma="have" stem="have" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="selling" lemma="sell" stem="sell" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="50" lemma="50" stem="50" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="14" string="per" lemma="per" stem="per" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="cent" lemma="cent" stem="cent" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="less" lemma="less" stem="less" pos="RBR" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="than" lemma="than" stem="than" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="last" lemma="last" stem="last" pos="JJ" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="19" string="year" lemma="year" stem="year" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="20" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="21" string="shortlist" lemma="shortlist" stem="shortlist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (IN In) (NP (NP (NN spite)) (PP (IN of) (NP (JJ heavy) (NN publicity))))) (, ,) (NP (NP (DT the) (NNS novels)) (VP (VBN nominated))) (VP (VBP have) (VP (VBN been) (VP (VBG selling) (NP (NP (CD 50)) (PP (IN per) (NP (NN cent)))) (ADVP (RBR less)) (PP (IN than) (NP (NP-TMP (JJ last) (NN year) (POS 's)) (NN shortlist)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the novels nominated" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="novels" />
            <token id="9" string="nominated" />
          </tokens>
        </chunking>
        <chunking id="2" string="selling 50 per cent less than last year 's shortlist" type="VP">
          <tokens>
            <token id="12" string="selling" />
            <token id="13" string="50" />
            <token id="14" string="per" />
            <token id="15" string="cent" />
            <token id="16" string="less" />
            <token id="17" string="than" />
            <token id="18" string="last" />
            <token id="19" string="year" />
            <token id="20" string="'s" />
            <token id="21" string="shortlist" />
          </tokens>
        </chunking>
        <chunking id="3" string="cent" type="NP">
          <tokens>
            <token id="15" string="cent" />
          </tokens>
        </chunking>
        <chunking id="4" string="50 per cent" type="NP">
          <tokens>
            <token id="13" string="50" />
            <token id="14" string="per" />
            <token id="15" string="cent" />
          </tokens>
        </chunking>
        <chunking id="5" string="have been selling 50 per cent less than last year 's shortlist" type="VP">
          <tokens>
            <token id="10" string="have" />
            <token id="11" string="been" />
            <token id="12" string="selling" />
            <token id="13" string="50" />
            <token id="14" string="per" />
            <token id="15" string="cent" />
            <token id="16" string="less" />
            <token id="17" string="than" />
            <token id="18" string="last" />
            <token id="19" string="year" />
            <token id="20" string="'s" />
            <token id="21" string="shortlist" />
          </tokens>
        </chunking>
        <chunking id="6" string="been selling 50 per cent less than last year 's shortlist" type="VP">
          <tokens>
            <token id="11" string="been" />
            <token id="12" string="selling" />
            <token id="13" string="50" />
            <token id="14" string="per" />
            <token id="15" string="cent" />
            <token id="16" string="less" />
            <token id="17" string="than" />
            <token id="18" string="last" />
            <token id="19" string="year" />
            <token id="20" string="'s" />
            <token id="21" string="shortlist" />
          </tokens>
        </chunking>
        <chunking id="7" string="spite of heavy publicity" type="NP">
          <tokens>
            <token id="2" string="spite" />
            <token id="3" string="of" />
            <token id="4" string="heavy" />
            <token id="5" string="publicity" />
          </tokens>
        </chunking>
        <chunking id="8" string="spite" type="NP">
          <tokens>
            <token id="2" string="spite" />
          </tokens>
        </chunking>
        <chunking id="9" string="nominated" type="VP">
          <tokens>
            <token id="9" string="nominated" />
          </tokens>
        </chunking>
        <chunking id="10" string="last year 's shortlist" type="NP">
          <tokens>
            <token id="18" string="last" />
            <token id="19" string="year" />
            <token id="20" string="'s" />
            <token id="21" string="shortlist" />
          </tokens>
        </chunking>
        <chunking id="11" string="50" type="NP">
          <tokens>
            <token id="13" string="50" />
          </tokens>
        </chunking>
        <chunking id="12" string="heavy publicity" type="NP">
          <tokens>
            <token id="4" string="heavy" />
            <token id="5" string="publicity" />
          </tokens>
        </chunking>
        <chunking id="13" string="the novels" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="novels" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="case">
          <governor id="5">publicity</governor>
          <dependent id="1">In</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="1">In</governor>
          <dependent id="2">spite</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="1">In</governor>
          <dependent id="3">of</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">publicity</governor>
          <dependent id="4">heavy</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">selling</governor>
          <dependent id="5">publicity</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">novels</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">selling</governor>
          <dependent id="8">novels</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="8">novels</governor>
          <dependent id="9">nominated</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="12">selling</governor>
          <dependent id="10">have</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="12">selling</governor>
          <dependent id="11">been</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="12">selling</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="12">selling</governor>
          <dependent id="13">50</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">cent</governor>
          <dependent id="14">per</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">50</governor>
          <dependent id="15">cent</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="12">selling</governor>
          <dependent id="16">less</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">shortlist</governor>
          <dependent id="17">than</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">year</governor>
          <dependent id="18">last</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="21">shortlist</governor>
          <dependent id="19">year</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">year</governor>
          <dependent id="20">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">selling</governor>
          <dependent id="21">shortlist</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="50" type="NUMBER" score="0.0">
          <tokens>
            <token id="13" string="50" />
          </tokens>
        </entity>
        <entity id="2" string="last year" type="DATE" score="0.0">
          <tokens>
            <token id="18" string="last" />
            <token id="19" string="year" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="21" has_coreference="false">
      <content>Last year&amp;apost;s winner, Roddy Doyle&amp;apost;s Paddy Clarke Ha Ha Ha, sold 350,000 copies.</content>
      <tokens>
        <token id="1" string="Last" lemma="last" stem="last" pos="JJ" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="year" lemma="year" stem="year" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="winner" lemma="winner" stem="winner" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Roddy" lemma="Roddy" stem="roddi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="7" string="Doyle" lemma="Doyle" stem="doyl" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="8" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="Paddy" lemma="Paddy" stem="paddi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="10" string="Clarke" lemma="Clarke" stem="clark" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="11" string="Ha" lemma="Ha" stem="ha" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="12" string="Ha" lemma="Ha" stem="ha" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="13" string="Ha" lemma="Ha" stem="ha" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="sold" lemma="sell" stem="sold" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="350,000" lemma="350,000" stem="350,000" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="17" string="copies" lemma="copy" stem="copi" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NP (JJ Last) (NN year) (POS 's)) (NN winner)) (, ,) (NP (NP (NNP Roddy) (NNP Doyle) (POS 's)) (NNP Paddy) (NNP Clarke) (NNP Ha) (NNP Ha) (NNP Ha)) (, ,)) (VP (VBD sold) (NP (CD 350,000) (NNS copies))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Last year 's" type="NP">
          <tokens>
            <token id="1" string="Last" />
            <token id="2" string="year" />
            <token id="3" string="'s" />
          </tokens>
        </chunking>
        <chunking id="2" string="Roddy Doyle 's Paddy Clarke Ha Ha Ha" type="NP">
          <tokens>
            <token id="6" string="Roddy" />
            <token id="7" string="Doyle" />
            <token id="8" string="'s" />
            <token id="9" string="Paddy" />
            <token id="10" string="Clarke" />
            <token id="11" string="Ha" />
            <token id="12" string="Ha" />
            <token id="13" string="Ha" />
          </tokens>
        </chunking>
        <chunking id="3" string="350,000 copies" type="NP">
          <tokens>
            <token id="16" string="350,000" />
            <token id="17" string="copies" />
          </tokens>
        </chunking>
        <chunking id="4" string="Last year 's winner , Roddy Doyle 's Paddy Clarke Ha Ha Ha ," type="NP">
          <tokens>
            <token id="1" string="Last" />
            <token id="2" string="year" />
            <token id="3" string="'s" />
            <token id="4" string="winner" />
            <token id="5" string="," />
            <token id="6" string="Roddy" />
            <token id="7" string="Doyle" />
            <token id="8" string="'s" />
            <token id="9" string="Paddy" />
            <token id="10" string="Clarke" />
            <token id="11" string="Ha" />
            <token id="12" string="Ha" />
            <token id="13" string="Ha" />
            <token id="14" string="," />
          </tokens>
        </chunking>
        <chunking id="5" string="sold 350,000 copies" type="VP">
          <tokens>
            <token id="15" string="sold" />
            <token id="16" string="350,000" />
            <token id="17" string="copies" />
          </tokens>
        </chunking>
        <chunking id="6" string="Last year 's winner" type="NP">
          <tokens>
            <token id="1" string="Last" />
            <token id="2" string="year" />
            <token id="3" string="'s" />
            <token id="4" string="winner" />
          </tokens>
        </chunking>
        <chunking id="7" string="Roddy Doyle 's" type="NP">
          <tokens>
            <token id="6" string="Roddy" />
            <token id="7" string="Doyle" />
            <token id="8" string="'s" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="amod">
          <governor id="2">year</governor>
          <dependent id="1">Last</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="4">winner</governor>
          <dependent id="2">year</dependent>
        </dependency>
        <dependency type="case">
          <governor id="2">year</governor>
          <dependent id="3">'s</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">sold</governor>
          <dependent id="4">winner</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">Doyle</governor>
          <dependent id="6">Roddy</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="13">Ha</governor>
          <dependent id="7">Doyle</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">Doyle</governor>
          <dependent id="8">'s</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="13">Ha</governor>
          <dependent id="9">Paddy</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="13">Ha</governor>
          <dependent id="10">Clarke</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="13">Ha</governor>
          <dependent id="11">Ha</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="13">Ha</governor>
          <dependent id="12">Ha</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="4">winner</governor>
          <dependent id="13">Ha</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="15">sold</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="17">copies</governor>
          <dependent id="16">350,000</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">sold</governor>
          <dependent id="17">copies</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="350,000" type="NUMBER" score="0.0">
          <tokens>
            <token id="16" string="350,000" />
          </tokens>
        </entity>
        <entity id="2" string="Paddy Clarke Ha Ha Ha" type="PERSON" score="0.0">
          <tokens>
            <token id="9" string="Paddy" />
            <token id="10" string="Clarke" />
            <token id="11" string="Ha" />
            <token id="12" string="Ha" />
            <token id="13" string="Ha" />
          </tokens>
        </entity>
        <entity id="3" string="Last year" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Last" />
            <token id="2" string="year" />
          </tokens>
        </entity>
        <entity id="4" string="Roddy Doyle" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Roddy" />
            <token id="7" string="Doyle" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="22" has_coreference="true">
      <content>&amp;apost;Kelman&amp;apost;s novel is unlikely to come anywhere near that,&amp;apost; one bookseller said.</content>
      <tokens>
        <token id="1" string="'" lemma="`" stem="'" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="Kelman" lemma="Kelman" stem="kelman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="3" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="4" string="novel" lemma="novel" stem="novel" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="unlikely" lemma="unlikely" stem="unlik" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="come" lemma="come" stem="come" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="anywhere" lemma="anywhere" stem="anywher" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="near" lemma="near" stem="near" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="that" lemma="that" stem="that" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="one" lemma="one" stem="on" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="15" string="bookseller" lemma="bookseller" stem="booksel" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` `) (S (NP (NP (NNP Kelman) (POS 's)) (NN novel)) (VP (VBZ is) (ADJP (JJ unlikely) (S (VP (TO to) (VP (VB come) (ADVP (RB anywhere)) (PP (IN near) (NP (DT that))))))))) (, ,) ('' ') (NP (CD one) (NN bookseller)) (VP (VBD said)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="is unlikely to come anywhere near that" type="VP">
          <tokens>
            <token id="5" string="is" />
            <token id="6" string="unlikely" />
            <token id="7" string="to" />
            <token id="8" string="come" />
            <token id="9" string="anywhere" />
            <token id="10" string="near" />
            <token id="11" string="that" />
          </tokens>
        </chunking>
        <chunking id="2" string="that" type="NP">
          <tokens>
            <token id="11" string="that" />
          </tokens>
        </chunking>
        <chunking id="3" string="Kelman 's novel" type="NP">
          <tokens>
            <token id="2" string="Kelman" />
            <token id="3" string="'s" />
            <token id="4" string="novel" />
          </tokens>
        </chunking>
        <chunking id="4" string="come anywhere near that" type="VP">
          <tokens>
            <token id="8" string="come" />
            <token id="9" string="anywhere" />
            <token id="10" string="near" />
            <token id="11" string="that" />
          </tokens>
        </chunking>
        <chunking id="5" string="one bookseller" type="NP">
          <tokens>
            <token id="14" string="one" />
            <token id="15" string="bookseller" />
          </tokens>
        </chunking>
        <chunking id="6" string="unlikely to come anywhere near that" type="ADJP">
          <tokens>
            <token id="6" string="unlikely" />
            <token id="7" string="to" />
            <token id="8" string="come" />
            <token id="9" string="anywhere" />
            <token id="10" string="near" />
            <token id="11" string="that" />
          </tokens>
        </chunking>
        <chunking id="7" string="said" type="VP">
          <tokens>
            <token id="16" string="said" />
          </tokens>
        </chunking>
        <chunking id="8" string="Kelman 's" type="NP">
          <tokens>
            <token id="2" string="Kelman" />
            <token id="3" string="'s" />
          </tokens>
        </chunking>
        <chunking id="9" string="to come anywhere near that" type="VP">
          <tokens>
            <token id="7" string="to" />
            <token id="8" string="come" />
            <token id="9" string="anywhere" />
            <token id="10" string="near" />
            <token id="11" string="that" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="4">novel</governor>
          <dependent id="2">Kelman</dependent>
        </dependency>
        <dependency type="case">
          <governor id="2">Kelman</governor>
          <dependent id="3">'s</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">unlikely</governor>
          <dependent id="4">novel</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="6">unlikely</governor>
          <dependent id="5">is</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="16">said</governor>
          <dependent id="6">unlikely</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="8">come</governor>
          <dependent id="7">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="6">unlikely</governor>
          <dependent id="8">come</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="8">come</governor>
          <dependent id="9">anywhere</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">that</governor>
          <dependent id="10">near</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">come</governor>
          <dependent id="11">that</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="15">bookseller</governor>
          <dependent id="14">one</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">said</governor>
          <dependent id="15">bookseller</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="16">said</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="one" type="NUMBER" score="0.0">
          <tokens>
            <token id="14" string="one" />
          </tokens>
        </entity>
        <entity id="2" string="Kelman" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Kelman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="23" has_coreference="true">
      <content>Mr Kelman was also shortlisted for the prize in 1989 for his novel A Disaffection.</content>
      <tokens>
        <token id="1" string="Mr" lemma="Mr" stem="mr" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="2" string="Kelman" lemma="Kelman" stem="kelman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="shortlisted" lemma="shortlist" stem="shortlist" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="prize" lemma="prize" stem="prize" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="1989" lemma="1989" stem="1989" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="11" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="novel" lemma="novel" stem="novel" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="A" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="Disaffection" lemma="disaffection" stem="disaffect" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Mr) (NNP Kelman)) (VP (VBD was) (ADVP (RB also)) (VP (VBN shortlisted) (PP (IN for) (NP (NP (DT the) (NN prize)) (PP (IN in) (NP (CD 1989))))) (PP (IN for) (NP (NP (PRP$ his) (NN novel)) (NP (DT A) (NN Disaffection)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="shortlisted for the prize in 1989 for his novel A Disaffection" type="VP">
          <tokens>
            <token id="5" string="shortlisted" />
            <token id="6" string="for" />
            <token id="7" string="the" />
            <token id="8" string="prize" />
            <token id="9" string="in" />
            <token id="10" string="1989" />
            <token id="11" string="for" />
            <token id="12" string="his" />
            <token id="13" string="novel" />
            <token id="14" string="A" />
            <token id="15" string="Disaffection" />
          </tokens>
        </chunking>
        <chunking id="2" string="Mr Kelman" type="NP">
          <tokens>
            <token id="1" string="Mr" />
            <token id="2" string="Kelman" />
          </tokens>
        </chunking>
        <chunking id="3" string="A Disaffection" type="NP">
          <tokens>
            <token id="14" string="A" />
            <token id="15" string="Disaffection" />
          </tokens>
        </chunking>
        <chunking id="4" string="the prize in 1989" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="prize" />
            <token id="9" string="in" />
            <token id="10" string="1989" />
          </tokens>
        </chunking>
        <chunking id="5" string="his novel A Disaffection" type="NP">
          <tokens>
            <token id="12" string="his" />
            <token id="13" string="novel" />
            <token id="14" string="A" />
            <token id="15" string="Disaffection" />
          </tokens>
        </chunking>
        <chunking id="6" string="was also shortlisted for the prize in 1989 for his novel A Disaffection" type="VP">
          <tokens>
            <token id="3" string="was" />
            <token id="4" string="also" />
            <token id="5" string="shortlisted" />
            <token id="6" string="for" />
            <token id="7" string="the" />
            <token id="8" string="prize" />
            <token id="9" string="in" />
            <token id="10" string="1989" />
            <token id="11" string="for" />
            <token id="12" string="his" />
            <token id="13" string="novel" />
            <token id="14" string="A" />
            <token id="15" string="Disaffection" />
          </tokens>
        </chunking>
        <chunking id="7" string="the prize" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="prize" />
          </tokens>
        </chunking>
        <chunking id="8" string="1989" type="NP">
          <tokens>
            <token id="10" string="1989" />
          </tokens>
        </chunking>
        <chunking id="9" string="his novel" type="NP">
          <tokens>
            <token id="12" string="his" />
            <token id="13" string="novel" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">Kelman</governor>
          <dependent id="1">Mr</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="5">shortlisted</governor>
          <dependent id="2">Kelman</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="5">shortlisted</governor>
          <dependent id="3">was</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">shortlisted</governor>
          <dependent id="4">also</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">shortlisted</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">prize</governor>
          <dependent id="6">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">prize</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">shortlisted</governor>
          <dependent id="8">prize</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">1989</governor>
          <dependent id="9">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">prize</governor>
          <dependent id="10">1989</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">novel</governor>
          <dependent id="11">for</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="13">novel</governor>
          <dependent id="12">his</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">shortlisted</governor>
          <dependent id="13">novel</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">Disaffection</governor>
          <dependent id="14">A</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="13">novel</governor>
          <dependent id="15">Disaffection</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Kelman" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Kelman" />
          </tokens>
        </entity>
        <entity id="2" string="1989" type="DATE" score="0.0">
          <tokens>
            <token id="10" string="1989" />
          </tokens>
        </entity>
      </entities>
    </sentence>
  </sentences>
  <coreferences>
    <coreference id="2" type="PROPER">
      <referenced ids_tokens="22-23-24-25" string="Booker Prize for fiction" id_sentence="1" />
      <mentions>
        <mention ids_tokens="33-34" string="the prize" id_sentence="2" />
        <mention ids_tokens="19-20" string="the prize" id_sentence="11" />
        <mention ids_tokens="18-19" string="the prize" id_sentence="13" />
        <mention ids_tokens="1-3" string="The Booker prize" id_sentence="14" />
        <mention ids_tokens="7" string="its" id_sentence="14" />
      </mentions>
    </coreference>
    <coreference id="3" type="PROPER">
      <referenced ids_tokens="30-31-32-33-34-35-36-37-38-39-40-41-42-43-44-45-46" string="Mr James Kelman for How Late It Was , How Late , published by Secker and Warburg" id_sentence="1" />
      <mentions>
        <mention ids_tokens="13-15" string="Mr Kelman's" id_sentence="3" />
        <mention ids_tokens="1" string="Kelman" id_sentence="9" />
        <mention ids_tokens="1-3" string="Mr Kelman's" id_sentence="11" />
        <mention ids_tokens="2" string="his" id_sentence="12" />
        <mention ids_tokens="6-15" string="Mr Kelman , perceived by some as a Scottish isolationist" id_sentence="12" />
        <mention ids_tokens="6-7" string="Mr Kelman" id_sentence="12" />
        <mention ids_tokens="7" string="Kelman" id_sentence="12" />
        <mention ids_tokens="30-31" string="James Kelman" id_sentence="13" />
        <mention ids_tokens="40" string="he" id_sentence="13" />
        <mention ids_tokens="42" string="he" id_sentence="13" />
        <mention ids_tokens="2-3" string="Kelman's" id_sentence="22" />
        <mention ids_tokens="1-2" string="Mr Kelman" id_sentence="23" />
        <mention ids_tokens="12" string="his" id_sentence="23" />
      </mentions>
    </coreference>
    <coreference id="5" type="NOMINAL">
      <referenced ids_tokens="9-10" string="the booksellers" id_sentence="18" />
      <mentions>
        <mention ids_tokens="23" string="booksellers" id_sentence="2" />
      </mentions>
    </coreference>
    <coreference id="6" type="PROPER">
      <referenced ids_tokens="2-3" string="John Bayley" id_sentence="3" />
      <mentions>
        <mention ids_tokens="15" string="he" id_sentence="4" />
      </mentions>
    </coreference>
    <coreference id="7" type="NOMINAL">
      <referenced ids_tokens="13-14-15-16-17-18" string="Mr Kelman 's book in particular" id_sentence="3" />
      <mentions>
        <mention ids_tokens="1-4" string="Mr Kelman's book" id_sentence="11" />
      </mentions>
    </coreference>
    <coreference id="8" type="PROPER">
      <referenced ids_tokens="2-3" string="Julia Neuberger" id_sentence="5" />
      <mentions>
        <mention ids_tokens="13" string="she" id_sentence="6" />
      </mentions>
    </coreference>
    <coreference id="9" type="NOMINAL">
      <referenced ids_tokens="5-6-7-8-9-10" string="a drunken Scotsman railing against bureaucracy" id_sentence="6" />
      <mentions>
        <mention ids_tokens="9-11" string="the judging panel" id_sentence="5" />
      </mentions>
    </coreference>
    <coreference id="11" type="PROPER">
      <referenced ids_tokens="8" string="three" id_sentence="9" />
      <mentions>
        <mention ids_tokens="2" string="it" id_sentence="10" />
        <mention ids_tokens="5-7" string="a short story" id_sentence="10" />
        <mention ids_tokens="8" string="it" id_sentence="10" />
      </mentions>
    </coreference>
  </coreferences>
</document>
