<?xml version="1.0" encoding="UTF-8"?>
<document id="0" name="LA011990-0091">
  <sentences>
    <sentence id="1" has_coreference="true">
      <content>Ray Buckey and his mother, Peggy McMartin Buckey, were found not guilty Thursday of molesting children at the family-run McMartin Pre-School in Manhattan Beach, a verdict which brought to a close the longest and costliest criminal trial in history.</content>
      <tokens>
        <token id="1" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="2" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="3" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="4" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="Peggy" lemma="Peggy" stem="peggi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="8" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="9" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="found" lemma="find" stem="found" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="guilty" lemma="guilty" stem="guilti" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="Thursday" lemma="Thursday" stem="thursdai" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="16" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="molesting" lemma="molest" stem="molest" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="19" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="21" string="family-run" lemma="family-run" stem="family-run" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="22" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="23" string="Pre-School" lemma="Pre-School" stem="pre-school" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="24" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="Manhattan" lemma="Manhattan" stem="manhattan" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="26" string="Beach" lemma="Beach" stem="beach" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="27" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="verdict" lemma="verdict" stem="verdict" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="which" lemma="which" stem="which" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="brought" lemma="bring" stem="brought" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="close" lemma="close" stem="close" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="longest" lemma="longest" stem="longest" pos="JJS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="37" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="costliest" lemma="costliest" stem="costliest" pos="JJS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="criminal" lemma="criminal" stem="crimin" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="40" string="trial" lemma="trial" stem="trial" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="41" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="42" string="history" lemma="history" stem="histori" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="43" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NP (NNP Ray) (NNP Buckey)) (CC and) (NP (PRP$ his) (NN mother))) (, ,) (NP (NNP Peggy) (NNP McMartin) (NNP Buckey)) (, ,)) (VP (VBD were) (VP (VBN found) (NP-TMP (RB not) (JJ guilty) (NNP Thursday)) (PP (IN of) (S (VP (VBG molesting) (NP (NNS children)) (PP (IN at) (NP (DT the) (JJ family-run) (NNP McMartin) (NNP Pre-School))) (PP (IN in) (NP (NP (NNP Manhattan) (NNP Beach)) (, ,) (NP (NP (DT a) (NN verdict)) (SBAR (WHNP (WDT which)) (S (VP (VBD brought) (PP (TO to) (NP (DT a) (NP (RB close) (DT the) (JJS longest)) (CC and) (NP (JJS costliest) (JJ criminal) (NN trial)))) (PP (IN in) (NP (NN history)))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Peggy McMartin Buckey" type="NP">
          <tokens>
            <token id="7" string="Peggy" />
            <token id="8" string="McMartin" />
            <token id="9" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="2" string="a close the longest and costliest criminal trial" type="NP">
          <tokens>
            <token id="33" string="a" />
            <token id="34" string="close" />
            <token id="35" string="the" />
            <token id="36" string="longest" />
            <token id="37" string="and" />
            <token id="38" string="costliest" />
            <token id="39" string="criminal" />
            <token id="40" string="trial" />
          </tokens>
        </chunking>
        <chunking id="3" string="the family-run McMartin Pre-School" type="NP">
          <tokens>
            <token id="20" string="the" />
            <token id="21" string="family-run" />
            <token id="22" string="McMartin" />
            <token id="23" string="Pre-School" />
          </tokens>
        </chunking>
        <chunking id="4" string="brought to a close the longest and costliest criminal trial in history" type="VP">
          <tokens>
            <token id="31" string="brought" />
            <token id="32" string="to" />
            <token id="33" string="a" />
            <token id="34" string="close" />
            <token id="35" string="the" />
            <token id="36" string="longest" />
            <token id="37" string="and" />
            <token id="38" string="costliest" />
            <token id="39" string="criminal" />
            <token id="40" string="trial" />
            <token id="41" string="in" />
            <token id="42" string="history" />
          </tokens>
        </chunking>
        <chunking id="5" string="history" type="NP">
          <tokens>
            <token id="42" string="history" />
          </tokens>
        </chunking>
        <chunking id="6" string="Ray Buckey and his mother" type="NP">
          <tokens>
            <token id="1" string="Ray" />
            <token id="2" string="Buckey" />
            <token id="3" string="and" />
            <token id="4" string="his" />
            <token id="5" string="mother" />
          </tokens>
        </chunking>
        <chunking id="7" string="Ray Buckey" type="NP">
          <tokens>
            <token id="1" string="Ray" />
            <token id="2" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="8" string="costliest criminal trial" type="NP">
          <tokens>
            <token id="38" string="costliest" />
            <token id="39" string="criminal" />
            <token id="40" string="trial" />
          </tokens>
        </chunking>
        <chunking id="9" string="a verdict which brought to a close the longest and costliest criminal trial in history" type="NP">
          <tokens>
            <token id="28" string="a" />
            <token id="29" string="verdict" />
            <token id="30" string="which" />
            <token id="31" string="brought" />
            <token id="32" string="to" />
            <token id="33" string="a" />
            <token id="34" string="close" />
            <token id="35" string="the" />
            <token id="36" string="longest" />
            <token id="37" string="and" />
            <token id="38" string="costliest" />
            <token id="39" string="criminal" />
            <token id="40" string="trial" />
            <token id="41" string="in" />
            <token id="42" string="history" />
          </tokens>
        </chunking>
        <chunking id="10" string="Ray Buckey and his mother , Peggy McMartin Buckey ," type="NP">
          <tokens>
            <token id="1" string="Ray" />
            <token id="2" string="Buckey" />
            <token id="3" string="and" />
            <token id="4" string="his" />
            <token id="5" string="mother" />
            <token id="6" string="," />
            <token id="7" string="Peggy" />
            <token id="8" string="McMartin" />
            <token id="9" string="Buckey" />
            <token id="10" string="," />
          </tokens>
        </chunking>
        <chunking id="11" string="found not guilty Thursday of molesting children at the family-run McMartin Pre-School in Manhattan Beach , a verdict which brought to a close the longest and costliest criminal trial in history" type="VP">
          <tokens>
            <token id="12" string="found" />
            <token id="13" string="not" />
            <token id="14" string="guilty" />
            <token id="15" string="Thursday" />
            <token id="16" string="of" />
            <token id="17" string="molesting" />
            <token id="18" string="children" />
            <token id="19" string="at" />
            <token id="20" string="the" />
            <token id="21" string="family-run" />
            <token id="22" string="McMartin" />
            <token id="23" string="Pre-School" />
            <token id="24" string="in" />
            <token id="25" string="Manhattan" />
            <token id="26" string="Beach" />
            <token id="27" string="," />
            <token id="28" string="a" />
            <token id="29" string="verdict" />
            <token id="30" string="which" />
            <token id="31" string="brought" />
            <token id="32" string="to" />
            <token id="33" string="a" />
            <token id="34" string="close" />
            <token id="35" string="the" />
            <token id="36" string="longest" />
            <token id="37" string="and" />
            <token id="38" string="costliest" />
            <token id="39" string="criminal" />
            <token id="40" string="trial" />
            <token id="41" string="in" />
            <token id="42" string="history" />
          </tokens>
        </chunking>
        <chunking id="12" string="were found not guilty Thursday of molesting children at the family-run McMartin Pre-School in Manhattan Beach , a verdict which brought to a close the longest and costliest criminal trial in history" type="VP">
          <tokens>
            <token id="11" string="were" />
            <token id="12" string="found" />
            <token id="13" string="not" />
            <token id="14" string="guilty" />
            <token id="15" string="Thursday" />
            <token id="16" string="of" />
            <token id="17" string="molesting" />
            <token id="18" string="children" />
            <token id="19" string="at" />
            <token id="20" string="the" />
            <token id="21" string="family-run" />
            <token id="22" string="McMartin" />
            <token id="23" string="Pre-School" />
            <token id="24" string="in" />
            <token id="25" string="Manhattan" />
            <token id="26" string="Beach" />
            <token id="27" string="," />
            <token id="28" string="a" />
            <token id="29" string="verdict" />
            <token id="30" string="which" />
            <token id="31" string="brought" />
            <token id="32" string="to" />
            <token id="33" string="a" />
            <token id="34" string="close" />
            <token id="35" string="the" />
            <token id="36" string="longest" />
            <token id="37" string="and" />
            <token id="38" string="costliest" />
            <token id="39" string="criminal" />
            <token id="40" string="trial" />
            <token id="41" string="in" />
            <token id="42" string="history" />
          </tokens>
        </chunking>
        <chunking id="13" string="children" type="NP">
          <tokens>
            <token id="18" string="children" />
          </tokens>
        </chunking>
        <chunking id="14" string="molesting children at the family-run McMartin Pre-School in Manhattan Beach , a verdict which brought to a close the longest and costliest criminal trial in history" type="VP">
          <tokens>
            <token id="17" string="molesting" />
            <token id="18" string="children" />
            <token id="19" string="at" />
            <token id="20" string="the" />
            <token id="21" string="family-run" />
            <token id="22" string="McMartin" />
            <token id="23" string="Pre-School" />
            <token id="24" string="in" />
            <token id="25" string="Manhattan" />
            <token id="26" string="Beach" />
            <token id="27" string="," />
            <token id="28" string="a" />
            <token id="29" string="verdict" />
            <token id="30" string="which" />
            <token id="31" string="brought" />
            <token id="32" string="to" />
            <token id="33" string="a" />
            <token id="34" string="close" />
            <token id="35" string="the" />
            <token id="36" string="longest" />
            <token id="37" string="and" />
            <token id="38" string="costliest" />
            <token id="39" string="criminal" />
            <token id="40" string="trial" />
            <token id="41" string="in" />
            <token id="42" string="history" />
          </tokens>
        </chunking>
        <chunking id="15" string="Manhattan Beach , a verdict which brought to a close the longest and costliest criminal trial in history" type="NP">
          <tokens>
            <token id="25" string="Manhattan" />
            <token id="26" string="Beach" />
            <token id="27" string="," />
            <token id="28" string="a" />
            <token id="29" string="verdict" />
            <token id="30" string="which" />
            <token id="31" string="brought" />
            <token id="32" string="to" />
            <token id="33" string="a" />
            <token id="34" string="close" />
            <token id="35" string="the" />
            <token id="36" string="longest" />
            <token id="37" string="and" />
            <token id="38" string="costliest" />
            <token id="39" string="criminal" />
            <token id="40" string="trial" />
            <token id="41" string="in" />
            <token id="42" string="history" />
          </tokens>
        </chunking>
        <chunking id="16" string="which brought to a close the longest and costliest criminal trial in history" type="SBAR">
          <tokens>
            <token id="30" string="which" />
            <token id="31" string="brought" />
            <token id="32" string="to" />
            <token id="33" string="a" />
            <token id="34" string="close" />
            <token id="35" string="the" />
            <token id="36" string="longest" />
            <token id="37" string="and" />
            <token id="38" string="costliest" />
            <token id="39" string="criminal" />
            <token id="40" string="trial" />
            <token id="41" string="in" />
            <token id="42" string="history" />
          </tokens>
        </chunking>
        <chunking id="17" string="his mother" type="NP">
          <tokens>
            <token id="4" string="his" />
            <token id="5" string="mother" />
          </tokens>
        </chunking>
        <chunking id="18" string="a verdict" type="NP">
          <tokens>
            <token id="28" string="a" />
            <token id="29" string="verdict" />
          </tokens>
        </chunking>
        <chunking id="19" string="close the longest" type="NP">
          <tokens>
            <token id="34" string="close" />
            <token id="35" string="the" />
            <token id="36" string="longest" />
          </tokens>
        </chunking>
        <chunking id="20" string="Manhattan Beach" type="NP">
          <tokens>
            <token id="25" string="Manhattan" />
            <token id="26" string="Beach" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">Buckey</governor>
          <dependent id="1">Ray</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="12">found</governor>
          <dependent id="2">Buckey</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="2">Buckey</governor>
          <dependent id="3">and</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="5">mother</governor>
          <dependent id="4">his</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="2">Buckey</governor>
          <dependent id="5">mother</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">Buckey</governor>
          <dependent id="7">Peggy</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">Buckey</governor>
          <dependent id="8">McMartin</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="2">Buckey</governor>
          <dependent id="9">Buckey</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="12">found</governor>
          <dependent id="11">were</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="12">found</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="15">Thursday</governor>
          <dependent id="13">not</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">Thursday</governor>
          <dependent id="14">guilty</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="12">found</governor>
          <dependent id="15">Thursday</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="17">molesting</governor>
          <dependent id="16">of</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="12">found</governor>
          <dependent id="17">molesting</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="17">molesting</governor>
          <dependent id="18">children</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">Pre-School</governor>
          <dependent id="19">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="23">Pre-School</governor>
          <dependent id="20">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="23">Pre-School</governor>
          <dependent id="21">family-run</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="23">Pre-School</governor>
          <dependent id="22">McMartin</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">molesting</governor>
          <dependent id="23">Pre-School</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">Beach</governor>
          <dependent id="24">in</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="26">Beach</governor>
          <dependent id="25">Manhattan</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">molesting</governor>
          <dependent id="26">Beach</dependent>
        </dependency>
        <dependency type="det">
          <governor id="29">verdict</governor>
          <dependent id="28">a</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="26">Beach</governor>
          <dependent id="29">verdict</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="31">brought</governor>
          <dependent id="30">which</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="29">verdict</governor>
          <dependent id="31">brought</dependent>
        </dependency>
        <dependency type="case">
          <governor id="36">longest</governor>
          <dependent id="32">to</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="36">longest</governor>
          <dependent id="33">a</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="36">longest</governor>
          <dependent id="34">close</dependent>
        </dependency>
        <dependency type="det">
          <governor id="36">longest</governor>
          <dependent id="35">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="31">brought</governor>
          <dependent id="36">longest</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="36">longest</governor>
          <dependent id="37">and</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="40">trial</governor>
          <dependent id="38">costliest</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="40">trial</governor>
          <dependent id="39">criminal</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="36">longest</governor>
          <dependent id="40">trial</dependent>
        </dependency>
        <dependency type="case">
          <governor id="42">history</governor>
          <dependent id="41">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="31">brought</governor>
          <dependent id="42">history</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Peggy McMartin Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="7" string="Peggy" />
            <token id="8" string="McMartin" />
            <token id="9" string="Buckey" />
          </tokens>
        </entity>
        <entity id="2" string="Thursday" type="DATE" score="0.0">
          <tokens>
            <token id="15" string="Thursday" />
          </tokens>
        </entity>
        <entity id="3" string="McMartin Pre-School" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="22" string="McMartin" />
            <token id="23" string="Pre-School" />
          </tokens>
        </entity>
        <entity id="4" string="Ray Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Ray" />
            <token id="2" string="Buckey" />
          </tokens>
        </entity>
        <entity id="5" string="Manhattan Beach" type="LOCATION" score="0.0">
          <tokens>
            <token id="25" string="Manhattan" />
            <token id="26" string="Beach" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="2" has_coreference="true">
      <content>An eight-man, four-woman jury -- 10 of the members parents themselves -- acquitted the Buckeys of 52 counts of molestation after deliberating for nine weeks over evidence that had been presented over the course of more than two years.</content>
      <tokens>
        <token id="1" string="An" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="eight-man" lemma="eight-man" stem="eight-man" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="four-woman" lemma="four-woman" stem="four-woman" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="10" lemma="10" stem="10" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="8" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="members" lemma="member" stem="member" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="parents" lemma="parent" stem="parent" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="themselves" lemma="themselves" stem="themselv" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="acquitted" lemma="acquit" stem="acquit" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="16" string="Buckeys" lemma="Buckeys" stem="buckei" pos="NNPS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="17" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="52" lemma="52" stem="52" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="true" />
        <token id="19" string="counts" lemma="count" stem="count" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="20" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="21" string="molestation" lemma="molestation" stem="molest" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="22" string="after" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="deliberating" lemma="deliberate" stem="deliber" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="nine" lemma="nine" stem="nine" pos="CD" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="26" string="weeks" lemma="week" stem="week" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="27" string="over" lemma="over" stem="over" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="evidence" lemma="evidence" stem="evid" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="presented" lemma="present" stem="present" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="over" lemma="over" stem="over" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="course" lemma="course" stem="cours" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="36" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="more" lemma="more" stem="more" pos="JJR" type="Word" isStopWord="true" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="38" string="than" lemma="than" stem="than" pos="IN" type="Word" isStopWord="true" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="39" string="two" lemma="two" stem="two" pos="CD" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="40" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="41" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT An) (NN eight-man)) (, ,) (NP (NP (JJ four-woman) (NN jury)) (PRN (: --) (S (NP (NP (CD 10)) (PP (IN of) (NP (DT the) (NNS members) (NNS parents)))) (NP (PRP themselves))) (: --)))) (VP (VBD acquitted) (NP (NP (DT the) (NNPS Buckeys)) (PP (IN of) (NP (NP (CD 52) (NNS counts)) (PP (IN of) (NP (NN molestation)))))) (PP (IN after) (S (VP (VBG deliberating) (PP (IN for) (NP (NP (CD nine) (NNS weeks)) (PP (IN over) (NP (NN evidence))) (SBAR (WHNP (IN that)) (S (VP (VBD had) (VP (VBN been) (VP (VBN presented) (PP (IN over) (NP (NP (DT the) (NN course)) (PP (IN of) (NP (QP (JJR more) (IN than) (CD two)) (NNS years)))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="molestation" type="NP">
          <tokens>
            <token id="21" string="molestation" />
          </tokens>
        </chunking>
        <chunking id="2" string="presented over the course of more than two years" type="VP">
          <tokens>
            <token id="32" string="presented" />
            <token id="33" string="over" />
            <token id="34" string="the" />
            <token id="35" string="course" />
            <token id="36" string="of" />
            <token id="37" string="more" />
            <token id="38" string="than" />
            <token id="39" string="two" />
            <token id="40" string="years" />
          </tokens>
        </chunking>
        <chunking id="3" string="four-woman jury" type="NP">
          <tokens>
            <token id="4" string="four-woman" />
            <token id="5" string="jury" />
          </tokens>
        </chunking>
        <chunking id="4" string="An eight-man , four-woman jury -- 10 of the members parents themselves --" type="NP">
          <tokens>
            <token id="1" string="An" />
            <token id="2" string="eight-man" />
            <token id="3" string="," />
            <token id="4" string="four-woman" />
            <token id="5" string="jury" />
            <token id="6" string="--" />
            <token id="7" string="10" />
            <token id="8" string="of" />
            <token id="9" string="the" />
            <token id="10" string="members" />
            <token id="11" string="parents" />
            <token id="12" string="themselves" />
            <token id="13" string="--" />
          </tokens>
        </chunking>
        <chunking id="5" string="the Buckeys of 52 counts of molestation" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="Buckeys" />
            <token id="17" string="of" />
            <token id="18" string="52" />
            <token id="19" string="counts" />
            <token id="20" string="of" />
            <token id="21" string="molestation" />
          </tokens>
        </chunking>
        <chunking id="6" string="the members parents" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="members" />
            <token id="11" string="parents" />
          </tokens>
        </chunking>
        <chunking id="7" string="more than two years" type="NP">
          <tokens>
            <token id="37" string="more" />
            <token id="38" string="than" />
            <token id="39" string="two" />
            <token id="40" string="years" />
          </tokens>
        </chunking>
        <chunking id="8" string="the course of more than two years" type="NP">
          <tokens>
            <token id="34" string="the" />
            <token id="35" string="course" />
            <token id="36" string="of" />
            <token id="37" string="more" />
            <token id="38" string="than" />
            <token id="39" string="two" />
            <token id="40" string="years" />
          </tokens>
        </chunking>
        <chunking id="9" string="An eight-man" type="NP">
          <tokens>
            <token id="1" string="An" />
            <token id="2" string="eight-man" />
          </tokens>
        </chunking>
        <chunking id="10" string="10 of the members parents" type="NP">
          <tokens>
            <token id="7" string="10" />
            <token id="8" string="of" />
            <token id="9" string="the" />
            <token id="10" string="members" />
            <token id="11" string="parents" />
          </tokens>
        </chunking>
        <chunking id="11" string="that had been presented over the course of more than two years" type="SBAR">
          <tokens>
            <token id="29" string="that" />
            <token id="30" string="had" />
            <token id="31" string="been" />
            <token id="32" string="presented" />
            <token id="33" string="over" />
            <token id="34" string="the" />
            <token id="35" string="course" />
            <token id="36" string="of" />
            <token id="37" string="more" />
            <token id="38" string="than" />
            <token id="39" string="two" />
            <token id="40" string="years" />
          </tokens>
        </chunking>
        <chunking id="12" string="52 counts of molestation" type="NP">
          <tokens>
            <token id="18" string="52" />
            <token id="19" string="counts" />
            <token id="20" string="of" />
            <token id="21" string="molestation" />
          </tokens>
        </chunking>
        <chunking id="13" string="evidence" type="NP">
          <tokens>
            <token id="28" string="evidence" />
          </tokens>
        </chunking>
        <chunking id="14" string="nine weeks over evidence that had been presented over the course of more than two years" type="NP">
          <tokens>
            <token id="25" string="nine" />
            <token id="26" string="weeks" />
            <token id="27" string="over" />
            <token id="28" string="evidence" />
            <token id="29" string="that" />
            <token id="30" string="had" />
            <token id="31" string="been" />
            <token id="32" string="presented" />
            <token id="33" string="over" />
            <token id="34" string="the" />
            <token id="35" string="course" />
            <token id="36" string="of" />
            <token id="37" string="more" />
            <token id="38" string="than" />
            <token id="39" string="two" />
            <token id="40" string="years" />
          </tokens>
        </chunking>
        <chunking id="15" string="52 counts" type="NP">
          <tokens>
            <token id="18" string="52" />
            <token id="19" string="counts" />
          </tokens>
        </chunking>
        <chunking id="16" string="had been presented over the course of more than two years" type="VP">
          <tokens>
            <token id="30" string="had" />
            <token id="31" string="been" />
            <token id="32" string="presented" />
            <token id="33" string="over" />
            <token id="34" string="the" />
            <token id="35" string="course" />
            <token id="36" string="of" />
            <token id="37" string="more" />
            <token id="38" string="than" />
            <token id="39" string="two" />
            <token id="40" string="years" />
          </tokens>
        </chunking>
        <chunking id="17" string="acquitted the Buckeys of 52 counts of molestation after deliberating for nine weeks over evidence that had been presented over the course of more than two years" type="VP">
          <tokens>
            <token id="14" string="acquitted" />
            <token id="15" string="the" />
            <token id="16" string="Buckeys" />
            <token id="17" string="of" />
            <token id="18" string="52" />
            <token id="19" string="counts" />
            <token id="20" string="of" />
            <token id="21" string="molestation" />
            <token id="22" string="after" />
            <token id="23" string="deliberating" />
            <token id="24" string="for" />
            <token id="25" string="nine" />
            <token id="26" string="weeks" />
            <token id="27" string="over" />
            <token id="28" string="evidence" />
            <token id="29" string="that" />
            <token id="30" string="had" />
            <token id="31" string="been" />
            <token id="32" string="presented" />
            <token id="33" string="over" />
            <token id="34" string="the" />
            <token id="35" string="course" />
            <token id="36" string="of" />
            <token id="37" string="more" />
            <token id="38" string="than" />
            <token id="39" string="two" />
            <token id="40" string="years" />
          </tokens>
        </chunking>
        <chunking id="18" string="four-woman jury -- 10 of the members parents themselves --" type="NP">
          <tokens>
            <token id="4" string="four-woman" />
            <token id="5" string="jury" />
            <token id="6" string="--" />
            <token id="7" string="10" />
            <token id="8" string="of" />
            <token id="9" string="the" />
            <token id="10" string="members" />
            <token id="11" string="parents" />
            <token id="12" string="themselves" />
            <token id="13" string="--" />
          </tokens>
        </chunking>
        <chunking id="19" string="themselves" type="NP">
          <tokens>
            <token id="12" string="themselves" />
          </tokens>
        </chunking>
        <chunking id="20" string="deliberating for nine weeks over evidence that had been presented over the course of more than two years" type="VP">
          <tokens>
            <token id="23" string="deliberating" />
            <token id="24" string="for" />
            <token id="25" string="nine" />
            <token id="26" string="weeks" />
            <token id="27" string="over" />
            <token id="28" string="evidence" />
            <token id="29" string="that" />
            <token id="30" string="had" />
            <token id="31" string="been" />
            <token id="32" string="presented" />
            <token id="33" string="over" />
            <token id="34" string="the" />
            <token id="35" string="course" />
            <token id="36" string="of" />
            <token id="37" string="more" />
            <token id="38" string="than" />
            <token id="39" string="two" />
            <token id="40" string="years" />
          </tokens>
        </chunking>
        <chunking id="21" string="the Buckeys" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="Buckeys" />
          </tokens>
        </chunking>
        <chunking id="22" string="been presented over the course of more than two years" type="VP">
          <tokens>
            <token id="31" string="been" />
            <token id="32" string="presented" />
            <token id="33" string="over" />
            <token id="34" string="the" />
            <token id="35" string="course" />
            <token id="36" string="of" />
            <token id="37" string="more" />
            <token id="38" string="than" />
            <token id="39" string="two" />
            <token id="40" string="years" />
          </tokens>
        </chunking>
        <chunking id="23" string="the course" type="NP">
          <tokens>
            <token id="34" string="the" />
            <token id="35" string="course" />
          </tokens>
        </chunking>
        <chunking id="24" string="10" type="NP">
          <tokens>
            <token id="7" string="10" />
          </tokens>
        </chunking>
        <chunking id="25" string="nine weeks" type="NP">
          <tokens>
            <token id="25" string="nine" />
            <token id="26" string="weeks" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">eight-man</governor>
          <dependent id="1">An</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">acquitted</governor>
          <dependent id="2">eight-man</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">jury</governor>
          <dependent id="4">four-woman</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="2">eight-man</governor>
          <dependent id="5">jury</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="12">themselves</governor>
          <dependent id="7">10</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">parents</governor>
          <dependent id="8">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">parents</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">parents</governor>
          <dependent id="10">members</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">10</governor>
          <dependent id="11">parents</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="5">jury</governor>
          <dependent id="12">themselves</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="14">acquitted</dependent>
        </dependency>
        <dependency type="det">
          <governor id="16">Buckeys</governor>
          <dependent id="15">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="14">acquitted</governor>
          <dependent id="16">Buckeys</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">counts</governor>
          <dependent id="17">of</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="19">counts</governor>
          <dependent id="18">52</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">Buckeys</governor>
          <dependent id="19">counts</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">molestation</governor>
          <dependent id="20">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">counts</governor>
          <dependent id="21">molestation</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="23">deliberating</governor>
          <dependent id="22">after</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="14">acquitted</governor>
          <dependent id="23">deliberating</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">weeks</governor>
          <dependent id="24">for</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="26">weeks</governor>
          <dependent id="25">nine</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">deliberating</governor>
          <dependent id="26">weeks</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">evidence</governor>
          <dependent id="27">over</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">weeks</governor>
          <dependent id="28">evidence</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="32">presented</governor>
          <dependent id="29">that</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="32">presented</governor>
          <dependent id="30">had</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="32">presented</governor>
          <dependent id="31">been</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="26">weeks</governor>
          <dependent id="32">presented</dependent>
        </dependency>
        <dependency type="case">
          <governor id="35">course</governor>
          <dependent id="33">over</dependent>
        </dependency>
        <dependency type="det">
          <governor id="35">course</governor>
          <dependent id="34">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="32">presented</governor>
          <dependent id="35">course</dependent>
        </dependency>
        <dependency type="case">
          <governor id="40">years</governor>
          <dependent id="36">of</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="39">two</governor>
          <dependent id="37">more</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="37">more</governor>
          <dependent id="38">than</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="40">years</governor>
          <dependent id="39">two</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="35">course</governor>
          <dependent id="40">years</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="more than two years" type="DURATION" score="0.0">
          <tokens>
            <token id="37" string="more" />
            <token id="38" string="than" />
            <token id="39" string="two" />
            <token id="40" string="years" />
          </tokens>
        </entity>
        <entity id="2" string="52" type="NUMBER" score="0.0">
          <tokens>
            <token id="18" string="52" />
          </tokens>
        </entity>
        <entity id="3" string="10" type="NUMBER" score="0.0">
          <tokens>
            <token id="7" string="10" />
          </tokens>
        </entity>
        <entity id="4" string="nine weeks" type="DURATION" score="0.0">
          <tokens>
            <token id="25" string="nine" />
            <token id="26" string="weeks" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="3" has_coreference="true">
      <content>The jury reported it was deadlocked on 13 remaining counts, and a mistrial was declared on those allegations.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="reported" lemma="report" stem="report" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="deadlocked" lemma="deadlock" stem="deadlock" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="13" lemma="13" stem="13" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="9" string="remaining" lemma="remain" stem="remain" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="counts" lemma="count" stem="count" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="mistrial" lemma="mistrial" stem="mistrial" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="declared" lemma="declare" stem="declar" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="those" lemma="those" stem="those" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="allegations" lemma="allegation" stem="alleg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (DT The) (NN jury)) (VP (VBD reported) (SBAR (S (NP (PRP it)) (VP (VBD was) (VP (VBN deadlocked) (PP (IN on) (NP (CD 13))) (S (VP (VBG remaining) (NP (NNS counts)))))))))) (, ,) (CC and) (S (NP (DT a) (NN mistrial)) (VP (VBD was) (VP (VBN declared) (PP (IN on) (NP (DT those) (NNS allegations)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="was deadlocked on 13 remaining counts" type="VP">
          <tokens>
            <token id="5" string="was" />
            <token id="6" string="deadlocked" />
            <token id="7" string="on" />
            <token id="8" string="13" />
            <token id="9" string="remaining" />
            <token id="10" string="counts" />
          </tokens>
        </chunking>
        <chunking id="2" string="it was deadlocked on 13 remaining counts" type="SBAR">
          <tokens>
            <token id="4" string="it" />
            <token id="5" string="was" />
            <token id="6" string="deadlocked" />
            <token id="7" string="on" />
            <token id="8" string="13" />
            <token id="9" string="remaining" />
            <token id="10" string="counts" />
          </tokens>
        </chunking>
        <chunking id="3" string="13" type="NP">
          <tokens>
            <token id="8" string="13" />
          </tokens>
        </chunking>
        <chunking id="4" string="counts" type="NP">
          <tokens>
            <token id="10" string="counts" />
          </tokens>
        </chunking>
        <chunking id="5" string="declared on those allegations" type="VP">
          <tokens>
            <token id="16" string="declared" />
            <token id="17" string="on" />
            <token id="18" string="those" />
            <token id="19" string="allegations" />
          </tokens>
        </chunking>
        <chunking id="6" string="was declared on those allegations" type="VP">
          <tokens>
            <token id="15" string="was" />
            <token id="16" string="declared" />
            <token id="17" string="on" />
            <token id="18" string="those" />
            <token id="19" string="allegations" />
          </tokens>
        </chunking>
        <chunking id="7" string="it" type="NP">
          <tokens>
            <token id="4" string="it" />
          </tokens>
        </chunking>
        <chunking id="8" string="deadlocked on 13 remaining counts" type="VP">
          <tokens>
            <token id="6" string="deadlocked" />
            <token id="7" string="on" />
            <token id="8" string="13" />
            <token id="9" string="remaining" />
            <token id="10" string="counts" />
          </tokens>
        </chunking>
        <chunking id="9" string="remaining counts" type="VP">
          <tokens>
            <token id="9" string="remaining" />
            <token id="10" string="counts" />
          </tokens>
        </chunking>
        <chunking id="10" string="those allegations" type="NP">
          <tokens>
            <token id="18" string="those" />
            <token id="19" string="allegations" />
          </tokens>
        </chunking>
        <chunking id="11" string="reported it was deadlocked on 13 remaining counts" type="VP">
          <tokens>
            <token id="3" string="reported" />
            <token id="4" string="it" />
            <token id="5" string="was" />
            <token id="6" string="deadlocked" />
            <token id="7" string="on" />
            <token id="8" string="13" />
            <token id="9" string="remaining" />
            <token id="10" string="counts" />
          </tokens>
        </chunking>
        <chunking id="12" string="a mistrial" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="mistrial" />
          </tokens>
        </chunking>
        <chunking id="13" string="The jury" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="jury" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">jury</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">reported</governor>
          <dependent id="2">jury</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">reported</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="6">deadlocked</governor>
          <dependent id="4">it</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="6">deadlocked</governor>
          <dependent id="5">was</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">reported</governor>
          <dependent id="6">deadlocked</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">13</governor>
          <dependent id="7">on</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">deadlocked</governor>
          <dependent id="8">13</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="6">deadlocked</governor>
          <dependent id="9">remaining</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="9">remaining</governor>
          <dependent id="10">counts</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="3">reported</governor>
          <dependent id="12">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">mistrial</governor>
          <dependent id="13">a</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="16">declared</governor>
          <dependent id="14">mistrial</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="16">declared</governor>
          <dependent id="15">was</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="3">reported</governor>
          <dependent id="16">declared</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">allegations</governor>
          <dependent id="17">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">allegations</governor>
          <dependent id="18">those</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">declared</governor>
          <dependent id="19">allegations</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="13" type="NUMBER" score="0.0">
          <tokens>
            <token id="8" string="13" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="4" has_coreference="true">
      <content>Ray Buckey, 31, left the courtroom without comment -- free on his own recognizance until a decision is reached on whether the district attorney will retry him on the 13 undecided counts.</content>
      <tokens>
        <token id="1" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="31" lemma="31" stem="31" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="true" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="left" lemma="leave" stem="left" pos="VBD" type="Word" isStopWord="false" ner="IDEOLOGY" is_referenced="true" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="courtroom" lemma="courtroom" stem="courtroom" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="without" lemma="without" stem="without" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="comment" lemma="comment" stem="comment" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="free" lemma="free" stem="free" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="own" lemma="own" stem="own" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="recognizance" lemma="recognizance" stem="recogniz" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="until" lemma="until" stem="until" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="19" string="decision" lemma="decision" stem="decis" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="20" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="reached" lemma="reach" stem="reach" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="whether" lemma="whether" stem="whether" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="25" string="district" lemma="district" stem="district" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="26" string="attorney" lemma="attorney" stem="attornei" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="27" string="will" lemma="will" stem="will" pos="MD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="retry" lemma="retry" stem="retri" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="him" lemma="he" stem="him" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="32" string="13" lemma="13" stem="13" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="33" string="undecided" lemma="undecided" stem="undecid" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="34" string="counts" lemma="count" stem="count" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="35" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Ray) (NNP Buckey)) (, ,) (NP (CD 31)) (, ,)) (VP (VP (VBD left) (NP (DT the) (NN courtroom)) (PP (IN without) (NP (NN comment)))) (: --) (VP (JJ free) (PP (IN on) (NP (PRP$ his) (JJ own) (NN recognizance)))) (SBAR (IN until) (S (NP (DT a) (NN decision)) (VP (VBZ is) (VP (VBN reached) (PP (IN on) (SBAR (IN whether) (S (NP (DT the) (NN district) (NN attorney)) (VP (MD will) (VP (VB retry) (NP (PRP him)) (PP (IN on) (NP (DT the) (CD 13) (JJ undecided) (NNS counts))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a decision" type="NP">
          <tokens>
            <token id="18" string="a" />
            <token id="19" string="decision" />
          </tokens>
        </chunking>
        <chunking id="2" string="reached on whether the district attorney will retry him on the 13 undecided counts" type="VP">
          <tokens>
            <token id="21" string="reached" />
            <token id="22" string="on" />
            <token id="23" string="whether" />
            <token id="24" string="the" />
            <token id="25" string="district" />
            <token id="26" string="attorney" />
            <token id="27" string="will" />
            <token id="28" string="retry" />
            <token id="29" string="him" />
            <token id="30" string="on" />
            <token id="31" string="the" />
            <token id="32" string="13" />
            <token id="33" string="undecided" />
            <token id="34" string="counts" />
          </tokens>
        </chunking>
        <chunking id="3" string="Ray Buckey , 31 ," type="NP">
          <tokens>
            <token id="1" string="Ray" />
            <token id="2" string="Buckey" />
            <token id="3" string="," />
            <token id="4" string="31" />
            <token id="5" string="," />
          </tokens>
        </chunking>
        <chunking id="4" string="the district attorney" type="NP">
          <tokens>
            <token id="24" string="the" />
            <token id="25" string="district" />
            <token id="26" string="attorney" />
          </tokens>
        </chunking>
        <chunking id="5" string="him" type="NP">
          <tokens>
            <token id="29" string="him" />
          </tokens>
        </chunking>
        <chunking id="6" string="left the courtroom without comment -- free on his own recognizance until a decision is reached on whether the district attorney will retry him on the 13 undecided counts" type="VP">
          <tokens>
            <token id="6" string="left" />
            <token id="7" string="the" />
            <token id="8" string="courtroom" />
            <token id="9" string="without" />
            <token id="10" string="comment" />
            <token id="11" string="--" />
            <token id="12" string="free" />
            <token id="13" string="on" />
            <token id="14" string="his" />
            <token id="15" string="own" />
            <token id="16" string="recognizance" />
            <token id="17" string="until" />
            <token id="18" string="a" />
            <token id="19" string="decision" />
            <token id="20" string="is" />
            <token id="21" string="reached" />
            <token id="22" string="on" />
            <token id="23" string="whether" />
            <token id="24" string="the" />
            <token id="25" string="district" />
            <token id="26" string="attorney" />
            <token id="27" string="will" />
            <token id="28" string="retry" />
            <token id="29" string="him" />
            <token id="30" string="on" />
            <token id="31" string="the" />
            <token id="32" string="13" />
            <token id="33" string="undecided" />
            <token id="34" string="counts" />
          </tokens>
        </chunking>
        <chunking id="7" string="Ray Buckey" type="NP">
          <tokens>
            <token id="1" string="Ray" />
            <token id="2" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="8" string="until a decision is reached on whether the district attorney will retry him on the 13 undecided counts" type="SBAR">
          <tokens>
            <token id="17" string="until" />
            <token id="18" string="a" />
            <token id="19" string="decision" />
            <token id="20" string="is" />
            <token id="21" string="reached" />
            <token id="22" string="on" />
            <token id="23" string="whether" />
            <token id="24" string="the" />
            <token id="25" string="district" />
            <token id="26" string="attorney" />
            <token id="27" string="will" />
            <token id="28" string="retry" />
            <token id="29" string="him" />
            <token id="30" string="on" />
            <token id="31" string="the" />
            <token id="32" string="13" />
            <token id="33" string="undecided" />
            <token id="34" string="counts" />
          </tokens>
        </chunking>
        <chunking id="9" string="the 13 undecided counts" type="NP">
          <tokens>
            <token id="31" string="the" />
            <token id="32" string="13" />
            <token id="33" string="undecided" />
            <token id="34" string="counts" />
          </tokens>
        </chunking>
        <chunking id="10" string="free on his own recognizance" type="VP">
          <tokens>
            <token id="12" string="free" />
            <token id="13" string="on" />
            <token id="14" string="his" />
            <token id="15" string="own" />
            <token id="16" string="recognizance" />
          </tokens>
        </chunking>
        <chunking id="11" string="the courtroom" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="courtroom" />
          </tokens>
        </chunking>
        <chunking id="12" string="will retry him on the 13 undecided counts" type="VP">
          <tokens>
            <token id="27" string="will" />
            <token id="28" string="retry" />
            <token id="29" string="him" />
            <token id="30" string="on" />
            <token id="31" string="the" />
            <token id="32" string="13" />
            <token id="33" string="undecided" />
            <token id="34" string="counts" />
          </tokens>
        </chunking>
        <chunking id="13" string="retry him on the 13 undecided counts" type="VP">
          <tokens>
            <token id="28" string="retry" />
            <token id="29" string="him" />
            <token id="30" string="on" />
            <token id="31" string="the" />
            <token id="32" string="13" />
            <token id="33" string="undecided" />
            <token id="34" string="counts" />
          </tokens>
        </chunking>
        <chunking id="14" string="left the courtroom without comment" type="VP">
          <tokens>
            <token id="6" string="left" />
            <token id="7" string="the" />
            <token id="8" string="courtroom" />
            <token id="9" string="without" />
            <token id="10" string="comment" />
          </tokens>
        </chunking>
        <chunking id="15" string="is reached on whether the district attorney will retry him on the 13 undecided counts" type="VP">
          <tokens>
            <token id="20" string="is" />
            <token id="21" string="reached" />
            <token id="22" string="on" />
            <token id="23" string="whether" />
            <token id="24" string="the" />
            <token id="25" string="district" />
            <token id="26" string="attorney" />
            <token id="27" string="will" />
            <token id="28" string="retry" />
            <token id="29" string="him" />
            <token id="30" string="on" />
            <token id="31" string="the" />
            <token id="32" string="13" />
            <token id="33" string="undecided" />
            <token id="34" string="counts" />
          </tokens>
        </chunking>
        <chunking id="16" string="comment" type="NP">
          <tokens>
            <token id="10" string="comment" />
          </tokens>
        </chunking>
        <chunking id="17" string="his own recognizance" type="NP">
          <tokens>
            <token id="14" string="his" />
            <token id="15" string="own" />
            <token id="16" string="recognizance" />
          </tokens>
        </chunking>
        <chunking id="18" string="whether the district attorney will retry him on the 13 undecided counts" type="SBAR">
          <tokens>
            <token id="23" string="whether" />
            <token id="24" string="the" />
            <token id="25" string="district" />
            <token id="26" string="attorney" />
            <token id="27" string="will" />
            <token id="28" string="retry" />
            <token id="29" string="him" />
            <token id="30" string="on" />
            <token id="31" string="the" />
            <token id="32" string="13" />
            <token id="33" string="undecided" />
            <token id="34" string="counts" />
          </tokens>
        </chunking>
        <chunking id="19" string="31" type="NP">
          <tokens>
            <token id="4" string="31" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">Buckey</governor>
          <dependent id="1">Ray</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">left</governor>
          <dependent id="2">Buckey</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="2">Buckey</governor>
          <dependent id="4">31</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">left</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">courtroom</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">left</governor>
          <dependent id="8">courtroom</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">comment</governor>
          <dependent id="9">without</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">left</governor>
          <dependent id="10">comment</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="6">left</governor>
          <dependent id="12">free</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">recognizance</governor>
          <dependent id="13">on</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="16">recognizance</governor>
          <dependent id="14">his</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="16">recognizance</governor>
          <dependent id="15">own</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">free</governor>
          <dependent id="16">recognizance</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="21">reached</governor>
          <dependent id="17">until</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">decision</governor>
          <dependent id="18">a</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="21">reached</governor>
          <dependent id="19">decision</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="21">reached</governor>
          <dependent id="20">is</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="6">left</governor>
          <dependent id="21">reached</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="28">retry</governor>
          <dependent id="22">on</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="28">retry</governor>
          <dependent id="23">whether</dependent>
        </dependency>
        <dependency type="det">
          <governor id="26">attorney</governor>
          <dependent id="24">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="26">attorney</governor>
          <dependent id="25">district</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="28">retry</governor>
          <dependent id="26">attorney</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="28">retry</governor>
          <dependent id="27">will</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="21">reached</governor>
          <dependent id="28">retry</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="28">retry</governor>
          <dependent id="29">him</dependent>
        </dependency>
        <dependency type="case">
          <governor id="34">counts</governor>
          <dependent id="30">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="34">counts</governor>
          <dependent id="31">the</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="34">counts</governor>
          <dependent id="32">13</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="34">counts</governor>
          <dependent id="33">undecided</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="28">retry</governor>
          <dependent id="34">counts</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="13" type="NUMBER" score="0.0">
          <tokens>
            <token id="32" string="13" />
          </tokens>
        </entity>
        <entity id="2" string="left" type="IDEOLOGY" score="0.0">
          <tokens>
            <token id="6" string="left" />
          </tokens>
        </entity>
        <entity id="3" string="Ray Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Ray" />
            <token id="2" string="Buckey" />
          </tokens>
        </entity>
        <entity id="4" string="31" type="NUMBER" score="0.0">
          <tokens>
            <token id="4" string="31" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="5" has_coreference="true">
      <content>But his 63-year-old mother and co-defendant reacted with anger.</content>
      <tokens>
        <token id="1" string="But" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="3" string="63-year-old" lemma="63-year-old" stem="63-year-old" pos="JJ" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="4" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="6" string="co-defendant" lemma="co-defendant" stem="co-defend" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="reacted" lemma="react" stem="react" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="anger" lemma="anger" stem="anger" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (CC But) (NP (NP (PRP$ his) (JJ 63-year-old) (NN mother)) (CC and) (NP (NN co-defendant))) (VP (VBD reacted) (PP (IN with) (NP (NN anger)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="co-defendant" type="NP">
          <tokens>
            <token id="6" string="co-defendant" />
          </tokens>
        </chunking>
        <chunking id="2" string="his 63-year-old mother" type="NP">
          <tokens>
            <token id="2" string="his" />
            <token id="3" string="63-year-old" />
            <token id="4" string="mother" />
          </tokens>
        </chunking>
        <chunking id="3" string="anger" type="NP">
          <tokens>
            <token id="9" string="anger" />
          </tokens>
        </chunking>
        <chunking id="4" string="his 63-year-old mother and co-defendant" type="NP">
          <tokens>
            <token id="2" string="his" />
            <token id="3" string="63-year-old" />
            <token id="4" string="mother" />
            <token id="5" string="and" />
            <token id="6" string="co-defendant" />
          </tokens>
        </chunking>
        <chunking id="5" string="reacted with anger" type="VP">
          <tokens>
            <token id="7" string="reacted" />
            <token id="8" string="with" />
            <token id="9" string="anger" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="cc">
          <governor id="7">reacted</governor>
          <dependent id="1">But</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="4">mother</governor>
          <dependent id="2">his</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="4">mother</governor>
          <dependent id="3">63-year-old</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">reacted</governor>
          <dependent id="4">mother</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">mother</governor>
          <dependent id="5">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">mother</governor>
          <dependent id="6">co-defendant</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">reacted</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">anger</governor>
          <dependent id="8">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">reacted</governor>
          <dependent id="9">anger</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="63-year-old" type="DURATION" score="0.0">
          <tokens>
            <token id="3" string="63-year-old" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="6" has_coreference="true">
      <content>&amp;quot;I&amp;apost;ve gone through hell and now we&amp;apost;ve lost everything,&amp;quot; she said outside the court.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="'ve" lemma="have" stem="'ve" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="gone" lemma="go" stem="gone" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="through" lemma="through" stem="through" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="hell" lemma="hell" stem="hell" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="now" lemma="now" stem="now" pos="RB" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="9" string="we" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="'ve" lemma="have" stem="'ve" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="lost" lemma="lose" stem="lost" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="everything" lemma="everything" stem="everyth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="16" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="outside" lemma="outside" stem="outsid" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="19" string="court" lemma="court" stem="court" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="20" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (S (NP (PRP I)) (VP (VBP 've) (VP (VBN gone) (PP (IN through) (NP (NN hell)))))) (CC and) (S (ADVP (RB now)) (NP (PRP we)) (VP (VBP 've) (VP (VBN lost) (NP (NN everything)))))) (, ,) ('' '') (NP (PRP she)) (VP (VBD said) (PP (IN outside) (NP (DT the) (NN court)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="lost everything" type="VP">
          <tokens>
            <token id="11" string="lost" />
            <token id="12" string="everything" />
          </tokens>
        </chunking>
        <chunking id="2" string="'ve gone through hell" type="VP">
          <tokens>
            <token id="3" string="'ve" />
            <token id="4" string="gone" />
            <token id="5" string="through" />
            <token id="6" string="hell" />
          </tokens>
        </chunking>
        <chunking id="3" string="gone through hell" type="VP">
          <tokens>
            <token id="4" string="gone" />
            <token id="5" string="through" />
            <token id="6" string="hell" />
          </tokens>
        </chunking>
        <chunking id="4" string="I" type="NP">
          <tokens>
            <token id="2" string="I" />
          </tokens>
        </chunking>
        <chunking id="5" string="said outside the court" type="VP">
          <tokens>
            <token id="16" string="said" />
            <token id="17" string="outside" />
            <token id="18" string="the" />
            <token id="19" string="court" />
          </tokens>
        </chunking>
        <chunking id="6" string="'ve lost everything" type="VP">
          <tokens>
            <token id="10" string="'ve" />
            <token id="11" string="lost" />
            <token id="12" string="everything" />
          </tokens>
        </chunking>
        <chunking id="7" string="hell" type="NP">
          <tokens>
            <token id="6" string="hell" />
          </tokens>
        </chunking>
        <chunking id="8" string="we" type="NP">
          <tokens>
            <token id="9" string="we" />
          </tokens>
        </chunking>
        <chunking id="9" string="everything" type="NP">
          <tokens>
            <token id="12" string="everything" />
          </tokens>
        </chunking>
        <chunking id="10" string="she" type="NP">
          <tokens>
            <token id="15" string="she" />
          </tokens>
        </chunking>
        <chunking id="11" string="the court" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="court" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">gone</governor>
          <dependent id="2">I</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="4">gone</governor>
          <dependent id="3">'ve</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="16">said</governor>
          <dependent id="4">gone</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">hell</governor>
          <dependent id="5">through</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">gone</governor>
          <dependent id="6">hell</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">gone</governor>
          <dependent id="7">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">lost</governor>
          <dependent id="8">now</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">lost</governor>
          <dependent id="9">we</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="11">lost</governor>
          <dependent id="10">'ve</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">gone</governor>
          <dependent id="11">lost</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">lost</governor>
          <dependent id="12">everything</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">said</governor>
          <dependent id="15">she</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="16">said</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">court</governor>
          <dependent id="17">outside</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">court</governor>
          <dependent id="18">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">said</governor>
          <dependent id="19">court</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="now" type="DATE" score="0.0">
          <tokens>
            <token id="8" string="now" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="7" has_coreference="true">
      <content>&amp;quot;My concern was for my son and what they&amp;apost;ve done to him . . . because my son would never harm a child.&amp;quot;</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="My" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="concern" lemma="concern" stem="concern" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="my" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="7" string="son" lemma="son" stem="son" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="what" lemma="what" stem="what" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="'ve" lemma="have" stem="'ve" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="done" lemma="do" stem="done" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="him" lemma="he" stem="him" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string=". . ." lemma="..." stem=". . ." pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="because" lemma="because" stem="becaus" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="my" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="son" lemma="son" stem="son" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="would" lemma="would" stem="would" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="never" lemma="never" stem="never" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="harm" lemma="harm" stem="harm" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="child" lemma="child" stem="child" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (PRP$ My) (NN concern)) (VP (VBD was) (UCP (PP (IN for) (NP (PRP$ my) (NN son))) (CC and) (SBAR (WHNP (WP what)) (S (NP (PRP they)) (VP (VBP 've) (VP (VBN done) (PP (TO to) (NP (PRP him))) (: ...) (SBAR (IN because) (S (NP (PRP$ my) (NN son)) (VP (MD would) (ADVP (RB never)) (VP (VB harm) (NP (DT a) (NN child)))))))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="they" type="NP">
          <tokens>
            <token id="10" string="they" />
          </tokens>
        </chunking>
        <chunking id="2" string="done to him ... because my son would never harm a child" type="VP">
          <tokens>
            <token id="12" string="done" />
            <token id="13" string="to" />
            <token id="14" string="him" />
            <token id="15" string=". . ." />
            <token id="16" string="because" />
            <token id="17" string="my" />
            <token id="18" string="son" />
            <token id="19" string="would" />
            <token id="20" string="never" />
            <token id="21" string="harm" />
            <token id="22" string="a" />
            <token id="23" string="child" />
          </tokens>
        </chunking>
        <chunking id="3" string="my son" type="NP">
          <tokens>
            <token id="6" string="my" />
            <token id="7" string="son" />
          </tokens>
        </chunking>
        <chunking id="4" string="what they 've done to him ... because my son would never harm a child" type="SBAR">
          <tokens>
            <token id="9" string="what" />
            <token id="10" string="they" />
            <token id="11" string="'ve" />
            <token id="12" string="done" />
            <token id="13" string="to" />
            <token id="14" string="him" />
            <token id="15" string=". . ." />
            <token id="16" string="because" />
            <token id="17" string="my" />
            <token id="18" string="son" />
            <token id="19" string="would" />
            <token id="20" string="never" />
            <token id="21" string="harm" />
            <token id="22" string="a" />
            <token id="23" string="child" />
          </tokens>
        </chunking>
        <chunking id="5" string="was for my son and what they 've done to him ... because my son would never harm a child" type="VP">
          <tokens>
            <token id="4" string="was" />
            <token id="5" string="for" />
            <token id="6" string="my" />
            <token id="7" string="son" />
            <token id="8" string="and" />
            <token id="9" string="what" />
            <token id="10" string="they" />
            <token id="11" string="'ve" />
            <token id="12" string="done" />
            <token id="13" string="to" />
            <token id="14" string="him" />
            <token id="15" string=". . ." />
            <token id="16" string="because" />
            <token id="17" string="my" />
            <token id="18" string="son" />
            <token id="19" string="would" />
            <token id="20" string="never" />
            <token id="21" string="harm" />
            <token id="22" string="a" />
            <token id="23" string="child" />
          </tokens>
        </chunking>
        <chunking id="6" string="because my son would never harm a child" type="SBAR">
          <tokens>
            <token id="16" string="because" />
            <token id="17" string="my" />
            <token id="18" string="son" />
            <token id="19" string="would" />
            <token id="20" string="never" />
            <token id="21" string="harm" />
            <token id="22" string="a" />
            <token id="23" string="child" />
          </tokens>
        </chunking>
        <chunking id="7" string="a child" type="NP">
          <tokens>
            <token id="22" string="a" />
            <token id="23" string="child" />
          </tokens>
        </chunking>
        <chunking id="8" string="'ve done to him ... because my son would never harm a child" type="VP">
          <tokens>
            <token id="11" string="'ve" />
            <token id="12" string="done" />
            <token id="13" string="to" />
            <token id="14" string="him" />
            <token id="15" string=". . ." />
            <token id="16" string="because" />
            <token id="17" string="my" />
            <token id="18" string="son" />
            <token id="19" string="would" />
            <token id="20" string="never" />
            <token id="21" string="harm" />
            <token id="22" string="a" />
            <token id="23" string="child" />
          </tokens>
        </chunking>
        <chunking id="9" string="him" type="NP">
          <tokens>
            <token id="14" string="him" />
          </tokens>
        </chunking>
        <chunking id="10" string="would never harm a child" type="VP">
          <tokens>
            <token id="19" string="would" />
            <token id="20" string="never" />
            <token id="21" string="harm" />
            <token id="22" string="a" />
            <token id="23" string="child" />
          </tokens>
        </chunking>
        <chunking id="11" string="harm a child" type="VP">
          <tokens>
            <token id="21" string="harm" />
            <token id="22" string="a" />
            <token id="23" string="child" />
          </tokens>
        </chunking>
        <chunking id="12" string="My concern" type="NP">
          <tokens>
            <token id="2" string="My" />
            <token id="3" string="concern" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="3">concern</governor>
          <dependent id="2">My</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">son</governor>
          <dependent id="3">concern</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="7">son</governor>
          <dependent id="4">was</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">son</governor>
          <dependent id="5">for</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="7">son</governor>
          <dependent id="6">my</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">son</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">son</governor>
          <dependent id="8">and</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="12">done</governor>
          <dependent id="9">what</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">done</governor>
          <dependent id="10">they</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="12">done</governor>
          <dependent id="11">'ve</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">son</governor>
          <dependent id="12">done</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">him</governor>
          <dependent id="13">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">done</governor>
          <dependent id="14">him</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="21">harm</governor>
          <dependent id="16">because</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="18">son</governor>
          <dependent id="17">my</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">harm</governor>
          <dependent id="18">son</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="21">harm</governor>
          <dependent id="19">would</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="21">harm</governor>
          <dependent id="20">never</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="12">done</governor>
          <dependent id="21">harm</dependent>
        </dependency>
        <dependency type="det">
          <governor id="23">child</governor>
          <dependent id="22">a</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="21">harm</governor>
          <dependent id="23">child</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="8" has_coreference="true">
      <content>Most jurors said later that they believed the children who testified had been molested, but that the prosecution had failed to prove beyond a reasonable doubt that the Buckeys were culpable.</content>
      <tokens>
        <token id="1" string="Most" lemma="most" stem="most" pos="JJS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="jurors" lemma="juror" stem="juror" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="later" lemma="later" stem="later" pos="RBR" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="believed" lemma="believe" stem="believ" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="testified" lemma="testify" stem="testifi" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="molested" lemma="molest" stem="molest" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="19" string="prosecution" lemma="prosecution" stem="prosecut" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="20" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="failed" lemma="fail" stem="fail" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="prove" lemma="prove" stem="prove" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="beyond" lemma="beyond" stem="beyond" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="reasonable" lemma="reasonable" stem="reason" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="doubt" lemma="doubt" stem="doubt" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="30" string="Buckeys" lemma="Buckeys" stem="buckei" pos="NNPS" type="Word" isStopWord="false" ner="MISC" is_referenced="true" is_refers="false" />
        <token id="31" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="culpable" lemma="culpable" stem="culpabl" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (JJS Most) (NNS jurors)) (VP (VBD said) (ADVP (RBR later)) (SBAR (IN that) (S (NP (PRP they)) (VP (VBD believed) (NP (NP (DT the) (NNS children)) (SBAR (WHNP (WP who)) (S (VP (VBD testified) (SBAR (SBAR (S (VP (VBD had) (VP (VBN been) (VP (VBN molested)))))) (, ,) (CC but) (SBAR (IN that) (S (NP (DT the) (NN prosecution)) (VP (VBD had) (VP (VBN failed) (S (VP (TO to) (VP (VB prove) (PP (IN beyond) (NP (DT a) (JJ reasonable) (NN doubt))) (SBAR (IN that) (S (NP (DT the) (NNPS Buckeys)) (VP (VBD were) (ADJP (JJ culpable))))))))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="believed the children who testified had been molested , but that the prosecution had failed to prove beyond a reasonable doubt that the Buckeys were culpable" type="VP">
          <tokens>
            <token id="7" string="believed" />
            <token id="8" string="the" />
            <token id="9" string="children" />
            <token id="10" string="who" />
            <token id="11" string="testified" />
            <token id="12" string="had" />
            <token id="13" string="been" />
            <token id="14" string="molested" />
            <token id="15" string="," />
            <token id="16" string="but" />
            <token id="17" string="that" />
            <token id="18" string="the" />
            <token id="19" string="prosecution" />
            <token id="20" string="had" />
            <token id="21" string="failed" />
            <token id="22" string="to" />
            <token id="23" string="prove" />
            <token id="24" string="beyond" />
            <token id="25" string="a" />
            <token id="26" string="reasonable" />
            <token id="27" string="doubt" />
            <token id="28" string="that" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
            <token id="31" string="were" />
            <token id="32" string="culpable" />
          </tokens>
        </chunking>
        <chunking id="2" string="were culpable" type="VP">
          <tokens>
            <token id="31" string="were" />
            <token id="32" string="culpable" />
          </tokens>
        </chunking>
        <chunking id="3" string="that the Buckeys were culpable" type="SBAR">
          <tokens>
            <token id="28" string="that" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
            <token id="31" string="were" />
            <token id="32" string="culpable" />
          </tokens>
        </chunking>
        <chunking id="4" string="said later that they believed the children who testified had been molested , but that the prosecution had failed to prove beyond a reasonable doubt that the Buckeys were culpable" type="VP">
          <tokens>
            <token id="3" string="said" />
            <token id="4" string="later" />
            <token id="5" string="that" />
            <token id="6" string="they" />
            <token id="7" string="believed" />
            <token id="8" string="the" />
            <token id="9" string="children" />
            <token id="10" string="who" />
            <token id="11" string="testified" />
            <token id="12" string="had" />
            <token id="13" string="been" />
            <token id="14" string="molested" />
            <token id="15" string="," />
            <token id="16" string="but" />
            <token id="17" string="that" />
            <token id="18" string="the" />
            <token id="19" string="prosecution" />
            <token id="20" string="had" />
            <token id="21" string="failed" />
            <token id="22" string="to" />
            <token id="23" string="prove" />
            <token id="24" string="beyond" />
            <token id="25" string="a" />
            <token id="26" string="reasonable" />
            <token id="27" string="doubt" />
            <token id="28" string="that" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
            <token id="31" string="were" />
            <token id="32" string="culpable" />
          </tokens>
        </chunking>
        <chunking id="5" string="had failed to prove beyond a reasonable doubt that the Buckeys were culpable" type="VP">
          <tokens>
            <token id="20" string="had" />
            <token id="21" string="failed" />
            <token id="22" string="to" />
            <token id="23" string="prove" />
            <token id="24" string="beyond" />
            <token id="25" string="a" />
            <token id="26" string="reasonable" />
            <token id="27" string="doubt" />
            <token id="28" string="that" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
            <token id="31" string="were" />
            <token id="32" string="culpable" />
          </tokens>
        </chunking>
        <chunking id="6" string="had been molested" type="SBAR">
          <tokens>
            <token id="12" string="had" />
            <token id="13" string="been" />
            <token id="14" string="molested" />
          </tokens>
        </chunking>
        <chunking id="7" string="that the prosecution had failed to prove beyond a reasonable doubt that the Buckeys were culpable" type="SBAR">
          <tokens>
            <token id="17" string="that" />
            <token id="18" string="the" />
            <token id="19" string="prosecution" />
            <token id="20" string="had" />
            <token id="21" string="failed" />
            <token id="22" string="to" />
            <token id="23" string="prove" />
            <token id="24" string="beyond" />
            <token id="25" string="a" />
            <token id="26" string="reasonable" />
            <token id="27" string="doubt" />
            <token id="28" string="that" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
            <token id="31" string="were" />
            <token id="32" string="culpable" />
          </tokens>
        </chunking>
        <chunking id="8" string="failed to prove beyond a reasonable doubt that the Buckeys were culpable" type="VP">
          <tokens>
            <token id="21" string="failed" />
            <token id="22" string="to" />
            <token id="23" string="prove" />
            <token id="24" string="beyond" />
            <token id="25" string="a" />
            <token id="26" string="reasonable" />
            <token id="27" string="doubt" />
            <token id="28" string="that" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
            <token id="31" string="were" />
            <token id="32" string="culpable" />
          </tokens>
        </chunking>
        <chunking id="9" string="that they believed the children who testified had been molested , but that the prosecution had failed to prove beyond a reasonable doubt that the Buckeys were culpable" type="SBAR">
          <tokens>
            <token id="5" string="that" />
            <token id="6" string="they" />
            <token id="7" string="believed" />
            <token id="8" string="the" />
            <token id="9" string="children" />
            <token id="10" string="who" />
            <token id="11" string="testified" />
            <token id="12" string="had" />
            <token id="13" string="been" />
            <token id="14" string="molested" />
            <token id="15" string="," />
            <token id="16" string="but" />
            <token id="17" string="that" />
            <token id="18" string="the" />
            <token id="19" string="prosecution" />
            <token id="20" string="had" />
            <token id="21" string="failed" />
            <token id="22" string="to" />
            <token id="23" string="prove" />
            <token id="24" string="beyond" />
            <token id="25" string="a" />
            <token id="26" string="reasonable" />
            <token id="27" string="doubt" />
            <token id="28" string="that" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
            <token id="31" string="were" />
            <token id="32" string="culpable" />
          </tokens>
        </chunking>
        <chunking id="10" string="they" type="NP">
          <tokens>
            <token id="6" string="they" />
          </tokens>
        </chunking>
        <chunking id="11" string="Most jurors" type="NP">
          <tokens>
            <token id="1" string="Most" />
            <token id="2" string="jurors" />
          </tokens>
        </chunking>
        <chunking id="12" string="the children" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="children" />
          </tokens>
        </chunking>
        <chunking id="13" string="been molested" type="VP">
          <tokens>
            <token id="13" string="been" />
            <token id="14" string="molested" />
          </tokens>
        </chunking>
        <chunking id="14" string="who testified had been molested , but that the prosecution had failed to prove beyond a reasonable doubt that the Buckeys were culpable" type="SBAR">
          <tokens>
            <token id="10" string="who" />
            <token id="11" string="testified" />
            <token id="12" string="had" />
            <token id="13" string="been" />
            <token id="14" string="molested" />
            <token id="15" string="," />
            <token id="16" string="but" />
            <token id="17" string="that" />
            <token id="18" string="the" />
            <token id="19" string="prosecution" />
            <token id="20" string="had" />
            <token id="21" string="failed" />
            <token id="22" string="to" />
            <token id="23" string="prove" />
            <token id="24" string="beyond" />
            <token id="25" string="a" />
            <token id="26" string="reasonable" />
            <token id="27" string="doubt" />
            <token id="28" string="that" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
            <token id="31" string="were" />
            <token id="32" string="culpable" />
          </tokens>
        </chunking>
        <chunking id="15" string="testified had been molested , but that the prosecution had failed to prove beyond a reasonable doubt that the Buckeys were culpable" type="VP">
          <tokens>
            <token id="11" string="testified" />
            <token id="12" string="had" />
            <token id="13" string="been" />
            <token id="14" string="molested" />
            <token id="15" string="," />
            <token id="16" string="but" />
            <token id="17" string="that" />
            <token id="18" string="the" />
            <token id="19" string="prosecution" />
            <token id="20" string="had" />
            <token id="21" string="failed" />
            <token id="22" string="to" />
            <token id="23" string="prove" />
            <token id="24" string="beyond" />
            <token id="25" string="a" />
            <token id="26" string="reasonable" />
            <token id="27" string="doubt" />
            <token id="28" string="that" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
            <token id="31" string="were" />
            <token id="32" string="culpable" />
          </tokens>
        </chunking>
        <chunking id="16" string="to prove beyond a reasonable doubt that the Buckeys were culpable" type="VP">
          <tokens>
            <token id="22" string="to" />
            <token id="23" string="prove" />
            <token id="24" string="beyond" />
            <token id="25" string="a" />
            <token id="26" string="reasonable" />
            <token id="27" string="doubt" />
            <token id="28" string="that" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
            <token id="31" string="were" />
            <token id="32" string="culpable" />
          </tokens>
        </chunking>
        <chunking id="17" string="the Buckeys" type="NP">
          <tokens>
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
          </tokens>
        </chunking>
        <chunking id="18" string="prove beyond a reasonable doubt that the Buckeys were culpable" type="VP">
          <tokens>
            <token id="23" string="prove" />
            <token id="24" string="beyond" />
            <token id="25" string="a" />
            <token id="26" string="reasonable" />
            <token id="27" string="doubt" />
            <token id="28" string="that" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
            <token id="31" string="were" />
            <token id="32" string="culpable" />
          </tokens>
        </chunking>
        <chunking id="19" string="had been molested , but that the prosecution had failed to prove beyond a reasonable doubt that the Buckeys were culpable" type="SBAR">
          <tokens>
            <token id="12" string="had" />
            <token id="13" string="been" />
            <token id="14" string="molested" />
            <token id="15" string="," />
            <token id="16" string="but" />
            <token id="17" string="that" />
            <token id="18" string="the" />
            <token id="19" string="prosecution" />
            <token id="20" string="had" />
            <token id="21" string="failed" />
            <token id="22" string="to" />
            <token id="23" string="prove" />
            <token id="24" string="beyond" />
            <token id="25" string="a" />
            <token id="26" string="reasonable" />
            <token id="27" string="doubt" />
            <token id="28" string="that" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
            <token id="31" string="were" />
            <token id="32" string="culpable" />
          </tokens>
        </chunking>
        <chunking id="20" string="the prosecution" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="prosecution" />
          </tokens>
        </chunking>
        <chunking id="21" string="a reasonable doubt" type="NP">
          <tokens>
            <token id="25" string="a" />
            <token id="26" string="reasonable" />
            <token id="27" string="doubt" />
          </tokens>
        </chunking>
        <chunking id="22" string="the children who testified had been molested , but that the prosecution had failed to prove beyond a reasonable doubt that the Buckeys were culpable" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="children" />
            <token id="10" string="who" />
            <token id="11" string="testified" />
            <token id="12" string="had" />
            <token id="13" string="been" />
            <token id="14" string="molested" />
            <token id="15" string="," />
            <token id="16" string="but" />
            <token id="17" string="that" />
            <token id="18" string="the" />
            <token id="19" string="prosecution" />
            <token id="20" string="had" />
            <token id="21" string="failed" />
            <token id="22" string="to" />
            <token id="23" string="prove" />
            <token id="24" string="beyond" />
            <token id="25" string="a" />
            <token id="26" string="reasonable" />
            <token id="27" string="doubt" />
            <token id="28" string="that" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
            <token id="31" string="were" />
            <token id="32" string="culpable" />
          </tokens>
        </chunking>
        <chunking id="23" string="molested" type="VP">
          <tokens>
            <token id="14" string="molested" />
          </tokens>
        </chunking>
        <chunking id="24" string="culpable" type="ADJP">
          <tokens>
            <token id="32" string="culpable" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="amod">
          <governor id="2">jurors</governor>
          <dependent id="1">Most</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">said</governor>
          <dependent id="2">jurors</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">said</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="3">said</governor>
          <dependent id="4">later</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="7">believed</governor>
          <dependent id="5">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">believed</governor>
          <dependent id="6">they</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">said</governor>
          <dependent id="7">believed</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">children</governor>
          <dependent id="8">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">believed</governor>
          <dependent id="9">children</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">testified</governor>
          <dependent id="10">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="9">children</governor>
          <dependent id="11">testified</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="14">molested</governor>
          <dependent id="12">had</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="14">molested</governor>
          <dependent id="13">been</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="11">testified</governor>
          <dependent id="14">molested</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="14">molested</governor>
          <dependent id="16">but</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="21">failed</governor>
          <dependent id="17">that</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">prosecution</governor>
          <dependent id="18">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">failed</governor>
          <dependent id="19">prosecution</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="21">failed</governor>
          <dependent id="20">had</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">molested</governor>
          <dependent id="21">failed</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="23">prove</governor>
          <dependent id="22">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="21">failed</governor>
          <dependent id="23">prove</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">doubt</governor>
          <dependent id="24">beyond</dependent>
        </dependency>
        <dependency type="det">
          <governor id="27">doubt</governor>
          <dependent id="25">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="27">doubt</governor>
          <dependent id="26">reasonable</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">prove</governor>
          <dependent id="27">doubt</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="32">culpable</governor>
          <dependent id="28">that</dependent>
        </dependency>
        <dependency type="det">
          <governor id="30">Buckeys</governor>
          <dependent id="29">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="32">culpable</governor>
          <dependent id="30">Buckeys</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="32">culpable</governor>
          <dependent id="31">were</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="23">prove</governor>
          <dependent id="32">culpable</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Buckeys" type="MISC" score="0.0">
          <tokens>
            <token id="30" string="Buckeys" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="9" has_coreference="true">
      <content>&amp;quot;I am not convinced that he (Ray Buckey) is innocent,&amp;quot; said juror Sally Cordova, 27, a supermarket checker.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="am" lemma="be" stem="am" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="convinced" lemma="convince" stem="convinc" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="(" lemma="-lrb-" stem="(" pos="-LRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="10" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="11" string=")" lemma="-rrb-" stem=")" pos="-RRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="innocent" lemma="innocent" stem="innoc" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="juror" lemma="juror" stem="juror" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="Sally" lemma="Sally" stem="salli" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="19" string="Cordova" lemma="Cordova" stem="cordova" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="27" lemma="27" stem="27" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="22" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="supermarket" lemma="supermarket" stem="supermarket" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="checker" lemma="checker" stem="checker" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (SINV (`` ``) (S (NP (PRP I)) (VP (VBP am) (RB not) (ADJP (VBN convinced) (SBAR (IN that) (S (NP (PRP he) (PRN (-LRB- -LRB-) (NP (NNP Ray) (NNP Buckey)) (-RRB- -RRB-))) (VP (VBZ is) (ADJP (JJ innocent)))))))) (, ,) ('' '') (VP (VBD said) (NP (NN juror))) (NP (NP (NNP Sally) (NNP Cordova)) (, ,) (NP (NP (CD 27)) (, ,) (NP (DT a) (NN supermarket) (NN checker)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="is innocent" type="VP">
          <tokens>
            <token id="12" string="is" />
            <token id="13" string="innocent" />
          </tokens>
        </chunking>
        <chunking id="2" string="27" type="NP">
          <tokens>
            <token id="21" string="27" />
          </tokens>
        </chunking>
        <chunking id="3" string="innocent" type="ADJP">
          <tokens>
            <token id="13" string="innocent" />
          </tokens>
        </chunking>
        <chunking id="4" string="juror" type="NP">
          <tokens>
            <token id="17" string="juror" />
          </tokens>
        </chunking>
        <chunking id="5" string="Sally Cordova" type="NP">
          <tokens>
            <token id="18" string="Sally" />
            <token id="19" string="Cordova" />
          </tokens>
        </chunking>
        <chunking id="6" string="I" type="NP">
          <tokens>
            <token id="2" string="I" />
          </tokens>
        </chunking>
        <chunking id="7" string="he -LRB- Ray Buckey -RRB-" type="NP">
          <tokens>
            <token id="7" string="he" />
            <token id="8" string="(" />
            <token id="9" string="Ray" />
            <token id="10" string="Buckey" />
            <token id="11" string=")" />
          </tokens>
        </chunking>
        <chunking id="8" string="said juror" type="VP">
          <tokens>
            <token id="16" string="said" />
            <token id="17" string="juror" />
          </tokens>
        </chunking>
        <chunking id="9" string="a supermarket checker" type="NP">
          <tokens>
            <token id="23" string="a" />
            <token id="24" string="supermarket" />
            <token id="25" string="checker" />
          </tokens>
        </chunking>
        <chunking id="10" string="Ray Buckey" type="NP">
          <tokens>
            <token id="9" string="Ray" />
            <token id="10" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="11" string="convinced that he -LRB- Ray Buckey -RRB- is innocent" type="ADJP">
          <tokens>
            <token id="5" string="convinced" />
            <token id="6" string="that" />
            <token id="7" string="he" />
            <token id="8" string="(" />
            <token id="9" string="Ray" />
            <token id="10" string="Buckey" />
            <token id="11" string=")" />
            <token id="12" string="is" />
            <token id="13" string="innocent" />
          </tokens>
        </chunking>
        <chunking id="12" string="that he -LRB- Ray Buckey -RRB- is innocent" type="SBAR">
          <tokens>
            <token id="6" string="that" />
            <token id="7" string="he" />
            <token id="8" string="(" />
            <token id="9" string="Ray" />
            <token id="10" string="Buckey" />
            <token id="11" string=")" />
            <token id="12" string="is" />
            <token id="13" string="innocent" />
          </tokens>
        </chunking>
        <chunking id="13" string="am not convinced that he -LRB- Ray Buckey -RRB- is innocent" type="VP">
          <tokens>
            <token id="3" string="am" />
            <token id="4" string="not" />
            <token id="5" string="convinced" />
            <token id="6" string="that" />
            <token id="7" string="he" />
            <token id="8" string="(" />
            <token id="9" string="Ray" />
            <token id="10" string="Buckey" />
            <token id="11" string=")" />
            <token id="12" string="is" />
            <token id="13" string="innocent" />
          </tokens>
        </chunking>
        <chunking id="14" string="Sally Cordova , 27 , a supermarket checker" type="NP">
          <tokens>
            <token id="18" string="Sally" />
            <token id="19" string="Cordova" />
            <token id="20" string="," />
            <token id="21" string="27" />
            <token id="22" string="," />
            <token id="23" string="a" />
            <token id="24" string="supermarket" />
            <token id="25" string="checker" />
          </tokens>
        </chunking>
        <chunking id="15" string="27 , a supermarket checker" type="NP">
          <tokens>
            <token id="21" string="27" />
            <token id="22" string="," />
            <token id="23" string="a" />
            <token id="24" string="supermarket" />
            <token id="25" string="checker" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="5">convinced</governor>
          <dependent id="2">I</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="5">convinced</governor>
          <dependent id="3">am</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="5">convinced</governor>
          <dependent id="4">not</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="16">said</governor>
          <dependent id="5">convinced</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="13">innocent</governor>
          <dependent id="6">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="13">innocent</governor>
          <dependent id="7">he</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">Buckey</governor>
          <dependent id="9">Ray</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="7">he</governor>
          <dependent id="10">Buckey</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="13">innocent</governor>
          <dependent id="12">is</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="5">convinced</governor>
          <dependent id="13">innocent</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="16">said</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="16">said</governor>
          <dependent id="17">juror</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">Cordova</governor>
          <dependent id="18">Sally</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">said</governor>
          <dependent id="19">Cordova</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="19">Cordova</governor>
          <dependent id="21">27</dependent>
        </dependency>
        <dependency type="det">
          <governor id="25">checker</governor>
          <dependent id="23">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">checker</governor>
          <dependent id="24">supermarket</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="21">27</governor>
          <dependent id="25">checker</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="27" type="NUMBER" score="0.0">
          <tokens>
            <token id="21" string="27" />
          </tokens>
        </entity>
        <entity id="2" string="Sally Cordova" type="PERSON" score="0.0">
          <tokens>
            <token id="18" string="Sally" />
            <token id="19" string="Cordova" />
          </tokens>
        </entity>
        <entity id="3" string="Ray Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="9" string="Ray" />
            <token id="10" string="Buckey" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="10" has_coreference="true">
      <content>&amp;quot;But it was not proven to me that he did it.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="But" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="proven" lemma="prove" stem="proven" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="me" lemma="I" stem="me" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="did" lemma="do" stem="did" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (CC But) (NP (PRP it)) (VP (VBD was) (RB not) (VP (VBN proven) (PP (TO to) (NP (PRP me))) (SBAR (IN that) (S (NP (PRP he)) (VP (VBD did) (NP (PRP it))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="did it" type="VP">
          <tokens>
            <token id="11" string="did" />
            <token id="12" string="it" />
          </tokens>
        </chunking>
        <chunking id="2" string="was not proven to me that he did it" type="VP">
          <tokens>
            <token id="4" string="was" />
            <token id="5" string="not" />
            <token id="6" string="proven" />
            <token id="7" string="to" />
            <token id="8" string="me" />
            <token id="9" string="that" />
            <token id="10" string="he" />
            <token id="11" string="did" />
            <token id="12" string="it" />
          </tokens>
        </chunking>
        <chunking id="3" string="me" type="NP">
          <tokens>
            <token id="8" string="me" />
          </tokens>
        </chunking>
        <chunking id="4" string="it" type="NP">
          <tokens>
            <token id="3" string="it" />
          </tokens>
        </chunking>
        <chunking id="5" string="that he did it" type="SBAR">
          <tokens>
            <token id="9" string="that" />
            <token id="10" string="he" />
            <token id="11" string="did" />
            <token id="12" string="it" />
          </tokens>
        </chunking>
        <chunking id="6" string="he" type="NP">
          <tokens>
            <token id="10" string="he" />
          </tokens>
        </chunking>
        <chunking id="7" string="proven to me that he did it" type="VP">
          <tokens>
            <token id="6" string="proven" />
            <token id="7" string="to" />
            <token id="8" string="me" />
            <token id="9" string="that" />
            <token id="10" string="he" />
            <token id="11" string="did" />
            <token id="12" string="it" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="cc">
          <governor id="6">proven</governor>
          <dependent id="2">But</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="6">proven</governor>
          <dependent id="3">it</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="6">proven</governor>
          <dependent id="4">was</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="6">proven</governor>
          <dependent id="5">not</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">proven</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">me</governor>
          <dependent id="7">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">proven</governor>
          <dependent id="8">me</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="11">did</governor>
          <dependent id="9">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">did</governor>
          <dependent id="10">he</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="6">proven</governor>
          <dependent id="11">did</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">did</governor>
          <dependent id="12">it</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="11" has_coreference="true">
      <content>Whether I believe he did it and whether it was proven are very different.&amp;quot;</content>
      <tokens>
        <token id="1" string="Whether" lemma="whether" stem="whether" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="believe" lemma="believe" stem="believ" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="did" lemma="do" stem="did" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="whether" lemma="whether" stem="whether" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="proven" lemma="prove" stem="proven" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="are" lemma="be" stem="ar" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="very" lemma="very" stem="veri" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="different" lemma="different" stem="differ" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (SBAR (IN Whether) (S (NP (PRP I)) (VP (VBP believe) (SBAR (SBAR (S (NP (PRP he)) (VP (VBD did) (NP (PRP it))))) (CC and) (SBAR (IN whether) (S (NP (PRP it)) (VP (VBD was) (VP (VBN proven))))))))) (VP (VBP are) (ADJP (RB very) (JJ different))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="he did it and whether it was proven" type="SBAR">
          <tokens>
            <token id="4" string="he" />
            <token id="5" string="did" />
            <token id="6" string="it" />
            <token id="7" string="and" />
            <token id="8" string="whether" />
            <token id="9" string="it" />
            <token id="10" string="was" />
            <token id="11" string="proven" />
          </tokens>
        </chunking>
        <chunking id="2" string="are very different" type="VP">
          <tokens>
            <token id="12" string="are" />
            <token id="13" string="very" />
            <token id="14" string="different" />
          </tokens>
        </chunking>
        <chunking id="3" string="whether it was proven" type="SBAR">
          <tokens>
            <token id="8" string="whether" />
            <token id="9" string="it" />
            <token id="10" string="was" />
            <token id="11" string="proven" />
          </tokens>
        </chunking>
        <chunking id="4" string="I" type="NP">
          <tokens>
            <token id="2" string="I" />
          </tokens>
        </chunking>
        <chunking id="5" string="proven" type="VP">
          <tokens>
            <token id="11" string="proven" />
          </tokens>
        </chunking>
        <chunking id="6" string="it" type="NP">
          <tokens>
            <token id="6" string="it" />
          </tokens>
        </chunking>
        <chunking id="7" string="he did it" type="SBAR">
          <tokens>
            <token id="4" string="he" />
            <token id="5" string="did" />
            <token id="6" string="it" />
          </tokens>
        </chunking>
        <chunking id="8" string="Whether I believe he did it and whether it was proven" type="SBAR">
          <tokens>
            <token id="1" string="Whether" />
            <token id="2" string="I" />
            <token id="3" string="believe" />
            <token id="4" string="he" />
            <token id="5" string="did" />
            <token id="6" string="it" />
            <token id="7" string="and" />
            <token id="8" string="whether" />
            <token id="9" string="it" />
            <token id="10" string="was" />
            <token id="11" string="proven" />
          </tokens>
        </chunking>
        <chunking id="9" string="did it" type="VP">
          <tokens>
            <token id="5" string="did" />
            <token id="6" string="it" />
          </tokens>
        </chunking>
        <chunking id="10" string="was proven" type="VP">
          <tokens>
            <token id="10" string="was" />
            <token id="11" string="proven" />
          </tokens>
        </chunking>
        <chunking id="11" string="believe he did it and whether it was proven" type="VP">
          <tokens>
            <token id="3" string="believe" />
            <token id="4" string="he" />
            <token id="5" string="did" />
            <token id="6" string="it" />
            <token id="7" string="and" />
            <token id="8" string="whether" />
            <token id="9" string="it" />
            <token id="10" string="was" />
            <token id="11" string="proven" />
          </tokens>
        </chunking>
        <chunking id="12" string="very different" type="ADJP">
          <tokens>
            <token id="13" string="very" />
            <token id="14" string="different" />
          </tokens>
        </chunking>
        <chunking id="13" string="he" type="NP">
          <tokens>
            <token id="4" string="he" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="mark">
          <governor id="3">believe</governor>
          <dependent id="1">Whether</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">believe</governor>
          <dependent id="2">I</dependent>
        </dependency>
        <dependency type="csubj">
          <governor id="14">different</governor>
          <dependent id="3">believe</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">did</governor>
          <dependent id="4">he</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">believe</governor>
          <dependent id="5">did</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">did</governor>
          <dependent id="6">it</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="5">did</governor>
          <dependent id="7">and</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="11">proven</governor>
          <dependent id="8">whether</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="11">proven</governor>
          <dependent id="9">it</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="11">proven</governor>
          <dependent id="10">was</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="5">did</governor>
          <dependent id="11">proven</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="14">different</governor>
          <dependent id="12">are</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="14">different</governor>
          <dependent id="13">very</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="14">different</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="12" has_coreference="true">
      <content>The end of the case came in a tense and crowded courtroom.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="end" lemma="end" stem="end" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="came" lemma="come" stem="came" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="tense" lemma="tense" stem="tens" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="crowded" lemma="crowded" stem="crowd" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="courtroom" lemma="courtroom" stem="courtroom" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT The) (NN end)) (PP (IN of) (NP (DT the) (NN case)))) (VP (VBD came) (PP (IN in) (NP (DT a) (JJ tense) (CC and) (JJ crowded) (NN courtroom)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="The end of the case" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="end" />
            <token id="3" string="of" />
            <token id="4" string="the" />
            <token id="5" string="case" />
          </tokens>
        </chunking>
        <chunking id="2" string="the case" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="case" />
          </tokens>
        </chunking>
        <chunking id="3" string="The end" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="end" />
          </tokens>
        </chunking>
        <chunking id="4" string="came in a tense and crowded courtroom" type="VP">
          <tokens>
            <token id="6" string="came" />
            <token id="7" string="in" />
            <token id="8" string="a" />
            <token id="9" string="tense" />
            <token id="10" string="and" />
            <token id="11" string="crowded" />
            <token id="12" string="courtroom" />
          </tokens>
        </chunking>
        <chunking id="5" string="a tense and crowded courtroom" type="NP">
          <tokens>
            <token id="8" string="a" />
            <token id="9" string="tense" />
            <token id="10" string="and" />
            <token id="11" string="crowded" />
            <token id="12" string="courtroom" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">end</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">came</governor>
          <dependent id="2">end</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">case</governor>
          <dependent id="3">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">case</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">end</governor>
          <dependent id="5">case</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">came</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">courtroom</governor>
          <dependent id="7">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">courtroom</governor>
          <dependent id="8">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">courtroom</governor>
          <dependent id="9">tense</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="9">tense</governor>
          <dependent id="10">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="9">tense</governor>
          <dependent id="11">crowded</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">came</governor>
          <dependent id="12">courtroom</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="13" has_coreference="true">
      <content>The Buckeys, who spent years in jail as the case progressed through the judicial system, had sat silently, staring straight ahead, as the jury filed into the room shortly before 10:30 a.m. and the gallery of reporters and spectators grew quiet in anticipation.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="Buckeys" lemma="Buckeys" stem="buckei" pos="NNPS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="spent" lemma="spend" stem="spent" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="6" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="true" />
        <token id="7" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="jail" lemma="jail" stem="jail" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="9" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="12" string="progressed" lemma="progress" stem="progress" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="13" string="through" lemma="through" stem="through" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="15" string="judicial" lemma="judicial" stem="judici" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="16" string="system" lemma="system" stem="system" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="17" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="sat" lemma="sit" stem="sat" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="silently" lemma="silently" stem="silent" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="staring" lemma="stare" stem="stare" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="straight" lemma="straight" stem="straight" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="ahead" lemma="ahead" stem="ahead" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="filed" lemma="file" stem="file" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="into" lemma="into" stem="into" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="room" lemma="room" stem="room" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="shortly" lemma="shortly" stem="shortli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="before" lemma="before" stem="befor" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="10:30" lemma="10:30" stem="10:30" pos="CD" type="Word" isStopWord="false" ner="TIME" is_referenced="false" is_refers="false" />
        <token id="36" string="a.m." lemma="a.m." stem="a.m." pos="NN" type="Word" isStopWord="false" ner="TIME" is_referenced="false" is_refers="false" />
        <token id="37" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="39" string="gallery" lemma="gallery" stem="galleri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="40" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="41" string="reporters" lemma="reporter" stem="report" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="42" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="43" string="spectators" lemma="spectator" stem="spectat" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="44" string="grew" lemma="grow" stem="grew" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="45" string="quiet" lemma="quiet" stem="quiet" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="46" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="47" string="anticipation" lemma="anticipation" stem="anticip" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="48" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT The) (NNPS Buckeys)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBD spent) (NP (NNS years)) (PP (IN in) (NP (NN jail))) (SBAR (IN as) (S (NP (DT the) (NN case)) (VP (VBD progressed) (PP (IN through) (NP (DT the) (JJ judicial) (NN system))))))))) (, ,)) (VP (VBD had) (VP (VBN sat) (ADVP (RB silently)) (, ,) (S (VP (VBG staring) (ADVP (RB straight)) (ADVP (RB ahead)) (, ,) (SBAR (IN as) (S (NP (DT the) (NN jury)) (VP (VBD filed) (PP (IN into) (NP (DT the) (NN room))) (ADVP (RB shortly)) (SBAR (IN before) (S (NP (NP (CD 10:30) (NN a.m.)) (CC and) (NP (NP (DT the) (NN gallery)) (PP (IN of) (NP (NNS reporters) (CC and) (NNS spectators))))) (VP (VBD grew) (ADJP (JJ quiet)) (PP (IN in) (NP (NN anticipation))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the gallery of reporters and spectators" type="NP">
          <tokens>
            <token id="38" string="the" />
            <token id="39" string="gallery" />
            <token id="40" string="of" />
            <token id="41" string="reporters" />
            <token id="42" string="and" />
            <token id="43" string="spectators" />
          </tokens>
        </chunking>
        <chunking id="2" string="the case" type="NP">
          <tokens>
            <token id="10" string="the" />
            <token id="11" string="case" />
          </tokens>
        </chunking>
        <chunking id="3" string="The Buckeys , who spent years in jail as the case progressed through the judicial system ," type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Buckeys" />
            <token id="3" string="," />
            <token id="4" string="who" />
            <token id="5" string="spent" />
            <token id="6" string="years" />
            <token id="7" string="in" />
            <token id="8" string="jail" />
            <token id="9" string="as" />
            <token id="10" string="the" />
            <token id="11" string="case" />
            <token id="12" string="progressed" />
            <token id="13" string="through" />
            <token id="14" string="the" />
            <token id="15" string="judicial" />
            <token id="16" string="system" />
            <token id="17" string="," />
          </tokens>
        </chunking>
        <chunking id="4" string="jail" type="NP">
          <tokens>
            <token id="8" string="jail" />
          </tokens>
        </chunking>
        <chunking id="5" string="10:30 a.m. and the gallery of reporters and spectators" type="NP">
          <tokens>
            <token id="35" string="10:30" />
            <token id="36" string="a.m." />
            <token id="37" string="and" />
            <token id="38" string="the" />
            <token id="39" string="gallery" />
            <token id="40" string="of" />
            <token id="41" string="reporters" />
            <token id="42" string="and" />
            <token id="43" string="spectators" />
          </tokens>
        </chunking>
        <chunking id="6" string="filed into the room shortly before 10:30 a.m. and the gallery of reporters and spectators grew quiet in anticipation" type="VP">
          <tokens>
            <token id="29" string="filed" />
            <token id="30" string="into" />
            <token id="31" string="the" />
            <token id="32" string="room" />
            <token id="33" string="shortly" />
            <token id="34" string="before" />
            <token id="35" string="10:30" />
            <token id="36" string="a.m." />
            <token id="37" string="and" />
            <token id="38" string="the" />
            <token id="39" string="gallery" />
            <token id="40" string="of" />
            <token id="41" string="reporters" />
            <token id="42" string="and" />
            <token id="43" string="spectators" />
            <token id="44" string="grew" />
            <token id="45" string="quiet" />
            <token id="46" string="in" />
            <token id="47" string="anticipation" />
          </tokens>
        </chunking>
        <chunking id="7" string="spent years in jail as the case progressed through the judicial system" type="VP">
          <tokens>
            <token id="5" string="spent" />
            <token id="6" string="years" />
            <token id="7" string="in" />
            <token id="8" string="jail" />
            <token id="9" string="as" />
            <token id="10" string="the" />
            <token id="11" string="case" />
            <token id="12" string="progressed" />
            <token id="13" string="through" />
            <token id="14" string="the" />
            <token id="15" string="judicial" />
            <token id="16" string="system" />
          </tokens>
        </chunking>
        <chunking id="8" string="progressed through the judicial system" type="VP">
          <tokens>
            <token id="12" string="progressed" />
            <token id="13" string="through" />
            <token id="14" string="the" />
            <token id="15" string="judicial" />
            <token id="16" string="system" />
          </tokens>
        </chunking>
        <chunking id="9" string="had sat silently , staring straight ahead , as the jury filed into the room shortly before 10:30 a.m. and the gallery of reporters and spectators grew quiet in anticipation" type="VP">
          <tokens>
            <token id="18" string="had" />
            <token id="19" string="sat" />
            <token id="20" string="silently" />
            <token id="21" string="," />
            <token id="22" string="staring" />
            <token id="23" string="straight" />
            <token id="24" string="ahead" />
            <token id="25" string="," />
            <token id="26" string="as" />
            <token id="27" string="the" />
            <token id="28" string="jury" />
            <token id="29" string="filed" />
            <token id="30" string="into" />
            <token id="31" string="the" />
            <token id="32" string="room" />
            <token id="33" string="shortly" />
            <token id="34" string="before" />
            <token id="35" string="10:30" />
            <token id="36" string="a.m." />
            <token id="37" string="and" />
            <token id="38" string="the" />
            <token id="39" string="gallery" />
            <token id="40" string="of" />
            <token id="41" string="reporters" />
            <token id="42" string="and" />
            <token id="43" string="spectators" />
            <token id="44" string="grew" />
            <token id="45" string="quiet" />
            <token id="46" string="in" />
            <token id="47" string="anticipation" />
          </tokens>
        </chunking>
        <chunking id="10" string="sat silently , staring straight ahead , as the jury filed into the room shortly before 10:30 a.m. and the gallery of reporters and spectators grew quiet in anticipation" type="VP">
          <tokens>
            <token id="19" string="sat" />
            <token id="20" string="silently" />
            <token id="21" string="," />
            <token id="22" string="staring" />
            <token id="23" string="straight" />
            <token id="24" string="ahead" />
            <token id="25" string="," />
            <token id="26" string="as" />
            <token id="27" string="the" />
            <token id="28" string="jury" />
            <token id="29" string="filed" />
            <token id="30" string="into" />
            <token id="31" string="the" />
            <token id="32" string="room" />
            <token id="33" string="shortly" />
            <token id="34" string="before" />
            <token id="35" string="10:30" />
            <token id="36" string="a.m." />
            <token id="37" string="and" />
            <token id="38" string="the" />
            <token id="39" string="gallery" />
            <token id="40" string="of" />
            <token id="41" string="reporters" />
            <token id="42" string="and" />
            <token id="43" string="spectators" />
            <token id="44" string="grew" />
            <token id="45" string="quiet" />
            <token id="46" string="in" />
            <token id="47" string="anticipation" />
          </tokens>
        </chunking>
        <chunking id="11" string="before 10:30 a.m. and the gallery of reporters and spectators grew quiet in anticipation" type="SBAR">
          <tokens>
            <token id="34" string="before" />
            <token id="35" string="10:30" />
            <token id="36" string="a.m." />
            <token id="37" string="and" />
            <token id="38" string="the" />
            <token id="39" string="gallery" />
            <token id="40" string="of" />
            <token id="41" string="reporters" />
            <token id="42" string="and" />
            <token id="43" string="spectators" />
            <token id="44" string="grew" />
            <token id="45" string="quiet" />
            <token id="46" string="in" />
            <token id="47" string="anticipation" />
          </tokens>
        </chunking>
        <chunking id="12" string="as the case progressed through the judicial system" type="SBAR">
          <tokens>
            <token id="9" string="as" />
            <token id="10" string="the" />
            <token id="11" string="case" />
            <token id="12" string="progressed" />
            <token id="13" string="through" />
            <token id="14" string="the" />
            <token id="15" string="judicial" />
            <token id="16" string="system" />
          </tokens>
        </chunking>
        <chunking id="13" string="the jury" type="NP">
          <tokens>
            <token id="27" string="the" />
            <token id="28" string="jury" />
          </tokens>
        </chunking>
        <chunking id="14" string="the judicial system" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="judicial" />
            <token id="16" string="system" />
          </tokens>
        </chunking>
        <chunking id="15" string="grew quiet in anticipation" type="VP">
          <tokens>
            <token id="44" string="grew" />
            <token id="45" string="quiet" />
            <token id="46" string="in" />
            <token id="47" string="anticipation" />
          </tokens>
        </chunking>
        <chunking id="16" string="who spent years in jail as the case progressed through the judicial system" type="SBAR">
          <tokens>
            <token id="4" string="who" />
            <token id="5" string="spent" />
            <token id="6" string="years" />
            <token id="7" string="in" />
            <token id="8" string="jail" />
            <token id="9" string="as" />
            <token id="10" string="the" />
            <token id="11" string="case" />
            <token id="12" string="progressed" />
            <token id="13" string="through" />
            <token id="14" string="the" />
            <token id="15" string="judicial" />
            <token id="16" string="system" />
          </tokens>
        </chunking>
        <chunking id="17" string="10:30 a.m." type="NP">
          <tokens>
            <token id="35" string="10:30" />
            <token id="36" string="a.m." />
          </tokens>
        </chunking>
        <chunking id="18" string="the gallery" type="NP">
          <tokens>
            <token id="38" string="the" />
            <token id="39" string="gallery" />
          </tokens>
        </chunking>
        <chunking id="19" string="the room" type="NP">
          <tokens>
            <token id="31" string="the" />
            <token id="32" string="room" />
          </tokens>
        </chunking>
        <chunking id="20" string="years" type="NP">
          <tokens>
            <token id="6" string="years" />
          </tokens>
        </chunking>
        <chunking id="21" string="as the jury filed into the room shortly before 10:30 a.m. and the gallery of reporters and spectators grew quiet in anticipation" type="SBAR">
          <tokens>
            <token id="26" string="as" />
            <token id="27" string="the" />
            <token id="28" string="jury" />
            <token id="29" string="filed" />
            <token id="30" string="into" />
            <token id="31" string="the" />
            <token id="32" string="room" />
            <token id="33" string="shortly" />
            <token id="34" string="before" />
            <token id="35" string="10:30" />
            <token id="36" string="a.m." />
            <token id="37" string="and" />
            <token id="38" string="the" />
            <token id="39" string="gallery" />
            <token id="40" string="of" />
            <token id="41" string="reporters" />
            <token id="42" string="and" />
            <token id="43" string="spectators" />
            <token id="44" string="grew" />
            <token id="45" string="quiet" />
            <token id="46" string="in" />
            <token id="47" string="anticipation" />
          </tokens>
        </chunking>
        <chunking id="22" string="staring straight ahead , as the jury filed into the room shortly before 10:30 a.m. and the gallery of reporters and spectators grew quiet in anticipation" type="VP">
          <tokens>
            <token id="22" string="staring" />
            <token id="23" string="straight" />
            <token id="24" string="ahead" />
            <token id="25" string="," />
            <token id="26" string="as" />
            <token id="27" string="the" />
            <token id="28" string="jury" />
            <token id="29" string="filed" />
            <token id="30" string="into" />
            <token id="31" string="the" />
            <token id="32" string="room" />
            <token id="33" string="shortly" />
            <token id="34" string="before" />
            <token id="35" string="10:30" />
            <token id="36" string="a.m." />
            <token id="37" string="and" />
            <token id="38" string="the" />
            <token id="39" string="gallery" />
            <token id="40" string="of" />
            <token id="41" string="reporters" />
            <token id="42" string="and" />
            <token id="43" string="spectators" />
            <token id="44" string="grew" />
            <token id="45" string="quiet" />
            <token id="46" string="in" />
            <token id="47" string="anticipation" />
          </tokens>
        </chunking>
        <chunking id="23" string="anticipation" type="NP">
          <tokens>
            <token id="47" string="anticipation" />
          </tokens>
        </chunking>
        <chunking id="24" string="reporters and spectators" type="NP">
          <tokens>
            <token id="41" string="reporters" />
            <token id="42" string="and" />
            <token id="43" string="spectators" />
          </tokens>
        </chunking>
        <chunking id="25" string="The Buckeys" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Buckeys" />
          </tokens>
        </chunking>
        <chunking id="26" string="quiet" type="ADJP">
          <tokens>
            <token id="45" string="quiet" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">Buckeys</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">sat</governor>
          <dependent id="2">Buckeys</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">spent</governor>
          <dependent id="4">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="2">Buckeys</governor>
          <dependent id="5">spent</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="5">spent</governor>
          <dependent id="6">years</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">jail</governor>
          <dependent id="7">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">spent</governor>
          <dependent id="8">jail</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="12">progressed</governor>
          <dependent id="9">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">case</governor>
          <dependent id="10">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">progressed</governor>
          <dependent id="11">case</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="5">spent</governor>
          <dependent id="12">progressed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">system</governor>
          <dependent id="13">through</dependent>
        </dependency>
        <dependency type="det">
          <governor id="16">system</governor>
          <dependent id="14">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="16">system</governor>
          <dependent id="15">judicial</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">progressed</governor>
          <dependent id="16">system</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="19">sat</governor>
          <dependent id="18">had</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="19">sat</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="19">sat</governor>
          <dependent id="20">silently</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="19">sat</governor>
          <dependent id="22">staring</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="22">staring</governor>
          <dependent id="23">straight</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="22">staring</governor>
          <dependent id="24">ahead</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="29">filed</governor>
          <dependent id="26">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="28">jury</governor>
          <dependent id="27">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="29">filed</governor>
          <dependent id="28">jury</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="22">staring</governor>
          <dependent id="29">filed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="32">room</governor>
          <dependent id="30">into</dependent>
        </dependency>
        <dependency type="det">
          <governor id="32">room</governor>
          <dependent id="31">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="29">filed</governor>
          <dependent id="32">room</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="29">filed</governor>
          <dependent id="33">shortly</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="44">grew</governor>
          <dependent id="34">before</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="36">a.m.</governor>
          <dependent id="35">10:30</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="44">grew</governor>
          <dependent id="36">a.m.</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="36">a.m.</governor>
          <dependent id="37">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="39">gallery</governor>
          <dependent id="38">the</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="36">a.m.</governor>
          <dependent id="39">gallery</dependent>
        </dependency>
        <dependency type="case">
          <governor id="41">reporters</governor>
          <dependent id="40">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="39">gallery</governor>
          <dependent id="41">reporters</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="41">reporters</governor>
          <dependent id="42">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="41">reporters</governor>
          <dependent id="43">spectators</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="29">filed</governor>
          <dependent id="44">grew</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="44">grew</governor>
          <dependent id="45">quiet</dependent>
        </dependency>
        <dependency type="case">
          <governor id="47">anticipation</governor>
          <dependent id="46">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="44">grew</governor>
          <dependent id="47">anticipation</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="10:30 a.m." type="TIME" score="0.0">
          <tokens>
            <token id="35" string="10:30" />
            <token id="36" string="a.m." />
          </tokens>
        </entity>
        <entity id="2" string="years" type="DURATION" score="0.0">
          <tokens>
            <token id="6" string="years" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="14" has_coreference="true">
      <content>&amp;quot;All the verdicts are the same,&amp;quot; said Los Angeles Superior Court Judge William Pounders, handing them to his court clerk.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="All" lemma="all" stem="all" pos="PDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="verdicts" lemma="verdict" stem="verdict" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="are" lemma="be" stem="ar" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="same" lemma="same" stem="same" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="Los" lemma="Los" stem="lo" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="12" string="Angeles" lemma="Angeles" stem="angele" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="13" string="Superior" lemma="Superior" stem="superior" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="14" string="Court" lemma="Court" stem="court" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="15" string="Judge" lemma="Judge" stem="judg" pos="NNP" type="Word" isStopWord="false" ner="TITLE" is_referenced="true" is_refers="false" />
        <token id="16" string="William" lemma="William" stem="william" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="17" string="Pounders" lemma="Pounders" stem="pounder" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="18" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="handing" lemma="hand" stem="hand" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="them" lemma="they" stem="them" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="21" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="23" string="court" lemma="court" stem="court" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="24" string="clerk" lemma="clerk" stem="clerk" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="25" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (SINV (`` ``) (S (NP (PDT All) (DT the) (NNS verdicts)) (VP (VBP are) (NP (DT the) (JJ same)))) (, ,) ('' '') (VP (VBD said)) (NP (NNP Los) (NNP Angeles) (NNP Superior) (NNP Court) (NNP Judge) (NNP William) (NNP Pounders)) (, ,) (S (VP (VBG handing) (NP (PRP them)) (PP (TO to) (NP (PRP$ his) (NN court) (NN clerk))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="All the verdicts" type="NP">
          <tokens>
            <token id="2" string="All" />
            <token id="3" string="the" />
            <token id="4" string="verdicts" />
          </tokens>
        </chunking>
        <chunking id="2" string="are the same" type="VP">
          <tokens>
            <token id="5" string="are" />
            <token id="6" string="the" />
            <token id="7" string="same" />
          </tokens>
        </chunking>
        <chunking id="3" string="Los Angeles Superior Court Judge William Pounders" type="NP">
          <tokens>
            <token id="11" string="Los" />
            <token id="12" string="Angeles" />
            <token id="13" string="Superior" />
            <token id="14" string="Court" />
            <token id="15" string="Judge" />
            <token id="16" string="William" />
            <token id="17" string="Pounders" />
          </tokens>
        </chunking>
        <chunking id="4" string="his court clerk" type="NP">
          <tokens>
            <token id="22" string="his" />
            <token id="23" string="court" />
            <token id="24" string="clerk" />
          </tokens>
        </chunking>
        <chunking id="5" string="the same" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="same" />
          </tokens>
        </chunking>
        <chunking id="6" string="handing them to his court clerk" type="VP">
          <tokens>
            <token id="19" string="handing" />
            <token id="20" string="them" />
            <token id="21" string="to" />
            <token id="22" string="his" />
            <token id="23" string="court" />
            <token id="24" string="clerk" />
          </tokens>
        </chunking>
        <chunking id="7" string="said" type="VP">
          <tokens>
            <token id="10" string="said" />
          </tokens>
        </chunking>
        <chunking id="8" string="them" type="NP">
          <tokens>
            <token id="20" string="them" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det:predet">
          <governor id="4">verdicts</governor>
          <dependent id="2">All</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">verdicts</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">same</governor>
          <dependent id="4">verdicts</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="7">same</governor>
          <dependent id="5">are</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">same</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="10">said</governor>
          <dependent id="7">same</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="10">said</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="17">Pounders</governor>
          <dependent id="11">Los</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="17">Pounders</governor>
          <dependent id="12">Angeles</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="17">Pounders</governor>
          <dependent id="13">Superior</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="17">Pounders</governor>
          <dependent id="14">Court</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="17">Pounders</governor>
          <dependent id="15">Judge</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="17">Pounders</governor>
          <dependent id="16">William</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">said</governor>
          <dependent id="17">Pounders</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="10">said</governor>
          <dependent id="19">handing</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="19">handing</governor>
          <dependent id="20">them</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">clerk</governor>
          <dependent id="21">to</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="24">clerk</governor>
          <dependent id="22">his</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="24">clerk</governor>
          <dependent id="23">court</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">handing</governor>
          <dependent id="24">clerk</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="William Pounders" type="PERSON" score="0.0">
          <tokens>
            <token id="16" string="William" />
            <token id="17" string="Pounders" />
          </tokens>
        </entity>
        <entity id="2" string="Los Angeles Superior Court" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="11" string="Los" />
            <token id="12" string="Angeles" />
            <token id="13" string="Superior" />
            <token id="14" string="Court" />
          </tokens>
        </entity>
        <entity id="3" string="Judge" type="TITLE" score="0.0">
          <tokens>
            <token id="15" string="Judge" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="15" has_coreference="true">
      <content>Clerk Stan Ferrell listed the counts numerically and then, reading from the verdict form, delivered in a single, almost anti-climactic sentence the historic trial&amp;apost;s close: &amp;quot;We the jury in the above entitled actions find the defendants not guilty.&amp;quot;</content>
      <tokens>
        <token id="1" string="Clerk" lemma="clerk" stem="clerk" pos="NN" type="Word" isStopWord="false" ner="TITLE" is_referenced="false" is_refers="false" />
        <token id="2" string="Stan" lemma="Stan" stem="stan" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="3" string="Ferrell" lemma="Ferrell" stem="ferrel" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="4" string="listed" lemma="list" stem="list" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="counts" lemma="count" stem="count" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="numerically" lemma="numerically" stem="numer" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="then" lemma="then" stem="then" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="reading" lemma="read" stem="read" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="verdict" lemma="verdict" stem="verdict" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="form" lemma="form" stem="form" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="delivered" lemma="deliver" stem="deliv" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="single" lemma="single" stem="singl" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="almost" lemma="almost" stem="almost" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="anti-climactic" lemma="anti-climactic" stem="anti-climact" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="sentence" lemma="sentence" stem="sentenc" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="historic" lemma="historic" stem="histor" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="trial" lemma="trial" stem="trial" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="close" lemma="close" stem="close" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="We" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="33" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="34" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="35" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="36" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="37" string="above" lemma="above" stem="abov" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="38" string="entitled" lemma="entitle" stem="entitl" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="39" string="actions" lemma="action" stem="action" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="40" string="find" lemma="find" stem="find" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="41" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="42" string="defendants" lemma="defendant" stem="defend" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="43" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="44" string="guilty" lemma="guilty" stem="guilti" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="45" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="46" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NN Clerk) (NNP Stan) (NNP Ferrell)) (VP (VBD listed) (NP (DT the) (NNS counts)) (ADVP (RB numerically) (CC and) (RB then)) (, ,) (S (VP (VBG reading) (PP (IN from) (NP (NP (DT the) (NN verdict) (NN form)) (, ,) (VP (VBN delivered) (PP (IN in) (NP (NP (NP (DT a) (JJ single) (, ,) (ADJP (RB almost) (JJ anti-climactic)) (NN sentence)) (NP (NP (DT the) (JJ historic) (NN trial) (POS 's)) (NN close))) (: :) (`` ``) (NP (NP (PRP We)) (SBAR (S (NP (NP (DT the) (NN jury)) (PP (IN in) (NP (DT the) (ADJP (JJ above) (VBN entitled)) (NNS actions)))) (VP (VBP find) (S (NP (DT the) (NNS defendants)) (ADJP (RB not) (JJ guilty))))))))))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="the jury in the above entitled actions" type="NP">
          <tokens>
            <token id="33" string="the" />
            <token id="34" string="jury" />
            <token id="35" string="in" />
            <token id="36" string="the" />
            <token id="37" string="above" />
            <token id="38" string="entitled" />
            <token id="39" string="actions" />
          </tokens>
        </chunking>
        <chunking id="2" string="find the defendants not guilty" type="VP">
          <tokens>
            <token id="40" string="find" />
            <token id="41" string="the" />
            <token id="42" string="defendants" />
            <token id="43" string="not" />
            <token id="44" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="3" string="the counts" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="counts" />
          </tokens>
        </chunking>
        <chunking id="4" string="above entitled" type="ADJP">
          <tokens>
            <token id="37" string="above" />
            <token id="38" string="entitled" />
          </tokens>
        </chunking>
        <chunking id="5" string="reading from the verdict form , delivered in a single , almost anti-climactic sentence the historic trial 's close : `` We the jury in the above entitled actions find the defendants not guilty" type="VP">
          <tokens>
            <token id="11" string="reading" />
            <token id="12" string="from" />
            <token id="13" string="the" />
            <token id="14" string="verdict" />
            <token id="15" string="form" />
            <token id="16" string="," />
            <token id="17" string="delivered" />
            <token id="18" string="in" />
            <token id="19" string="a" />
            <token id="20" string="single" />
            <token id="21" string="," />
            <token id="22" string="almost" />
            <token id="23" string="anti-climactic" />
            <token id="24" string="sentence" />
            <token id="25" string="the" />
            <token id="26" string="historic" />
            <token id="27" string="trial" />
            <token id="28" string="'s" />
            <token id="29" string="close" />
            <token id="30" string=":" />
            <token id="31" string="&quot;" />
            <token id="32" string="We" />
            <token id="33" string="the" />
            <token id="34" string="jury" />
            <token id="35" string="in" />
            <token id="36" string="the" />
            <token id="37" string="above" />
            <token id="38" string="entitled" />
            <token id="39" string="actions" />
            <token id="40" string="find" />
            <token id="41" string="the" />
            <token id="42" string="defendants" />
            <token id="43" string="not" />
            <token id="44" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="6" string="the verdict form" type="NP">
          <tokens>
            <token id="13" string="the" />
            <token id="14" string="verdict" />
            <token id="15" string="form" />
          </tokens>
        </chunking>
        <chunking id="7" string="a single , almost anti-climactic sentence the historic trial 's close : `` We the jury in the above entitled actions find the defendants not guilty" type="NP">
          <tokens>
            <token id="19" string="a" />
            <token id="20" string="single" />
            <token id="21" string="," />
            <token id="22" string="almost" />
            <token id="23" string="anti-climactic" />
            <token id="24" string="sentence" />
            <token id="25" string="the" />
            <token id="26" string="historic" />
            <token id="27" string="trial" />
            <token id="28" string="'s" />
            <token id="29" string="close" />
            <token id="30" string=":" />
            <token id="31" string="&quot;" />
            <token id="32" string="We" />
            <token id="33" string="the" />
            <token id="34" string="jury" />
            <token id="35" string="in" />
            <token id="36" string="the" />
            <token id="37" string="above" />
            <token id="38" string="entitled" />
            <token id="39" string="actions" />
            <token id="40" string="find" />
            <token id="41" string="the" />
            <token id="42" string="defendants" />
            <token id="43" string="not" />
            <token id="44" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="8" string="the jury in the above entitled actions find the defendants not guilty" type="SBAR">
          <tokens>
            <token id="33" string="the" />
            <token id="34" string="jury" />
            <token id="35" string="in" />
            <token id="36" string="the" />
            <token id="37" string="above" />
            <token id="38" string="entitled" />
            <token id="39" string="actions" />
            <token id="40" string="find" />
            <token id="41" string="the" />
            <token id="42" string="defendants" />
            <token id="43" string="not" />
            <token id="44" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="9" string="the above entitled actions" type="NP">
          <tokens>
            <token id="36" string="the" />
            <token id="37" string="above" />
            <token id="38" string="entitled" />
            <token id="39" string="actions" />
          </tokens>
        </chunking>
        <chunking id="10" string="Clerk Stan Ferrell" type="NP">
          <tokens>
            <token id="1" string="Clerk" />
            <token id="2" string="Stan" />
            <token id="3" string="Ferrell" />
          </tokens>
        </chunking>
        <chunking id="11" string="listed the counts numerically and then , reading from the verdict form , delivered in a single , almost anti-climactic sentence the historic trial 's close : `` We the jury in the above entitled actions find the defendants not guilty" type="VP">
          <tokens>
            <token id="4" string="listed" />
            <token id="5" string="the" />
            <token id="6" string="counts" />
            <token id="7" string="numerically" />
            <token id="8" string="and" />
            <token id="9" string="then" />
            <token id="10" string="," />
            <token id="11" string="reading" />
            <token id="12" string="from" />
            <token id="13" string="the" />
            <token id="14" string="verdict" />
            <token id="15" string="form" />
            <token id="16" string="," />
            <token id="17" string="delivered" />
            <token id="18" string="in" />
            <token id="19" string="a" />
            <token id="20" string="single" />
            <token id="21" string="," />
            <token id="22" string="almost" />
            <token id="23" string="anti-climactic" />
            <token id="24" string="sentence" />
            <token id="25" string="the" />
            <token id="26" string="historic" />
            <token id="27" string="trial" />
            <token id="28" string="'s" />
            <token id="29" string="close" />
            <token id="30" string=":" />
            <token id="31" string="&quot;" />
            <token id="32" string="We" />
            <token id="33" string="the" />
            <token id="34" string="jury" />
            <token id="35" string="in" />
            <token id="36" string="the" />
            <token id="37" string="above" />
            <token id="38" string="entitled" />
            <token id="39" string="actions" />
            <token id="40" string="find" />
            <token id="41" string="the" />
            <token id="42" string="defendants" />
            <token id="43" string="not" />
            <token id="44" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="12" string="We" type="NP">
          <tokens>
            <token id="32" string="We" />
          </tokens>
        </chunking>
        <chunking id="13" string="not guilty" type="ADJP">
          <tokens>
            <token id="43" string="not" />
            <token id="44" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="14" string="the verdict form , delivered in a single , almost anti-climactic sentence the historic trial 's close : `` We the jury in the above entitled actions find the defendants not guilty" type="NP">
          <tokens>
            <token id="13" string="the" />
            <token id="14" string="verdict" />
            <token id="15" string="form" />
            <token id="16" string="," />
            <token id="17" string="delivered" />
            <token id="18" string="in" />
            <token id="19" string="a" />
            <token id="20" string="single" />
            <token id="21" string="," />
            <token id="22" string="almost" />
            <token id="23" string="anti-climactic" />
            <token id="24" string="sentence" />
            <token id="25" string="the" />
            <token id="26" string="historic" />
            <token id="27" string="trial" />
            <token id="28" string="'s" />
            <token id="29" string="close" />
            <token id="30" string=":" />
            <token id="31" string="&quot;" />
            <token id="32" string="We" />
            <token id="33" string="the" />
            <token id="34" string="jury" />
            <token id="35" string="in" />
            <token id="36" string="the" />
            <token id="37" string="above" />
            <token id="38" string="entitled" />
            <token id="39" string="actions" />
            <token id="40" string="find" />
            <token id="41" string="the" />
            <token id="42" string="defendants" />
            <token id="43" string="not" />
            <token id="44" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="15" string="delivered in a single , almost anti-climactic sentence the historic trial 's close : `` We the jury in the above entitled actions find the defendants not guilty" type="VP">
          <tokens>
            <token id="17" string="delivered" />
            <token id="18" string="in" />
            <token id="19" string="a" />
            <token id="20" string="single" />
            <token id="21" string="," />
            <token id="22" string="almost" />
            <token id="23" string="anti-climactic" />
            <token id="24" string="sentence" />
            <token id="25" string="the" />
            <token id="26" string="historic" />
            <token id="27" string="trial" />
            <token id="28" string="'s" />
            <token id="29" string="close" />
            <token id="30" string=":" />
            <token id="31" string="&quot;" />
            <token id="32" string="We" />
            <token id="33" string="the" />
            <token id="34" string="jury" />
            <token id="35" string="in" />
            <token id="36" string="the" />
            <token id="37" string="above" />
            <token id="38" string="entitled" />
            <token id="39" string="actions" />
            <token id="40" string="find" />
            <token id="41" string="the" />
            <token id="42" string="defendants" />
            <token id="43" string="not" />
            <token id="44" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="16" string="the historic trial 's close" type="NP">
          <tokens>
            <token id="25" string="the" />
            <token id="26" string="historic" />
            <token id="27" string="trial" />
            <token id="28" string="'s" />
            <token id="29" string="close" />
          </tokens>
        </chunking>
        <chunking id="17" string="We the jury in the above entitled actions find the defendants not guilty" type="NP">
          <tokens>
            <token id="32" string="We" />
            <token id="33" string="the" />
            <token id="34" string="jury" />
            <token id="35" string="in" />
            <token id="36" string="the" />
            <token id="37" string="above" />
            <token id="38" string="entitled" />
            <token id="39" string="actions" />
            <token id="40" string="find" />
            <token id="41" string="the" />
            <token id="42" string="defendants" />
            <token id="43" string="not" />
            <token id="44" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="18" string="almost anti-climactic" type="ADJP">
          <tokens>
            <token id="22" string="almost" />
            <token id="23" string="anti-climactic" />
          </tokens>
        </chunking>
        <chunking id="19" string="the defendants" type="NP">
          <tokens>
            <token id="41" string="the" />
            <token id="42" string="defendants" />
          </tokens>
        </chunking>
        <chunking id="20" string="a single , almost anti-climactic sentence" type="NP">
          <tokens>
            <token id="19" string="a" />
            <token id="20" string="single" />
            <token id="21" string="," />
            <token id="22" string="almost" />
            <token id="23" string="anti-climactic" />
            <token id="24" string="sentence" />
          </tokens>
        </chunking>
        <chunking id="21" string="a single , almost anti-climactic sentence the historic trial 's close" type="NP">
          <tokens>
            <token id="19" string="a" />
            <token id="20" string="single" />
            <token id="21" string="," />
            <token id="22" string="almost" />
            <token id="23" string="anti-climactic" />
            <token id="24" string="sentence" />
            <token id="25" string="the" />
            <token id="26" string="historic" />
            <token id="27" string="trial" />
            <token id="28" string="'s" />
            <token id="29" string="close" />
          </tokens>
        </chunking>
        <chunking id="22" string="the historic trial 's" type="NP">
          <tokens>
            <token id="25" string="the" />
            <token id="26" string="historic" />
            <token id="27" string="trial" />
            <token id="28" string="'s" />
          </tokens>
        </chunking>
        <chunking id="23" string="the jury" type="NP">
          <tokens>
            <token id="33" string="the" />
            <token id="34" string="jury" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="3">Ferrell</governor>
          <dependent id="1">Clerk</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">Ferrell</governor>
          <dependent id="2">Stan</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">listed</governor>
          <dependent id="3">Ferrell</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">listed</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">counts</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">listed</governor>
          <dependent id="6">counts</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">listed</governor>
          <dependent id="7">numerically</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">numerically</governor>
          <dependent id="8">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">numerically</governor>
          <dependent id="9">then</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="4">listed</governor>
          <dependent id="11">reading</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">form</governor>
          <dependent id="12">from</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">form</governor>
          <dependent id="13">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">form</governor>
          <dependent id="14">verdict</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">reading</governor>
          <dependent id="15">form</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="15">form</governor>
          <dependent id="17">delivered</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">sentence</governor>
          <dependent id="18">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="24">sentence</governor>
          <dependent id="19">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="24">sentence</governor>
          <dependent id="20">single</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="23">anti-climactic</governor>
          <dependent id="22">almost</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="24">sentence</governor>
          <dependent id="23">anti-climactic</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">delivered</governor>
          <dependent id="24">sentence</dependent>
        </dependency>
        <dependency type="det">
          <governor id="27">trial</governor>
          <dependent id="25">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="27">trial</governor>
          <dependent id="26">historic</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="29">close</governor>
          <dependent id="27">trial</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">trial</governor>
          <dependent id="28">'s</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="24">sentence</governor>
          <dependent id="29">close</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="24">sentence</governor>
          <dependent id="32">We</dependent>
        </dependency>
        <dependency type="det">
          <governor id="34">jury</governor>
          <dependent id="33">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="40">find</governor>
          <dependent id="34">jury</dependent>
        </dependency>
        <dependency type="case">
          <governor id="39">actions</governor>
          <dependent id="35">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="39">actions</governor>
          <dependent id="36">the</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="38">entitled</governor>
          <dependent id="37">above</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="39">actions</governor>
          <dependent id="38">entitled</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="34">jury</governor>
          <dependent id="39">actions</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="32">We</governor>
          <dependent id="40">find</dependent>
        </dependency>
        <dependency type="det">
          <governor id="42">defendants</governor>
          <dependent id="41">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="44">guilty</governor>
          <dependent id="42">defendants</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="44">guilty</governor>
          <dependent id="43">not</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="40">find</governor>
          <dependent id="44">guilty</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Clerk" type="TITLE" score="0.0">
          <tokens>
            <token id="1" string="Clerk" />
          </tokens>
        </entity>
        <entity id="2" string="Stan Ferrell" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Stan" />
            <token id="3" string="Ferrell" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="16" has_coreference="false">
      <content>The quiet of the courtroom was split by a few gasps and sobs.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="quiet" lemma="quiet" stem="quiet" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="courtroom" lemma="courtroom" stem="courtroom" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="split" lemma="split" stem="split" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="few" lemma="few" stem="few" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="gasps" lemma="gasp" stem="gasp" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="sobs" lemma="sob" stem="sob" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT The) (NN quiet)) (PP (IN of) (NP (DT the) (NN courtroom)))) (VP (VBD was) (VP (VBN split) (PP (IN by) (NP (DT a) (JJ few) (NNS gasps) (CC and) (NNS sobs))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="The quiet of the courtroom" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="quiet" />
            <token id="3" string="of" />
            <token id="4" string="the" />
            <token id="5" string="courtroom" />
          </tokens>
        </chunking>
        <chunking id="2" string="was split by a few gasps and sobs" type="VP">
          <tokens>
            <token id="6" string="was" />
            <token id="7" string="split" />
            <token id="8" string="by" />
            <token id="9" string="a" />
            <token id="10" string="few" />
            <token id="11" string="gasps" />
            <token id="12" string="and" />
            <token id="13" string="sobs" />
          </tokens>
        </chunking>
        <chunking id="3" string="the courtroom" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="courtroom" />
          </tokens>
        </chunking>
        <chunking id="4" string="a few gasps and sobs" type="NP">
          <tokens>
            <token id="9" string="a" />
            <token id="10" string="few" />
            <token id="11" string="gasps" />
            <token id="12" string="and" />
            <token id="13" string="sobs" />
          </tokens>
        </chunking>
        <chunking id="5" string="split by a few gasps and sobs" type="VP">
          <tokens>
            <token id="7" string="split" />
            <token id="8" string="by" />
            <token id="9" string="a" />
            <token id="10" string="few" />
            <token id="11" string="gasps" />
            <token id="12" string="and" />
            <token id="13" string="sobs" />
          </tokens>
        </chunking>
        <chunking id="6" string="The quiet" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="quiet" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">quiet</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="7">split</governor>
          <dependent id="2">quiet</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">courtroom</governor>
          <dependent id="3">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">courtroom</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">quiet</governor>
          <dependent id="5">courtroom</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="7">split</governor>
          <dependent id="6">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">split</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">gasps</governor>
          <dependent id="8">by</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">gasps</governor>
          <dependent id="9">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">gasps</governor>
          <dependent id="10">few</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">split</governor>
          <dependent id="11">gasps</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="11">gasps</governor>
          <dependent id="12">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="11">gasps</governor>
          <dependent id="13">sobs</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="17" has_coreference="true">
      <content>At first, the defendants remained stoic, then their eyes began to rim with tears.</content>
      <tokens>
        <token id="1" string="At" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="first" lemma="first" stem="first" pos="RB" type="Word" isStopWord="false" ner="ORDINAL" is_referenced="true" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="defendants" lemma="defendant" stem="defend" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="remained" lemma="remain" stem="remain" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="stoic" lemma="stoic" stem="stoic" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="then" lemma="then" stem="then" pos="RB" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="11" string="eyes" lemma="eye" stem="ey" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="began" lemma="begin" stem="began" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="rim" lemma="rim" stem="rim" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="tears" lemma="tear" stem="tear" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="17" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (IN At) (ADVP (RB first))) (, ,) (NP (DT the) (NNS defendants)) (VP (VBD remained) (SBAR (S (NP (JJ stoic) (, ,) (RB then) (PRP$ their) (NNS eyes)) (VP (VBD began) (S (VP (TO to) (VP (VB rim) (PP (IN with) (NP (NNS tears)))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="tears" type="NP">
          <tokens>
            <token id="16" string="tears" />
          </tokens>
        </chunking>
        <chunking id="2" string="stoic , then their eyes began to rim with tears" type="SBAR">
          <tokens>
            <token id="7" string="stoic" />
            <token id="8" string="," />
            <token id="9" string="then" />
            <token id="10" string="their" />
            <token id="11" string="eyes" />
            <token id="12" string="began" />
            <token id="13" string="to" />
            <token id="14" string="rim" />
            <token id="15" string="with" />
            <token id="16" string="tears" />
          </tokens>
        </chunking>
        <chunking id="3" string="the defendants" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="defendants" />
          </tokens>
        </chunking>
        <chunking id="4" string="stoic , then their eyes" type="NP">
          <tokens>
            <token id="7" string="stoic" />
            <token id="8" string="," />
            <token id="9" string="then" />
            <token id="10" string="their" />
            <token id="11" string="eyes" />
          </tokens>
        </chunking>
        <chunking id="5" string="began to rim with tears" type="VP">
          <tokens>
            <token id="12" string="began" />
            <token id="13" string="to" />
            <token id="14" string="rim" />
            <token id="15" string="with" />
            <token id="16" string="tears" />
          </tokens>
        </chunking>
        <chunking id="6" string="to rim with tears" type="VP">
          <tokens>
            <token id="13" string="to" />
            <token id="14" string="rim" />
            <token id="15" string="with" />
            <token id="16" string="tears" />
          </tokens>
        </chunking>
        <chunking id="7" string="remained stoic , then their eyes began to rim with tears" type="VP">
          <tokens>
            <token id="6" string="remained" />
            <token id="7" string="stoic" />
            <token id="8" string="," />
            <token id="9" string="then" />
            <token id="10" string="their" />
            <token id="11" string="eyes" />
            <token id="12" string="began" />
            <token id="13" string="to" />
            <token id="14" string="rim" />
            <token id="15" string="with" />
            <token id="16" string="tears" />
          </tokens>
        </chunking>
        <chunking id="8" string="rim with tears" type="VP">
          <tokens>
            <token id="14" string="rim" />
            <token id="15" string="with" />
            <token id="16" string="tears" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="case">
          <governor id="2">first</governor>
          <dependent id="1">At</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="6">remained</governor>
          <dependent id="2">first</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">defendants</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">remained</governor>
          <dependent id="5">defendants</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">remained</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">eyes</governor>
          <dependent id="7">stoic</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">eyes</governor>
          <dependent id="9">then</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="11">eyes</governor>
          <dependent id="10">their</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">began</governor>
          <dependent id="11">eyes</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="6">remained</governor>
          <dependent id="12">began</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="14">rim</governor>
          <dependent id="13">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="12">began</governor>
          <dependent id="14">rim</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">tears</governor>
          <dependent id="15">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">rim</governor>
          <dependent id="16">tears</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="first" type="ORDINAL" score="0.0">
          <tokens>
            <token id="2" string="first" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="18" has_coreference="true">
      <content>Ray Buckey&amp;apost;s lower lip quivered.</content>
      <tokens>
        <token id="1" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="2" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="3" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="4" string="lower" lemma="lower" stem="lower" pos="JJR" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="lip" lemma="lip" stem="lip" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="quivered" lemma="quiver" stem="quiver" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Ray) (NNP Buckey) (POS 's)) (JJR lower) (NN lip)) (VP (VBD quivered)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Ray Buckey 's" type="NP">
          <tokens>
            <token id="1" string="Ray" />
            <token id="2" string="Buckey" />
            <token id="3" string="'s" />
          </tokens>
        </chunking>
        <chunking id="2" string="Ray Buckey 's lower lip" type="NP">
          <tokens>
            <token id="1" string="Ray" />
            <token id="2" string="Buckey" />
            <token id="3" string="'s" />
            <token id="4" string="lower" />
            <token id="5" string="lip" />
          </tokens>
        </chunking>
        <chunking id="3" string="quivered" type="VP">
          <tokens>
            <token id="6" string="quivered" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">Buckey</governor>
          <dependent id="1">Ray</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="5">lip</governor>
          <dependent id="2">Buckey</dependent>
        </dependency>
        <dependency type="case">
          <governor id="2">Buckey</governor>
          <dependent id="3">'s</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">lip</governor>
          <dependent id="4">lower</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">quivered</governor>
          <dependent id="5">lip</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">quivered</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Ray Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Ray" />
            <token id="2" string="Buckey" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="19" has_coreference="true">
      <content>His mother dabbed at her eyes with a handkerchief.</content>
      <tokens>
        <token id="1" string="His" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="dabbed" lemma="dab" stem="dab" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="eyes" lemma="eye" stem="ey" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="handkerchief" lemma="handkerchief" stem="handkerchief" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP$ His) (NN mother)) (VP (VBD dabbed) (PP (IN at) (NP (PRP$ her) (NNS eyes))) (PP (IN with) (NP (DT a) (NN handkerchief)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="dabbed at her eyes with a handkerchief" type="VP">
          <tokens>
            <token id="3" string="dabbed" />
            <token id="4" string="at" />
            <token id="5" string="her" />
            <token id="6" string="eyes" />
            <token id="7" string="with" />
            <token id="8" string="a" />
            <token id="9" string="handkerchief" />
          </tokens>
        </chunking>
        <chunking id="2" string="her eyes" type="NP">
          <tokens>
            <token id="5" string="her" />
            <token id="6" string="eyes" />
          </tokens>
        </chunking>
        <chunking id="3" string="His mother" type="NP">
          <tokens>
            <token id="1" string="His" />
            <token id="2" string="mother" />
          </tokens>
        </chunking>
        <chunking id="4" string="a handkerchief" type="NP">
          <tokens>
            <token id="8" string="a" />
            <token id="9" string="handkerchief" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="2">mother</governor>
          <dependent id="1">His</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">dabbed</governor>
          <dependent id="2">mother</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">dabbed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">eyes</governor>
          <dependent id="4">at</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="6">eyes</governor>
          <dependent id="5">her</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">dabbed</governor>
          <dependent id="6">eyes</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">handkerchief</governor>
          <dependent id="7">with</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">handkerchief</governor>
          <dependent id="8">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">dabbed</governor>
          <dependent id="9">handkerchief</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="20" has_coreference="true">
      <content>They said nothing.</content>
      <tokens>
        <token id="1" string="They" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="nothing" lemma="nothing" stem="noth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP They)) (VP (VBD said) (NP (NN nothing))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="They" type="NP">
          <tokens>
            <token id="1" string="They" />
          </tokens>
        </chunking>
        <chunking id="2" string="said nothing" type="VP">
          <tokens>
            <token id="2" string="said" />
            <token id="3" string="nothing" />
          </tokens>
        </chunking>
        <chunking id="3" string="nothing" type="NP">
          <tokens>
            <token id="3" string="nothing" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">said</governor>
          <dependent id="1">They</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">said</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">said</governor>
          <dependent id="3">nothing</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="21" has_coreference="true">
      <content>Afterward, Peggy McMartin Buckey insisted she was not bitter.</content>
      <tokens>
        <token id="1" string="Afterward" lemma="afterward" stem="afterward" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="Peggy" lemma="Peggy" stem="peggi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="4" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="5" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="6" string="insisted" lemma="insist" stem="insist" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="bitter" lemma="bitter" stem="bitter" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (ADVP (RB Afterward)) (, ,) (NP (NNP Peggy) (NNP McMartin) (NNP Buckey)) (VP (VBD insisted) (SBAR (S (NP (PRP she)) (VP (VBD was) (RB not) (ADJP (JJ bitter)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Peggy McMartin Buckey" type="NP">
          <tokens>
            <token id="3" string="Peggy" />
            <token id="4" string="McMartin" />
            <token id="5" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="2" string="insisted she was not bitter" type="VP">
          <tokens>
            <token id="6" string="insisted" />
            <token id="7" string="she" />
            <token id="8" string="was" />
            <token id="9" string="not" />
            <token id="10" string="bitter" />
          </tokens>
        </chunking>
        <chunking id="3" string="was not bitter" type="VP">
          <tokens>
            <token id="8" string="was" />
            <token id="9" string="not" />
            <token id="10" string="bitter" />
          </tokens>
        </chunking>
        <chunking id="4" string="bitter" type="ADJP">
          <tokens>
            <token id="10" string="bitter" />
          </tokens>
        </chunking>
        <chunking id="5" string="she was not bitter" type="SBAR">
          <tokens>
            <token id="7" string="she" />
            <token id="8" string="was" />
            <token id="9" string="not" />
            <token id="10" string="bitter" />
          </tokens>
        </chunking>
        <chunking id="6" string="she" type="NP">
          <tokens>
            <token id="7" string="she" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="6">insisted</governor>
          <dependent id="1">Afterward</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">Buckey</governor>
          <dependent id="3">Peggy</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">Buckey</governor>
          <dependent id="4">McMartin</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">insisted</governor>
          <dependent id="5">Buckey</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">insisted</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">bitter</governor>
          <dependent id="7">she</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="10">bitter</governor>
          <dependent id="8">was</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="10">bitter</governor>
          <dependent id="9">not</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="6">insisted</governor>
          <dependent id="10">bitter</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Peggy McMartin Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="3" string="Peggy" />
            <token id="4" string="McMartin" />
            <token id="5" string="Buckey" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="22" has_coreference="true">
      <content>&amp;quot;I knew God would set my son and me free because we have done nothing,&amp;quot; she said as she was driven away.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="knew" lemma="know" stem="knew" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="God" lemma="God" stem="god" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="would" lemma="would" stem="would" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="set" lemma="set" stem="set" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="my" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="son" lemma="son" stem="son" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="me" lemma="I" stem="me" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="free" lemma="free" stem="free" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="because" lemma="because" stem="becaus" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="we" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="have" lemma="have" stem="have" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="done" lemma="do" stem="done" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="nothing" lemma="nothing" stem="noth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="20" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="23" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="driven" lemma="drive" stem="driven" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="away" lemma="away" stem="awai" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (NP (PRP I)) (VP (VBD knew) (SBAR (S (S (NP (NNP God)) (VP (MD would) (VP (VB set) (NP (PRP$ my) (NN son))))) (CC and) (S (NP (PRP me)) (ADJP (JJ free))))) (SBAR (IN because) (S (NP (PRP we)) (VP (VBP have) (VP (VBN done) (NP (NN nothing)))))))) (, ,) ('' '') (NP (PRP she)) (VP (VBD said) (SBAR (IN as) (S (NP (PRP she)) (VP (VBD was) (VP (VBN driven) (ADVP (RB away))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="done nothing" type="VP">
          <tokens>
            <token id="15" string="done" />
            <token id="16" string="nothing" />
          </tokens>
        </chunking>
        <chunking id="2" string="my son" type="NP">
          <tokens>
            <token id="7" string="my" />
            <token id="8" string="son" />
          </tokens>
        </chunking>
        <chunking id="3" string="nothing" type="NP">
          <tokens>
            <token id="16" string="nothing" />
          </tokens>
        </chunking>
        <chunking id="4" string="I" type="NP">
          <tokens>
            <token id="2" string="I" />
          </tokens>
        </chunking>
        <chunking id="5" string="was driven away" type="VP">
          <tokens>
            <token id="23" string="was" />
            <token id="24" string="driven" />
            <token id="25" string="away" />
          </tokens>
        </chunking>
        <chunking id="6" string="driven away" type="VP">
          <tokens>
            <token id="24" string="driven" />
            <token id="25" string="away" />
          </tokens>
        </chunking>
        <chunking id="7" string="we" type="NP">
          <tokens>
            <token id="13" string="we" />
          </tokens>
        </chunking>
        <chunking id="8" string="said as she was driven away" type="VP">
          <tokens>
            <token id="20" string="said" />
            <token id="21" string="as" />
            <token id="22" string="she" />
            <token id="23" string="was" />
            <token id="24" string="driven" />
            <token id="25" string="away" />
          </tokens>
        </chunking>
        <chunking id="9" string="because we have done nothing" type="SBAR">
          <tokens>
            <token id="12" string="because" />
            <token id="13" string="we" />
            <token id="14" string="have" />
            <token id="15" string="done" />
            <token id="16" string="nothing" />
          </tokens>
        </chunking>
        <chunking id="10" string="she" type="NP">
          <tokens>
            <token id="19" string="she" />
          </tokens>
        </chunking>
        <chunking id="11" string="as she was driven away" type="SBAR">
          <tokens>
            <token id="21" string="as" />
            <token id="22" string="she" />
            <token id="23" string="was" />
            <token id="24" string="driven" />
            <token id="25" string="away" />
          </tokens>
        </chunking>
        <chunking id="12" string="would set my son" type="VP">
          <tokens>
            <token id="5" string="would" />
            <token id="6" string="set" />
            <token id="7" string="my" />
            <token id="8" string="son" />
          </tokens>
        </chunking>
        <chunking id="13" string="me" type="NP">
          <tokens>
            <token id="10" string="me" />
          </tokens>
        </chunking>
        <chunking id="14" string="knew God would set my son and me free because we have done nothing" type="VP">
          <tokens>
            <token id="3" string="knew" />
            <token id="4" string="God" />
            <token id="5" string="would" />
            <token id="6" string="set" />
            <token id="7" string="my" />
            <token id="8" string="son" />
            <token id="9" string="and" />
            <token id="10" string="me" />
            <token id="11" string="free" />
            <token id="12" string="because" />
            <token id="13" string="we" />
            <token id="14" string="have" />
            <token id="15" string="done" />
            <token id="16" string="nothing" />
          </tokens>
        </chunking>
        <chunking id="15" string="God" type="NP">
          <tokens>
            <token id="4" string="God" />
          </tokens>
        </chunking>
        <chunking id="16" string="free" type="ADJP">
          <tokens>
            <token id="11" string="free" />
          </tokens>
        </chunking>
        <chunking id="17" string="set my son" type="VP">
          <tokens>
            <token id="6" string="set" />
            <token id="7" string="my" />
            <token id="8" string="son" />
          </tokens>
        </chunking>
        <chunking id="18" string="have done nothing" type="VP">
          <tokens>
            <token id="14" string="have" />
            <token id="15" string="done" />
            <token id="16" string="nothing" />
          </tokens>
        </chunking>
        <chunking id="19" string="God would set my son and me free" type="SBAR">
          <tokens>
            <token id="4" string="God" />
            <token id="5" string="would" />
            <token id="6" string="set" />
            <token id="7" string="my" />
            <token id="8" string="son" />
            <token id="9" string="and" />
            <token id="10" string="me" />
            <token id="11" string="free" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">knew</governor>
          <dependent id="2">I</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="20">said</governor>
          <dependent id="3">knew</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">set</governor>
          <dependent id="4">God</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="6">set</governor>
          <dependent id="5">would</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="3">knew</governor>
          <dependent id="6">set</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="8">son</governor>
          <dependent id="7">my</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">set</governor>
          <dependent id="8">son</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="6">set</governor>
          <dependent id="9">and</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="11">free</governor>
          <dependent id="10">me</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="6">set</governor>
          <dependent id="11">free</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="15">done</governor>
          <dependent id="12">because</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">done</governor>
          <dependent id="13">we</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="15">done</governor>
          <dependent id="14">have</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="3">knew</governor>
          <dependent id="15">done</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">done</governor>
          <dependent id="16">nothing</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="20">said</governor>
          <dependent id="19">she</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="20">said</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="24">driven</governor>
          <dependent id="21">as</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="24">driven</governor>
          <dependent id="22">she</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="24">driven</governor>
          <dependent id="23">was</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="20">said</governor>
          <dependent id="24">driven</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="24">driven</governor>
          <dependent id="25">away</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="23" has_coreference="true">
      <content>Her optimism was not matched by her son, however, who she said had &amp;quot;had fear, definitely,&amp;quot; about what the jury would decide.</content>
      <tokens>
        <token id="1" string="Her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="optimism" lemma="optimism" stem="optim" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="matched" lemma="match" stem="match" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="son" lemma="son" stem="son" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="however" lemma="however" stem="howev" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="fear" lemma="fear" stem="fear" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="definitely" lemma="definitely" stem="definit" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="what" lemma="what" stem="what" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="would" lemma="would" stem="would" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="decide" lemma="decide" stem="decid" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP$ Her) (NN optimism)) (VP (VBD was) (RB not) (VP (VBN matched) (PP (IN by) (NP (PRP$ her) (NN son))) (, ,) (ADVP (RB however)) (, ,) (SBAR (WHNP (WP who)) (S (NP (PRP she)) (VP (VBD said) (SBAR (S (VP (VBD had) (NP (SBAR (`` ``) (S (VP (VBD had) (NP (NN fear)) (, ,) (ADVP (RB definitely))))) (, ,) ('' '') (PP (IN about) (SBAR (WHNP (WP what)) (S (NP (DT the) (NN jury)) (VP (MD would) (VP (VB decide))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="said had `` had fear , definitely , '' about what the jury would decide" type="VP">
          <tokens>
            <token id="14" string="said" />
            <token id="15" string="had" />
            <token id="16" string="&quot;" />
            <token id="17" string="had" />
            <token id="18" string="fear" />
            <token id="19" string="," />
            <token id="20" string="definitely" />
            <token id="21" string="," />
            <token id="22" string="&quot;" />
            <token id="23" string="about" />
            <token id="24" string="what" />
            <token id="25" string="the" />
            <token id="26" string="jury" />
            <token id="27" string="would" />
            <token id="28" string="decide" />
          </tokens>
        </chunking>
        <chunking id="2" string="had fear , definitely" type="VP">
          <tokens>
            <token id="17" string="had" />
            <token id="18" string="fear" />
            <token id="19" string="," />
            <token id="20" string="definitely" />
          </tokens>
        </chunking>
        <chunking id="3" string="fear" type="NP">
          <tokens>
            <token id="18" string="fear" />
          </tokens>
        </chunking>
        <chunking id="4" string="decide" type="VP">
          <tokens>
            <token id="28" string="decide" />
          </tokens>
        </chunking>
        <chunking id="5" string="matched by her son , however , who she said had `` had fear , definitely , '' about what the jury would decide" type="VP">
          <tokens>
            <token id="5" string="matched" />
            <token id="6" string="by" />
            <token id="7" string="her" />
            <token id="8" string="son" />
            <token id="9" string="," />
            <token id="10" string="however" />
            <token id="11" string="," />
            <token id="12" string="who" />
            <token id="13" string="she" />
            <token id="14" string="said" />
            <token id="15" string="had" />
            <token id="16" string="&quot;" />
            <token id="17" string="had" />
            <token id="18" string="fear" />
            <token id="19" string="," />
            <token id="20" string="definitely" />
            <token id="21" string="," />
            <token id="22" string="&quot;" />
            <token id="23" string="about" />
            <token id="24" string="what" />
            <token id="25" string="the" />
            <token id="26" string="jury" />
            <token id="27" string="would" />
            <token id="28" string="decide" />
          </tokens>
        </chunking>
        <chunking id="6" string="was not matched by her son , however , who she said had `` had fear , definitely , '' about what the jury would decide" type="VP">
          <tokens>
            <token id="3" string="was" />
            <token id="4" string="not" />
            <token id="5" string="matched" />
            <token id="6" string="by" />
            <token id="7" string="her" />
            <token id="8" string="son" />
            <token id="9" string="," />
            <token id="10" string="however" />
            <token id="11" string="," />
            <token id="12" string="who" />
            <token id="13" string="she" />
            <token id="14" string="said" />
            <token id="15" string="had" />
            <token id="16" string="&quot;" />
            <token id="17" string="had" />
            <token id="18" string="fear" />
            <token id="19" string="," />
            <token id="20" string="definitely" />
            <token id="21" string="," />
            <token id="22" string="&quot;" />
            <token id="23" string="about" />
            <token id="24" string="what" />
            <token id="25" string="the" />
            <token id="26" string="jury" />
            <token id="27" string="would" />
            <token id="28" string="decide" />
          </tokens>
        </chunking>
        <chunking id="7" string="Her optimism" type="NP">
          <tokens>
            <token id="1" string="Her" />
            <token id="2" string="optimism" />
          </tokens>
        </chunking>
        <chunking id="8" string="who she said had `` had fear , definitely , '' about what the jury would decide" type="SBAR">
          <tokens>
            <token id="12" string="who" />
            <token id="13" string="she" />
            <token id="14" string="said" />
            <token id="15" string="had" />
            <token id="16" string="&quot;" />
            <token id="17" string="had" />
            <token id="18" string="fear" />
            <token id="19" string="," />
            <token id="20" string="definitely" />
            <token id="21" string="," />
            <token id="22" string="&quot;" />
            <token id="23" string="about" />
            <token id="24" string="what" />
            <token id="25" string="the" />
            <token id="26" string="jury" />
            <token id="27" string="would" />
            <token id="28" string="decide" />
          </tokens>
        </chunking>
        <chunking id="9" string="she" type="NP">
          <tokens>
            <token id="13" string="she" />
          </tokens>
        </chunking>
        <chunking id="10" string="what the jury would decide" type="SBAR">
          <tokens>
            <token id="24" string="what" />
            <token id="25" string="the" />
            <token id="26" string="jury" />
            <token id="27" string="would" />
            <token id="28" string="decide" />
          </tokens>
        </chunking>
        <chunking id="11" string="had `` had fear , definitely , '' about what the jury would decide" type="SBAR">
          <tokens>
            <token id="15" string="had" />
            <token id="16" string="&quot;" />
            <token id="17" string="had" />
            <token id="18" string="fear" />
            <token id="19" string="," />
            <token id="20" string="definitely" />
            <token id="21" string="," />
            <token id="22" string="&quot;" />
            <token id="23" string="about" />
            <token id="24" string="what" />
            <token id="25" string="the" />
            <token id="26" string="jury" />
            <token id="27" string="would" />
            <token id="28" string="decide" />
          </tokens>
        </chunking>
        <chunking id="12" string="`` had fear , definitely , '' about what the jury would decide" type="NP">
          <tokens>
            <token id="16" string="&quot;" />
            <token id="17" string="had" />
            <token id="18" string="fear" />
            <token id="19" string="," />
            <token id="20" string="definitely" />
            <token id="21" string="," />
            <token id="22" string="&quot;" />
            <token id="23" string="about" />
            <token id="24" string="what" />
            <token id="25" string="the" />
            <token id="26" string="jury" />
            <token id="27" string="would" />
            <token id="28" string="decide" />
          </tokens>
        </chunking>
        <chunking id="13" string="`` had fear , definitely" type="SBAR">
          <tokens>
            <token id="16" string="&quot;" />
            <token id="17" string="had" />
            <token id="18" string="fear" />
            <token id="19" string="," />
            <token id="20" string="definitely" />
          </tokens>
        </chunking>
        <chunking id="14" string="her son" type="NP">
          <tokens>
            <token id="7" string="her" />
            <token id="8" string="son" />
          </tokens>
        </chunking>
        <chunking id="15" string="the jury" type="NP">
          <tokens>
            <token id="25" string="the" />
            <token id="26" string="jury" />
          </tokens>
        </chunking>
        <chunking id="16" string="would decide" type="VP">
          <tokens>
            <token id="27" string="would" />
            <token id="28" string="decide" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="2">optimism</governor>
          <dependent id="1">Her</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="5">matched</governor>
          <dependent id="2">optimism</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="5">matched</governor>
          <dependent id="3">was</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="5">matched</governor>
          <dependent id="4">not</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">matched</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">son</governor>
          <dependent id="6">by</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="8">son</governor>
          <dependent id="7">her</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">matched</governor>
          <dependent id="8">son</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">matched</governor>
          <dependent id="10">however</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="14">said</governor>
          <dependent id="12">who</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">said</governor>
          <dependent id="13">she</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="5">matched</governor>
          <dependent id="14">said</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="14">said</governor>
          <dependent id="15">had</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">had</governor>
          <dependent id="17">had</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="17">had</governor>
          <dependent id="18">fear</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="17">had</governor>
          <dependent id="20">definitely</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="28">decide</governor>
          <dependent id="23">about</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="28">decide</governor>
          <dependent id="24">what</dependent>
        </dependency>
        <dependency type="det">
          <governor id="26">jury</governor>
          <dependent id="25">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="28">decide</governor>
          <dependent id="26">jury</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="28">decide</governor>
          <dependent id="27">would</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="17">had</governor>
          <dependent id="28">decide</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="24" has_coreference="true">
      <content>&amp;quot;I held his hand,&amp;quot; Buckey&amp;apost;s mother said after the verdicts were announced, &amp;quot;and I told him: &amp;apost;I told you, Ray,&amp;apost; and he felt like he wanted to cry but he didn&amp;apost;t.&amp;quot;</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="held" lemma="hold" stem="held" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="hand" lemma="hand" stem="hand" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="9" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="10" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="after" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="verdicts" lemma="verdict" stem="verdict" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="announced" lemma="announce" stem="announc" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="told" lemma="tell" stem="told" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="him" lemma="he" stem="him" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="23" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="told" lemma="tell" stem="told" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="you" lemma="you" stem="you" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="29" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="30" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="34" string="felt" lemma="feel" stem="felt" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="like" lemma="like" stem="like" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="36" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="37" string="wanted" lemma="want" stem="want" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="38" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="39" string="cry" lemma="cry" stem="cry" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="40" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="41" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="42" string="did" lemma="do" stem="did" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="43" string="n't" lemma="not" stem="n't" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="44" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="45" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (S (NP (PRP I)) (VP (VBD held) (NP (PRP$ his) (NN hand))) (PRN (, ,) ('' '') (S (NP (NP (NNP Buckey) (POS 's)) (NN mother)) (VP (VBD said) (SBAR (IN after) (S (NP (DT the) (NNS verdicts)) (VP (VBD were) (VP (VBN announced))))))) (, ,) (`` ``))) (CC and) (S (NP (PRP I)) (VP (VBD told) (NP (PRP him))))) (: :) ('' ') (S (S (NP (PRP I)) (VP (VBD told) (NP (NP (PRP you)) (, ,) (NP (NNP Ray))))) (, ,) ('' ') (CC and) (S (NP (PRP he)) (VP (VBD felt) (SBAR (IN like) (S (NP (PRP he)) (VP (VBD wanted) (S (VP (TO to) (VP (VB cry) (SBAR (CC but) (S (NP (PRP he)) (VP (VBD did) (RB n't))))))))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="the verdicts" type="NP">
          <tokens>
            <token id="13" string="the" />
            <token id="14" string="verdicts" />
          </tokens>
        </chunking>
        <chunking id="2" string="were announced" type="VP">
          <tokens>
            <token id="15" string="were" />
            <token id="16" string="announced" />
          </tokens>
        </chunking>
        <chunking id="3" string="held his hand" type="VP">
          <tokens>
            <token id="3" string="held" />
            <token id="4" string="his" />
            <token id="5" string="hand" />
          </tokens>
        </chunking>
        <chunking id="4" string="his hand" type="NP">
          <tokens>
            <token id="4" string="his" />
            <token id="5" string="hand" />
          </tokens>
        </chunking>
        <chunking id="5" string="felt like he wanted to cry but he did n't" type="VP">
          <tokens>
            <token id="34" string="felt" />
            <token id="35" string="like" />
            <token id="36" string="he" />
            <token id="37" string="wanted" />
            <token id="38" string="to" />
            <token id="39" string="cry" />
            <token id="40" string="but" />
            <token id="41" string="he" />
            <token id="42" string="did" />
            <token id="43" string="n't" />
          </tokens>
        </chunking>
        <chunking id="6" string="told him" type="VP">
          <tokens>
            <token id="21" string="told" />
            <token id="22" string="him" />
          </tokens>
        </chunking>
        <chunking id="7" string="I" type="NP">
          <tokens>
            <token id="2" string="I" />
          </tokens>
        </chunking>
        <chunking id="8" string="Buckey 's" type="NP">
          <tokens>
            <token id="8" string="Buckey" />
            <token id="9" string="'s" />
          </tokens>
        </chunking>
        <chunking id="9" string="told you , Ray" type="VP">
          <tokens>
            <token id="26" string="told" />
            <token id="27" string="you" />
            <token id="28" string="," />
            <token id="29" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="10" string="Ray" type="NP">
          <tokens>
            <token id="29" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="11" string="wanted to cry but he did n't" type="VP">
          <tokens>
            <token id="37" string="wanted" />
            <token id="38" string="to" />
            <token id="39" string="cry" />
            <token id="40" string="but" />
            <token id="41" string="he" />
            <token id="42" string="did" />
            <token id="43" string="n't" />
          </tokens>
        </chunking>
        <chunking id="12" string="him" type="NP">
          <tokens>
            <token id="22" string="him" />
          </tokens>
        </chunking>
        <chunking id="13" string="to cry but he did n't" type="VP">
          <tokens>
            <token id="38" string="to" />
            <token id="39" string="cry" />
            <token id="40" string="but" />
            <token id="41" string="he" />
            <token id="42" string="did" />
            <token id="43" string="n't" />
          </tokens>
        </chunking>
        <chunking id="14" string="said after the verdicts were announced" type="VP">
          <tokens>
            <token id="11" string="said" />
            <token id="12" string="after" />
            <token id="13" string="the" />
            <token id="14" string="verdicts" />
            <token id="15" string="were" />
            <token id="16" string="announced" />
          </tokens>
        </chunking>
        <chunking id="15" string="announced" type="VP">
          <tokens>
            <token id="16" string="announced" />
          </tokens>
        </chunking>
        <chunking id="16" string="but he did n't" type="SBAR">
          <tokens>
            <token id="40" string="but" />
            <token id="41" string="he" />
            <token id="42" string="did" />
            <token id="43" string="n't" />
          </tokens>
        </chunking>
        <chunking id="17" string="Buckey 's mother" type="NP">
          <tokens>
            <token id="8" string="Buckey" />
            <token id="9" string="'s" />
            <token id="10" string="mother" />
          </tokens>
        </chunking>
        <chunking id="18" string="did n't" type="VP">
          <tokens>
            <token id="42" string="did" />
            <token id="43" string="n't" />
          </tokens>
        </chunking>
        <chunking id="19" string="he" type="NP">
          <tokens>
            <token id="33" string="he" />
          </tokens>
        </chunking>
        <chunking id="20" string="after the verdicts were announced" type="SBAR">
          <tokens>
            <token id="12" string="after" />
            <token id="13" string="the" />
            <token id="14" string="verdicts" />
            <token id="15" string="were" />
            <token id="16" string="announced" />
          </tokens>
        </chunking>
        <chunking id="21" string="like he wanted to cry but he did n't" type="SBAR">
          <tokens>
            <token id="35" string="like" />
            <token id="36" string="he" />
            <token id="37" string="wanted" />
            <token id="38" string="to" />
            <token id="39" string="cry" />
            <token id="40" string="but" />
            <token id="41" string="he" />
            <token id="42" string="did" />
            <token id="43" string="n't" />
          </tokens>
        </chunking>
        <chunking id="22" string="you , Ray" type="NP">
          <tokens>
            <token id="27" string="you" />
            <token id="28" string="," />
            <token id="29" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="23" string="cry but he did n't" type="VP">
          <tokens>
            <token id="39" string="cry" />
            <token id="40" string="but" />
            <token id="41" string="he" />
            <token id="42" string="did" />
            <token id="43" string="n't" />
          </tokens>
        </chunking>
        <chunking id="24" string="you" type="NP">
          <tokens>
            <token id="27" string="you" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">held</governor>
          <dependent id="2">I</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">held</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="5">hand</governor>
          <dependent id="4">his</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">held</governor>
          <dependent id="5">hand</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="10">mother</governor>
          <dependent id="8">Buckey</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">Buckey</governor>
          <dependent id="9">'s</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">said</governor>
          <dependent id="10">mother</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="3">held</governor>
          <dependent id="11">said</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="16">announced</governor>
          <dependent id="12">after</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">verdicts</governor>
          <dependent id="13">the</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="16">announced</governor>
          <dependent id="14">verdicts</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="16">announced</governor>
          <dependent id="15">were</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="11">said</governor>
          <dependent id="16">announced</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="3">held</governor>
          <dependent id="19">and</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">told</governor>
          <dependent id="20">I</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="3">held</governor>
          <dependent id="21">told</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="21">told</governor>
          <dependent id="22">him</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="26">told</governor>
          <dependent id="25">I</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="3">held</governor>
          <dependent id="26">told</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="26">told</governor>
          <dependent id="27">you</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="27">you</governor>
          <dependent id="29">Ray</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="26">told</governor>
          <dependent id="32">and</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="34">felt</governor>
          <dependent id="33">he</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="26">told</governor>
          <dependent id="34">felt</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="37">wanted</governor>
          <dependent id="35">like</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="37">wanted</governor>
          <dependent id="36">he</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="34">felt</governor>
          <dependent id="37">wanted</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="39">cry</governor>
          <dependent id="38">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="37">wanted</governor>
          <dependent id="39">cry</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="42">did</governor>
          <dependent id="40">but</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="42">did</governor>
          <dependent id="41">he</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="39">cry</governor>
          <dependent id="42">did</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="42">did</governor>
          <dependent id="43">n't</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Buckey" />
          </tokens>
        </entity>
        <entity id="2" string="Ray" type="PERSON" score="0.0">
          <tokens>
            <token id="29" string="Ray" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="25" has_coreference="true">
      <content>The acquittals concluded the longest criminal trial in history, a case that stemmed from a 2 1/2-year-old&amp;apost;s report to his mother six years ago that he had been sodomized at his school by a &amp;quot;Mr. Ray.&amp;quot;</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="acquittals" lemma="acquittal" stem="acquitt" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="concluded" lemma="conclude" stem="conclud" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="longest" lemma="longest" stem="longest" pos="JJS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="criminal" lemma="criminal" stem="crimin" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="trial" lemma="trial" stem="trial" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="history" lemma="history" stem="histori" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="12" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="13" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="stemmed" lemma="stem" stem="stem" pos="VBD" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="16" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="17" string="2 1/2" lemma="2 1/2" stem="2 1/2" pos="CD" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="18" string="-" lemma="-" stem="-" pos=":" type="Symbol" isStopWord="true" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="19" string="year-old" lemma="year-old" stem="year-old" pos="NN" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="20" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="21" string="report" lemma="report" stem="report" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="22" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="23" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="24" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="25" string="six" lemma="six" stem="six" pos="CD" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="26" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="27" string="ago" lemma="ago" stem="ago" pos="IN" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="28" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="29" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="30" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="31" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="32" string="sodomized" lemma="sodomize" stem="sodom" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="33" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="34" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="35" string="school" lemma="school" stem="school" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="36" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="37" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="38" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="39" string="Mr." lemma="Mr." stem="mr." pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="40" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="41" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="42" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NNS acquittals)) (VP (VBD concluded) (NP (NP (DT the) (JJS longest) (JJ criminal) (NN trial)) (PP (IN in) (NP (NP (NN history)) (, ,) (NP (NP (DT a) (NN case)) (SBAR (WHNP (WDT that)) (S (VP (VBD stemmed) (PP (IN from) (NP (NP (QP (DT a) (CD 2 1/2))) (: -) (NP (NP (NN year-old) (POS 's)) (NN report)))) (PP (TO to) (NP (NP (PRP$ his) (NN mother)) (ADVP (NP (CD six) (NNS years)) (IN ago)))) (SBAR (IN that) (S (NP (PRP he)) (VP (VBD had) (VP (VBN been) (VP (VBN sodomized) (PP (IN at) (NP (PRP$ his) (NN school))) (PP (IN by) (NP (DT a) (`` ``) (NNP Mr.) (NNP Ray)))))))))))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="concluded the longest criminal trial in history , a case that stemmed from a 2 1/2 - year-old 's report to his mother six years ago that he had been sodomized at his school by a `` Mr. Ray" type="VP">
          <tokens>
            <token id="3" string="concluded" />
            <token id="4" string="the" />
            <token id="5" string="longest" />
            <token id="6" string="criminal" />
            <token id="7" string="trial" />
            <token id="8" string="in" />
            <token id="9" string="history" />
            <token id="10" string="," />
            <token id="11" string="a" />
            <token id="12" string="case" />
            <token id="13" string="that" />
            <token id="14" string="stemmed" />
            <token id="15" string="from" />
            <token id="16" string="a" />
            <token id="17" string="2 1/2" />
            <token id="18" string="-" />
            <token id="19" string="year-old" />
            <token id="20" string="'s" />
            <token id="21" string="report" />
            <token id="22" string="to" />
            <token id="23" string="his" />
            <token id="24" string="mother" />
            <token id="25" string="six" />
            <token id="26" string="years" />
            <token id="27" string="ago" />
            <token id="28" string="that" />
            <token id="29" string="he" />
            <token id="30" string="had" />
            <token id="31" string="been" />
            <token id="32" string="sodomized" />
            <token id="33" string="at" />
            <token id="34" string="his" />
            <token id="35" string="school" />
            <token id="36" string="by" />
            <token id="37" string="a" />
            <token id="38" string="&quot;" />
            <token id="39" string="Mr." />
            <token id="40" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="2" string="that he had been sodomized at his school by a `` Mr. Ray" type="SBAR">
          <tokens>
            <token id="28" string="that" />
            <token id="29" string="he" />
            <token id="30" string="had" />
            <token id="31" string="been" />
            <token id="32" string="sodomized" />
            <token id="33" string="at" />
            <token id="34" string="his" />
            <token id="35" string="school" />
            <token id="36" string="by" />
            <token id="37" string="a" />
            <token id="38" string="&quot;" />
            <token id="39" string="Mr." />
            <token id="40" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="3" string="history , a case that stemmed from a 2 1/2 - year-old 's report to his mother six years ago that he had been sodomized at his school by a `` Mr. Ray" type="NP">
          <tokens>
            <token id="9" string="history" />
            <token id="10" string="," />
            <token id="11" string="a" />
            <token id="12" string="case" />
            <token id="13" string="that" />
            <token id="14" string="stemmed" />
            <token id="15" string="from" />
            <token id="16" string="a" />
            <token id="17" string="2 1/2" />
            <token id="18" string="-" />
            <token id="19" string="year-old" />
            <token id="20" string="'s" />
            <token id="21" string="report" />
            <token id="22" string="to" />
            <token id="23" string="his" />
            <token id="24" string="mother" />
            <token id="25" string="six" />
            <token id="26" string="years" />
            <token id="27" string="ago" />
            <token id="28" string="that" />
            <token id="29" string="he" />
            <token id="30" string="had" />
            <token id="31" string="been" />
            <token id="32" string="sodomized" />
            <token id="33" string="at" />
            <token id="34" string="his" />
            <token id="35" string="school" />
            <token id="36" string="by" />
            <token id="37" string="a" />
            <token id="38" string="&quot;" />
            <token id="39" string="Mr." />
            <token id="40" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="4" string="year-old 's report" type="NP">
          <tokens>
            <token id="19" string="year-old" />
            <token id="20" string="'s" />
            <token id="21" string="report" />
          </tokens>
        </chunking>
        <chunking id="5" string="been sodomized at his school by a `` Mr. Ray" type="VP">
          <tokens>
            <token id="31" string="been" />
            <token id="32" string="sodomized" />
            <token id="33" string="at" />
            <token id="34" string="his" />
            <token id="35" string="school" />
            <token id="36" string="by" />
            <token id="37" string="a" />
            <token id="38" string="&quot;" />
            <token id="39" string="Mr." />
            <token id="40" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="6" string="a case that stemmed from a 2 1/2 - year-old 's report to his mother six years ago that he had been sodomized at his school by a `` Mr. Ray" type="NP">
          <tokens>
            <token id="11" string="a" />
            <token id="12" string="case" />
            <token id="13" string="that" />
            <token id="14" string="stemmed" />
            <token id="15" string="from" />
            <token id="16" string="a" />
            <token id="17" string="2 1/2" />
            <token id="18" string="-" />
            <token id="19" string="year-old" />
            <token id="20" string="'s" />
            <token id="21" string="report" />
            <token id="22" string="to" />
            <token id="23" string="his" />
            <token id="24" string="mother" />
            <token id="25" string="six" />
            <token id="26" string="years" />
            <token id="27" string="ago" />
            <token id="28" string="that" />
            <token id="29" string="he" />
            <token id="30" string="had" />
            <token id="31" string="been" />
            <token id="32" string="sodomized" />
            <token id="33" string="at" />
            <token id="34" string="his" />
            <token id="35" string="school" />
            <token id="36" string="by" />
            <token id="37" string="a" />
            <token id="38" string="&quot;" />
            <token id="39" string="Mr." />
            <token id="40" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="7" string="a 2 1/2" type="NP">
          <tokens>
            <token id="16" string="a" />
            <token id="17" string="2 1/2" />
          </tokens>
        </chunking>
        <chunking id="8" string="sodomized at his school by a `` Mr. Ray" type="VP">
          <tokens>
            <token id="32" string="sodomized" />
            <token id="33" string="at" />
            <token id="34" string="his" />
            <token id="35" string="school" />
            <token id="36" string="by" />
            <token id="37" string="a" />
            <token id="38" string="&quot;" />
            <token id="39" string="Mr." />
            <token id="40" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="9" string="The acquittals" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="acquittals" />
          </tokens>
        </chunking>
        <chunking id="10" string="the longest criminal trial in history , a case that stemmed from a 2 1/2 - year-old 's report to his mother six years ago that he had been sodomized at his school by a `` Mr. Ray" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="longest" />
            <token id="6" string="criminal" />
            <token id="7" string="trial" />
            <token id="8" string="in" />
            <token id="9" string="history" />
            <token id="10" string="," />
            <token id="11" string="a" />
            <token id="12" string="case" />
            <token id="13" string="that" />
            <token id="14" string="stemmed" />
            <token id="15" string="from" />
            <token id="16" string="a" />
            <token id="17" string="2 1/2" />
            <token id="18" string="-" />
            <token id="19" string="year-old" />
            <token id="20" string="'s" />
            <token id="21" string="report" />
            <token id="22" string="to" />
            <token id="23" string="his" />
            <token id="24" string="mother" />
            <token id="25" string="six" />
            <token id="26" string="years" />
            <token id="27" string="ago" />
            <token id="28" string="that" />
            <token id="29" string="he" />
            <token id="30" string="had" />
            <token id="31" string="been" />
            <token id="32" string="sodomized" />
            <token id="33" string="at" />
            <token id="34" string="his" />
            <token id="35" string="school" />
            <token id="36" string="by" />
            <token id="37" string="a" />
            <token id="38" string="&quot;" />
            <token id="39" string="Mr." />
            <token id="40" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="11" string="history" type="NP">
          <tokens>
            <token id="9" string="history" />
          </tokens>
        </chunking>
        <chunking id="12" string="his mother six years ago" type="NP">
          <tokens>
            <token id="23" string="his" />
            <token id="24" string="mother" />
            <token id="25" string="six" />
            <token id="26" string="years" />
            <token id="27" string="ago" />
          </tokens>
        </chunking>
        <chunking id="13" string="had been sodomized at his school by a `` Mr. Ray" type="VP">
          <tokens>
            <token id="30" string="had" />
            <token id="31" string="been" />
            <token id="32" string="sodomized" />
            <token id="33" string="at" />
            <token id="34" string="his" />
            <token id="35" string="school" />
            <token id="36" string="by" />
            <token id="37" string="a" />
            <token id="38" string="&quot;" />
            <token id="39" string="Mr." />
            <token id="40" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="14" string="a `` Mr. Ray" type="NP">
          <tokens>
            <token id="37" string="a" />
            <token id="38" string="&quot;" />
            <token id="39" string="Mr." />
            <token id="40" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="15" string="six years" type="NP">
          <tokens>
            <token id="25" string="six" />
            <token id="26" string="years" />
          </tokens>
        </chunking>
        <chunking id="16" string="stemmed from a 2 1/2 - year-old 's report to his mother six years ago that he had been sodomized at his school by a `` Mr. Ray" type="VP">
          <tokens>
            <token id="14" string="stemmed" />
            <token id="15" string="from" />
            <token id="16" string="a" />
            <token id="17" string="2 1/2" />
            <token id="18" string="-" />
            <token id="19" string="year-old" />
            <token id="20" string="'s" />
            <token id="21" string="report" />
            <token id="22" string="to" />
            <token id="23" string="his" />
            <token id="24" string="mother" />
            <token id="25" string="six" />
            <token id="26" string="years" />
            <token id="27" string="ago" />
            <token id="28" string="that" />
            <token id="29" string="he" />
            <token id="30" string="had" />
            <token id="31" string="been" />
            <token id="32" string="sodomized" />
            <token id="33" string="at" />
            <token id="34" string="his" />
            <token id="35" string="school" />
            <token id="36" string="by" />
            <token id="37" string="a" />
            <token id="38" string="&quot;" />
            <token id="39" string="Mr." />
            <token id="40" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="17" string="a 2 1/2 - year-old 's report" type="NP">
          <tokens>
            <token id="16" string="a" />
            <token id="17" string="2 1/2" />
            <token id="18" string="-" />
            <token id="19" string="year-old" />
            <token id="20" string="'s" />
            <token id="21" string="report" />
          </tokens>
        </chunking>
        <chunking id="18" string="the longest criminal trial" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="longest" />
            <token id="6" string="criminal" />
            <token id="7" string="trial" />
          </tokens>
        </chunking>
        <chunking id="19" string="his school" type="NP">
          <tokens>
            <token id="34" string="his" />
            <token id="35" string="school" />
          </tokens>
        </chunking>
        <chunking id="20" string="his mother" type="NP">
          <tokens>
            <token id="23" string="his" />
            <token id="24" string="mother" />
          </tokens>
        </chunking>
        <chunking id="21" string="a case" type="NP">
          <tokens>
            <token id="11" string="a" />
            <token id="12" string="case" />
          </tokens>
        </chunking>
        <chunking id="22" string="that stemmed from a 2 1/2 - year-old 's report to his mother six years ago that he had been sodomized at his school by a `` Mr. Ray" type="SBAR">
          <tokens>
            <token id="13" string="that" />
            <token id="14" string="stemmed" />
            <token id="15" string="from" />
            <token id="16" string="a" />
            <token id="17" string="2 1/2" />
            <token id="18" string="-" />
            <token id="19" string="year-old" />
            <token id="20" string="'s" />
            <token id="21" string="report" />
            <token id="22" string="to" />
            <token id="23" string="his" />
            <token id="24" string="mother" />
            <token id="25" string="six" />
            <token id="26" string="years" />
            <token id="27" string="ago" />
            <token id="28" string="that" />
            <token id="29" string="he" />
            <token id="30" string="had" />
            <token id="31" string="been" />
            <token id="32" string="sodomized" />
            <token id="33" string="at" />
            <token id="34" string="his" />
            <token id="35" string="school" />
            <token id="36" string="by" />
            <token id="37" string="a" />
            <token id="38" string="&quot;" />
            <token id="39" string="Mr." />
            <token id="40" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="23" string="year-old 's" type="NP">
          <tokens>
            <token id="19" string="year-old" />
            <token id="20" string="'s" />
          </tokens>
        </chunking>
        <chunking id="24" string="he" type="NP">
          <tokens>
            <token id="29" string="he" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">acquittals</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">concluded</governor>
          <dependent id="2">acquittals</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">concluded</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">trial</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">trial</governor>
          <dependent id="5">longest</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">trial</governor>
          <dependent id="6">criminal</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">concluded</governor>
          <dependent id="7">trial</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">history</governor>
          <dependent id="8">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">trial</governor>
          <dependent id="9">history</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">case</governor>
          <dependent id="11">a</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="9">history</governor>
          <dependent id="12">case</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">stemmed</governor>
          <dependent id="13">that</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="12">case</governor>
          <dependent id="14">stemmed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">2 1/2</governor>
          <dependent id="15">from</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="17">2 1/2</governor>
          <dependent id="16">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">stemmed</governor>
          <dependent id="17">2 1/2</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="21">report</governor>
          <dependent id="19">year-old</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">year-old</governor>
          <dependent id="20">'s</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="17">2 1/2</governor>
          <dependent id="21">report</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">mother</governor>
          <dependent id="22">to</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="24">mother</governor>
          <dependent id="23">his</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">stemmed</governor>
          <dependent id="24">mother</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="26">years</governor>
          <dependent id="25">six</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="24">mother</governor>
          <dependent id="26">years</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">years</governor>
          <dependent id="27">ago</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="32">sodomized</governor>
          <dependent id="28">that</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="32">sodomized</governor>
          <dependent id="29">he</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="32">sodomized</governor>
          <dependent id="30">had</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="32">sodomized</governor>
          <dependent id="31">been</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="14">stemmed</governor>
          <dependent id="32">sodomized</dependent>
        </dependency>
        <dependency type="case">
          <governor id="35">school</governor>
          <dependent id="33">at</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="35">school</governor>
          <dependent id="34">his</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="32">sodomized</governor>
          <dependent id="35">school</dependent>
        </dependency>
        <dependency type="case">
          <governor id="40">Ray</governor>
          <dependent id="36">by</dependent>
        </dependency>
        <dependency type="det">
          <governor id="40">Ray</governor>
          <dependent id="37">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="40">Ray</governor>
          <dependent id="39">Mr.</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="32">sodomized</governor>
          <dependent id="40">Ray</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="2 1/2 - year-old" type="DURATION" score="0.0">
          <tokens>
            <token id="17" string="2 1/2" />
            <token id="18" string="-" />
            <token id="19" string="year-old" />
          </tokens>
        </entity>
        <entity id="2" string="six years ago" type="DATE" score="0.0">
          <tokens>
            <token id="25" string="six" />
            <token id="26" string="years" />
            <token id="27" string="ago" />
          </tokens>
        </entity>
        <entity id="3" string="Ray" type="PERSON" score="0.0">
          <tokens>
            <token id="40" string="Ray" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="26" has_coreference="true">
      <content>The case ultimately cost taxpayers more than $15 million, altered scores of lives and careers, and provided a national focal point for the issue of child abuse.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="ultimately" lemma="ultimately" stem="ultim" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="cost" lemma="cost" stem="cost" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="taxpayers" lemma="taxpayer" stem="taxpay" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="more" lemma="more" stem="more" pos="JJR" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="than" lemma="than" stem="than" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="$" lemma="$" stem="$" pos="$" type="Symbol" isStopWord="true" ner="MONEY" is_referenced="false" is_refers="false" />
        <token id="9" string="15" lemma="15" stem="15" pos="CD" type="Number" isStopWord="false" ner="MONEY" is_referenced="false" is_refers="false" />
        <token id="10" string="million" lemma="million" stem="million" pos="CD" type="Word" isStopWord="false" ner="MONEY" is_referenced="false" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="altered" lemma="altered" stem="alter" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="scores" lemma="score" stem="score" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="lives" lemma="life" stem="live" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="careers" lemma="career" stem="career" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="provided" lemma="provide" stem="provid" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="national" lemma="national" stem="nation" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="focal" lemma="focal" stem="focal" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="point" lemma="point" stem="point" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="issue" lemma="issue" stem="issu" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="child" lemma="child" stem="child" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="abuse" lemma="abuse" stem="abus" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN case)) (ADVP (RB ultimately)) (VP (VP (VBD cost) (NP (NNS taxpayers)) (NP (NP (QP (JJR more) (IN than) ($ $) (CD 15) (CD million))) (, ,) (NP (NP (JJ altered) (NNS scores)) (PP (IN of) (NP (NNS lives) (CC and) (NNS careers)))))) (, ,) (CC and) (VP (VBD provided) (NP (DT a) (JJ national) (JJ focal) (NN point)) (PP (IN for) (NP (NP (DT the) (NN issue)) (PP (IN of) (NP (NN child) (NN abuse))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="more than $ 15 million" type="NP">
          <tokens>
            <token id="6" string="more" />
            <token id="7" string="than" />
            <token id="8" string="$" />
            <token id="9" string="15" />
            <token id="10" string="million" />
          </tokens>
        </chunking>
        <chunking id="2" string="provided a national focal point for the issue of child abuse" type="VP">
          <tokens>
            <token id="20" string="provided" />
            <token id="21" string="a" />
            <token id="22" string="national" />
            <token id="23" string="focal" />
            <token id="24" string="point" />
            <token id="25" string="for" />
            <token id="26" string="the" />
            <token id="27" string="issue" />
            <token id="28" string="of" />
            <token id="29" string="child" />
            <token id="30" string="abuse" />
          </tokens>
        </chunking>
        <chunking id="3" string="the issue" type="NP">
          <tokens>
            <token id="26" string="the" />
            <token id="27" string="issue" />
          </tokens>
        </chunking>
        <chunking id="4" string="taxpayers" type="NP">
          <tokens>
            <token id="5" string="taxpayers" />
          </tokens>
        </chunking>
        <chunking id="5" string="child abuse" type="NP">
          <tokens>
            <token id="29" string="child" />
            <token id="30" string="abuse" />
          </tokens>
        </chunking>
        <chunking id="6" string="the issue of child abuse" type="NP">
          <tokens>
            <token id="26" string="the" />
            <token id="27" string="issue" />
            <token id="28" string="of" />
            <token id="29" string="child" />
            <token id="30" string="abuse" />
          </tokens>
        </chunking>
        <chunking id="7" string="The case" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="case" />
          </tokens>
        </chunking>
        <chunking id="8" string="cost taxpayers more than $ 15 million , altered scores of lives and careers , and provided a national focal point for the issue of child abuse" type="VP">
          <tokens>
            <token id="4" string="cost" />
            <token id="5" string="taxpayers" />
            <token id="6" string="more" />
            <token id="7" string="than" />
            <token id="8" string="$" />
            <token id="9" string="15" />
            <token id="10" string="million" />
            <token id="11" string="," />
            <token id="12" string="altered" />
            <token id="13" string="scores" />
            <token id="14" string="of" />
            <token id="15" string="lives" />
            <token id="16" string="and" />
            <token id="17" string="careers" />
            <token id="18" string="," />
            <token id="19" string="and" />
            <token id="20" string="provided" />
            <token id="21" string="a" />
            <token id="22" string="national" />
            <token id="23" string="focal" />
            <token id="24" string="point" />
            <token id="25" string="for" />
            <token id="26" string="the" />
            <token id="27" string="issue" />
            <token id="28" string="of" />
            <token id="29" string="child" />
            <token id="30" string="abuse" />
          </tokens>
        </chunking>
        <chunking id="9" string="more than $ 15 million , altered scores of lives and careers" type="NP">
          <tokens>
            <token id="6" string="more" />
            <token id="7" string="than" />
            <token id="8" string="$" />
            <token id="9" string="15" />
            <token id="10" string="million" />
            <token id="11" string="," />
            <token id="12" string="altered" />
            <token id="13" string="scores" />
            <token id="14" string="of" />
            <token id="15" string="lives" />
            <token id="16" string="and" />
            <token id="17" string="careers" />
          </tokens>
        </chunking>
        <chunking id="10" string="cost taxpayers more than $ 15 million , altered scores of lives and careers" type="VP">
          <tokens>
            <token id="4" string="cost" />
            <token id="5" string="taxpayers" />
            <token id="6" string="more" />
            <token id="7" string="than" />
            <token id="8" string="$" />
            <token id="9" string="15" />
            <token id="10" string="million" />
            <token id="11" string="," />
            <token id="12" string="altered" />
            <token id="13" string="scores" />
            <token id="14" string="of" />
            <token id="15" string="lives" />
            <token id="16" string="and" />
            <token id="17" string="careers" />
          </tokens>
        </chunking>
        <chunking id="11" string="altered scores" type="NP">
          <tokens>
            <token id="12" string="altered" />
            <token id="13" string="scores" />
          </tokens>
        </chunking>
        <chunking id="12" string="a national focal point" type="NP">
          <tokens>
            <token id="21" string="a" />
            <token id="22" string="national" />
            <token id="23" string="focal" />
            <token id="24" string="point" />
          </tokens>
        </chunking>
        <chunking id="13" string="altered scores of lives and careers" type="NP">
          <tokens>
            <token id="12" string="altered" />
            <token id="13" string="scores" />
            <token id="14" string="of" />
            <token id="15" string="lives" />
            <token id="16" string="and" />
            <token id="17" string="careers" />
          </tokens>
        </chunking>
        <chunking id="14" string="lives and careers" type="NP">
          <tokens>
            <token id="15" string="lives" />
            <token id="16" string="and" />
            <token id="17" string="careers" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">case</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">cost</governor>
          <dependent id="2">case</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">cost</governor>
          <dependent id="3">ultimately</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">cost</dependent>
        </dependency>
        <dependency type="iobj">
          <governor id="4">cost</governor>
          <dependent id="5">taxpayers</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="8">$</governor>
          <dependent id="6">more</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="6">more</governor>
          <dependent id="7">than</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">cost</governor>
          <dependent id="8">$</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">million</governor>
          <dependent id="9">15</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="8">$</governor>
          <dependent id="10">million</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">scores</governor>
          <dependent id="12">altered</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="8">$</governor>
          <dependent id="13">scores</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">lives</governor>
          <dependent id="14">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">scores</governor>
          <dependent id="15">lives</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="15">lives</governor>
          <dependent id="16">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="15">lives</governor>
          <dependent id="17">careers</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">cost</governor>
          <dependent id="19">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">cost</governor>
          <dependent id="20">provided</dependent>
        </dependency>
        <dependency type="det">
          <governor id="24">point</governor>
          <dependent id="21">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="24">point</governor>
          <dependent id="22">national</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="24">point</governor>
          <dependent id="23">focal</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="20">provided</governor>
          <dependent id="24">point</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">issue</governor>
          <dependent id="25">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="27">issue</governor>
          <dependent id="26">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">provided</governor>
          <dependent id="27">issue</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">abuse</governor>
          <dependent id="28">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="30">abuse</governor>
          <dependent id="29">child</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">issue</governor>
          <dependent id="30">abuse</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="$ 15 million" type="MONEY" score="0.0">
          <tokens>
            <token id="8" string="$" />
            <token id="9" string="15" />
            <token id="10" string="million" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="27" has_coreference="true">
      <content>Judge Pounders said he was not surprised by the verdicts: &amp;quot;I found it difficult to determine how they would view the evidence in the first place.</content>
      <tokens>
        <token id="1" string="Judge" lemma="Judge" stem="judg" pos="NNP" type="Word" isStopWord="false" ner="TITLE" is_referenced="false" is_refers="true" />
        <token id="2" string="Pounders" lemma="Pounders" stem="pounder" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="surprised" lemma="surprise" stem="surpris" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="verdicts" lemma="verdict" stem="verdict" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="found" lemma="find" stem="found" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="16" string="difficult" lemma="difficult" stem="difficult" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="determine" lemma="determine" stem="determin" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="how" lemma="how" stem="how" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="21" string="would" lemma="would" stem="would" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="view" lemma="view" stem="view" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="evidence" lemma="evidence" stem="evid" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="first" lemma="first" stem="first" pos="JJ" type="Word" isStopWord="false" ner="ORDINAL" is_referenced="false" is_refers="false" />
        <token id="28" string="place" lemma="place" stem="place" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (NNP Judge) (NNP Pounders)) (VP (VBD said) (SBAR (S (NP (PRP he)) (VP (VBD was) (RB not) (VP (VBN surprised) (PP (IN by) (NP (DT the) (NNS verdicts))))))))) (: :) (`` ``) (S (NP (PRP I)) (VP (VBD found) (S (NP (PRP it)) (ADJP (JJ difficult) (S (VP (TO to) (VP (VB determine) (SBAR (WHADVP (WRB how)) (S (NP (PRP they)) (VP (MD would) (VP (VB view) (NP (NP (DT the) (NN evidence)) (PP (IN in) (NP (DT the) (JJ first) (NN place))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the verdicts" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="verdicts" />
          </tokens>
        </chunking>
        <chunking id="2" string="Judge Pounders" type="NP">
          <tokens>
            <token id="1" string="Judge" />
            <token id="2" string="Pounders" />
          </tokens>
        </chunking>
        <chunking id="3" string="said he was not surprised by the verdicts" type="VP">
          <tokens>
            <token id="3" string="said" />
            <token id="4" string="he" />
            <token id="5" string="was" />
            <token id="6" string="not" />
            <token id="7" string="surprised" />
            <token id="8" string="by" />
            <token id="9" string="the" />
            <token id="10" string="verdicts" />
          </tokens>
        </chunking>
        <chunking id="4" string="surprised by the verdicts" type="VP">
          <tokens>
            <token id="7" string="surprised" />
            <token id="8" string="by" />
            <token id="9" string="the" />
            <token id="10" string="verdicts" />
          </tokens>
        </chunking>
        <chunking id="5" string="found it difficult to determine how they would view the evidence in the first place" type="VP">
          <tokens>
            <token id="14" string="found" />
            <token id="15" string="it" />
            <token id="16" string="difficult" />
            <token id="17" string="to" />
            <token id="18" string="determine" />
            <token id="19" string="how" />
            <token id="20" string="they" />
            <token id="21" string="would" />
            <token id="22" string="view" />
            <token id="23" string="the" />
            <token id="24" string="evidence" />
            <token id="25" string="in" />
            <token id="26" string="the" />
            <token id="27" string="first" />
            <token id="28" string="place" />
          </tokens>
        </chunking>
        <chunking id="6" string="the evidence" type="NP">
          <tokens>
            <token id="23" string="the" />
            <token id="24" string="evidence" />
          </tokens>
        </chunking>
        <chunking id="7" string="I" type="NP">
          <tokens>
            <token id="13" string="I" />
          </tokens>
        </chunking>
        <chunking id="8" string="it" type="NP">
          <tokens>
            <token id="15" string="it" />
          </tokens>
        </chunking>
        <chunking id="9" string="determine how they would view the evidence in the first place" type="VP">
          <tokens>
            <token id="18" string="determine" />
            <token id="19" string="how" />
            <token id="20" string="they" />
            <token id="21" string="would" />
            <token id="22" string="view" />
            <token id="23" string="the" />
            <token id="24" string="evidence" />
            <token id="25" string="in" />
            <token id="26" string="the" />
            <token id="27" string="first" />
            <token id="28" string="place" />
          </tokens>
        </chunking>
        <chunking id="10" string="how" type="WHADVP">
          <tokens>
            <token id="19" string="how" />
          </tokens>
        </chunking>
        <chunking id="11" string="he was not surprised by the verdicts" type="SBAR">
          <tokens>
            <token id="4" string="he" />
            <token id="5" string="was" />
            <token id="6" string="not" />
            <token id="7" string="surprised" />
            <token id="8" string="by" />
            <token id="9" string="the" />
            <token id="10" string="verdicts" />
          </tokens>
        </chunking>
        <chunking id="12" string="they" type="NP">
          <tokens>
            <token id="20" string="they" />
          </tokens>
        </chunking>
        <chunking id="13" string="the first place" type="NP">
          <tokens>
            <token id="26" string="the" />
            <token id="27" string="first" />
            <token id="28" string="place" />
          </tokens>
        </chunking>
        <chunking id="14" string="how they would view the evidence in the first place" type="SBAR">
          <tokens>
            <token id="19" string="how" />
            <token id="20" string="they" />
            <token id="21" string="would" />
            <token id="22" string="view" />
            <token id="23" string="the" />
            <token id="24" string="evidence" />
            <token id="25" string="in" />
            <token id="26" string="the" />
            <token id="27" string="first" />
            <token id="28" string="place" />
          </tokens>
        </chunking>
        <chunking id="15" string="to determine how they would view the evidence in the first place" type="VP">
          <tokens>
            <token id="17" string="to" />
            <token id="18" string="determine" />
            <token id="19" string="how" />
            <token id="20" string="they" />
            <token id="21" string="would" />
            <token id="22" string="view" />
            <token id="23" string="the" />
            <token id="24" string="evidence" />
            <token id="25" string="in" />
            <token id="26" string="the" />
            <token id="27" string="first" />
            <token id="28" string="place" />
          </tokens>
        </chunking>
        <chunking id="16" string="the evidence in the first place" type="NP">
          <tokens>
            <token id="23" string="the" />
            <token id="24" string="evidence" />
            <token id="25" string="in" />
            <token id="26" string="the" />
            <token id="27" string="first" />
            <token id="28" string="place" />
          </tokens>
        </chunking>
        <chunking id="17" string="difficult to determine how they would view the evidence in the first place" type="ADJP">
          <tokens>
            <token id="16" string="difficult" />
            <token id="17" string="to" />
            <token id="18" string="determine" />
            <token id="19" string="how" />
            <token id="20" string="they" />
            <token id="21" string="would" />
            <token id="22" string="view" />
            <token id="23" string="the" />
            <token id="24" string="evidence" />
            <token id="25" string="in" />
            <token id="26" string="the" />
            <token id="27" string="first" />
            <token id="28" string="place" />
          </tokens>
        </chunking>
        <chunking id="18" string="was not surprised by the verdicts" type="VP">
          <tokens>
            <token id="5" string="was" />
            <token id="6" string="not" />
            <token id="7" string="surprised" />
            <token id="8" string="by" />
            <token id="9" string="the" />
            <token id="10" string="verdicts" />
          </tokens>
        </chunking>
        <chunking id="19" string="would view the evidence in the first place" type="VP">
          <tokens>
            <token id="21" string="would" />
            <token id="22" string="view" />
            <token id="23" string="the" />
            <token id="24" string="evidence" />
            <token id="25" string="in" />
            <token id="26" string="the" />
            <token id="27" string="first" />
            <token id="28" string="place" />
          </tokens>
        </chunking>
        <chunking id="20" string="he" type="NP">
          <tokens>
            <token id="4" string="he" />
          </tokens>
        </chunking>
        <chunking id="21" string="view the evidence in the first place" type="VP">
          <tokens>
            <token id="22" string="view" />
            <token id="23" string="the" />
            <token id="24" string="evidence" />
            <token id="25" string="in" />
            <token id="26" string="the" />
            <token id="27" string="first" />
            <token id="28" string="place" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">Pounders</governor>
          <dependent id="1">Judge</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">said</governor>
          <dependent id="2">Pounders</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">said</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="7">surprised</governor>
          <dependent id="4">he</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="7">surprised</governor>
          <dependent id="5">was</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="7">surprised</governor>
          <dependent id="6">not</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">said</governor>
          <dependent id="7">surprised</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">verdicts</governor>
          <dependent id="8">by</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">verdicts</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">surprised</governor>
          <dependent id="10">verdicts</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">found</governor>
          <dependent id="13">I</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="3">said</governor>
          <dependent id="14">found</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">difficult</governor>
          <dependent id="15">it</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="14">found</governor>
          <dependent id="16">difficult</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="18">determine</governor>
          <dependent id="17">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="16">difficult</governor>
          <dependent id="18">determine</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="22">view</governor>
          <dependent id="19">how</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">view</governor>
          <dependent id="20">they</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="22">view</governor>
          <dependent id="21">would</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="18">determine</governor>
          <dependent id="22">view</dependent>
        </dependency>
        <dependency type="det">
          <governor id="24">evidence</governor>
          <dependent id="23">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="22">view</governor>
          <dependent id="24">evidence</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">place</governor>
          <dependent id="25">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="28">place</governor>
          <dependent id="26">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="28">place</governor>
          <dependent id="27">first</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="24">evidence</governor>
          <dependent id="28">place</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="first" type="ORDINAL" score="0.0">
          <tokens>
            <token id="27" string="first" />
          </tokens>
        </entity>
        <entity id="2" string="Pounders" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Pounders" />
          </tokens>
        </entity>
        <entity id="3" string="Judge" type="TITLE" score="0.0">
          <tokens>
            <token id="1" string="Judge" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="28" has_coreference="true">
      <content>I thought that based on the evidence that was presented, the jury could do almost anything and still find rational support for it.</content>
      <tokens>
        <token id="1" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="thought" lemma="think" stem="thought" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="based" lemma="base" stem="base" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="evidence" lemma="evidence" stem="evid" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="presented" lemma="present" stem="present" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="could" lemma="could" stem="could" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="do" lemma="do" stem="do" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="almost" lemma="almost" stem="almost" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="anything" lemma="anything" stem="anyth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="still" lemma="still" stem="still" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="find" lemma="find" stem="find" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="rational" lemma="rational" stem="ration" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="support" lemma="support" stem="support" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP I)) (VP (VBD thought) (SBAR (IN that) (S (PP (VBN based) (PP (IN on) (NP (NP (DT the) (NN evidence)) (SBAR (WHNP (IN that)) (S (VP (VBD was) (VP (VBN presented)))))))) (, ,) (NP (DT the) (NN jury)) (VP (MD could) (VP (VP (VB do) (NP (RB almost) (NN anything))) (CC and) (ADVP (RB still)) (VP (VB find) (NP (JJ rational) (NN support)) (PP (IN for) (NP (PRP it))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the evidence that was presented" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="evidence" />
            <token id="8" string="that" />
            <token id="9" string="was" />
            <token id="10" string="presented" />
          </tokens>
        </chunking>
        <chunking id="2" string="the evidence" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="evidence" />
          </tokens>
        </chunking>
        <chunking id="3" string="find rational support for it" type="VP">
          <tokens>
            <token id="20" string="find" />
            <token id="21" string="rational" />
            <token id="22" string="support" />
            <token id="23" string="for" />
            <token id="24" string="it" />
          </tokens>
        </chunking>
        <chunking id="4" string="I" type="NP">
          <tokens>
            <token id="1" string="I" />
          </tokens>
        </chunking>
        <chunking id="5" string="it" type="NP">
          <tokens>
            <token id="24" string="it" />
          </tokens>
        </chunking>
        <chunking id="6" string="was presented" type="VP">
          <tokens>
            <token id="9" string="was" />
            <token id="10" string="presented" />
          </tokens>
        </chunking>
        <chunking id="7" string="almost anything" type="NP">
          <tokens>
            <token id="16" string="almost" />
            <token id="17" string="anything" />
          </tokens>
        </chunking>
        <chunking id="8" string="presented" type="VP">
          <tokens>
            <token id="10" string="presented" />
          </tokens>
        </chunking>
        <chunking id="9" string="do almost anything and still find rational support for it" type="VP">
          <tokens>
            <token id="15" string="do" />
            <token id="16" string="almost" />
            <token id="17" string="anything" />
            <token id="18" string="and" />
            <token id="19" string="still" />
            <token id="20" string="find" />
            <token id="21" string="rational" />
            <token id="22" string="support" />
            <token id="23" string="for" />
            <token id="24" string="it" />
          </tokens>
        </chunking>
        <chunking id="10" string="rational support" type="NP">
          <tokens>
            <token id="21" string="rational" />
            <token id="22" string="support" />
          </tokens>
        </chunking>
        <chunking id="11" string="that based on the evidence that was presented , the jury could do almost anything and still find rational support for it" type="SBAR">
          <tokens>
            <token id="3" string="that" />
            <token id="4" string="based" />
            <token id="5" string="on" />
            <token id="6" string="the" />
            <token id="7" string="evidence" />
            <token id="8" string="that" />
            <token id="9" string="was" />
            <token id="10" string="presented" />
            <token id="11" string="," />
            <token id="12" string="the" />
            <token id="13" string="jury" />
            <token id="14" string="could" />
            <token id="15" string="do" />
            <token id="16" string="almost" />
            <token id="17" string="anything" />
            <token id="18" string="and" />
            <token id="19" string="still" />
            <token id="20" string="find" />
            <token id="21" string="rational" />
            <token id="22" string="support" />
            <token id="23" string="for" />
            <token id="24" string="it" />
          </tokens>
        </chunking>
        <chunking id="12" string="thought that based on the evidence that was presented , the jury could do almost anything and still find rational support for it" type="VP">
          <tokens>
            <token id="2" string="thought" />
            <token id="3" string="that" />
            <token id="4" string="based" />
            <token id="5" string="on" />
            <token id="6" string="the" />
            <token id="7" string="evidence" />
            <token id="8" string="that" />
            <token id="9" string="was" />
            <token id="10" string="presented" />
            <token id="11" string="," />
            <token id="12" string="the" />
            <token id="13" string="jury" />
            <token id="14" string="could" />
            <token id="15" string="do" />
            <token id="16" string="almost" />
            <token id="17" string="anything" />
            <token id="18" string="and" />
            <token id="19" string="still" />
            <token id="20" string="find" />
            <token id="21" string="rational" />
            <token id="22" string="support" />
            <token id="23" string="for" />
            <token id="24" string="it" />
          </tokens>
        </chunking>
        <chunking id="13" string="that was presented" type="SBAR">
          <tokens>
            <token id="8" string="that" />
            <token id="9" string="was" />
            <token id="10" string="presented" />
          </tokens>
        </chunking>
        <chunking id="14" string="do almost anything" type="VP">
          <tokens>
            <token id="15" string="do" />
            <token id="16" string="almost" />
            <token id="17" string="anything" />
          </tokens>
        </chunking>
        <chunking id="15" string="the jury" type="NP">
          <tokens>
            <token id="12" string="the" />
            <token id="13" string="jury" />
          </tokens>
        </chunking>
        <chunking id="16" string="could do almost anything and still find rational support for it" type="VP">
          <tokens>
            <token id="14" string="could" />
            <token id="15" string="do" />
            <token id="16" string="almost" />
            <token id="17" string="anything" />
            <token id="18" string="and" />
            <token id="19" string="still" />
            <token id="20" string="find" />
            <token id="21" string="rational" />
            <token id="22" string="support" />
            <token id="23" string="for" />
            <token id="24" string="it" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">thought</governor>
          <dependent id="1">I</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">thought</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="15">do</governor>
          <dependent id="3">that</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">evidence</governor>
          <dependent id="4">based</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="4">based</governor>
          <dependent id="5">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">evidence</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="15">do</governor>
          <dependent id="7">evidence</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="10">presented</governor>
          <dependent id="8">that</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="10">presented</governor>
          <dependent id="9">was</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="7">evidence</governor>
          <dependent id="10">presented</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">jury</governor>
          <dependent id="12">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">do</governor>
          <dependent id="13">jury</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="15">do</governor>
          <dependent id="14">could</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="2">thought</governor>
          <dependent id="15">do</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="17">anything</governor>
          <dependent id="16">almost</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">do</governor>
          <dependent id="17">anything</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="15">do</governor>
          <dependent id="18">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="20">find</governor>
          <dependent id="19">still</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="15">do</governor>
          <dependent id="20">find</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="22">support</governor>
          <dependent id="21">rational</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="20">find</governor>
          <dependent id="22">support</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">it</governor>
          <dependent id="23">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">find</governor>
          <dependent id="24">it</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="29" has_coreference="true">
      <content>&amp;quot;It is very difficult to test the credibility of children, and when you go beyond that, the natural tendency for adults is to look for corroboration.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="very" lemma="very" stem="veri" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="difficult" lemma="difficult" stem="difficult" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="test" lemma="test" stem="test" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="credibility" lemma="credibility" stem="credibl" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="you" lemma="you" stem="you" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="go" lemma="go" stem="go" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="beyond" lemma="beyond" stem="beyond" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="that" lemma="that" stem="that" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="natural" lemma="natural" stem="natur" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="tendency" lemma="tendency" stem="tendenc" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="adults" lemma="adult" stem="adult" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="look" lemma="look" stem="look" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="corroboration" lemma="corroboration" stem="corrobor" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (NP (PRP It)) (VP (VBZ is) (ADJP (RB very) (JJ difficult)) (S (VP (TO to) (VP (VB test) (NP (NP (DT the) (NN credibility)) (PP (IN of) (NP (NNS children))))))))) (, ,) (CC and) (S (SBAR (WHADVP (WRB when)) (S (NP (PRP you)) (VP (VBP go) (PP (IN beyond) (NP (DT that)))))) (, ,) (NP (NP (DT the) (JJ natural) (NN tendency)) (PP (IN for) (NP (NNS adults)))) (VP (VBZ is) (S (VP (TO to) (VP (VB look) (PP (IN for) (NP (NN corroboration)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="look for corroboration" type="VP">
          <tokens>
            <token id="27" string="look" />
            <token id="28" string="for" />
            <token id="29" string="corroboration" />
          </tokens>
        </chunking>
        <chunking id="2" string="to test the credibility of children" type="VP">
          <tokens>
            <token id="6" string="to" />
            <token id="7" string="test" />
            <token id="8" string="the" />
            <token id="9" string="credibility" />
            <token id="10" string="of" />
            <token id="11" string="children" />
          </tokens>
        </chunking>
        <chunking id="3" string="adults" type="NP">
          <tokens>
            <token id="24" string="adults" />
          </tokens>
        </chunking>
        <chunking id="4" string="It" type="NP">
          <tokens>
            <token id="2" string="It" />
          </tokens>
        </chunking>
        <chunking id="5" string="test the credibility of children" type="VP">
          <tokens>
            <token id="7" string="test" />
            <token id="8" string="the" />
            <token id="9" string="credibility" />
            <token id="10" string="of" />
            <token id="11" string="children" />
          </tokens>
        </chunking>
        <chunking id="6" string="when" type="WHADVP">
          <tokens>
            <token id="14" string="when" />
          </tokens>
        </chunking>
        <chunking id="7" string="that" type="NP">
          <tokens>
            <token id="18" string="that" />
          </tokens>
        </chunking>
        <chunking id="8" string="the natural tendency" type="NP">
          <tokens>
            <token id="20" string="the" />
            <token id="21" string="natural" />
            <token id="22" string="tendency" />
          </tokens>
        </chunking>
        <chunking id="9" string="go beyond that" type="VP">
          <tokens>
            <token id="16" string="go" />
            <token id="17" string="beyond" />
            <token id="18" string="that" />
          </tokens>
        </chunking>
        <chunking id="10" string="children" type="NP">
          <tokens>
            <token id="11" string="children" />
          </tokens>
        </chunking>
        <chunking id="11" string="corroboration" type="NP">
          <tokens>
            <token id="29" string="corroboration" />
          </tokens>
        </chunking>
        <chunking id="12" string="is very difficult to test the credibility of children" type="VP">
          <tokens>
            <token id="3" string="is" />
            <token id="4" string="very" />
            <token id="5" string="difficult" />
            <token id="6" string="to" />
            <token id="7" string="test" />
            <token id="8" string="the" />
            <token id="9" string="credibility" />
            <token id="10" string="of" />
            <token id="11" string="children" />
          </tokens>
        </chunking>
        <chunking id="13" string="very difficult" type="ADJP">
          <tokens>
            <token id="4" string="very" />
            <token id="5" string="difficult" />
          </tokens>
        </chunking>
        <chunking id="14" string="when you go beyond that" type="SBAR">
          <tokens>
            <token id="14" string="when" />
            <token id="15" string="you" />
            <token id="16" string="go" />
            <token id="17" string="beyond" />
            <token id="18" string="that" />
          </tokens>
        </chunking>
        <chunking id="15" string="is to look for corroboration" type="VP">
          <tokens>
            <token id="25" string="is" />
            <token id="26" string="to" />
            <token id="27" string="look" />
            <token id="28" string="for" />
            <token id="29" string="corroboration" />
          </tokens>
        </chunking>
        <chunking id="16" string="the credibility of children" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="credibility" />
            <token id="10" string="of" />
            <token id="11" string="children" />
          </tokens>
        </chunking>
        <chunking id="17" string="to look for corroboration" type="VP">
          <tokens>
            <token id="26" string="to" />
            <token id="27" string="look" />
            <token id="28" string="for" />
            <token id="29" string="corroboration" />
          </tokens>
        </chunking>
        <chunking id="18" string="the credibility" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="credibility" />
          </tokens>
        </chunking>
        <chunking id="19" string="the natural tendency for adults" type="NP">
          <tokens>
            <token id="20" string="the" />
            <token id="21" string="natural" />
            <token id="22" string="tendency" />
            <token id="23" string="for" />
            <token id="24" string="adults" />
          </tokens>
        </chunking>
        <chunking id="20" string="you" type="NP">
          <tokens>
            <token id="15" string="you" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="5">difficult</governor>
          <dependent id="2">It</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="5">difficult</governor>
          <dependent id="3">is</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">difficult</governor>
          <dependent id="4">very</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">difficult</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="7">test</governor>
          <dependent id="6">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="5">difficult</governor>
          <dependent id="7">test</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">credibility</governor>
          <dependent id="8">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">test</governor>
          <dependent id="9">credibility</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">children</governor>
          <dependent id="10">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">credibility</governor>
          <dependent id="11">children</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="5">difficult</governor>
          <dependent id="13">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="16">go</governor>
          <dependent id="14">when</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">go</governor>
          <dependent id="15">you</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="25">is</governor>
          <dependent id="16">go</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">that</governor>
          <dependent id="17">beyond</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">go</governor>
          <dependent id="18">that</dependent>
        </dependency>
        <dependency type="det">
          <governor id="22">tendency</governor>
          <dependent id="20">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="22">tendency</governor>
          <dependent id="21">natural</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="25">is</governor>
          <dependent id="22">tendency</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">adults</governor>
          <dependent id="23">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="22">tendency</governor>
          <dependent id="24">adults</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="5">difficult</governor>
          <dependent id="25">is</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="27">look</governor>
          <dependent id="26">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="25">is</governor>
          <dependent id="27">look</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">corroboration</governor>
          <dependent id="28">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">look</governor>
          <dependent id="29">corroboration</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="30" has_coreference="true">
      <content>It was very difficult to find corroboration in this case.&amp;quot;</content>
      <tokens>
        <token id="1" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="very" lemma="very" stem="veri" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="difficult" lemma="difficult" stem="difficult" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="find" lemma="find" stem="find" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="corroboration" lemma="corroboration" stem="corrobor" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="11" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP It)) (VP (VBD was) (ADJP (RB very) (JJ difficult) (S (VP (TO to) (VP (VB find) (NP (NN corroboration)) (PP (IN in) (NP (DT this) (NN case)))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="very difficult to find corroboration in this case" type="ADJP">
          <tokens>
            <token id="3" string="very" />
            <token id="4" string="difficult" />
            <token id="5" string="to" />
            <token id="6" string="find" />
            <token id="7" string="corroboration" />
            <token id="8" string="in" />
            <token id="9" string="this" />
            <token id="10" string="case" />
          </tokens>
        </chunking>
        <chunking id="2" string="corroboration" type="NP">
          <tokens>
            <token id="7" string="corroboration" />
          </tokens>
        </chunking>
        <chunking id="3" string="to find corroboration in this case" type="VP">
          <tokens>
            <token id="5" string="to" />
            <token id="6" string="find" />
            <token id="7" string="corroboration" />
            <token id="8" string="in" />
            <token id="9" string="this" />
            <token id="10" string="case" />
          </tokens>
        </chunking>
        <chunking id="4" string="this case" type="NP">
          <tokens>
            <token id="9" string="this" />
            <token id="10" string="case" />
          </tokens>
        </chunking>
        <chunking id="5" string="It" type="NP">
          <tokens>
            <token id="1" string="It" />
          </tokens>
        </chunking>
        <chunking id="6" string="was very difficult to find corroboration in this case" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="very" />
            <token id="4" string="difficult" />
            <token id="5" string="to" />
            <token id="6" string="find" />
            <token id="7" string="corroboration" />
            <token id="8" string="in" />
            <token id="9" string="this" />
            <token id="10" string="case" />
          </tokens>
        </chunking>
        <chunking id="7" string="find corroboration in this case" type="VP">
          <tokens>
            <token id="6" string="find" />
            <token id="7" string="corroboration" />
            <token id="8" string="in" />
            <token id="9" string="this" />
            <token id="10" string="case" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">difficult</governor>
          <dependent id="1">It</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="4">difficult</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">difficult</governor>
          <dependent id="3">very</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">difficult</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="6">find</governor>
          <dependent id="5">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="4">difficult</governor>
          <dependent id="6">find</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">find</governor>
          <dependent id="7">corroboration</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">case</governor>
          <dependent id="8">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">case</governor>
          <dependent id="9">this</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">find</governor>
          <dependent id="10">case</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="31" has_coreference="true">
      <content>Asked if the case was worth its cost in money, lives and stress, Pounders said he thinks society benefited from the case.</content>
      <tokens>
        <token id="1" string="Asked" lemma="ask" stem="asked" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="if" lemma="if" stem="if" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="worth" lemma="worth" stem="worth" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="its" lemma="its" stem="it" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="cost" lemma="cost" stem="cost" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="money" lemma="money" stem="monei" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="lives" lemma="life" stem="live" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="stress" lemma="stress" stem="stress" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="Pounders" lemma="Pounders" stem="pounder" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="17" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="thinks" lemma="think" stem="think" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="society" lemma="society" stem="societi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="benefited" lemma="benefit" stem="benefit" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (VP (VBN Asked) (SBAR (IN if) (S (NP (DT the) (NN case)) (VP (VBD was) (ADJP (JJ worth) (NP (NP (PRP$ its) (NN cost)) (PP (IN in) (NP (NN money) (, ,) (NNS lives) (CC and) (NN stress)))))))))) (, ,) (NP (NNP Pounders)) (VP (VBD said) (SBAR (S (NP (PRP he)) (VP (VBZ thinks) (SBAR (S (NP (NN society)) (VP (VBD benefited) (PP (IN from) (NP (DT the) (NN case)))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Asked if the case was worth its cost in money , lives and stress" type="VP">
          <tokens>
            <token id="1" string="Asked" />
            <token id="2" string="if" />
            <token id="3" string="the" />
            <token id="4" string="case" />
            <token id="5" string="was" />
            <token id="6" string="worth" />
            <token id="7" string="its" />
            <token id="8" string="cost" />
            <token id="9" string="in" />
            <token id="10" string="money" />
            <token id="11" string="," />
            <token id="12" string="lives" />
            <token id="13" string="and" />
            <token id="14" string="stress" />
          </tokens>
        </chunking>
        <chunking id="2" string="the case" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="case" />
          </tokens>
        </chunking>
        <chunking id="3" string="society benefited from the case" type="SBAR">
          <tokens>
            <token id="20" string="society" />
            <token id="21" string="benefited" />
            <token id="22" string="from" />
            <token id="23" string="the" />
            <token id="24" string="case" />
          </tokens>
        </chunking>
        <chunking id="4" string="thinks society benefited from the case" type="VP">
          <tokens>
            <token id="19" string="thinks" />
            <token id="20" string="society" />
            <token id="21" string="benefited" />
            <token id="22" string="from" />
            <token id="23" string="the" />
            <token id="24" string="case" />
          </tokens>
        </chunking>
        <chunking id="5" string="worth its cost in money , lives and stress" type="ADJP">
          <tokens>
            <token id="6" string="worth" />
            <token id="7" string="its" />
            <token id="8" string="cost" />
            <token id="9" string="in" />
            <token id="10" string="money" />
            <token id="11" string="," />
            <token id="12" string="lives" />
            <token id="13" string="and" />
            <token id="14" string="stress" />
          </tokens>
        </chunking>
        <chunking id="6" string="its cost in money , lives and stress" type="NP">
          <tokens>
            <token id="7" string="its" />
            <token id="8" string="cost" />
            <token id="9" string="in" />
            <token id="10" string="money" />
            <token id="11" string="," />
            <token id="12" string="lives" />
            <token id="13" string="and" />
            <token id="14" string="stress" />
          </tokens>
        </chunking>
        <chunking id="7" string="Pounders" type="NP">
          <tokens>
            <token id="16" string="Pounders" />
          </tokens>
        </chunking>
        <chunking id="8" string="he thinks society benefited from the case" type="SBAR">
          <tokens>
            <token id="18" string="he" />
            <token id="19" string="thinks" />
            <token id="20" string="society" />
            <token id="21" string="benefited" />
            <token id="22" string="from" />
            <token id="23" string="the" />
            <token id="24" string="case" />
          </tokens>
        </chunking>
        <chunking id="9" string="benefited from the case" type="VP">
          <tokens>
            <token id="21" string="benefited" />
            <token id="22" string="from" />
            <token id="23" string="the" />
            <token id="24" string="case" />
          </tokens>
        </chunking>
        <chunking id="10" string="money , lives and stress" type="NP">
          <tokens>
            <token id="10" string="money" />
            <token id="11" string="," />
            <token id="12" string="lives" />
            <token id="13" string="and" />
            <token id="14" string="stress" />
          </tokens>
        </chunking>
        <chunking id="11" string="its cost" type="NP">
          <tokens>
            <token id="7" string="its" />
            <token id="8" string="cost" />
          </tokens>
        </chunking>
        <chunking id="12" string="said he thinks society benefited from the case" type="VP">
          <tokens>
            <token id="17" string="said" />
            <token id="18" string="he" />
            <token id="19" string="thinks" />
            <token id="20" string="society" />
            <token id="21" string="benefited" />
            <token id="22" string="from" />
            <token id="23" string="the" />
            <token id="24" string="case" />
          </tokens>
        </chunking>
        <chunking id="13" string="if the case was worth its cost in money , lives and stress" type="SBAR">
          <tokens>
            <token id="2" string="if" />
            <token id="3" string="the" />
            <token id="4" string="case" />
            <token id="5" string="was" />
            <token id="6" string="worth" />
            <token id="7" string="its" />
            <token id="8" string="cost" />
            <token id="9" string="in" />
            <token id="10" string="money" />
            <token id="11" string="," />
            <token id="12" string="lives" />
            <token id="13" string="and" />
            <token id="14" string="stress" />
          </tokens>
        </chunking>
        <chunking id="14" string="was worth its cost in money , lives and stress" type="VP">
          <tokens>
            <token id="5" string="was" />
            <token id="6" string="worth" />
            <token id="7" string="its" />
            <token id="8" string="cost" />
            <token id="9" string="in" />
            <token id="10" string="money" />
            <token id="11" string="," />
            <token id="12" string="lives" />
            <token id="13" string="and" />
            <token id="14" string="stress" />
          </tokens>
        </chunking>
        <chunking id="15" string="he" type="NP">
          <tokens>
            <token id="18" string="he" />
          </tokens>
        </chunking>
        <chunking id="16" string="society" type="NP">
          <tokens>
            <token id="20" string="society" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advcl">
          <governor id="17">said</governor>
          <dependent id="1">Asked</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="6">worth</governor>
          <dependent id="2">if</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">case</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">worth</governor>
          <dependent id="4">case</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="6">worth</governor>
          <dependent id="5">was</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="1">Asked</governor>
          <dependent id="6">worth</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="8">cost</governor>
          <dependent id="7">its</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="6">worth</governor>
          <dependent id="8">cost</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">money</governor>
          <dependent id="9">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">cost</governor>
          <dependent id="10">money</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="10">money</governor>
          <dependent id="12">lives</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="10">money</governor>
          <dependent id="13">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="10">money</governor>
          <dependent id="14">stress</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="17">said</governor>
          <dependent id="16">Pounders</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="17">said</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">thinks</governor>
          <dependent id="18">he</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="17">said</governor>
          <dependent id="19">thinks</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">benefited</governor>
          <dependent id="20">society</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="19">thinks</governor>
          <dependent id="21">benefited</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">case</governor>
          <dependent id="22">from</dependent>
        </dependency>
        <dependency type="det">
          <governor id="24">case</governor>
          <dependent id="23">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="21">benefited</governor>
          <dependent id="24">case</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Pounders" type="PERSON" score="0.0">
          <tokens>
            <token id="16" string="Pounders" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="32" has_coreference="true">
      <content>&amp;quot;People will be much more alert to the possibility of this type of offense taking place at preschool and at schools,&amp;quot; Pounders said.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="People" lemma="people" stem="peopl" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="will" lemma="will" stem="will" pos="MD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="be" lemma="be" stem="be" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="much" lemma="much" stem="much" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="more" lemma="more" stem="more" pos="RBR" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="alert" lemma="alert" stem="alert" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="possibility" lemma="possibility" stem="possibl" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="type" lemma="type" stem="type" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="offense" lemma="offense" stem="offens" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="taking" lemma="take" stem="take" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="place" lemma="place" stem="place" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="preschool" lemma="preschool" stem="preschool" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="schools" lemma="school" stem="school" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="Pounders" lemma="Pounders" stem="pounder" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="26" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (NP (NNS People)) (VP (MD will) (VP (VB be) (ADJP (ADJP (RB much) (RBR more) (JJ alert)) (PP (TO to) (NP (NP (DT the) (NN possibility)) (PP (IN of) (NP (NP (DT this) (NN type)) (PP (IN of) (NP (NN offense)))))))) (S (VP (VBG taking) (NP (NN place)) (PP (PP (IN at) (NP (JJ preschool))) (CC and) (PP (IN at) (NP (NNS schools))))))))) (, ,) ('' '') (NP (NNP Pounders)) (VP (VBD said)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="be much more alert to the possibility of this type of offense taking place at preschool and at schools" type="VP">
          <tokens>
            <token id="4" string="be" />
            <token id="5" string="much" />
            <token id="6" string="more" />
            <token id="7" string="alert" />
            <token id="8" string="to" />
            <token id="9" string="the" />
            <token id="10" string="possibility" />
            <token id="11" string="of" />
            <token id="12" string="this" />
            <token id="13" string="type" />
            <token id="14" string="of" />
            <token id="15" string="offense" />
            <token id="16" string="taking" />
            <token id="17" string="place" />
            <token id="18" string="at" />
            <token id="19" string="preschool" />
            <token id="20" string="and" />
            <token id="21" string="at" />
            <token id="22" string="schools" />
          </tokens>
        </chunking>
        <chunking id="2" string="much more alert" type="ADJP">
          <tokens>
            <token id="5" string="much" />
            <token id="6" string="more" />
            <token id="7" string="alert" />
          </tokens>
        </chunking>
        <chunking id="3" string="taking place at preschool and at schools" type="VP">
          <tokens>
            <token id="16" string="taking" />
            <token id="17" string="place" />
            <token id="18" string="at" />
            <token id="19" string="preschool" />
            <token id="20" string="and" />
            <token id="21" string="at" />
            <token id="22" string="schools" />
          </tokens>
        </chunking>
        <chunking id="4" string="offense" type="NP">
          <tokens>
            <token id="15" string="offense" />
          </tokens>
        </chunking>
        <chunking id="5" string="People" type="NP">
          <tokens>
            <token id="2" string="People" />
          </tokens>
        </chunking>
        <chunking id="6" string="the possibility of this type of offense" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="possibility" />
            <token id="11" string="of" />
            <token id="12" string="this" />
            <token id="13" string="type" />
            <token id="14" string="of" />
            <token id="15" string="offense" />
          </tokens>
        </chunking>
        <chunking id="7" string="Pounders" type="NP">
          <tokens>
            <token id="25" string="Pounders" />
          </tokens>
        </chunking>
        <chunking id="8" string="will be much more alert to the possibility of this type of offense taking place at preschool and at schools" type="VP">
          <tokens>
            <token id="3" string="will" />
            <token id="4" string="be" />
            <token id="5" string="much" />
            <token id="6" string="more" />
            <token id="7" string="alert" />
            <token id="8" string="to" />
            <token id="9" string="the" />
            <token id="10" string="possibility" />
            <token id="11" string="of" />
            <token id="12" string="this" />
            <token id="13" string="type" />
            <token id="14" string="of" />
            <token id="15" string="offense" />
            <token id="16" string="taking" />
            <token id="17" string="place" />
            <token id="18" string="at" />
            <token id="19" string="preschool" />
            <token id="20" string="and" />
            <token id="21" string="at" />
            <token id="22" string="schools" />
          </tokens>
        </chunking>
        <chunking id="9" string="schools" type="NP">
          <tokens>
            <token id="22" string="schools" />
          </tokens>
        </chunking>
        <chunking id="10" string="the possibility" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="possibility" />
          </tokens>
        </chunking>
        <chunking id="11" string="this type of offense" type="NP">
          <tokens>
            <token id="12" string="this" />
            <token id="13" string="type" />
            <token id="14" string="of" />
            <token id="15" string="offense" />
          </tokens>
        </chunking>
        <chunking id="12" string="this type" type="NP">
          <tokens>
            <token id="12" string="this" />
            <token id="13" string="type" />
          </tokens>
        </chunking>
        <chunking id="13" string="place" type="NP">
          <tokens>
            <token id="17" string="place" />
          </tokens>
        </chunking>
        <chunking id="14" string="much more alert to the possibility of this type of offense" type="ADJP">
          <tokens>
            <token id="5" string="much" />
            <token id="6" string="more" />
            <token id="7" string="alert" />
            <token id="8" string="to" />
            <token id="9" string="the" />
            <token id="10" string="possibility" />
            <token id="11" string="of" />
            <token id="12" string="this" />
            <token id="13" string="type" />
            <token id="14" string="of" />
            <token id="15" string="offense" />
          </tokens>
        </chunking>
        <chunking id="15" string="preschool" type="NP">
          <tokens>
            <token id="19" string="preschool" />
          </tokens>
        </chunking>
        <chunking id="16" string="said" type="VP">
          <tokens>
            <token id="26" string="said" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="7">alert</governor>
          <dependent id="2">People</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="7">alert</governor>
          <dependent id="3">will</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="7">alert</governor>
          <dependent id="4">be</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="7">alert</governor>
          <dependent id="5">much</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="7">alert</governor>
          <dependent id="6">more</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="26">said</governor>
          <dependent id="7">alert</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">possibility</governor>
          <dependent id="8">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">possibility</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">alert</governor>
          <dependent id="10">possibility</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">type</governor>
          <dependent id="11">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">type</governor>
          <dependent id="12">this</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">possibility</governor>
          <dependent id="13">type</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">offense</governor>
          <dependent id="14">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">type</governor>
          <dependent id="15">offense</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="7">alert</governor>
          <dependent id="16">taking</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="16">taking</governor>
          <dependent id="16">taking</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="16">taking</governor>
          <dependent id="17">place</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">preschool</governor>
          <dependent id="18">at</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">taking</governor>
          <dependent id="19">preschool</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="16">taking</governor>
          <dependent id="20">and</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">schools</governor>
          <dependent id="21">at</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">taking</governor>
          <dependent id="22">schools</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="26">said</governor>
          <dependent id="25">Pounders</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="26">said</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Pounders" type="PERSON" score="0.0">
          <tokens>
            <token id="25" string="Pounders" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="33" has_coreference="true">
      <content>&amp;quot;People are no longer dropping their children off and walking away blindly.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="People" lemma="people" stem="peopl" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="are" lemma="be" stem="ar" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="no" lemma="no" stem="no" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="longer" lemma="longer" stem="longer" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="dropping" lemma="drop" stem="drop" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="8" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="off" lemma="off" stem="off" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="walking" lemma="walk" stem="walk" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="away" lemma="away" stem="awai" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="blindly" lemma="blindly" stem="blindli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (NNS People)) (VP (VBP are) (ADVP (RB no) (RB longer)) (VP (VP (VBG dropping) (NP (PRP$ their) (NNS children)) (ADVP (RB off))) (CC and) (VP (VBG walking) (ADVP (RB away) (RB blindly))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="walking away blindly" type="VP">
          <tokens>
            <token id="11" string="walking" />
            <token id="12" string="away" />
            <token id="13" string="blindly" />
          </tokens>
        </chunking>
        <chunking id="2" string="dropping their children off and walking away blindly" type="VP">
          <tokens>
            <token id="6" string="dropping" />
            <token id="7" string="their" />
            <token id="8" string="children" />
            <token id="9" string="off" />
            <token id="10" string="and" />
            <token id="11" string="walking" />
            <token id="12" string="away" />
            <token id="13" string="blindly" />
          </tokens>
        </chunking>
        <chunking id="3" string="dropping their children off" type="VP">
          <tokens>
            <token id="6" string="dropping" />
            <token id="7" string="their" />
            <token id="8" string="children" />
            <token id="9" string="off" />
          </tokens>
        </chunking>
        <chunking id="4" string="People" type="NP">
          <tokens>
            <token id="2" string="People" />
          </tokens>
        </chunking>
        <chunking id="5" string="are no longer dropping their children off and walking away blindly" type="VP">
          <tokens>
            <token id="3" string="are" />
            <token id="4" string="no" />
            <token id="5" string="longer" />
            <token id="6" string="dropping" />
            <token id="7" string="their" />
            <token id="8" string="children" />
            <token id="9" string="off" />
            <token id="10" string="and" />
            <token id="11" string="walking" />
            <token id="12" string="away" />
            <token id="13" string="blindly" />
          </tokens>
        </chunking>
        <chunking id="6" string="their children" type="NP">
          <tokens>
            <token id="7" string="their" />
            <token id="8" string="children" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="6">dropping</governor>
          <dependent id="2">People</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="6">dropping</governor>
          <dependent id="3">are</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="5">longer</governor>
          <dependent id="4">no</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="6">dropping</governor>
          <dependent id="5">longer</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">dropping</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="8">children</governor>
          <dependent id="7">their</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">dropping</governor>
          <dependent id="8">children</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="6">dropping</governor>
          <dependent id="9">off</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="6">dropping</governor>
          <dependent id="10">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="6">dropping</governor>
          <dependent id="11">walking</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="13">blindly</governor>
          <dependent id="12">away</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">walking</governor>
          <dependent id="13">blindly</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="34" has_coreference="true">
      <content>They look back.</content>
      <tokens>
        <token id="1" string="They" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="look" lemma="look" stem="look" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="back" lemma="back" stem="back" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP They)) (VP (VBP look) (ADVP (RB back))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="They" type="NP">
          <tokens>
            <token id="1" string="They" />
          </tokens>
        </chunking>
        <chunking id="2" string="look back" type="VP">
          <tokens>
            <token id="2" string="look" />
            <token id="3" string="back" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">look</governor>
          <dependent id="1">They</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">look</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="2">look</governor>
          <dependent id="3">back</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="35" has_coreference="true">
      <content>They check things out more.&amp;quot;</content>
      <tokens>
        <token id="1" string="They" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="check" lemma="check" stem="check" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="things" lemma="thing" stem="thing" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="out" lemma="out" stem="out" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="more" lemma="more" stem="more" pos="RBR" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP They)) (VP (VBP check) (NP (NNS things)) (ADVP (RP out)) (ADVP (RBR more))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="They" type="NP">
          <tokens>
            <token id="1" string="They" />
          </tokens>
        </chunking>
        <chunking id="2" string="check things out more" type="VP">
          <tokens>
            <token id="2" string="check" />
            <token id="3" string="things" />
            <token id="4" string="out" />
            <token id="5" string="more" />
          </tokens>
        </chunking>
        <chunking id="3" string="things" type="NP">
          <tokens>
            <token id="3" string="things" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">check</governor>
          <dependent id="1">They</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">check</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">check</governor>
          <dependent id="3">things</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="2">check</governor>
          <dependent id="4">out</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="2">check</governor>
          <dependent id="5">more</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="36" has_coreference="true">
      <content>In the crowded courthouse hallways, parents of alleged victims lashed out at the prosecutors for bungling the case and insisted that their children had not been well served by the judicial system.</content>
      <tokens>
        <token id="1" string="In" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="crowded" lemma="crowded" stem="crowd" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="courthouse" lemma="courthouse" stem="courthous" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="hallways" lemma="hallway" stem="hallwai" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="parents" lemma="parent" stem="parent" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="alleged" lemma="allege" stem="alleg" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="victims" lemma="victim" stem="victim" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="lashed" lemma="lash" stem="lash" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="out" lemma="out" stem="out" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="prosecutors" lemma="prosecutor" stem="prosecutor" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="16" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="bungling" lemma="bungle" stem="bungl" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="insisted" lemma="insist" stem="insist" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="well" lemma="well" stem="well" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="served" lemma="serve" stem="serv" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="judicial" lemma="judicial" stem="judici" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="system" lemma="system" stem="system" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (IN In) (NP (DT the) (JJ crowded) (NN courthouse) (NNS hallways))) (, ,) (NP (NP (NNS parents)) (PP (IN of) (NP (VBN alleged) (NNS victims)))) (VP (VP (VBD lashed) (PRT (RP out)) (PP (IN at) (NP (DT the) (NNS prosecutors))) (PP (IN for) (S (VP (VBG bungling) (NP (DT the) (NN case)))))) (CC and) (VP (VBD insisted) (SBAR (IN that) (S (NP (PRP$ their) (NNS children)) (VP (VBD had) (RB not) (VP (VBN been) (VP (ADVP (RB well)) (VBN served) (PP (IN by) (NP (DT the) (JJ judicial) (NN system)))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="parents of alleged victims" type="NP">
          <tokens>
            <token id="7" string="parents" />
            <token id="8" string="of" />
            <token id="9" string="alleged" />
            <token id="10" string="victims" />
          </tokens>
        </chunking>
        <chunking id="2" string="the judicial system" type="NP">
          <tokens>
            <token id="31" string="the" />
            <token id="32" string="judicial" />
            <token id="33" string="system" />
          </tokens>
        </chunking>
        <chunking id="3" string="the crowded courthouse hallways" type="NP">
          <tokens>
            <token id="2" string="the" />
            <token id="3" string="crowded" />
            <token id="4" string="courthouse" />
            <token id="5" string="hallways" />
          </tokens>
        </chunking>
        <chunking id="4" string="the case" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="case" />
          </tokens>
        </chunking>
        <chunking id="5" string="insisted that their children had not been well served by the judicial system" type="VP">
          <tokens>
            <token id="21" string="insisted" />
            <token id="22" string="that" />
            <token id="23" string="their" />
            <token id="24" string="children" />
            <token id="25" string="had" />
            <token id="26" string="not" />
            <token id="27" string="been" />
            <token id="28" string="well" />
            <token id="29" string="served" />
            <token id="30" string="by" />
            <token id="31" string="the" />
            <token id="32" string="judicial" />
            <token id="33" string="system" />
          </tokens>
        </chunking>
        <chunking id="6" string="alleged victims" type="NP">
          <tokens>
            <token id="9" string="alleged" />
            <token id="10" string="victims" />
          </tokens>
        </chunking>
        <chunking id="7" string="been well served by the judicial system" type="VP">
          <tokens>
            <token id="27" string="been" />
            <token id="28" string="well" />
            <token id="29" string="served" />
            <token id="30" string="by" />
            <token id="31" string="the" />
            <token id="32" string="judicial" />
            <token id="33" string="system" />
          </tokens>
        </chunking>
        <chunking id="8" string="their children" type="NP">
          <tokens>
            <token id="23" string="their" />
            <token id="24" string="children" />
          </tokens>
        </chunking>
        <chunking id="9" string="bungling the case" type="VP">
          <tokens>
            <token id="17" string="bungling" />
            <token id="18" string="the" />
            <token id="19" string="case" />
          </tokens>
        </chunking>
        <chunking id="10" string="the prosecutors" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="prosecutors" />
          </tokens>
        </chunking>
        <chunking id="11" string="well served by the judicial system" type="VP">
          <tokens>
            <token id="28" string="well" />
            <token id="29" string="served" />
            <token id="30" string="by" />
            <token id="31" string="the" />
            <token id="32" string="judicial" />
            <token id="33" string="system" />
          </tokens>
        </chunking>
        <chunking id="12" string="that their children had not been well served by the judicial system" type="SBAR">
          <tokens>
            <token id="22" string="that" />
            <token id="23" string="their" />
            <token id="24" string="children" />
            <token id="25" string="had" />
            <token id="26" string="not" />
            <token id="27" string="been" />
            <token id="28" string="well" />
            <token id="29" string="served" />
            <token id="30" string="by" />
            <token id="31" string="the" />
            <token id="32" string="judicial" />
            <token id="33" string="system" />
          </tokens>
        </chunking>
        <chunking id="13" string="lashed out at the prosecutors for bungling the case" type="VP">
          <tokens>
            <token id="11" string="lashed" />
            <token id="12" string="out" />
            <token id="13" string="at" />
            <token id="14" string="the" />
            <token id="15" string="prosecutors" />
            <token id="16" string="for" />
            <token id="17" string="bungling" />
            <token id="18" string="the" />
            <token id="19" string="case" />
          </tokens>
        </chunking>
        <chunking id="14" string="parents" type="NP">
          <tokens>
            <token id="7" string="parents" />
          </tokens>
        </chunking>
        <chunking id="15" string="lashed out at the prosecutors for bungling the case and insisted that their children had not been well served by the judicial system" type="VP">
          <tokens>
            <token id="11" string="lashed" />
            <token id="12" string="out" />
            <token id="13" string="at" />
            <token id="14" string="the" />
            <token id="15" string="prosecutors" />
            <token id="16" string="for" />
            <token id="17" string="bungling" />
            <token id="18" string="the" />
            <token id="19" string="case" />
            <token id="20" string="and" />
            <token id="21" string="insisted" />
            <token id="22" string="that" />
            <token id="23" string="their" />
            <token id="24" string="children" />
            <token id="25" string="had" />
            <token id="26" string="not" />
            <token id="27" string="been" />
            <token id="28" string="well" />
            <token id="29" string="served" />
            <token id="30" string="by" />
            <token id="31" string="the" />
            <token id="32" string="judicial" />
            <token id="33" string="system" />
          </tokens>
        </chunking>
        <chunking id="16" string="had not been well served by the judicial system" type="VP">
          <tokens>
            <token id="25" string="had" />
            <token id="26" string="not" />
            <token id="27" string="been" />
            <token id="28" string="well" />
            <token id="29" string="served" />
            <token id="30" string="by" />
            <token id="31" string="the" />
            <token id="32" string="judicial" />
            <token id="33" string="system" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="case">
          <governor id="5">hallways</governor>
          <dependent id="1">In</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">hallways</governor>
          <dependent id="2">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">hallways</governor>
          <dependent id="3">crowded</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">hallways</governor>
          <dependent id="4">courthouse</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">lashed</governor>
          <dependent id="5">hallways</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">lashed</governor>
          <dependent id="7">parents</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">victims</governor>
          <dependent id="8">of</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="10">victims</governor>
          <dependent id="9">alleged</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">parents</governor>
          <dependent id="10">victims</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="11">lashed</dependent>
        </dependency>
        <dependency type="compound:prt">
          <governor id="11">lashed</governor>
          <dependent id="12">out</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">prosecutors</governor>
          <dependent id="13">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">prosecutors</governor>
          <dependent id="14">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">lashed</governor>
          <dependent id="15">prosecutors</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="17">bungling</governor>
          <dependent id="16">for</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="11">lashed</governor>
          <dependent id="17">bungling</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">case</governor>
          <dependent id="18">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="17">bungling</governor>
          <dependent id="19">case</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="11">lashed</governor>
          <dependent id="20">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="11">lashed</governor>
          <dependent id="21">insisted</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="29">served</governor>
          <dependent id="22">that</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="24">children</governor>
          <dependent id="23">their</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="29">served</governor>
          <dependent id="24">children</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="29">served</governor>
          <dependent id="25">had</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="29">served</governor>
          <dependent id="26">not</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="29">served</governor>
          <dependent id="27">been</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="29">served</governor>
          <dependent id="28">well</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="21">insisted</governor>
          <dependent id="29">served</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">system</governor>
          <dependent id="30">by</dependent>
        </dependency>
        <dependency type="det">
          <governor id="33">system</governor>
          <dependent id="31">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="33">system</governor>
          <dependent id="32">judicial</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="29">served</governor>
          <dependent id="33">system</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="37" has_coreference="true">
      <content>&amp;quot;The system doesn&amp;apost;t allow us to protect kids,&amp;quot; said the father of two alleged victims.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="system" lemma="system" stem="system" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="4" string="does" lemma="do" stem="doe" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="n't" lemma="not" stem="n't" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="allow" lemma="allow" stem="allow" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="us" lemma="we" stem="u" pos="PRP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="protect" lemma="protect" stem="protect" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="kids" lemma="kid" stem="kid" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="father" lemma="father" stem="father" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="16" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="17" string="two" lemma="two" stem="two" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="18" string="alleged" lemma="alleged" stem="alleg" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="19" string="victims" lemma="victim" stem="victim" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="20" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (SINV (`` ``) (S (NP (DT The) (NN system)) (VP (VBZ does) (RB n't) (VP (VB allow) (S (NP (PRP us)) (VP (TO to) (VP (VB protect) (NP (NNS kids)))))))) (, ,) ('' '') (VP (VBD said)) (NP (NP (DT the) (NN father)) (PP (IN of) (NP (CD two) (JJ alleged) (NNS victims)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="allow us to protect kids" type="VP">
          <tokens>
            <token id="6" string="allow" />
            <token id="7" string="us" />
            <token id="8" string="to" />
            <token id="9" string="protect" />
            <token id="10" string="kids" />
          </tokens>
        </chunking>
        <chunking id="2" string="two alleged victims" type="NP">
          <tokens>
            <token id="17" string="two" />
            <token id="18" string="alleged" />
            <token id="19" string="victims" />
          </tokens>
        </chunking>
        <chunking id="3" string="the father" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="father" />
          </tokens>
        </chunking>
        <chunking id="4" string="does n't allow us to protect kids" type="VP">
          <tokens>
            <token id="4" string="does" />
            <token id="5" string="n't" />
            <token id="6" string="allow" />
            <token id="7" string="us" />
            <token id="8" string="to" />
            <token id="9" string="protect" />
            <token id="10" string="kids" />
          </tokens>
        </chunking>
        <chunking id="5" string="to protect kids" type="VP">
          <tokens>
            <token id="8" string="to" />
            <token id="9" string="protect" />
            <token id="10" string="kids" />
          </tokens>
        </chunking>
        <chunking id="6" string="the father of two alleged victims" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="father" />
            <token id="16" string="of" />
            <token id="17" string="two" />
            <token id="18" string="alleged" />
            <token id="19" string="victims" />
          </tokens>
        </chunking>
        <chunking id="7" string="protect kids" type="VP">
          <tokens>
            <token id="9" string="protect" />
            <token id="10" string="kids" />
          </tokens>
        </chunking>
        <chunking id="8" string="The system" type="NP">
          <tokens>
            <token id="2" string="The" />
            <token id="3" string="system" />
          </tokens>
        </chunking>
        <chunking id="9" string="us" type="NP">
          <tokens>
            <token id="7" string="us" />
          </tokens>
        </chunking>
        <chunking id="10" string="said" type="VP">
          <tokens>
            <token id="13" string="said" />
          </tokens>
        </chunking>
        <chunking id="11" string="kids" type="NP">
          <tokens>
            <token id="10" string="kids" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">system</governor>
          <dependent id="2">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">allow</governor>
          <dependent id="3">system</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="6">allow</governor>
          <dependent id="4">does</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="6">allow</governor>
          <dependent id="5">n't</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="13">said</governor>
          <dependent id="6">allow</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">allow</governor>
          <dependent id="7">us</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="9">protect</governor>
          <dependent id="8">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="6">allow</governor>
          <dependent id="9">protect</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="9">protect</governor>
          <dependent id="10">kids</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="13">said</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">father</governor>
          <dependent id="14">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="13">said</governor>
          <dependent id="15">father</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">victims</governor>
          <dependent id="16">of</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="19">victims</governor>
          <dependent id="17">two</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">victims</governor>
          <dependent id="18">alleged</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">father</governor>
          <dependent id="19">victims</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="two" type="NUMBER" score="0.0">
          <tokens>
            <token id="17" string="two" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="38" has_coreference="true">
      <content>&amp;quot;I have no doubt these children were abused.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="have" lemma="have" stem="have" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="no" lemma="no" stem="no" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="doubt" lemma="doubt" stem="doubt" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="these" lemma="these" stem="these" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="7" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="8" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="abused" lemma="abuse" stem="abus" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (PRP I)) (VP (VBP have) (NP (NP (DT no) (NN doubt)) (SBAR (S (NP (DT these) (NNS children)) (VP (VBD were) (VP (VBN abused))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="these children" type="NP">
          <tokens>
            <token id="6" string="these" />
            <token id="7" string="children" />
          </tokens>
        </chunking>
        <chunking id="2" string="I" type="NP">
          <tokens>
            <token id="2" string="I" />
          </tokens>
        </chunking>
        <chunking id="3" string="have no doubt these children were abused" type="VP">
          <tokens>
            <token id="3" string="have" />
            <token id="4" string="no" />
            <token id="5" string="doubt" />
            <token id="6" string="these" />
            <token id="7" string="children" />
            <token id="8" string="were" />
            <token id="9" string="abused" />
          </tokens>
        </chunking>
        <chunking id="4" string="these children were abused" type="SBAR">
          <tokens>
            <token id="6" string="these" />
            <token id="7" string="children" />
            <token id="8" string="were" />
            <token id="9" string="abused" />
          </tokens>
        </chunking>
        <chunking id="5" string="no doubt" type="NP">
          <tokens>
            <token id="4" string="no" />
            <token id="5" string="doubt" />
          </tokens>
        </chunking>
        <chunking id="6" string="no doubt these children were abused" type="NP">
          <tokens>
            <token id="4" string="no" />
            <token id="5" string="doubt" />
            <token id="6" string="these" />
            <token id="7" string="children" />
            <token id="8" string="were" />
            <token id="9" string="abused" />
          </tokens>
        </chunking>
        <chunking id="7" string="abused" type="VP">
          <tokens>
            <token id="9" string="abused" />
          </tokens>
        </chunking>
        <chunking id="8" string="were abused" type="VP">
          <tokens>
            <token id="8" string="were" />
            <token id="9" string="abused" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">have</governor>
          <dependent id="2">I</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">have</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="5">doubt</governor>
          <dependent id="4">no</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">have</governor>
          <dependent id="5">doubt</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">children</governor>
          <dependent id="6">these</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="9">abused</governor>
          <dependent id="7">children</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="9">abused</governor>
          <dependent id="8">were</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="5">doubt</governor>
          <dependent id="9">abused</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="39" has_coreference="true">
      <content>Said a tearful mother: &amp;quot;I guess maybe it&amp;apost;s OK to (molest) kids!</content>
      <tokens>
        <token id="1" string="Said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="3" string="tearful" lemma="tearful" stem="tear" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="guess" lemma="guess" stem="guess" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="maybe" lemma="maybe" stem="mayb" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="OK" lemma="ok" stem="ok" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="(" lemma="-lrb-" stem="(" pos="-LRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="molest" lemma="molest" stem="molest" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string=")" lemma="-rrb-" stem=")" pos="-RRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="kids" lemma="kid" stem="kid" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="!" lemma="!" stem="!" pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (SINV (VP (VBD Said)) (NP (DT a) (JJ tearful) (NN mother)) (: :) (`` ``) (S (NP (PRP I)) (VP (VBP guess) (S (ADVP (RB maybe)) (NP (PRP it)) (VP (VBZ 's) (ADJP (JJ OK) (S (VP (TO to) (VP (-LRB- -LRB-) (VB molest) (-RRB- -RRB-) (NP (NNS kids)))))))))) (. !)))</syntactictree>
      <chunkings>
        <chunking id="1" string="-LRB- molest -RRB- kids" type="VP">
          <tokens>
            <token id="14" string="(" />
            <token id="15" string="molest" />
            <token id="16" string=")" />
            <token id="17" string="kids" />
          </tokens>
        </chunking>
        <chunking id="2" string="guess maybe it 's OK to -LRB- molest -RRB- kids" type="VP">
          <tokens>
            <token id="8" string="guess" />
            <token id="9" string="maybe" />
            <token id="10" string="it" />
            <token id="11" string="'s" />
            <token id="12" string="OK" />
            <token id="13" string="to" />
            <token id="14" string="(" />
            <token id="15" string="molest" />
            <token id="16" string=")" />
            <token id="17" string="kids" />
          </tokens>
        </chunking>
        <chunking id="3" string="to -LRB- molest -RRB- kids" type="VP">
          <tokens>
            <token id="13" string="to" />
            <token id="14" string="(" />
            <token id="15" string="molest" />
            <token id="16" string=")" />
            <token id="17" string="kids" />
          </tokens>
        </chunking>
        <chunking id="4" string="I" type="NP">
          <tokens>
            <token id="7" string="I" />
          </tokens>
        </chunking>
        <chunking id="5" string="it" type="NP">
          <tokens>
            <token id="10" string="it" />
          </tokens>
        </chunking>
        <chunking id="6" string="a tearful mother" type="NP">
          <tokens>
            <token id="2" string="a" />
            <token id="3" string="tearful" />
            <token id="4" string="mother" />
          </tokens>
        </chunking>
        <chunking id="7" string="'s OK to -LRB- molest -RRB- kids" type="VP">
          <tokens>
            <token id="11" string="'s" />
            <token id="12" string="OK" />
            <token id="13" string="to" />
            <token id="14" string="(" />
            <token id="15" string="molest" />
            <token id="16" string=")" />
            <token id="17" string="kids" />
          </tokens>
        </chunking>
        <chunking id="8" string="OK to -LRB- molest -RRB- kids" type="ADJP">
          <tokens>
            <token id="12" string="OK" />
            <token id="13" string="to" />
            <token id="14" string="(" />
            <token id="15" string="molest" />
            <token id="16" string=")" />
            <token id="17" string="kids" />
          </tokens>
        </chunking>
        <chunking id="9" string="kids" type="NP">
          <tokens>
            <token id="17" string="kids" />
          </tokens>
        </chunking>
        <chunking id="10" string="Said" type="VP">
          <tokens>
            <token id="1" string="Said" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Said</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">mother</governor>
          <dependent id="2">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="4">mother</governor>
          <dependent id="3">tearful</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="1">Said</governor>
          <dependent id="4">mother</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">guess</governor>
          <dependent id="7">I</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="1">Said</governor>
          <dependent id="8">guess</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="12">OK</governor>
          <dependent id="9">maybe</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">OK</governor>
          <dependent id="10">it</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="12">OK</governor>
          <dependent id="11">'s</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="8">guess</governor>
          <dependent id="12">OK</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="15">molest</governor>
          <dependent id="13">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="12">OK</governor>
          <dependent id="15">molest</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">molest</governor>
          <dependent id="17">kids</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="40" has_coreference="true">
      <content>I can&amp;apost;t believe this.&amp;quot;</content>
      <tokens>
        <token id="1" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="ca" lemma="can" stem="ca" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="n't" lemma="not" stem="n't" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="believe" lemma="believe" stem="believ" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP I)) (VP (MD ca) (RB n't) (VP (VB believe) (NP (DT this)))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="ca n't believe this" type="VP">
          <tokens>
            <token id="2" string="ca" />
            <token id="3" string="n't" />
            <token id="4" string="believe" />
            <token id="5" string="this" />
          </tokens>
        </chunking>
        <chunking id="2" string="I" type="NP">
          <tokens>
            <token id="1" string="I" />
          </tokens>
        </chunking>
        <chunking id="3" string="believe this" type="VP">
          <tokens>
            <token id="4" string="believe" />
            <token id="5" string="this" />
          </tokens>
        </chunking>
        <chunking id="4" string="this" type="NP">
          <tokens>
            <token id="5" string="this" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">believe</governor>
          <dependent id="1">I</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="4">believe</governor>
          <dependent id="2">ca</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="4">believe</governor>
          <dependent id="3">n't</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">believe</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">believe</governor>
          <dependent id="5">this</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="41" has_coreference="true">
      <content>The children themselves reacted calmly and stood by their accounts.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="themselves" lemma="themselves" stem="themselv" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="reacted" lemma="react" stem="react" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="calmly" lemma="calmly" stem="calmli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="stood" lemma="stand" stem="stood" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="accounts" lemma="account" stem="account" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NNS children)) (ADVP (PRP themselves)) (VP (VP (VBD reacted) (ADVP (RB calmly))) (CC and) (VP (VBD stood) (PP (IN by) (NP (PRP$ their) (NNS accounts))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="their accounts" type="NP">
          <tokens>
            <token id="9" string="their" />
            <token id="10" string="accounts" />
          </tokens>
        </chunking>
        <chunking id="2" string="reacted calmly and stood by their accounts" type="VP">
          <tokens>
            <token id="4" string="reacted" />
            <token id="5" string="calmly" />
            <token id="6" string="and" />
            <token id="7" string="stood" />
            <token id="8" string="by" />
            <token id="9" string="their" />
            <token id="10" string="accounts" />
          </tokens>
        </chunking>
        <chunking id="3" string="The children" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="children" />
          </tokens>
        </chunking>
        <chunking id="4" string="stood by their accounts" type="VP">
          <tokens>
            <token id="7" string="stood" />
            <token id="8" string="by" />
            <token id="9" string="their" />
            <token id="10" string="accounts" />
          </tokens>
        </chunking>
        <chunking id="5" string="reacted calmly" type="VP">
          <tokens>
            <token id="4" string="reacted" />
            <token id="5" string="calmly" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">children</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">reacted</governor>
          <dependent id="2">children</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">reacted</governor>
          <dependent id="3">themselves</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">reacted</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">reacted</governor>
          <dependent id="5">calmly</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">reacted</governor>
          <dependent id="6">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">reacted</governor>
          <dependent id="7">stood</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">accounts</governor>
          <dependent id="8">by</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="10">accounts</governor>
          <dependent id="9">their</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">stood</governor>
          <dependent id="10">accounts</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="42" has_coreference="true">
      <content>&amp;quot;We all know that we are telling the truth,&amp;quot; said a 15-year-old boy who had testified.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="We" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="know" lemma="know" stem="know" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="we" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="are" lemma="be" stem="ar" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="telling" lemma="tell" stem="tell" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="truth" lemma="truth" stem="truth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="15-year-old" lemma="15-year-old" stem="15-year-old" pos="JJ" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="16" string="boy" lemma="boy" stem="boi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="testified" lemma="testify" stem="testifi" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (SINV (`` ``) (S (NP (PRP We)) (ADVP (DT all)) (VP (VBP know) (SBAR (IN that) (S (NP (PRP we)) (VP (VBP are) (VP (VBG telling) (NP (DT the) (NN truth)))))))) (, ,) ('' '') (VP (VBD said)) (NP (NP (DT a) (JJ 15-year-old) (NN boy)) (SBAR (WHNP (WP who)) (S (VP (VBD had) (VP (VBN testified)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="that we are telling the truth" type="SBAR">
          <tokens>
            <token id="5" string="that" />
            <token id="6" string="we" />
            <token id="7" string="are" />
            <token id="8" string="telling" />
            <token id="9" string="the" />
            <token id="10" string="truth" />
          </tokens>
        </chunking>
        <chunking id="2" string="a 15-year-old boy" type="NP">
          <tokens>
            <token id="14" string="a" />
            <token id="15" string="15-year-old" />
            <token id="16" string="boy" />
          </tokens>
        </chunking>
        <chunking id="3" string="know that we are telling the truth" type="VP">
          <tokens>
            <token id="4" string="know" />
            <token id="5" string="that" />
            <token id="6" string="we" />
            <token id="7" string="are" />
            <token id="8" string="telling" />
            <token id="9" string="the" />
            <token id="10" string="truth" />
          </tokens>
        </chunking>
        <chunking id="4" string="are telling the truth" type="VP">
          <tokens>
            <token id="7" string="are" />
            <token id="8" string="telling" />
            <token id="9" string="the" />
            <token id="10" string="truth" />
          </tokens>
        </chunking>
        <chunking id="5" string="We" type="NP">
          <tokens>
            <token id="2" string="We" />
          </tokens>
        </chunking>
        <chunking id="6" string="we" type="NP">
          <tokens>
            <token id="6" string="we" />
          </tokens>
        </chunking>
        <chunking id="7" string="who had testified" type="SBAR">
          <tokens>
            <token id="17" string="who" />
            <token id="18" string="had" />
            <token id="19" string="testified" />
          </tokens>
        </chunking>
        <chunking id="8" string="testified" type="VP">
          <tokens>
            <token id="19" string="testified" />
          </tokens>
        </chunking>
        <chunking id="9" string="telling the truth" type="VP">
          <tokens>
            <token id="8" string="telling" />
            <token id="9" string="the" />
            <token id="10" string="truth" />
          </tokens>
        </chunking>
        <chunking id="10" string="said" type="VP">
          <tokens>
            <token id="13" string="said" />
          </tokens>
        </chunking>
        <chunking id="11" string="the truth" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="truth" />
          </tokens>
        </chunking>
        <chunking id="12" string="had testified" type="VP">
          <tokens>
            <token id="18" string="had" />
            <token id="19" string="testified" />
          </tokens>
        </chunking>
        <chunking id="13" string="a 15-year-old boy who had testified" type="NP">
          <tokens>
            <token id="14" string="a" />
            <token id="15" string="15-year-old" />
            <token id="16" string="boy" />
            <token id="17" string="who" />
            <token id="18" string="had" />
            <token id="19" string="testified" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">know</governor>
          <dependent id="2">We</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">know</governor>
          <dependent id="3">all</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="13">said</governor>
          <dependent id="4">know</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="8">telling</governor>
          <dependent id="5">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">telling</governor>
          <dependent id="6">we</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="8">telling</governor>
          <dependent id="7">are</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="4">know</governor>
          <dependent id="8">telling</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">truth</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="8">telling</governor>
          <dependent id="10">truth</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="13">said</dependent>
        </dependency>
        <dependency type="det">
          <governor id="16">boy</governor>
          <dependent id="14">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="16">boy</governor>
          <dependent id="15">15-year-old</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="13">said</governor>
          <dependent id="16">boy</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">testified</governor>
          <dependent id="17">who</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="19">testified</governor>
          <dependent id="18">had</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="16">boy</governor>
          <dependent id="19">testified</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="15-year-old" type="DURATION" score="0.0">
          <tokens>
            <token id="15" string="15-year-old" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="43" has_coreference="true">
      <content>&amp;quot;No matter what the jury says, whatever anybody says, this is the truth.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="No" lemma="no" stem="no" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="matter" lemma="matter" stem="matter" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="what" lemma="what" stem="what" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="says" lemma="say" stem="sai" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="whatever" lemma="whatever" stem="whatev" pos="WDT" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="anybody" lemma="anybody" stem="anybodi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="says" lemma="say" stem="sai" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="16" string="truth" lemma="truth" stem="truth" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="17" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (ADVP (DT No) (NN matter) (SBAR (WHNP (WP what)) (S (NP (DT the) (NN jury)) (VP (VBZ says) (, ,) (SBAR (WHNP (WDT whatever)) (S (NP (NN anybody)) (VP (VBZ says)))))))) (, ,) (NP (DT this)) (VP (VBZ is) (NP (DT the) (NN truth))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="says" type="VP">
          <tokens>
            <token id="11" string="says" />
          </tokens>
        </chunking>
        <chunking id="2" string="anybody" type="NP">
          <tokens>
            <token id="10" string="anybody" />
          </tokens>
        </chunking>
        <chunking id="3" string="says , whatever anybody says" type="VP">
          <tokens>
            <token id="7" string="says" />
            <token id="8" string="," />
            <token id="9" string="whatever" />
            <token id="10" string="anybody" />
            <token id="11" string="says" />
          </tokens>
        </chunking>
        <chunking id="4" string="what the jury says , whatever anybody says" type="SBAR">
          <tokens>
            <token id="4" string="what" />
            <token id="5" string="the" />
            <token id="6" string="jury" />
            <token id="7" string="says" />
            <token id="8" string="," />
            <token id="9" string="whatever" />
            <token id="10" string="anybody" />
            <token id="11" string="says" />
          </tokens>
        </chunking>
        <chunking id="5" string="is the truth" type="VP">
          <tokens>
            <token id="14" string="is" />
            <token id="15" string="the" />
            <token id="16" string="truth" />
          </tokens>
        </chunking>
        <chunking id="6" string="this" type="NP">
          <tokens>
            <token id="13" string="this" />
          </tokens>
        </chunking>
        <chunking id="7" string="whatever anybody says" type="SBAR">
          <tokens>
            <token id="9" string="whatever" />
            <token id="10" string="anybody" />
            <token id="11" string="says" />
          </tokens>
        </chunking>
        <chunking id="8" string="the jury" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="jury" />
          </tokens>
        </chunking>
        <chunking id="9" string="the truth" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="truth" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="neg">
          <governor id="3">matter</governor>
          <dependent id="2">No</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="16">truth</governor>
          <dependent id="3">matter</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">says</governor>
          <dependent id="4">what</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">jury</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">says</governor>
          <dependent id="6">jury</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">matter</governor>
          <dependent id="7">says</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">says</governor>
          <dependent id="9">whatever</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">says</governor>
          <dependent id="10">anybody</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="7">says</governor>
          <dependent id="11">says</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">truth</governor>
          <dependent id="13">this</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="16">truth</governor>
          <dependent id="14">is</dependent>
        </dependency>
        <dependency type="det">
          <governor id="16">truth</governor>
          <dependent id="15">the</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="16">truth</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="44" has_coreference="true">
      <content>We were molested.&amp;quot;</content>
      <tokens>
        <token id="1" string="We" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="molested" lemma="molest" stem="molest" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP We)) (VP (VBD were) (VP (VBN molested))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="were molested" type="VP">
          <tokens>
            <token id="2" string="were" />
            <token id="3" string="molested" />
          </tokens>
        </chunking>
        <chunking id="2" string="We" type="NP">
          <tokens>
            <token id="1" string="We" />
          </tokens>
        </chunking>
        <chunking id="3" string="molested" type="VP">
          <tokens>
            <token id="3" string="molested" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="3">molested</governor>
          <dependent id="1">We</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="3">molested</governor>
          <dependent id="2">were</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">molested</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="45" has_coreference="true">
      <content>Another of the children said that, when the clerk read &amp;quot;not guilty&amp;quot; on the verdict form, &amp;quot;my stomach just hollowed out.</content>
      <tokens>
        <token id="1" string="Another" lemma="another" stem="another" pos="DT" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="clerk" lemma="clerk" stem="clerk" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="11" string="read" lemma="read" stem="read" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="guilty" lemma="guilty" stem="guilti" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="verdict" lemma="verdict" stem="verdict" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="form" lemma="form" stem="form" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="my" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="23" string="stomach" lemma="stomach" stem="stomach" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="just" lemma="just" stem="just" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="hollowed" lemma="hollow" stem="hollow" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="out" lemma="out" stem="out" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT Another)) (PP (IN of) (NP (DT the) (NNS children)))) (VP (VBD said) (SBAR (IN that) (PRN (, ,) (SBAR (WHADVP (WRB when)) (S (NP (DT the) (NN clerk)) (VP (VBD read) (ADJP (`` ``) (RB not) (JJ guilty) ('' '') (PP (IN on) (NP (DT the) (NN verdict) (NN form))))))) (, ,)) (`` ``) (S (NP (PRP$ my) (NN stomach)) (ADVP (RB just)) (VP (VBD hollowed) (PRT (RP out)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="said that , when the clerk read `` not guilty '' on the verdict form , `` my stomach just hollowed out" type="VP">
          <tokens>
            <token id="5" string="said" />
            <token id="6" string="that" />
            <token id="7" string="," />
            <token id="8" string="when" />
            <token id="9" string="the" />
            <token id="10" string="clerk" />
            <token id="11" string="read" />
            <token id="12" string="&quot;" />
            <token id="13" string="not" />
            <token id="14" string="guilty" />
            <token id="15" string="&quot;" />
            <token id="16" string="on" />
            <token id="17" string="the" />
            <token id="18" string="verdict" />
            <token id="19" string="form" />
            <token id="20" string="," />
            <token id="21" string="&quot;" />
            <token id="22" string="my" />
            <token id="23" string="stomach" />
            <token id="24" string="just" />
            <token id="25" string="hollowed" />
            <token id="26" string="out" />
          </tokens>
        </chunking>
        <chunking id="2" string="my stomach" type="NP">
          <tokens>
            <token id="22" string="my" />
            <token id="23" string="stomach" />
          </tokens>
        </chunking>
        <chunking id="3" string="the verdict form" type="NP">
          <tokens>
            <token id="17" string="the" />
            <token id="18" string="verdict" />
            <token id="19" string="form" />
          </tokens>
        </chunking>
        <chunking id="4" string="that , when the clerk read `` not guilty '' on the verdict form , `` my stomach just hollowed out" type="SBAR">
          <tokens>
            <token id="6" string="that" />
            <token id="7" string="," />
            <token id="8" string="when" />
            <token id="9" string="the" />
            <token id="10" string="clerk" />
            <token id="11" string="read" />
            <token id="12" string="&quot;" />
            <token id="13" string="not" />
            <token id="14" string="guilty" />
            <token id="15" string="&quot;" />
            <token id="16" string="on" />
            <token id="17" string="the" />
            <token id="18" string="verdict" />
            <token id="19" string="form" />
            <token id="20" string="," />
            <token id="21" string="&quot;" />
            <token id="22" string="my" />
            <token id="23" string="stomach" />
            <token id="24" string="just" />
            <token id="25" string="hollowed" />
            <token id="26" string="out" />
          </tokens>
        </chunking>
        <chunking id="5" string="Another of the children" type="NP">
          <tokens>
            <token id="1" string="Another" />
            <token id="2" string="of" />
            <token id="3" string="the" />
            <token id="4" string="children" />
          </tokens>
        </chunking>
        <chunking id="6" string="when" type="WHADVP">
          <tokens>
            <token id="8" string="when" />
          </tokens>
        </chunking>
        <chunking id="7" string="`` not guilty '' on the verdict form" type="ADJP">
          <tokens>
            <token id="12" string="&quot;" />
            <token id="13" string="not" />
            <token id="14" string="guilty" />
            <token id="15" string="&quot;" />
            <token id="16" string="on" />
            <token id="17" string="the" />
            <token id="18" string="verdict" />
            <token id="19" string="form" />
          </tokens>
        </chunking>
        <chunking id="8" string="the children" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="children" />
          </tokens>
        </chunking>
        <chunking id="9" string="when the clerk read `` not guilty '' on the verdict form" type="SBAR">
          <tokens>
            <token id="8" string="when" />
            <token id="9" string="the" />
            <token id="10" string="clerk" />
            <token id="11" string="read" />
            <token id="12" string="&quot;" />
            <token id="13" string="not" />
            <token id="14" string="guilty" />
            <token id="15" string="&quot;" />
            <token id="16" string="on" />
            <token id="17" string="the" />
            <token id="18" string="verdict" />
            <token id="19" string="form" />
          </tokens>
        </chunking>
        <chunking id="10" string="the clerk" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="clerk" />
          </tokens>
        </chunking>
        <chunking id="11" string="read `` not guilty '' on the verdict form" type="VP">
          <tokens>
            <token id="11" string="read" />
            <token id="12" string="&quot;" />
            <token id="13" string="not" />
            <token id="14" string="guilty" />
            <token id="15" string="&quot;" />
            <token id="16" string="on" />
            <token id="17" string="the" />
            <token id="18" string="verdict" />
            <token id="19" string="form" />
          </tokens>
        </chunking>
        <chunking id="12" string="hollowed out" type="VP">
          <tokens>
            <token id="25" string="hollowed" />
            <token id="26" string="out" />
          </tokens>
        </chunking>
        <chunking id="13" string="Another" type="NP">
          <tokens>
            <token id="1" string="Another" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="5">said</governor>
          <dependent id="1">Another</dependent>
        </dependency>
        <dependency type="case">
          <governor id="4">children</governor>
          <dependent id="2">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">children</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="1">Another</governor>
          <dependent id="4">children</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">said</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="25">hollowed</governor>
          <dependent id="6">that</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">read</governor>
          <dependent id="8">when</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">clerk</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">read</governor>
          <dependent id="10">clerk</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="25">hollowed</governor>
          <dependent id="11">read</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="14">guilty</governor>
          <dependent id="13">not</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="11">read</governor>
          <dependent id="14">guilty</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">form</governor>
          <dependent id="16">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">form</governor>
          <dependent id="17">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">form</governor>
          <dependent id="18">verdict</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">guilty</governor>
          <dependent id="19">form</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="23">stomach</governor>
          <dependent id="22">my</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="25">hollowed</governor>
          <dependent id="23">stomach</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="25">hollowed</governor>
          <dependent id="24">just</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="5">said</governor>
          <dependent id="25">hollowed</dependent>
        </dependency>
        <dependency type="compound:prt">
          <governor id="25">hollowed</governor>
          <dependent id="26">out</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="46" has_coreference="true">
      <content>I was beyond tears.</content>
      <tokens>
        <token id="1" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="beyond" lemma="beyond" stem="beyond" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="tears" lemma="tear" stem="tear" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP I)) (VP (VBD was) (PP (IN beyond) (NP (NNS tears)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="tears" type="NP">
          <tokens>
            <token id="4" string="tears" />
          </tokens>
        </chunking>
        <chunking id="2" string="I" type="NP">
          <tokens>
            <token id="1" string="I" />
          </tokens>
        </chunking>
        <chunking id="3" string="was beyond tears" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="beyond" />
            <token id="4" string="tears" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">tears</governor>
          <dependent id="1">I</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="4">tears</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="case">
          <governor id="4">tears</governor>
          <dependent id="3">beyond</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">tears</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="47" has_coreference="true">
      <content>Those words will stick with me probably the rest of my life.</content>
      <tokens>
        <token id="1" string="Those" lemma="those" stem="those" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="words" lemma="word" stem="word" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="will" lemma="will" stem="will" pos="MD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="stick" lemma="stick" stem="stick" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="me" lemma="I" stem="me" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="probably" lemma="probably" stem="probabl" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="rest" lemma="rest" stem="rest" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="my" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="life" lemma="life" stem="life" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT Those) (NNS words)) (VP (MD will) (VP (VB stick) (PP (IN with) (NP (PRP me))) (NP (NP (RB probably) (DT the) (NN rest)) (PP (IN of) (NP (PRP$ my) (NN life)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Those words" type="NP">
          <tokens>
            <token id="1" string="Those" />
            <token id="2" string="words" />
          </tokens>
        </chunking>
        <chunking id="2" string="probably the rest of my life" type="NP">
          <tokens>
            <token id="7" string="probably" />
            <token id="8" string="the" />
            <token id="9" string="rest" />
            <token id="10" string="of" />
            <token id="11" string="my" />
            <token id="12" string="life" />
          </tokens>
        </chunking>
        <chunking id="3" string="me" type="NP">
          <tokens>
            <token id="6" string="me" />
          </tokens>
        </chunking>
        <chunking id="4" string="probably the rest" type="NP">
          <tokens>
            <token id="7" string="probably" />
            <token id="8" string="the" />
            <token id="9" string="rest" />
          </tokens>
        </chunking>
        <chunking id="5" string="my life" type="NP">
          <tokens>
            <token id="11" string="my" />
            <token id="12" string="life" />
          </tokens>
        </chunking>
        <chunking id="6" string="will stick with me probably the rest of my life" type="VP">
          <tokens>
            <token id="3" string="will" />
            <token id="4" string="stick" />
            <token id="5" string="with" />
            <token id="6" string="me" />
            <token id="7" string="probably" />
            <token id="8" string="the" />
            <token id="9" string="rest" />
            <token id="10" string="of" />
            <token id="11" string="my" />
            <token id="12" string="life" />
          </tokens>
        </chunking>
        <chunking id="7" string="stick with me probably the rest of my life" type="VP">
          <tokens>
            <token id="4" string="stick" />
            <token id="5" string="with" />
            <token id="6" string="me" />
            <token id="7" string="probably" />
            <token id="8" string="the" />
            <token id="9" string="rest" />
            <token id="10" string="of" />
            <token id="11" string="my" />
            <token id="12" string="life" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">words</governor>
          <dependent id="1">Those</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">stick</governor>
          <dependent id="2">words</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="4">stick</governor>
          <dependent id="3">will</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">stick</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">me</governor>
          <dependent id="5">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">stick</governor>
          <dependent id="6">me</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="9">rest</governor>
          <dependent id="7">probably</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">rest</governor>
          <dependent id="8">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">stick</governor>
          <dependent id="9">rest</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">life</governor>
          <dependent id="10">of</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="12">life</governor>
          <dependent id="11">my</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">rest</governor>
          <dependent id="12">life</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="48" has_coreference="true">
      <content>I&amp;apost;ll never forget those two words.&amp;quot;</content>
      <tokens>
        <token id="1" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="'ll" lemma="will" stem="'ll" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="never" lemma="never" stem="never" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="forget" lemma="forget" stem="forget" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="those" lemma="those" stem="those" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="two" lemma="two" stem="two" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="7" string="words" lemma="word" stem="word" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP I)) (VP (MD 'll) (ADVP (RB never)) (VP (VB forget) (NP (DT those) (CD two) (NNS words)))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="I" type="NP">
          <tokens>
            <token id="1" string="I" />
          </tokens>
        </chunking>
        <chunking id="2" string="those two words" type="NP">
          <tokens>
            <token id="5" string="those" />
            <token id="6" string="two" />
            <token id="7" string="words" />
          </tokens>
        </chunking>
        <chunking id="3" string="forget those two words" type="VP">
          <tokens>
            <token id="4" string="forget" />
            <token id="5" string="those" />
            <token id="6" string="two" />
            <token id="7" string="words" />
          </tokens>
        </chunking>
        <chunking id="4" string="'ll never forget those two words" type="VP">
          <tokens>
            <token id="2" string="'ll" />
            <token id="3" string="never" />
            <token id="4" string="forget" />
            <token id="5" string="those" />
            <token id="6" string="two" />
            <token id="7" string="words" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">forget</governor>
          <dependent id="1">I</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="4">forget</governor>
          <dependent id="2">'ll</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="4">forget</governor>
          <dependent id="3">never</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">forget</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">words</governor>
          <dependent id="5">those</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="7">words</governor>
          <dependent id="6">two</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">forget</governor>
          <dependent id="7">words</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="two" type="NUMBER" score="0.0">
          <tokens>
            <token id="6" string="two" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="49" has_coreference="true">
      <content>Jurors were critical of the prosecutors&amp;apost; methods.</content>
      <tokens>
        <token id="1" string="Jurors" lemma="Jurors" stem="juror" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="2" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="critical" lemma="critical" stem="critic" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="prosecutors" lemma="prosecutor" stem="prosecutor" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="'" lemma="'" stem="'" pos="POS" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="methods" lemma="method" stem="method" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Jurors)) (VP (VBD were) (ADJP (JJ critical) (PP (IN of) (NP (NP (DT the) (NNS prosecutors) (POS ')) (NNS methods))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the prosecutors ' methods" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="prosecutors" />
            <token id="7" string="'" />
            <token id="8" string="methods" />
          </tokens>
        </chunking>
        <chunking id="2" string="Jurors" type="NP">
          <tokens>
            <token id="1" string="Jurors" />
          </tokens>
        </chunking>
        <chunking id="3" string="were critical of the prosecutors ' methods" type="VP">
          <tokens>
            <token id="2" string="were" />
            <token id="3" string="critical" />
            <token id="4" string="of" />
            <token id="5" string="the" />
            <token id="6" string="prosecutors" />
            <token id="7" string="'" />
            <token id="8" string="methods" />
          </tokens>
        </chunking>
        <chunking id="4" string="critical of the prosecutors ' methods" type="ADJP">
          <tokens>
            <token id="3" string="critical" />
            <token id="4" string="of" />
            <token id="5" string="the" />
            <token id="6" string="prosecutors" />
            <token id="7" string="'" />
            <token id="8" string="methods" />
          </tokens>
        </chunking>
        <chunking id="5" string="the prosecutors '" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="prosecutors" />
            <token id="7" string="'" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">critical</governor>
          <dependent id="1">Jurors</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="3">critical</governor>
          <dependent id="2">were</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">critical</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">methods</governor>
          <dependent id="4">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">prosecutors</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="8">methods</governor>
          <dependent id="6">prosecutors</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">prosecutors</governor>
          <dependent id="7">'</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">critical</governor>
          <dependent id="8">methods</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="50" has_coreference="true">
      <content>In particular, they cited the prosecution&amp;apost;s reliance on videotapes of controversial pretrial interviews of the young witnesses at a Los Angeles child-abuse center.</content>
      <tokens>
        <token id="1" string="In" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="particular" lemma="particular" stem="particular" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="cited" lemma="cite" stem="cite" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="prosecution" lemma="prosecution" stem="prosecut" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="reliance" lemma="reliance" stem="relianc" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="videotapes" lemma="videotape" stem="videotap" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="controversial" lemma="controversial" stem="controversi" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="pretrial" lemma="pretrial" stem="pretrial" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="interviews" lemma="interview" stem="interview" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="young" lemma="young" stem="young" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="witnesses" lemma="witness" stem="wit" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="Los" lemma="Los" stem="lo" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="23" string="Angeles" lemma="Angeles" stem="angele" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="24" string="child-abuse" lemma="child-abuse" stem="child-abus" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="center" lemma="center" stem="center" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (IN In) (ADJP (JJ particular))) (, ,) (NP (PRP they)) (VP (VBD cited) (NP (NP (DT the) (NN prosecution) (POS 's)) (NN reliance)) (PP (IN on) (NP (NP (NNS videotapes)) (PP (IN of) (NP (NP (JJ controversial) (JJ pretrial) (NNS interviews)) (PP (IN of) (NP (DT the) (JJ young) (NNS witnesses))))))) (PP (IN at) (NP (DT a) (NNP Los) (NNP Angeles) (JJ child-abuse) (NN center)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="they" type="NP">
          <tokens>
            <token id="4" string="they" />
          </tokens>
        </chunking>
        <chunking id="2" string="controversial pretrial interviews" type="NP">
          <tokens>
            <token id="13" string="controversial" />
            <token id="14" string="pretrial" />
            <token id="15" string="interviews" />
          </tokens>
        </chunking>
        <chunking id="3" string="a Los Angeles child-abuse center" type="NP">
          <tokens>
            <token id="21" string="a" />
            <token id="22" string="Los" />
            <token id="23" string="Angeles" />
            <token id="24" string="child-abuse" />
            <token id="25" string="center" />
          </tokens>
        </chunking>
        <chunking id="4" string="videotapes" type="NP">
          <tokens>
            <token id="11" string="videotapes" />
          </tokens>
        </chunking>
        <chunking id="5" string="controversial pretrial interviews of the young witnesses" type="NP">
          <tokens>
            <token id="13" string="controversial" />
            <token id="14" string="pretrial" />
            <token id="15" string="interviews" />
            <token id="16" string="of" />
            <token id="17" string="the" />
            <token id="18" string="young" />
            <token id="19" string="witnesses" />
          </tokens>
        </chunking>
        <chunking id="6" string="the prosecution 's" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="prosecution" />
            <token id="8" string="'s" />
          </tokens>
        </chunking>
        <chunking id="7" string="the young witnesses" type="NP">
          <tokens>
            <token id="17" string="the" />
            <token id="18" string="young" />
            <token id="19" string="witnesses" />
          </tokens>
        </chunking>
        <chunking id="8" string="particular" type="ADJP">
          <tokens>
            <token id="2" string="particular" />
          </tokens>
        </chunking>
        <chunking id="9" string="the prosecution 's reliance" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="prosecution" />
            <token id="8" string="'s" />
            <token id="9" string="reliance" />
          </tokens>
        </chunking>
        <chunking id="10" string="cited the prosecution 's reliance on videotapes of controversial pretrial interviews of the young witnesses at a Los Angeles child-abuse center" type="VP">
          <tokens>
            <token id="5" string="cited" />
            <token id="6" string="the" />
            <token id="7" string="prosecution" />
            <token id="8" string="'s" />
            <token id="9" string="reliance" />
            <token id="10" string="on" />
            <token id="11" string="videotapes" />
            <token id="12" string="of" />
            <token id="13" string="controversial" />
            <token id="14" string="pretrial" />
            <token id="15" string="interviews" />
            <token id="16" string="of" />
            <token id="17" string="the" />
            <token id="18" string="young" />
            <token id="19" string="witnesses" />
            <token id="20" string="at" />
            <token id="21" string="a" />
            <token id="22" string="Los" />
            <token id="23" string="Angeles" />
            <token id="24" string="child-abuse" />
            <token id="25" string="center" />
          </tokens>
        </chunking>
        <chunking id="11" string="videotapes of controversial pretrial interviews of the young witnesses" type="NP">
          <tokens>
            <token id="11" string="videotapes" />
            <token id="12" string="of" />
            <token id="13" string="controversial" />
            <token id="14" string="pretrial" />
            <token id="15" string="interviews" />
            <token id="16" string="of" />
            <token id="17" string="the" />
            <token id="18" string="young" />
            <token id="19" string="witnesses" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="case">
          <governor id="2">particular</governor>
          <dependent id="1">In</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="5">cited</governor>
          <dependent id="2">particular</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">cited</governor>
          <dependent id="4">they</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">cited</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">prosecution</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="9">reliance</governor>
          <dependent id="7">prosecution</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">prosecution</governor>
          <dependent id="8">'s</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">cited</governor>
          <dependent id="9">reliance</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">videotapes</governor>
          <dependent id="10">on</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">cited</governor>
          <dependent id="11">videotapes</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">interviews</governor>
          <dependent id="12">of</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">interviews</governor>
          <dependent id="13">controversial</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">interviews</governor>
          <dependent id="14">pretrial</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">videotapes</governor>
          <dependent id="15">interviews</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">witnesses</governor>
          <dependent id="16">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">witnesses</governor>
          <dependent id="17">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">witnesses</governor>
          <dependent id="18">young</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">interviews</governor>
          <dependent id="19">witnesses</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">center</governor>
          <dependent id="20">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="25">center</governor>
          <dependent id="21">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">center</governor>
          <dependent id="22">Los</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">center</governor>
          <dependent id="23">Angeles</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="25">center</governor>
          <dependent id="24">child-abuse</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">cited</governor>
          <dependent id="25">center</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Los Angeles" type="LOCATION" score="0.0">
          <tokens>
            <token id="22" string="Los" />
            <token id="23" string="Angeles" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="51" has_coreference="true">
      <content>They also criticized the Manhattan Beach Police Department&amp;apost;s decision, early in the investigation, to send out form letters to parents announcing an investigation had begun and soliciting information.</content>
      <tokens>
        <token id="1" string="They" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="criticized" lemma="criticize" stem="critic" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="Manhattan" lemma="Manhattan" stem="manhattan" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="6" string="Beach" lemma="Beach" stem="beach" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="7" string="Police" lemma="Police" stem="polic" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="8" string="Department" lemma="Department" stem="depart" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="9" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="decision" lemma="decision" stem="decis" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="early" lemma="early" stem="earli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="investigation" lemma="investigation" stem="investig" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="send" lemma="send" stem="send" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="out" lemma="out" stem="out" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="form" lemma="form" stem="form" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="letters" lemma="letter" stem="letter" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="parents" lemma="parent" stem="parent" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="announcing" lemma="announce" stem="announc" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="investigation" lemma="investigation" stem="investig" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="begun" lemma="begin" stem="begun" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="soliciting" lemma="solicit" stem="solicit" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="information" lemma="information" stem="inform" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP They)) (ADVP (RB also)) (VP (VBD criticized) (NP (NP (DT the) (NNP Manhattan) (NNP Beach) (NNP Police) (NNP Department) (POS 's)) (NN decision)) (, ,) (ADVP (RB early) (PP (IN in) (NP (DT the) (NN investigation)))) (, ,) (S (VP (TO to) (VP (VB send) (PRT (RP out)) (NP (NN form) (NNS letters)) (PP (TO to) (NP (NP (NNS parents)) (VP (VP (VBG announcing) (SBAR (S (NP (DT an) (NN investigation)) (VP (VBD had) (VP (VBN begun)))))) (CC and) (VP (VBG soliciting) (NP (NN information)))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="They" type="NP">
          <tokens>
            <token id="1" string="They" />
          </tokens>
        </chunking>
        <chunking id="2" string="criticized the Manhattan Beach Police Department 's decision , early in the investigation , to send out form letters to parents announcing an investigation had begun and soliciting information" type="VP">
          <tokens>
            <token id="3" string="criticized" />
            <token id="4" string="the" />
            <token id="5" string="Manhattan" />
            <token id="6" string="Beach" />
            <token id="7" string="Police" />
            <token id="8" string="Department" />
            <token id="9" string="'s" />
            <token id="10" string="decision" />
            <token id="11" string="," />
            <token id="12" string="early" />
            <token id="13" string="in" />
            <token id="14" string="the" />
            <token id="15" string="investigation" />
            <token id="16" string="," />
            <token id="17" string="to" />
            <token id="18" string="send" />
            <token id="19" string="out" />
            <token id="20" string="form" />
            <token id="21" string="letters" />
            <token id="22" string="to" />
            <token id="23" string="parents" />
            <token id="24" string="announcing" />
            <token id="25" string="an" />
            <token id="26" string="investigation" />
            <token id="27" string="had" />
            <token id="28" string="begun" />
            <token id="29" string="and" />
            <token id="30" string="soliciting" />
            <token id="31" string="information" />
          </tokens>
        </chunking>
        <chunking id="3" string="announcing an investigation had begun" type="VP">
          <tokens>
            <token id="24" string="announcing" />
            <token id="25" string="an" />
            <token id="26" string="investigation" />
            <token id="27" string="had" />
            <token id="28" string="begun" />
          </tokens>
        </chunking>
        <chunking id="4" string="the Manhattan Beach Police Department 's" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="Manhattan" />
            <token id="6" string="Beach" />
            <token id="7" string="Police" />
            <token id="8" string="Department" />
            <token id="9" string="'s" />
          </tokens>
        </chunking>
        <chunking id="5" string="the Manhattan Beach Police Department 's decision" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="Manhattan" />
            <token id="6" string="Beach" />
            <token id="7" string="Police" />
            <token id="8" string="Department" />
            <token id="9" string="'s" />
            <token id="10" string="decision" />
          </tokens>
        </chunking>
        <chunking id="6" string="to send out form letters to parents announcing an investigation had begun and soliciting information" type="VP">
          <tokens>
            <token id="17" string="to" />
            <token id="18" string="send" />
            <token id="19" string="out" />
            <token id="20" string="form" />
            <token id="21" string="letters" />
            <token id="22" string="to" />
            <token id="23" string="parents" />
            <token id="24" string="announcing" />
            <token id="25" string="an" />
            <token id="26" string="investigation" />
            <token id="27" string="had" />
            <token id="28" string="begun" />
            <token id="29" string="and" />
            <token id="30" string="soliciting" />
            <token id="31" string="information" />
          </tokens>
        </chunking>
        <chunking id="7" string="begun" type="VP">
          <tokens>
            <token id="28" string="begun" />
          </tokens>
        </chunking>
        <chunking id="8" string="an investigation" type="NP">
          <tokens>
            <token id="25" string="an" />
            <token id="26" string="investigation" />
          </tokens>
        </chunking>
        <chunking id="9" string="the investigation" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="investigation" />
          </tokens>
        </chunking>
        <chunking id="10" string="form letters" type="NP">
          <tokens>
            <token id="20" string="form" />
            <token id="21" string="letters" />
          </tokens>
        </chunking>
        <chunking id="11" string="soliciting information" type="VP">
          <tokens>
            <token id="30" string="soliciting" />
            <token id="31" string="information" />
          </tokens>
        </chunking>
        <chunking id="12" string="an investigation had begun" type="SBAR">
          <tokens>
            <token id="25" string="an" />
            <token id="26" string="investigation" />
            <token id="27" string="had" />
            <token id="28" string="begun" />
          </tokens>
        </chunking>
        <chunking id="13" string="had begun" type="VP">
          <tokens>
            <token id="27" string="had" />
            <token id="28" string="begun" />
          </tokens>
        </chunking>
        <chunking id="14" string="announcing an investigation had begun and soliciting information" type="VP">
          <tokens>
            <token id="24" string="announcing" />
            <token id="25" string="an" />
            <token id="26" string="investigation" />
            <token id="27" string="had" />
            <token id="28" string="begun" />
            <token id="29" string="and" />
            <token id="30" string="soliciting" />
            <token id="31" string="information" />
          </tokens>
        </chunking>
        <chunking id="15" string="parents announcing an investigation had begun and soliciting information" type="NP">
          <tokens>
            <token id="23" string="parents" />
            <token id="24" string="announcing" />
            <token id="25" string="an" />
            <token id="26" string="investigation" />
            <token id="27" string="had" />
            <token id="28" string="begun" />
            <token id="29" string="and" />
            <token id="30" string="soliciting" />
            <token id="31" string="information" />
          </tokens>
        </chunking>
        <chunking id="16" string="information" type="NP">
          <tokens>
            <token id="31" string="information" />
          </tokens>
        </chunking>
        <chunking id="17" string="send out form letters to parents announcing an investigation had begun and soliciting information" type="VP">
          <tokens>
            <token id="18" string="send" />
            <token id="19" string="out" />
            <token id="20" string="form" />
            <token id="21" string="letters" />
            <token id="22" string="to" />
            <token id="23" string="parents" />
            <token id="24" string="announcing" />
            <token id="25" string="an" />
            <token id="26" string="investigation" />
            <token id="27" string="had" />
            <token id="28" string="begun" />
            <token id="29" string="and" />
            <token id="30" string="soliciting" />
            <token id="31" string="information" />
          </tokens>
        </chunking>
        <chunking id="18" string="parents" type="NP">
          <tokens>
            <token id="23" string="parents" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">criticized</governor>
          <dependent id="1">They</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="3">criticized</governor>
          <dependent id="2">also</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">criticized</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">Department</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">Department</governor>
          <dependent id="5">Manhattan</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">Department</governor>
          <dependent id="6">Beach</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">Department</governor>
          <dependent id="7">Police</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="10">decision</governor>
          <dependent id="8">Department</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">Department</governor>
          <dependent id="9">'s</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">criticized</governor>
          <dependent id="10">decision</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="3">criticized</governor>
          <dependent id="12">early</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">investigation</governor>
          <dependent id="13">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">investigation</governor>
          <dependent id="14">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">early</governor>
          <dependent id="15">investigation</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="18">send</governor>
          <dependent id="17">to</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="3">criticized</governor>
          <dependent id="18">send</dependent>
        </dependency>
        <dependency type="compound:prt">
          <governor id="18">send</governor>
          <dependent id="19">out</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="21">letters</governor>
          <dependent id="20">form</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="18">send</governor>
          <dependent id="21">letters</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">parents</governor>
          <dependent id="22">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="18">send</governor>
          <dependent id="23">parents</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="23">parents</governor>
          <dependent id="24">announcing</dependent>
        </dependency>
        <dependency type="det">
          <governor id="26">investigation</governor>
          <dependent id="25">an</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="28">begun</governor>
          <dependent id="26">investigation</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="28">begun</governor>
          <dependent id="27">had</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="24">announcing</governor>
          <dependent id="28">begun</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="24">announcing</governor>
          <dependent id="29">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="24">announcing</governor>
          <dependent id="30">soliciting</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="30">soliciting</governor>
          <dependent id="31">information</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Manhattan Beach Police Department" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="5" string="Manhattan" />
            <token id="6" string="Beach" />
            <token id="7" string="Police" />
            <token id="8" string="Department" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="52" has_coreference="true">
      <content>&amp;quot;The children,&amp;quot; said juror John Breese, 51, a biomedical technician, &amp;quot;were never allowed to say in their own words what happened to them.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="juror" lemma="juror" stem="juror" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="9" string="Breese" lemma="Breese" stem="brees" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="51" lemma="51" stem="51" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="biomedical" lemma="biomedical" stem="biomed" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="technician" lemma="technician" stem="technician" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="never" lemma="never" stem="never" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="allowed" lemma="allow" stem="allow" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="say" lemma="say" stem="sai" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="25" string="own" lemma="own" stem="own" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="words" lemma="word" stem="word" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="what" lemma="what" stem="what" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="happened" lemma="happen" stem="happen" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="them" lemma="they" stem="them" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="31" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (DT The) (NNS children)) (PRN (, ,) ('' '') (SINV (VP (VBD said) (NP (NN juror))) (NP (NP (NNP John) (NNP Breese)) (, ,) (NP (NP (CD 51)) (, ,) (NP (DT a) (JJ biomedical) (NN technician))))) (, ,) (`` ``)) (VP (VBD were) (ADVP (RB never)) (VP (VBN allowed) (S (VP (TO to) (VP (VB say) (PP (IN in) (NP (NP (PRP$ their) (JJ own) (NNS words)) (SBAR (WHNP (WP what)) (S (VP (VBD happened) (PP (TO to) (NP (PRP them))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="51 , a biomedical technician" type="NP">
          <tokens>
            <token id="11" string="51" />
            <token id="12" string="," />
            <token id="13" string="a" />
            <token id="14" string="biomedical" />
            <token id="15" string="technician" />
          </tokens>
        </chunking>
        <chunking id="2" string="allowed to say in their own words what happened to them" type="VP">
          <tokens>
            <token id="20" string="allowed" />
            <token id="21" string="to" />
            <token id="22" string="say" />
            <token id="23" string="in" />
            <token id="24" string="their" />
            <token id="25" string="own" />
            <token id="26" string="words" />
            <token id="27" string="what" />
            <token id="28" string="happened" />
            <token id="29" string="to" />
            <token id="30" string="them" />
          </tokens>
        </chunking>
        <chunking id="3" string="juror" type="NP">
          <tokens>
            <token id="7" string="juror" />
          </tokens>
        </chunking>
        <chunking id="4" string="to say in their own words what happened to them" type="VP">
          <tokens>
            <token id="21" string="to" />
            <token id="22" string="say" />
            <token id="23" string="in" />
            <token id="24" string="their" />
            <token id="25" string="own" />
            <token id="26" string="words" />
            <token id="27" string="what" />
            <token id="28" string="happened" />
            <token id="29" string="to" />
            <token id="30" string="them" />
          </tokens>
        </chunking>
        <chunking id="5" string="The children" type="NP">
          <tokens>
            <token id="2" string="The" />
            <token id="3" string="children" />
          </tokens>
        </chunking>
        <chunking id="6" string="John Breese" type="NP">
          <tokens>
            <token id="8" string="John" />
            <token id="9" string="Breese" />
          </tokens>
        </chunking>
        <chunking id="7" string="John Breese , 51 , a biomedical technician" type="NP">
          <tokens>
            <token id="8" string="John" />
            <token id="9" string="Breese" />
            <token id="10" string="," />
            <token id="11" string="51" />
            <token id="12" string="," />
            <token id="13" string="a" />
            <token id="14" string="biomedical" />
            <token id="15" string="technician" />
          </tokens>
        </chunking>
        <chunking id="8" string="said juror" type="VP">
          <tokens>
            <token id="6" string="said" />
            <token id="7" string="juror" />
          </tokens>
        </chunking>
        <chunking id="9" string="happened to them" type="VP">
          <tokens>
            <token id="28" string="happened" />
            <token id="29" string="to" />
            <token id="30" string="them" />
          </tokens>
        </chunking>
        <chunking id="10" string="them" type="NP">
          <tokens>
            <token id="30" string="them" />
          </tokens>
        </chunking>
        <chunking id="11" string="their own words" type="NP">
          <tokens>
            <token id="24" string="their" />
            <token id="25" string="own" />
            <token id="26" string="words" />
          </tokens>
        </chunking>
        <chunking id="12" string="a biomedical technician" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="biomedical" />
            <token id="15" string="technician" />
          </tokens>
        </chunking>
        <chunking id="13" string="what happened to them" type="SBAR">
          <tokens>
            <token id="27" string="what" />
            <token id="28" string="happened" />
            <token id="29" string="to" />
            <token id="30" string="them" />
          </tokens>
        </chunking>
        <chunking id="14" string="say in their own words what happened to them" type="VP">
          <tokens>
            <token id="22" string="say" />
            <token id="23" string="in" />
            <token id="24" string="their" />
            <token id="25" string="own" />
            <token id="26" string="words" />
            <token id="27" string="what" />
            <token id="28" string="happened" />
            <token id="29" string="to" />
            <token id="30" string="them" />
          </tokens>
        </chunking>
        <chunking id="15" string="51" type="NP">
          <tokens>
            <token id="11" string="51" />
          </tokens>
        </chunking>
        <chunking id="16" string="were never allowed to say in their own words what happened to them" type="VP">
          <tokens>
            <token id="18" string="were" />
            <token id="19" string="never" />
            <token id="20" string="allowed" />
            <token id="21" string="to" />
            <token id="22" string="say" />
            <token id="23" string="in" />
            <token id="24" string="their" />
            <token id="25" string="own" />
            <token id="26" string="words" />
            <token id="27" string="what" />
            <token id="28" string="happened" />
            <token id="29" string="to" />
            <token id="30" string="them" />
          </tokens>
        </chunking>
        <chunking id="17" string="their own words what happened to them" type="NP">
          <tokens>
            <token id="24" string="their" />
            <token id="25" string="own" />
            <token id="26" string="words" />
            <token id="27" string="what" />
            <token id="28" string="happened" />
            <token id="29" string="to" />
            <token id="30" string="them" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">children</governor>
          <dependent id="2">The</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="20">allowed</governor>
          <dependent id="3">children</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="20">allowed</governor>
          <dependent id="6">said</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">said</governor>
          <dependent id="7">juror</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">Breese</governor>
          <dependent id="8">John</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">said</governor>
          <dependent id="9">Breese</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="9">Breese</governor>
          <dependent id="11">51</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">technician</governor>
          <dependent id="13">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">technician</governor>
          <dependent id="14">biomedical</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="11">51</governor>
          <dependent id="15">technician</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="20">allowed</governor>
          <dependent id="18">were</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="20">allowed</governor>
          <dependent id="19">never</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="20">allowed</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="22">say</governor>
          <dependent id="21">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="20">allowed</governor>
          <dependent id="22">say</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">words</governor>
          <dependent id="23">in</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="26">words</governor>
          <dependent id="24">their</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="26">words</governor>
          <dependent id="25">own</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="22">say</governor>
          <dependent id="26">words</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="28">happened</governor>
          <dependent id="27">what</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="26">words</governor>
          <dependent id="28">happened</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">them</governor>
          <dependent id="29">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="28">happened</governor>
          <dependent id="30">them</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John Breese" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="John" />
            <token id="9" string="Breese" />
          </tokens>
        </entity>
        <entity id="2" string="51" type="NUMBER" score="0.0">
          <tokens>
            <token id="11" string="51" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="53" has_coreference="true">
      <content>That was crucial.&amp;quot;</content>
      <tokens>
        <token id="1" string="That" lemma="that" stem="that" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="crucial" lemma="crucial" stem="crucial" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT That)) (VP (VBD was) (ADJP (JJ crucial))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="That" type="NP">
          <tokens>
            <token id="1" string="That" />
          </tokens>
        </chunking>
        <chunking id="2" string="crucial" type="ADJP">
          <tokens>
            <token id="3" string="crucial" />
          </tokens>
        </chunking>
        <chunking id="3" string="was crucial" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="crucial" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">crucial</governor>
          <dependent id="1">That</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="3">crucial</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">crucial</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="54" has_coreference="true">
      <content>Said jury foreman Luis Chang, an electrical engineer: &amp;quot;The key evidence that swayed me was the interview tapes.</content>
      <tokens>
        <token id="1" string="Said" lemma="Said" stem="said" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="foreman" lemma="foreman" stem="foreman" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="Luis" lemma="Luis" stem="lui" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="5" string="Chang" lemma="Chang" stem="chang" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="electrical" lemma="electrical" stem="electr" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="engineer" lemma="engineer" stem="engin" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="12" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="key" lemma="key" stem="kei" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string="evidence" lemma="evidence" stem="evid" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="16" string="swayed" lemma="sway" stem="swai" pos="VBD" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="17" string="me" lemma="I" stem="me" pos="PRP" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="18" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="19" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="20" string="interview" lemma="interview" stem="interview" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="21" string="tapes" lemma="tape" stem="tape" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="22" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NP (NNP Said) (NN jury) (NN foreman) (NNP Luis) (NNP Chang)) (, ,) (NP (DT an) (JJ electrical) (NN engineer))) (: :) (NP (NP (`` ``) (DT The) (JJ key) (NN evidence)) (SBAR (WHNP (IN that)) (S (VP (VBD swayed) (SBAR (S (NP (PRP me)) (VP (VBD was) (NP (DT the) (NN interview) (NNS tapes))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="that swayed me was the interview tapes" type="SBAR">
          <tokens>
            <token id="15" string="that" />
            <token id="16" string="swayed" />
            <token id="17" string="me" />
            <token id="18" string="was" />
            <token id="19" string="the" />
            <token id="20" string="interview" />
            <token id="21" string="tapes" />
          </tokens>
        </chunking>
        <chunking id="2" string="the interview tapes" type="NP">
          <tokens>
            <token id="19" string="the" />
            <token id="20" string="interview" />
            <token id="21" string="tapes" />
          </tokens>
        </chunking>
        <chunking id="3" string="Said jury foreman Luis Chang" type="NP">
          <tokens>
            <token id="1" string="Said" />
            <token id="2" string="jury" />
            <token id="3" string="foreman" />
            <token id="4" string="Luis" />
            <token id="5" string="Chang" />
          </tokens>
        </chunking>
        <chunking id="4" string="`` The key evidence that swayed me was the interview tapes" type="NP">
          <tokens>
            <token id="11" string="&quot;" />
            <token id="12" string="The" />
            <token id="13" string="key" />
            <token id="14" string="evidence" />
            <token id="15" string="that" />
            <token id="16" string="swayed" />
            <token id="17" string="me" />
            <token id="18" string="was" />
            <token id="19" string="the" />
            <token id="20" string="interview" />
            <token id="21" string="tapes" />
          </tokens>
        </chunking>
        <chunking id="5" string="me was the interview tapes" type="SBAR">
          <tokens>
            <token id="17" string="me" />
            <token id="18" string="was" />
            <token id="19" string="the" />
            <token id="20" string="interview" />
            <token id="21" string="tapes" />
          </tokens>
        </chunking>
        <chunking id="6" string="swayed me was the interview tapes" type="VP">
          <tokens>
            <token id="16" string="swayed" />
            <token id="17" string="me" />
            <token id="18" string="was" />
            <token id="19" string="the" />
            <token id="20" string="interview" />
            <token id="21" string="tapes" />
          </tokens>
        </chunking>
        <chunking id="7" string="was the interview tapes" type="VP">
          <tokens>
            <token id="18" string="was" />
            <token id="19" string="the" />
            <token id="20" string="interview" />
            <token id="21" string="tapes" />
          </tokens>
        </chunking>
        <chunking id="8" string="an electrical engineer" type="NP">
          <tokens>
            <token id="7" string="an" />
            <token id="8" string="electrical" />
            <token id="9" string="engineer" />
          </tokens>
        </chunking>
        <chunking id="9" string="me" type="NP">
          <tokens>
            <token id="17" string="me" />
          </tokens>
        </chunking>
        <chunking id="10" string="Said jury foreman Luis Chang , an electrical engineer : `` The key evidence that swayed me was the interview tapes ." type="NP">
          <tokens>
            <token id="1" string="Said" />
            <token id="2" string="jury" />
            <token id="3" string="foreman" />
            <token id="4" string="Luis" />
            <token id="5" string="Chang" />
            <token id="6" string="," />
            <token id="7" string="an" />
            <token id="8" string="electrical" />
            <token id="9" string="engineer" />
            <token id="10" string=":" />
            <token id="11" string="&quot;" />
            <token id="12" string="The" />
            <token id="13" string="key" />
            <token id="14" string="evidence" />
            <token id="15" string="that" />
            <token id="16" string="swayed" />
            <token id="17" string="me" />
            <token id="18" string="was" />
            <token id="19" string="the" />
            <token id="20" string="interview" />
            <token id="21" string="tapes" />
            <token id="22" string="." />
          </tokens>
        </chunking>
        <chunking id="11" string="Said jury foreman Luis Chang , an electrical engineer" type="NP">
          <tokens>
            <token id="1" string="Said" />
            <token id="2" string="jury" />
            <token id="3" string="foreman" />
            <token id="4" string="Luis" />
            <token id="5" string="Chang" />
            <token id="6" string="," />
            <token id="7" string="an" />
            <token id="8" string="electrical" />
            <token id="9" string="engineer" />
          </tokens>
        </chunking>
        <chunking id="12" string="`` The key evidence" type="NP">
          <tokens>
            <token id="11" string="&quot;" />
            <token id="12" string="The" />
            <token id="13" string="key" />
            <token id="14" string="evidence" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="5">Chang</governor>
          <dependent id="1">Said</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">Chang</governor>
          <dependent id="2">jury</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">Chang</governor>
          <dependent id="3">foreman</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">Chang</governor>
          <dependent id="4">Luis</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">Chang</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">engineer</governor>
          <dependent id="7">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">engineer</governor>
          <dependent id="8">electrical</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="5">Chang</governor>
          <dependent id="9">engineer</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">evidence</governor>
          <dependent id="12">The</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="14">evidence</governor>
          <dependent id="13">key</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="5">Chang</governor>
          <dependent id="14">evidence</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">swayed</governor>
          <dependent id="15">that</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="14">evidence</governor>
          <dependent id="16">swayed</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">tapes</governor>
          <dependent id="17">me</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="21">tapes</governor>
          <dependent id="18">was</dependent>
        </dependency>
        <dependency type="det">
          <governor id="21">tapes</governor>
          <dependent id="19">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="21">tapes</governor>
          <dependent id="20">interview</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="16">swayed</governor>
          <dependent id="21">tapes</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Luis Chang" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="Luis" />
            <token id="5" string="Chang" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="55" has_coreference="true">
      <content>They were too biased, too leading.</content>
      <tokens>
        <token id="1" string="They" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="too" lemma="too" stem="too" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="biased" lemma="bias" stem="bias" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="too" lemma="too" stem="too" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="leading" lemma="lead" stem="lead" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP They)) (VP (VBD were) (VP (ADVP (RB too)) (VBN biased) (, ,) (S (VP (ADVP (RB too)) (VBG leading))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="They" type="NP">
          <tokens>
            <token id="1" string="They" />
          </tokens>
        </chunking>
        <chunking id="2" string="were too biased , too leading" type="VP">
          <tokens>
            <token id="2" string="were" />
            <token id="3" string="too" />
            <token id="4" string="biased" />
            <token id="5" string="," />
            <token id="6" string="too" />
            <token id="7" string="leading" />
          </tokens>
        </chunking>
        <chunking id="3" string="too leading" type="VP">
          <tokens>
            <token id="6" string="too" />
            <token id="7" string="leading" />
          </tokens>
        </chunking>
        <chunking id="4" string="too biased , too leading" type="VP">
          <tokens>
            <token id="3" string="too" />
            <token id="4" string="biased" />
            <token id="5" string="," />
            <token id="6" string="too" />
            <token id="7" string="leading" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="4">biased</governor>
          <dependent id="1">They</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="4">biased</governor>
          <dependent id="2">were</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">biased</governor>
          <dependent id="3">too</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">biased</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="7">leading</governor>
          <dependent id="6">too</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="4">biased</governor>
          <dependent id="7">leading</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="56" has_coreference="true">
      <content>That&amp;apost;s the main crux of it.&amp;quot;</content>
      <tokens>
        <token id="1" string="That" lemma="that" stem="that" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="main" lemma="main" stem="main" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="crux" lemma="crux" stem="crux" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="6" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT That)) (VP (VBZ 's) (NP (NP (DT the) (JJ main) (NN crux)) (PP (IN of) (NP (PRP it))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="That" type="NP">
          <tokens>
            <token id="1" string="That" />
          </tokens>
        </chunking>
        <chunking id="2" string="the main crux" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="main" />
            <token id="5" string="crux" />
          </tokens>
        </chunking>
        <chunking id="3" string="'s the main crux of it" type="VP">
          <tokens>
            <token id="2" string="'s" />
            <token id="3" string="the" />
            <token id="4" string="main" />
            <token id="5" string="crux" />
            <token id="6" string="of" />
            <token id="7" string="it" />
          </tokens>
        </chunking>
        <chunking id="4" string="it" type="NP">
          <tokens>
            <token id="7" string="it" />
          </tokens>
        </chunking>
        <chunking id="5" string="the main crux of it" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="main" />
            <token id="5" string="crux" />
            <token id="6" string="of" />
            <token id="7" string="it" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="5">crux</governor>
          <dependent id="1">That</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="5">crux</governor>
          <dependent id="2">'s</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">crux</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">crux</governor>
          <dependent id="4">main</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">crux</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">it</governor>
          <dependent id="6">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">crux</governor>
          <dependent id="7">it</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="57" has_coreference="true">
      <content>All said they had entered the jury deliberation room with open minds, not only because the judge had so instructed, but also because the disjointed testimony had left them confused.</content>
      <tokens>
        <token id="1" string="All" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="entered" lemma="enter" stem="enter" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="deliberation" lemma="deliberation" stem="deliber" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="room" lemma="room" stem="room" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="open" lemma="open" stem="open" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="minds" lemma="mind" stem="mind" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="only" lemma="only" stem="onli" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="because" lemma="because" stem="becaus" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="judge" lemma="judge" stem="judg" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="19" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="so" lemma="so" stem="so" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="instructed" lemma="instruct" stem="instruct" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="because" lemma="because" stem="becaus" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="disjointed" lemma="disjointed" stem="disjoint" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="testimony" lemma="testimony" stem="testimoni" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="left" lemma="leave" stem="left" pos="VBN" type="Word" isStopWord="false" ner="IDEOLOGY" is_referenced="false" is_refers="false" />
        <token id="31" string="them" lemma="they" stem="them" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="32" string="confused" lemma="confused" stem="confus" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT All)) (VP (VBD said) (SBAR (S (NP (PRP they)) (VP (VBD had) (VP (VBN entered) (NP (NP (DT the) (NN jury) (NN deliberation) (NN room)) (PP (IN with) (NP (JJ open) (NNS minds)))) (, ,) (SBAR (RB not) (RB only) (IN because) (S (NP (DT the) (NN judge)) (VP (VBD had) (ADVP (RB so)) (VP (VBN instructed))))) (, ,) (CC but) (ADVP (RB also)) (SBAR (IN because) (S (NP (DT the) (JJ disjointed) (NN testimony)) (VP (VBD had) (VP (VBN left) (S (NP (PRP them)) (ADJP (JJ confused)))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="had entered the jury deliberation room with open minds , not only because the judge had so instructed , but also because the disjointed testimony had left them confused" type="VP">
          <tokens>
            <token id="4" string="had" />
            <token id="5" string="entered" />
            <token id="6" string="the" />
            <token id="7" string="jury" />
            <token id="8" string="deliberation" />
            <token id="9" string="room" />
            <token id="10" string="with" />
            <token id="11" string="open" />
            <token id="12" string="minds" />
            <token id="13" string="," />
            <token id="14" string="not" />
            <token id="15" string="only" />
            <token id="16" string="because" />
            <token id="17" string="the" />
            <token id="18" string="judge" />
            <token id="19" string="had" />
            <token id="20" string="so" />
            <token id="21" string="instructed" />
            <token id="22" string="," />
            <token id="23" string="but" />
            <token id="24" string="also" />
            <token id="25" string="because" />
            <token id="26" string="the" />
            <token id="27" string="disjointed" />
            <token id="28" string="testimony" />
            <token id="29" string="had" />
            <token id="30" string="left" />
            <token id="31" string="them" />
            <token id="32" string="confused" />
          </tokens>
        </chunking>
        <chunking id="2" string="All" type="NP">
          <tokens>
            <token id="1" string="All" />
          </tokens>
        </chunking>
        <chunking id="3" string="the jury deliberation room with open minds" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="jury" />
            <token id="8" string="deliberation" />
            <token id="9" string="room" />
            <token id="10" string="with" />
            <token id="11" string="open" />
            <token id="12" string="minds" />
          </tokens>
        </chunking>
        <chunking id="4" string="the disjointed testimony" type="NP">
          <tokens>
            <token id="26" string="the" />
            <token id="27" string="disjointed" />
            <token id="28" string="testimony" />
          </tokens>
        </chunking>
        <chunking id="5" string="the jury deliberation room" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="jury" />
            <token id="8" string="deliberation" />
            <token id="9" string="room" />
          </tokens>
        </chunking>
        <chunking id="6" string="had left them confused" type="VP">
          <tokens>
            <token id="29" string="had" />
            <token id="30" string="left" />
            <token id="31" string="them" />
            <token id="32" string="confused" />
          </tokens>
        </chunking>
        <chunking id="7" string="entered the jury deliberation room with open minds , not only because the judge had so instructed , but also because the disjointed testimony had left them confused" type="VP">
          <tokens>
            <token id="5" string="entered" />
            <token id="6" string="the" />
            <token id="7" string="jury" />
            <token id="8" string="deliberation" />
            <token id="9" string="room" />
            <token id="10" string="with" />
            <token id="11" string="open" />
            <token id="12" string="minds" />
            <token id="13" string="," />
            <token id="14" string="not" />
            <token id="15" string="only" />
            <token id="16" string="because" />
            <token id="17" string="the" />
            <token id="18" string="judge" />
            <token id="19" string="had" />
            <token id="20" string="so" />
            <token id="21" string="instructed" />
            <token id="22" string="," />
            <token id="23" string="but" />
            <token id="24" string="also" />
            <token id="25" string="because" />
            <token id="26" string="the" />
            <token id="27" string="disjointed" />
            <token id="28" string="testimony" />
            <token id="29" string="had" />
            <token id="30" string="left" />
            <token id="31" string="them" />
            <token id="32" string="confused" />
          </tokens>
        </chunking>
        <chunking id="8" string="said they had entered the jury deliberation room with open minds , not only because the judge had so instructed , but also because the disjointed testimony had left them confused" type="VP">
          <tokens>
            <token id="2" string="said" />
            <token id="3" string="they" />
            <token id="4" string="had" />
            <token id="5" string="entered" />
            <token id="6" string="the" />
            <token id="7" string="jury" />
            <token id="8" string="deliberation" />
            <token id="9" string="room" />
            <token id="10" string="with" />
            <token id="11" string="open" />
            <token id="12" string="minds" />
            <token id="13" string="," />
            <token id="14" string="not" />
            <token id="15" string="only" />
            <token id="16" string="because" />
            <token id="17" string="the" />
            <token id="18" string="judge" />
            <token id="19" string="had" />
            <token id="20" string="so" />
            <token id="21" string="instructed" />
            <token id="22" string="," />
            <token id="23" string="but" />
            <token id="24" string="also" />
            <token id="25" string="because" />
            <token id="26" string="the" />
            <token id="27" string="disjointed" />
            <token id="28" string="testimony" />
            <token id="29" string="had" />
            <token id="30" string="left" />
            <token id="31" string="them" />
            <token id="32" string="confused" />
          </tokens>
        </chunking>
        <chunking id="9" string="them" type="NP">
          <tokens>
            <token id="31" string="them" />
          </tokens>
        </chunking>
        <chunking id="10" string="they had entered the jury deliberation room with open minds , not only because the judge had so instructed , but also because the disjointed testimony had left them confused" type="SBAR">
          <tokens>
            <token id="3" string="they" />
            <token id="4" string="had" />
            <token id="5" string="entered" />
            <token id="6" string="the" />
            <token id="7" string="jury" />
            <token id="8" string="deliberation" />
            <token id="9" string="room" />
            <token id="10" string="with" />
            <token id="11" string="open" />
            <token id="12" string="minds" />
            <token id="13" string="," />
            <token id="14" string="not" />
            <token id="15" string="only" />
            <token id="16" string="because" />
            <token id="17" string="the" />
            <token id="18" string="judge" />
            <token id="19" string="had" />
            <token id="20" string="so" />
            <token id="21" string="instructed" />
            <token id="22" string="," />
            <token id="23" string="but" />
            <token id="24" string="also" />
            <token id="25" string="because" />
            <token id="26" string="the" />
            <token id="27" string="disjointed" />
            <token id="28" string="testimony" />
            <token id="29" string="had" />
            <token id="30" string="left" />
            <token id="31" string="them" />
            <token id="32" string="confused" />
          </tokens>
        </chunking>
        <chunking id="11" string="had so instructed" type="VP">
          <tokens>
            <token id="19" string="had" />
            <token id="20" string="so" />
            <token id="21" string="instructed" />
          </tokens>
        </chunking>
        <chunking id="12" string="they" type="NP">
          <tokens>
            <token id="3" string="they" />
          </tokens>
        </chunking>
        <chunking id="13" string="instructed" type="VP">
          <tokens>
            <token id="21" string="instructed" />
          </tokens>
        </chunking>
        <chunking id="14" string="confused" type="ADJP">
          <tokens>
            <token id="32" string="confused" />
          </tokens>
        </chunking>
        <chunking id="15" string="open minds" type="NP">
          <tokens>
            <token id="11" string="open" />
            <token id="12" string="minds" />
          </tokens>
        </chunking>
        <chunking id="16" string="not only because the judge had so instructed" type="SBAR">
          <tokens>
            <token id="14" string="not" />
            <token id="15" string="only" />
            <token id="16" string="because" />
            <token id="17" string="the" />
            <token id="18" string="judge" />
            <token id="19" string="had" />
            <token id="20" string="so" />
            <token id="21" string="instructed" />
          </tokens>
        </chunking>
        <chunking id="17" string="left them confused" type="VP">
          <tokens>
            <token id="30" string="left" />
            <token id="31" string="them" />
            <token id="32" string="confused" />
          </tokens>
        </chunking>
        <chunking id="18" string="because the disjointed testimony had left them confused" type="SBAR">
          <tokens>
            <token id="25" string="because" />
            <token id="26" string="the" />
            <token id="27" string="disjointed" />
            <token id="28" string="testimony" />
            <token id="29" string="had" />
            <token id="30" string="left" />
            <token id="31" string="them" />
            <token id="32" string="confused" />
          </tokens>
        </chunking>
        <chunking id="19" string="the judge" type="NP">
          <tokens>
            <token id="17" string="the" />
            <token id="18" string="judge" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">said</governor>
          <dependent id="1">All</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">said</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">entered</governor>
          <dependent id="3">they</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="5">entered</governor>
          <dependent id="4">had</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="2">said</governor>
          <dependent id="5">entered</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">room</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">room</governor>
          <dependent id="7">jury</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">room</governor>
          <dependent id="8">deliberation</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">entered</governor>
          <dependent id="9">room</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">minds</governor>
          <dependent id="10">with</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">minds</governor>
          <dependent id="11">open</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">room</governor>
          <dependent id="12">minds</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="21">instructed</governor>
          <dependent id="14">not</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="21">instructed</governor>
          <dependent id="15">only</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="21">instructed</governor>
          <dependent id="16">because</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">judge</governor>
          <dependent id="17">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">instructed</governor>
          <dependent id="18">judge</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="21">instructed</governor>
          <dependent id="19">had</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="21">instructed</governor>
          <dependent id="20">so</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="5">entered</governor>
          <dependent id="21">instructed</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="5">entered</governor>
          <dependent id="23">but</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="30">left</governor>
          <dependent id="24">also</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="30">left</governor>
          <dependent id="25">because</dependent>
        </dependency>
        <dependency type="det">
          <governor id="28">testimony</governor>
          <dependent id="26">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="28">testimony</governor>
          <dependent id="27">disjointed</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="30">left</governor>
          <dependent id="28">testimony</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="30">left</governor>
          <dependent id="29">had</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="5">entered</governor>
          <dependent id="30">left</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="32">confused</governor>
          <dependent id="31">them</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="30">left</governor>
          <dependent id="32">confused</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="left" type="IDEOLOGY" score="0.0">
          <tokens>
            <token id="30" string="left" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="58" has_coreference="true">
      <content>&amp;quot;I went into the jury room as confused and uncertain as I was the day I first sat in the jury box the first day of this trial,&amp;quot; said Brenda Williams, 38, a Pacific Bell service representative.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="went" lemma="go" stem="went" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="into" lemma="into" stem="into" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="room" lemma="room" stem="room" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="confused" lemma="confused" stem="confus" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="uncertain" lemma="uncertain" stem="uncertain" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="16" string="day" lemma="day" stem="dai" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="17" string="I" lemma="i" stem="i" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="18" string="first" lemma="first" stem="first" pos="JJ" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="19" string="sat" lemma="sit" stem="sat" pos="VBD" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="20" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="21" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="22" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="23" string="box" lemma="box" stem="box" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="24" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="25" string="first" lemma="first" stem="first" pos="JJ" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="26" string="day" lemma="day" stem="dai" pos="NN" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="27" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="28" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="29" string="trial" lemma="trial" stem="trial" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="30" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="Brenda" lemma="Brenda" stem="brenda" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="34" string="Williams" lemma="Williams" stem="william" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="35" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="38" lemma="38" stem="38" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="37" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="39" string="Pacific" lemma="Pacific" stem="pacif" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="40" string="Bell" lemma="Bell" stem="bell" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="41" string="service" lemma="service" stem="servic" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="42" string="representative" lemma="representative" stem="repres" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="43" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (SINV (`` ``) (S (NP (PRP I)) (VP (VBD went) (PP (IN into) (NP (DT the) (NN jury) (NN room))) (PP (IN as) (ADJP (JJ confused) (CC and) (JJ uncertain))) (SBAR (IN as) (S (NP (PRP I)) (VP (VBD was) (NP (NP (DT the) (NN day)) (SBAR (S (NP (CD I)) (ADVP (JJ first)) (VP (VBD sat) (PP (IN in) (NP (NP (DT the) (NN jury) (NN box)) (NP-TMP (DT the) (JJ first) (NN day)) (PP (IN of) (NP (DT this) (NN trial)))))))))))))) (, ,) ('' '') (VP (VBD said)) (NP (NP (NNP Brenda) (NNP Williams)) (, ,) (NP (NP (CD 38)) (, ,) (NP (DT a) (NNP Pacific) (NNP Bell) (NN service) (NN representative)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="I first sat in the jury box the first day of this trial" type="SBAR">
          <tokens>
            <token id="17" string="I" />
            <token id="18" string="first" />
            <token id="19" string="sat" />
            <token id="20" string="in" />
            <token id="21" string="the" />
            <token id="22" string="jury" />
            <token id="23" string="box" />
            <token id="24" string="the" />
            <token id="25" string="first" />
            <token id="26" string="day" />
            <token id="27" string="of" />
            <token id="28" string="this" />
            <token id="29" string="trial" />
          </tokens>
        </chunking>
        <chunking id="2" string="38" type="NP">
          <tokens>
            <token id="36" string="38" />
          </tokens>
        </chunking>
        <chunking id="3" string="as I was the day I first sat in the jury box the first day of this trial" type="SBAR">
          <tokens>
            <token id="12" string="as" />
            <token id="13" string="I" />
            <token id="14" string="was" />
            <token id="15" string="the" />
            <token id="16" string="day" />
            <token id="17" string="I" />
            <token id="18" string="first" />
            <token id="19" string="sat" />
            <token id="20" string="in" />
            <token id="21" string="the" />
            <token id="22" string="jury" />
            <token id="23" string="box" />
            <token id="24" string="the" />
            <token id="25" string="first" />
            <token id="26" string="day" />
            <token id="27" string="of" />
            <token id="28" string="this" />
            <token id="29" string="trial" />
          </tokens>
        </chunking>
        <chunking id="4" string="the day" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="day" />
          </tokens>
        </chunking>
        <chunking id="5" string="Brenda Williams , 38 , a Pacific Bell service representative" type="NP">
          <tokens>
            <token id="33" string="Brenda" />
            <token id="34" string="Williams" />
            <token id="35" string="," />
            <token id="36" string="38" />
            <token id="37" string="," />
            <token id="38" string="a" />
            <token id="39" string="Pacific" />
            <token id="40" string="Bell" />
            <token id="41" string="service" />
            <token id="42" string="representative" />
          </tokens>
        </chunking>
        <chunking id="6" string="I" type="NP">
          <tokens>
            <token id="2" string="I" />
          </tokens>
        </chunking>
        <chunking id="7" string="went into the jury room as confused and uncertain as I was the day I first sat in the jury box the first day of this trial" type="VP">
          <tokens>
            <token id="3" string="went" />
            <token id="4" string="into" />
            <token id="5" string="the" />
            <token id="6" string="jury" />
            <token id="7" string="room" />
            <token id="8" string="as" />
            <token id="9" string="confused" />
            <token id="10" string="and" />
            <token id="11" string="uncertain" />
            <token id="12" string="as" />
            <token id="13" string="I" />
            <token id="14" string="was" />
            <token id="15" string="the" />
            <token id="16" string="day" />
            <token id="17" string="I" />
            <token id="18" string="first" />
            <token id="19" string="sat" />
            <token id="20" string="in" />
            <token id="21" string="the" />
            <token id="22" string="jury" />
            <token id="23" string="box" />
            <token id="24" string="the" />
            <token id="25" string="first" />
            <token id="26" string="day" />
            <token id="27" string="of" />
            <token id="28" string="this" />
            <token id="29" string="trial" />
          </tokens>
        </chunking>
        <chunking id="8" string="was the day I first sat in the jury box the first day of this trial" type="VP">
          <tokens>
            <token id="14" string="was" />
            <token id="15" string="the" />
            <token id="16" string="day" />
            <token id="17" string="I" />
            <token id="18" string="first" />
            <token id="19" string="sat" />
            <token id="20" string="in" />
            <token id="21" string="the" />
            <token id="22" string="jury" />
            <token id="23" string="box" />
            <token id="24" string="the" />
            <token id="25" string="first" />
            <token id="26" string="day" />
            <token id="27" string="of" />
            <token id="28" string="this" />
            <token id="29" string="trial" />
          </tokens>
        </chunking>
        <chunking id="9" string="the jury box the first day of this trial" type="NP">
          <tokens>
            <token id="21" string="the" />
            <token id="22" string="jury" />
            <token id="23" string="box" />
            <token id="24" string="the" />
            <token id="25" string="first" />
            <token id="26" string="day" />
            <token id="27" string="of" />
            <token id="28" string="this" />
            <token id="29" string="trial" />
          </tokens>
        </chunking>
        <chunking id="10" string="the jury box" type="NP">
          <tokens>
            <token id="21" string="the" />
            <token id="22" string="jury" />
            <token id="23" string="box" />
          </tokens>
        </chunking>
        <chunking id="11" string="the day I first sat in the jury box the first day of this trial" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="day" />
            <token id="17" string="I" />
            <token id="18" string="first" />
            <token id="19" string="sat" />
            <token id="20" string="in" />
            <token id="21" string="the" />
            <token id="22" string="jury" />
            <token id="23" string="box" />
            <token id="24" string="the" />
            <token id="25" string="first" />
            <token id="26" string="day" />
            <token id="27" string="of" />
            <token id="28" string="this" />
            <token id="29" string="trial" />
          </tokens>
        </chunking>
        <chunking id="12" string="Brenda Williams" type="NP">
          <tokens>
            <token id="33" string="Brenda" />
            <token id="34" string="Williams" />
          </tokens>
        </chunking>
        <chunking id="13" string="the jury room" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="jury" />
            <token id="7" string="room" />
          </tokens>
        </chunking>
        <chunking id="14" string="confused and uncertain" type="ADJP">
          <tokens>
            <token id="9" string="confused" />
            <token id="10" string="and" />
            <token id="11" string="uncertain" />
          </tokens>
        </chunking>
        <chunking id="15" string="sat in the jury box the first day of this trial" type="VP">
          <tokens>
            <token id="19" string="sat" />
            <token id="20" string="in" />
            <token id="21" string="the" />
            <token id="22" string="jury" />
            <token id="23" string="box" />
            <token id="24" string="the" />
            <token id="25" string="first" />
            <token id="26" string="day" />
            <token id="27" string="of" />
            <token id="28" string="this" />
            <token id="29" string="trial" />
          </tokens>
        </chunking>
        <chunking id="16" string="a Pacific Bell service representative" type="NP">
          <tokens>
            <token id="38" string="a" />
            <token id="39" string="Pacific" />
            <token id="40" string="Bell" />
            <token id="41" string="service" />
            <token id="42" string="representative" />
          </tokens>
        </chunking>
        <chunking id="17" string="38 , a Pacific Bell service representative" type="NP">
          <tokens>
            <token id="36" string="38" />
            <token id="37" string="," />
            <token id="38" string="a" />
            <token id="39" string="Pacific" />
            <token id="40" string="Bell" />
            <token id="41" string="service" />
            <token id="42" string="representative" />
          </tokens>
        </chunking>
        <chunking id="18" string="this trial" type="NP">
          <tokens>
            <token id="28" string="this" />
            <token id="29" string="trial" />
          </tokens>
        </chunking>
        <chunking id="19" string="said" type="VP">
          <tokens>
            <token id="32" string="said" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">went</governor>
          <dependent id="2">I</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="32">said</governor>
          <dependent id="3">went</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">room</governor>
          <dependent id="4">into</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">room</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">room</governor>
          <dependent id="6">jury</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">went</governor>
          <dependent id="7">room</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">confused</governor>
          <dependent id="8">as</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="3">went</governor>
          <dependent id="9">confused</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="9">confused</governor>
          <dependent id="10">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="9">confused</governor>
          <dependent id="11">uncertain</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="16">day</governor>
          <dependent id="12">as</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">day</governor>
          <dependent id="13">I</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="16">day</governor>
          <dependent id="14">was</dependent>
        </dependency>
        <dependency type="det">
          <governor id="16">day</governor>
          <dependent id="15">the</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="3">went</governor>
          <dependent id="16">day</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">sat</governor>
          <dependent id="17">I</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="19">sat</governor>
          <dependent id="18">first</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="16">day</governor>
          <dependent id="19">sat</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">box</governor>
          <dependent id="20">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="23">box</governor>
          <dependent id="21">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="23">box</governor>
          <dependent id="22">jury</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">sat</governor>
          <dependent id="23">box</dependent>
        </dependency>
        <dependency type="det">
          <governor id="26">day</governor>
          <dependent id="24">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="26">day</governor>
          <dependent id="25">first</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="23">box</governor>
          <dependent id="26">day</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">trial</governor>
          <dependent id="27">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="29">trial</governor>
          <dependent id="28">this</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">box</governor>
          <dependent id="29">trial</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="32">said</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="34">Williams</governor>
          <dependent id="33">Brenda</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="32">said</governor>
          <dependent id="34">Williams</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="34">Williams</governor>
          <dependent id="36">38</dependent>
        </dependency>
        <dependency type="det">
          <governor id="42">representative</governor>
          <dependent id="38">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="42">representative</governor>
          <dependent id="39">Pacific</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="42">representative</governor>
          <dependent id="40">Bell</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="42">representative</governor>
          <dependent id="41">service</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="36">38</governor>
          <dependent id="42">representative</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="first sat" type="DATE" score="0.0">
          <tokens>
            <token id="18" string="first" />
            <token id="19" string="sat" />
          </tokens>
        </entity>
        <entity id="2" string="the first day" type="DURATION" score="0.0">
          <tokens>
            <token id="24" string="the" />
            <token id="25" string="first" />
            <token id="26" string="day" />
          </tokens>
        </entity>
        <entity id="3" string="38" type="NUMBER" score="0.0">
          <tokens>
            <token id="36" string="38" />
          </tokens>
        </entity>
        <entity id="4" string="Pacific Bell" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="39" string="Pacific" />
            <token id="40" string="Bell" />
          </tokens>
        </entity>
        <entity id="5" string="the day" type="DATE" score="0.0">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="day" />
          </tokens>
        </entity>
        <entity id="6" string="I" type="NUMBER" score="0.0">
          <tokens>
            <token id="17" string="I" />
          </tokens>
        </entity>
        <entity id="7" string="Brenda Williams" type="PERSON" score="0.0">
          <tokens>
            <token id="33" string="Brenda" />
            <token id="34" string="Williams" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="59" has_coreference="true">
      <content>&amp;quot;I felt I had missed out on something, maybe I had dozed off during some important testimony and I was hoping some of my fellow jurors had something in their notes that I didn&amp;apost;t have in mine that would make it all come together.&amp;quot;</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="felt" lemma="feel" stem="felt" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="missed" lemma="miss" stem="miss" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="out" lemma="out" stem="out" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="something" lemma="something" stem="someth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="maybe" lemma="maybe" stem="mayb" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="dozed" lemma="doze" stem="doze" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="off" lemma="off" stem="off" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="during" lemma="during" stem="dure" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="some" lemma="some" stem="some" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="18" string="important" lemma="important" stem="import" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="19" string="testimony" lemma="testimony" stem="testimoni" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="20" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="22" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="hoping" lemma="hope" stem="hope" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="some" lemma="some" stem="some" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="25" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="26" string="my" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="27" string="fellow" lemma="fellow" stem="fellow" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="28" string="jurors" lemma="juror" stem="juror" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="29" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="30" string="something" lemma="something" stem="someth" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="31" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="32" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="33" string="notes" lemma="note" stem="note" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="34" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="35" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="36" string="did" lemma="do" stem="did" pos="VBD" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="37" string="n't" lemma="not" stem="n't" pos="RB" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="38" string="have" lemma="have" stem="have" pos="VB" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="39" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="40" string="mine" lemma="mine" stem="mine" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="41" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="42" string="would" lemma="would" stem="would" pos="MD" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="43" string="make" lemma="make" stem="make" pos="VB" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="44" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="45" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="46" string="come" lemma="come" stem="come" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="47" string="together" lemma="together" stem="togeth" pos="RB" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="48" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="49" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (NP (PRP I)) (VP (VBD felt) (SBAR (S (NP (PRP I)) (VP (VBD had) (VP (VBN missed) (PRT (RP out)) (PP (IN on) (NP (NN something))))))))) (, ,) (S (ADVP (RB maybe)) (NP (PRP I)) (VP (VBD had) (VP (VBN dozed) (PRT (RP off)) (PP (IN during) (NP (DT some) (JJ important) (NN testimony)))))) (CC and) (S (NP (PRP I)) (VP (VBD was) (VP (VBG hoping) (NP (NP (DT some)) (PP (IN of) (NP (NP (PRP$ my) (JJ fellow) (NNS jurors)) (VP (VBD had) (NP (NN something)) (PP (IN in) (NP (PRP$ their) (NNS notes))) (SBAR (IN that) (S (NP (PRP I)) (VP (VBD did) (RB n't) (VP (VB have) (PP (IN in) (NP (NN mine)))))))) (SBAR (WHNP (WDT that)) (S (VP (MD would) (VP (VB make) (S (NP (PRP it)) (NP (NP (DT all)) (VP (VBN come) (ADVP (RB together))))))))))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="some important testimony" type="NP">
          <tokens>
            <token id="17" string="some" />
            <token id="18" string="important" />
            <token id="19" string="testimony" />
          </tokens>
        </chunking>
        <chunking id="2" string="mine" type="NP">
          <tokens>
            <token id="40" string="mine" />
          </tokens>
        </chunking>
        <chunking id="3" string="all" type="NP">
          <tokens>
            <token id="45" string="all" />
          </tokens>
        </chunking>
        <chunking id="4" string="did n't have in mine" type="VP">
          <tokens>
            <token id="36" string="did" />
            <token id="37" string="n't" />
            <token id="38" string="have" />
            <token id="39" string="in" />
            <token id="40" string="mine" />
          </tokens>
        </chunking>
        <chunking id="5" string="missed out on something" type="VP">
          <tokens>
            <token id="6" string="missed" />
            <token id="7" string="out" />
            <token id="8" string="on" />
            <token id="9" string="something" />
          </tokens>
        </chunking>
        <chunking id="6" string="would make it all come together" type="VP">
          <tokens>
            <token id="42" string="would" />
            <token id="43" string="make" />
            <token id="44" string="it" />
            <token id="45" string="all" />
            <token id="46" string="come" />
            <token id="47" string="together" />
          </tokens>
        </chunking>
        <chunking id="7" string="that would make it all come together" type="SBAR">
          <tokens>
            <token id="41" string="that" />
            <token id="42" string="would" />
            <token id="43" string="make" />
            <token id="44" string="it" />
            <token id="45" string="all" />
            <token id="46" string="come" />
            <token id="47" string="together" />
          </tokens>
        </chunking>
        <chunking id="8" string="it" type="NP">
          <tokens>
            <token id="44" string="it" />
          </tokens>
        </chunking>
        <chunking id="9" string="come together" type="VP">
          <tokens>
            <token id="46" string="come" />
            <token id="47" string="together" />
          </tokens>
        </chunking>
        <chunking id="10" string="something" type="NP">
          <tokens>
            <token id="9" string="something" />
          </tokens>
        </chunking>
        <chunking id="11" string="some of my fellow jurors had something in their notes that I did n't have in mine that would make it all come together" type="NP">
          <tokens>
            <token id="24" string="some" />
            <token id="25" string="of" />
            <token id="26" string="my" />
            <token id="27" string="fellow" />
            <token id="28" string="jurors" />
            <token id="29" string="had" />
            <token id="30" string="something" />
            <token id="31" string="in" />
            <token id="32" string="their" />
            <token id="33" string="notes" />
            <token id="34" string="that" />
            <token id="35" string="I" />
            <token id="36" string="did" />
            <token id="37" string="n't" />
            <token id="38" string="have" />
            <token id="39" string="in" />
            <token id="40" string="mine" />
            <token id="41" string="that" />
            <token id="42" string="would" />
            <token id="43" string="make" />
            <token id="44" string="it" />
            <token id="45" string="all" />
            <token id="46" string="come" />
            <token id="47" string="together" />
          </tokens>
        </chunking>
        <chunking id="12" string="all come together" type="NP">
          <tokens>
            <token id="45" string="all" />
            <token id="46" string="come" />
            <token id="47" string="together" />
          </tokens>
        </chunking>
        <chunking id="13" string="I had missed out on something" type="SBAR">
          <tokens>
            <token id="4" string="I" />
            <token id="5" string="had" />
            <token id="6" string="missed" />
            <token id="7" string="out" />
            <token id="8" string="on" />
            <token id="9" string="something" />
          </tokens>
        </chunking>
        <chunking id="14" string="had missed out on something" type="VP">
          <tokens>
            <token id="5" string="had" />
            <token id="6" string="missed" />
            <token id="7" string="out" />
            <token id="8" string="on" />
            <token id="9" string="something" />
          </tokens>
        </chunking>
        <chunking id="15" string="some" type="NP">
          <tokens>
            <token id="24" string="some" />
          </tokens>
        </chunking>
        <chunking id="16" string="I" type="NP">
          <tokens>
            <token id="2" string="I" />
          </tokens>
        </chunking>
        <chunking id="17" string="hoping some of my fellow jurors had something in their notes that I did n't have in mine that would make it all come together" type="VP">
          <tokens>
            <token id="23" string="hoping" />
            <token id="24" string="some" />
            <token id="25" string="of" />
            <token id="26" string="my" />
            <token id="27" string="fellow" />
            <token id="28" string="jurors" />
            <token id="29" string="had" />
            <token id="30" string="something" />
            <token id="31" string="in" />
            <token id="32" string="their" />
            <token id="33" string="notes" />
            <token id="34" string="that" />
            <token id="35" string="I" />
            <token id="36" string="did" />
            <token id="37" string="n't" />
            <token id="38" string="have" />
            <token id="39" string="in" />
            <token id="40" string="mine" />
            <token id="41" string="that" />
            <token id="42" string="would" />
            <token id="43" string="make" />
            <token id="44" string="it" />
            <token id="45" string="all" />
            <token id="46" string="come" />
            <token id="47" string="together" />
          </tokens>
        </chunking>
        <chunking id="18" string="was hoping some of my fellow jurors had something in their notes that I did n't have in mine that would make it all come together" type="VP">
          <tokens>
            <token id="22" string="was" />
            <token id="23" string="hoping" />
            <token id="24" string="some" />
            <token id="25" string="of" />
            <token id="26" string="my" />
            <token id="27" string="fellow" />
            <token id="28" string="jurors" />
            <token id="29" string="had" />
            <token id="30" string="something" />
            <token id="31" string="in" />
            <token id="32" string="their" />
            <token id="33" string="notes" />
            <token id="34" string="that" />
            <token id="35" string="I" />
            <token id="36" string="did" />
            <token id="37" string="n't" />
            <token id="38" string="have" />
            <token id="39" string="in" />
            <token id="40" string="mine" />
            <token id="41" string="that" />
            <token id="42" string="would" />
            <token id="43" string="make" />
            <token id="44" string="it" />
            <token id="45" string="all" />
            <token id="46" string="come" />
            <token id="47" string="together" />
          </tokens>
        </chunking>
        <chunking id="19" string="their notes" type="NP">
          <tokens>
            <token id="32" string="their" />
            <token id="33" string="notes" />
          </tokens>
        </chunking>
        <chunking id="20" string="my fellow jurors had something in their notes that I did n't have in mine that would make it all come together" type="NP">
          <tokens>
            <token id="26" string="my" />
            <token id="27" string="fellow" />
            <token id="28" string="jurors" />
            <token id="29" string="had" />
            <token id="30" string="something" />
            <token id="31" string="in" />
            <token id="32" string="their" />
            <token id="33" string="notes" />
            <token id="34" string="that" />
            <token id="35" string="I" />
            <token id="36" string="did" />
            <token id="37" string="n't" />
            <token id="38" string="have" />
            <token id="39" string="in" />
            <token id="40" string="mine" />
            <token id="41" string="that" />
            <token id="42" string="would" />
            <token id="43" string="make" />
            <token id="44" string="it" />
            <token id="45" string="all" />
            <token id="46" string="come" />
            <token id="47" string="together" />
          </tokens>
        </chunking>
        <chunking id="21" string="dozed off during some important testimony" type="VP">
          <tokens>
            <token id="14" string="dozed" />
            <token id="15" string="off" />
            <token id="16" string="during" />
            <token id="17" string="some" />
            <token id="18" string="important" />
            <token id="19" string="testimony" />
          </tokens>
        </chunking>
        <chunking id="22" string="make it all come together" type="VP">
          <tokens>
            <token id="43" string="make" />
            <token id="44" string="it" />
            <token id="45" string="all" />
            <token id="46" string="come" />
            <token id="47" string="together" />
          </tokens>
        </chunking>
        <chunking id="23" string="that I did n't have in mine" type="SBAR">
          <tokens>
            <token id="34" string="that" />
            <token id="35" string="I" />
            <token id="36" string="did" />
            <token id="37" string="n't" />
            <token id="38" string="have" />
            <token id="39" string="in" />
            <token id="40" string="mine" />
          </tokens>
        </chunking>
        <chunking id="24" string="had dozed off during some important testimony" type="VP">
          <tokens>
            <token id="13" string="had" />
            <token id="14" string="dozed" />
            <token id="15" string="off" />
            <token id="16" string="during" />
            <token id="17" string="some" />
            <token id="18" string="important" />
            <token id="19" string="testimony" />
          </tokens>
        </chunking>
        <chunking id="25" string="had something in their notes that I did n't have in mine" type="VP">
          <tokens>
            <token id="29" string="had" />
            <token id="30" string="something" />
            <token id="31" string="in" />
            <token id="32" string="their" />
            <token id="33" string="notes" />
            <token id="34" string="that" />
            <token id="35" string="I" />
            <token id="36" string="did" />
            <token id="37" string="n't" />
            <token id="38" string="have" />
            <token id="39" string="in" />
            <token id="40" string="mine" />
          </tokens>
        </chunking>
        <chunking id="26" string="felt I had missed out on something" type="VP">
          <tokens>
            <token id="3" string="felt" />
            <token id="4" string="I" />
            <token id="5" string="had" />
            <token id="6" string="missed" />
            <token id="7" string="out" />
            <token id="8" string="on" />
            <token id="9" string="something" />
          </tokens>
        </chunking>
        <chunking id="27" string="my fellow jurors" type="NP">
          <tokens>
            <token id="26" string="my" />
            <token id="27" string="fellow" />
            <token id="28" string="jurors" />
          </tokens>
        </chunking>
        <chunking id="28" string="have in mine" type="VP">
          <tokens>
            <token id="38" string="have" />
            <token id="39" string="in" />
            <token id="40" string="mine" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">felt</governor>
          <dependent id="2">I</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">felt</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">missed</governor>
          <dependent id="4">I</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="6">missed</governor>
          <dependent id="5">had</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">felt</governor>
          <dependent id="6">missed</dependent>
        </dependency>
        <dependency type="compound:prt">
          <governor id="6">missed</governor>
          <dependent id="7">out</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">something</governor>
          <dependent id="8">on</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">missed</governor>
          <dependent id="9">something</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="14">dozed</governor>
          <dependent id="11">maybe</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">dozed</governor>
          <dependent id="12">I</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="14">dozed</governor>
          <dependent id="13">had</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="3">felt</governor>
          <dependent id="14">dozed</dependent>
        </dependency>
        <dependency type="compound:prt">
          <governor id="14">dozed</governor>
          <dependent id="15">off</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">testimony</governor>
          <dependent id="16">during</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">testimony</governor>
          <dependent id="17">some</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">testimony</governor>
          <dependent id="18">important</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">dozed</governor>
          <dependent id="19">testimony</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="3">felt</governor>
          <dependent id="20">and</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="23">hoping</governor>
          <dependent id="21">I</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="23">hoping</governor>
          <dependent id="22">was</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="3">felt</governor>
          <dependent id="23">hoping</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="23">hoping</governor>
          <dependent id="24">some</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">jurors</governor>
          <dependent id="25">of</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="28">jurors</governor>
          <dependent id="26">my</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="28">jurors</governor>
          <dependent id="27">fellow</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="24">some</governor>
          <dependent id="28">jurors</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="28">jurors</governor>
          <dependent id="29">had</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="29">had</governor>
          <dependent id="30">something</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">notes</governor>
          <dependent id="31">in</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="33">notes</governor>
          <dependent id="32">their</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="29">had</governor>
          <dependent id="33">notes</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="38">have</governor>
          <dependent id="34">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="38">have</governor>
          <dependent id="35">I</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="38">have</governor>
          <dependent id="36">did</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="38">have</governor>
          <dependent id="37">n't</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="29">had</governor>
          <dependent id="38">have</dependent>
        </dependency>
        <dependency type="case">
          <governor id="40">mine</governor>
          <dependent id="39">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="38">have</governor>
          <dependent id="40">mine</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="43">make</governor>
          <dependent id="41">that</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="43">make</governor>
          <dependent id="42">would</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="28">jurors</governor>
          <dependent id="43">make</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="45">all</governor>
          <dependent id="44">it</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="43">make</governor>
          <dependent id="45">all</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="45">all</governor>
          <dependent id="46">come</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="46">come</governor>
          <dependent id="47">together</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="60" has_coreference="true">
      <content>Said juror Mark Bassett, 33, a computer specialist: &amp;quot;We just didn&amp;apost;t really know what happened.&amp;quot;</content>
      <tokens>
        <token id="1" string="Said" lemma="Said" stem="said" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="juror" lemma="juror" stem="juror" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="Mark" lemma="Mark" stem="mark" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="4" string="Bassett" lemma="Bassett" stem="bassett" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="33" lemma="33" stem="33" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="computer" lemma="computer" stem="comput" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="specialist" lemma="specialist" stem="specialist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="We" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="just" lemma="just" stem="just" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="did" lemma="do" stem="did" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="n't" lemma="not" stem="n't" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="really" lemma="really" stem="realli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="know" lemma="know" stem="know" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="what" lemma="what" stem="what" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="happened" lemma="happen" stem="happen" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (FRAG (NP (NP (NNP Said) (NN juror) (NNP Mark) (NNP Bassett)) (, ,) (NP (CD 33)) (, ,) (NP (DT a) (NN computer) (NN specialist)))) (: :) (S (NP (`` ``) (PRP We)) (ADVP (RB just)) (VP (VBD did) (RB n't) (ADVP (RB really)) (VP (VB know) (SBAR (WHNP (WP what)) (S (VP (VBD happened))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="Said juror Mark Bassett" type="NP">
          <tokens>
            <token id="1" string="Said" />
            <token id="2" string="juror" />
            <token id="3" string="Mark" />
            <token id="4" string="Bassett" />
          </tokens>
        </chunking>
        <chunking id="2" string="Said juror Mark Bassett , 33 , a computer specialist" type="NP">
          <tokens>
            <token id="1" string="Said" />
            <token id="2" string="juror" />
            <token id="3" string="Mark" />
            <token id="4" string="Bassett" />
            <token id="5" string="," />
            <token id="6" string="33" />
            <token id="7" string="," />
            <token id="8" string="a" />
            <token id="9" string="computer" />
            <token id="10" string="specialist" />
          </tokens>
        </chunking>
        <chunking id="3" string="what happened" type="SBAR">
          <tokens>
            <token id="19" string="what" />
            <token id="20" string="happened" />
          </tokens>
        </chunking>
        <chunking id="4" string="`` We" type="NP">
          <tokens>
            <token id="12" string="&quot;" />
            <token id="13" string="We" />
          </tokens>
        </chunking>
        <chunking id="5" string="know what happened" type="VP">
          <tokens>
            <token id="18" string="know" />
            <token id="19" string="what" />
            <token id="20" string="happened" />
          </tokens>
        </chunking>
        <chunking id="6" string="did n't really know what happened" type="VP">
          <tokens>
            <token id="15" string="did" />
            <token id="16" string="n't" />
            <token id="17" string="really" />
            <token id="18" string="know" />
            <token id="19" string="what" />
            <token id="20" string="happened" />
          </tokens>
        </chunking>
        <chunking id="7" string="a computer specialist" type="NP">
          <tokens>
            <token id="8" string="a" />
            <token id="9" string="computer" />
            <token id="10" string="specialist" />
          </tokens>
        </chunking>
        <chunking id="8" string="33" type="NP">
          <tokens>
            <token id="6" string="33" />
          </tokens>
        </chunking>
        <chunking id="9" string="happened" type="VP">
          <tokens>
            <token id="20" string="happened" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="4">Bassett</governor>
          <dependent id="1">Said</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">Bassett</governor>
          <dependent id="2">juror</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">Bassett</governor>
          <dependent id="3">Mark</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="18">know</governor>
          <dependent id="4">Bassett</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="4">Bassett</governor>
          <dependent id="6">33</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">specialist</governor>
          <dependent id="8">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">specialist</governor>
          <dependent id="9">computer</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="4">Bassett</governor>
          <dependent id="10">specialist</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="18">know</governor>
          <dependent id="13">We</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="18">know</governor>
          <dependent id="14">just</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="18">know</governor>
          <dependent id="15">did</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="18">know</governor>
          <dependent id="16">n't</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="18">know</governor>
          <dependent id="17">really</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="18">know</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="20">happened</governor>
          <dependent id="19">what</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="18">know</governor>
          <dependent id="20">happened</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Mark Bassett" type="PERSON" score="0.0">
          <tokens>
            <token id="3" string="Mark" />
            <token id="4" string="Bassett" />
          </tokens>
        </entity>
        <entity id="2" string="33" type="NUMBER" score="0.0">
          <tokens>
            <token id="6" string="33" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="61" has_coreference="false">
      <content>Deputy Dist.</content>
      <tokens>
        <token id="1" string="Deputy" lemma="Deputy" stem="deputi" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="Dist" lemma="Dist" stem="dist" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NNP Deputy) (NNP Dist) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Deputy Dist ." type="NP">
          <tokens>
            <token id="1" string="Deputy" />
            <token id="2" string="Dist" />
            <token id="3" string="." />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">Dist</governor>
          <dependent id="1">Deputy</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">Dist</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="62" has_coreference="true">
      <content>Atty Lael Rubin, who prosecuted the case, said a decision has not been made about whether to pursue the 13 counts involved in a mistrial.</content>
      <tokens>
        <token id="1" string="Atty" lemma="Atty" stem="atty" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="Lael" lemma="Lael" stem="lael" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="Rubin" lemma="Rubin" stem="rubin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="prosecuted" lemma="prosecute" stem="prosecut" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="decision" lemma="decision" stem="decis" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="made" lemma="make" stem="made" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="whether" lemma="whether" stem="whether" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="pursue" lemma="pursue" stem="pursu" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="22" string="13" lemma="13" stem="13" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="23" string="counts" lemma="count" stem="count" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="24" string="involved" lemma="involve" stem="involv" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="25" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="26" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="27" string="mistrial" lemma="mistrial" stem="mistrial" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="28" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Atty) (NNP Lael) (NNP Rubin)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBD prosecuted) (NP (DT the) (NN case))))) (, ,)) (VP (VBD said) (SBAR (S (NP (DT a) (NN decision)) (VP (VBZ has) (RB not) (VP (VBN been) (VP (VBN made) (PP (IN about) (SBAR (IN whether) (S (VP (TO to) (VP (VB pursue) (NP (NP (DT the) (CD 13) (NNS counts)) (VP (VBN involved) (PP (IN in) (NP (DT a) (NN mistrial)))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="has not been made about whether to pursue the 13 counts involved in a mistrial" type="VP">
          <tokens>
            <token id="13" string="has" />
            <token id="14" string="not" />
            <token id="15" string="been" />
            <token id="16" string="made" />
            <token id="17" string="about" />
            <token id="18" string="whether" />
            <token id="19" string="to" />
            <token id="20" string="pursue" />
            <token id="21" string="the" />
            <token id="22" string="13" />
            <token id="23" string="counts" />
            <token id="24" string="involved" />
            <token id="25" string="in" />
            <token id="26" string="a" />
            <token id="27" string="mistrial" />
          </tokens>
        </chunking>
        <chunking id="2" string="the 13 counts" type="NP">
          <tokens>
            <token id="21" string="the" />
            <token id="22" string="13" />
            <token id="23" string="counts" />
          </tokens>
        </chunking>
        <chunking id="3" string="a decision" type="NP">
          <tokens>
            <token id="11" string="a" />
            <token id="12" string="decision" />
          </tokens>
        </chunking>
        <chunking id="4" string="whether to pursue the 13 counts involved in a mistrial" type="SBAR">
          <tokens>
            <token id="18" string="whether" />
            <token id="19" string="to" />
            <token id="20" string="pursue" />
            <token id="21" string="the" />
            <token id="22" string="13" />
            <token id="23" string="counts" />
            <token id="24" string="involved" />
            <token id="25" string="in" />
            <token id="26" string="a" />
            <token id="27" string="mistrial" />
          </tokens>
        </chunking>
        <chunking id="5" string="the case" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="case" />
          </tokens>
        </chunking>
        <chunking id="6" string="prosecuted the case" type="VP">
          <tokens>
            <token id="6" string="prosecuted" />
            <token id="7" string="the" />
            <token id="8" string="case" />
          </tokens>
        </chunking>
        <chunking id="7" string="said a decision has not been made about whether to pursue the 13 counts involved in a mistrial" type="VP">
          <tokens>
            <token id="10" string="said" />
            <token id="11" string="a" />
            <token id="12" string="decision" />
            <token id="13" string="has" />
            <token id="14" string="not" />
            <token id="15" string="been" />
            <token id="16" string="made" />
            <token id="17" string="about" />
            <token id="18" string="whether" />
            <token id="19" string="to" />
            <token id="20" string="pursue" />
            <token id="21" string="the" />
            <token id="22" string="13" />
            <token id="23" string="counts" />
            <token id="24" string="involved" />
            <token id="25" string="in" />
            <token id="26" string="a" />
            <token id="27" string="mistrial" />
          </tokens>
        </chunking>
        <chunking id="8" string="Atty Lael Rubin , who prosecuted the case ," type="NP">
          <tokens>
            <token id="1" string="Atty" />
            <token id="2" string="Lael" />
            <token id="3" string="Rubin" />
            <token id="4" string="," />
            <token id="5" string="who" />
            <token id="6" string="prosecuted" />
            <token id="7" string="the" />
            <token id="8" string="case" />
            <token id="9" string="," />
          </tokens>
        </chunking>
        <chunking id="9" string="made about whether to pursue the 13 counts involved in a mistrial" type="VP">
          <tokens>
            <token id="16" string="made" />
            <token id="17" string="about" />
            <token id="18" string="whether" />
            <token id="19" string="to" />
            <token id="20" string="pursue" />
            <token id="21" string="the" />
            <token id="22" string="13" />
            <token id="23" string="counts" />
            <token id="24" string="involved" />
            <token id="25" string="in" />
            <token id="26" string="a" />
            <token id="27" string="mistrial" />
          </tokens>
        </chunking>
        <chunking id="10" string="pursue the 13 counts involved in a mistrial" type="VP">
          <tokens>
            <token id="20" string="pursue" />
            <token id="21" string="the" />
            <token id="22" string="13" />
            <token id="23" string="counts" />
            <token id="24" string="involved" />
            <token id="25" string="in" />
            <token id="26" string="a" />
            <token id="27" string="mistrial" />
          </tokens>
        </chunking>
        <chunking id="11" string="Atty Lael Rubin" type="NP">
          <tokens>
            <token id="1" string="Atty" />
            <token id="2" string="Lael" />
            <token id="3" string="Rubin" />
          </tokens>
        </chunking>
        <chunking id="12" string="a decision has not been made about whether to pursue the 13 counts involved in a mistrial" type="SBAR">
          <tokens>
            <token id="11" string="a" />
            <token id="12" string="decision" />
            <token id="13" string="has" />
            <token id="14" string="not" />
            <token id="15" string="been" />
            <token id="16" string="made" />
            <token id="17" string="about" />
            <token id="18" string="whether" />
            <token id="19" string="to" />
            <token id="20" string="pursue" />
            <token id="21" string="the" />
            <token id="22" string="13" />
            <token id="23" string="counts" />
            <token id="24" string="involved" />
            <token id="25" string="in" />
            <token id="26" string="a" />
            <token id="27" string="mistrial" />
          </tokens>
        </chunking>
        <chunking id="13" string="involved in a mistrial" type="VP">
          <tokens>
            <token id="24" string="involved" />
            <token id="25" string="in" />
            <token id="26" string="a" />
            <token id="27" string="mistrial" />
          </tokens>
        </chunking>
        <chunking id="14" string="who prosecuted the case" type="SBAR">
          <tokens>
            <token id="5" string="who" />
            <token id="6" string="prosecuted" />
            <token id="7" string="the" />
            <token id="8" string="case" />
          </tokens>
        </chunking>
        <chunking id="15" string="been made about whether to pursue the 13 counts involved in a mistrial" type="VP">
          <tokens>
            <token id="15" string="been" />
            <token id="16" string="made" />
            <token id="17" string="about" />
            <token id="18" string="whether" />
            <token id="19" string="to" />
            <token id="20" string="pursue" />
            <token id="21" string="the" />
            <token id="22" string="13" />
            <token id="23" string="counts" />
            <token id="24" string="involved" />
            <token id="25" string="in" />
            <token id="26" string="a" />
            <token id="27" string="mistrial" />
          </tokens>
        </chunking>
        <chunking id="16" string="a mistrial" type="NP">
          <tokens>
            <token id="26" string="a" />
            <token id="27" string="mistrial" />
          </tokens>
        </chunking>
        <chunking id="17" string="to pursue the 13 counts involved in a mistrial" type="VP">
          <tokens>
            <token id="19" string="to" />
            <token id="20" string="pursue" />
            <token id="21" string="the" />
            <token id="22" string="13" />
            <token id="23" string="counts" />
            <token id="24" string="involved" />
            <token id="25" string="in" />
            <token id="26" string="a" />
            <token id="27" string="mistrial" />
          </tokens>
        </chunking>
        <chunking id="18" string="the 13 counts involved in a mistrial" type="NP">
          <tokens>
            <token id="21" string="the" />
            <token id="22" string="13" />
            <token id="23" string="counts" />
            <token id="24" string="involved" />
            <token id="25" string="in" />
            <token id="26" string="a" />
            <token id="27" string="mistrial" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="3">Rubin</governor>
          <dependent id="1">Atty</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">Rubin</governor>
          <dependent id="2">Lael</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">said</governor>
          <dependent id="3">Rubin</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">prosecuted</governor>
          <dependent id="5">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="3">Rubin</governor>
          <dependent id="6">prosecuted</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">case</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">prosecuted</governor>
          <dependent id="8">case</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="10">said</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">decision</governor>
          <dependent id="11">a</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="16">made</governor>
          <dependent id="12">decision</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="16">made</governor>
          <dependent id="13">has</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="16">made</governor>
          <dependent id="14">not</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="16">made</governor>
          <dependent id="15">been</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="10">said</governor>
          <dependent id="16">made</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="20">pursue</governor>
          <dependent id="17">about</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="20">pursue</governor>
          <dependent id="18">whether</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="20">pursue</governor>
          <dependent id="19">to</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="16">made</governor>
          <dependent id="20">pursue</dependent>
        </dependency>
        <dependency type="det">
          <governor id="23">counts</governor>
          <dependent id="21">the</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="23">counts</governor>
          <dependent id="22">13</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="20">pursue</governor>
          <dependent id="23">counts</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="23">counts</governor>
          <dependent id="24">involved</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">mistrial</governor>
          <dependent id="25">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="27">mistrial</governor>
          <dependent id="26">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="24">involved</governor>
          <dependent id="27">mistrial</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="13" type="NUMBER" score="0.0">
          <tokens>
            <token id="22" string="13" />
          </tokens>
        </entity>
        <entity id="2" string="Lael Rubin" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Lael" />
            <token id="3" string="Rubin" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="63" has_coreference="true">
      <content>These counts involve 12 molestation charges against Ray Buckey, and a conspiracy count that accused both him and his mother.</content>
      <tokens>
        <token id="1" string="These" lemma="these" stem="these" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="counts" lemma="count" stem="count" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="involve" lemma="involve" stem="involv" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="12" lemma="12" stem="12" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="5" string="molestation" lemma="molestation" stem="molest" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="charges" lemma="charge" stem="charg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="against" lemma="against" stem="against" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="9" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="conspiracy" lemma="conspiracy" stem="conspiraci" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="count" lemma="count" stem="count" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="accused" lemma="accuse" stem="accus" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="both" lemma="both" stem="both" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="him" lemma="he" stem="him" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="21" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT These) (NNS counts)) (VP (VBP involve) (NP (NP (NP (CD 12) (NN molestation) (NNS charges)) (PP (IN against) (NP (NNP Ray) (NNP Buckey)))) (, ,) (CC and) (NP (NP (DT a) (NN conspiracy) (NN count)) (SBAR (WHNP (WDT that)) (S (VP (VBD accused) (NP (DT both) (NP (PRP him)) (CC and) (NP (PRP$ his) (NN mother))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="both him and his mother" type="NP">
          <tokens>
            <token id="17" string="both" />
            <token id="18" string="him" />
            <token id="19" string="and" />
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
        <chunking id="2" string="involve 12 molestation charges against Ray Buckey , and a conspiracy count that accused both him and his mother" type="VP">
          <tokens>
            <token id="3" string="involve" />
            <token id="4" string="12" />
            <token id="5" string="molestation" />
            <token id="6" string="charges" />
            <token id="7" string="against" />
            <token id="8" string="Ray" />
            <token id="9" string="Buckey" />
            <token id="10" string="," />
            <token id="11" string="and" />
            <token id="12" string="a" />
            <token id="13" string="conspiracy" />
            <token id="14" string="count" />
            <token id="15" string="that" />
            <token id="16" string="accused" />
            <token id="17" string="both" />
            <token id="18" string="him" />
            <token id="19" string="and" />
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
        <chunking id="3" string="him" type="NP">
          <tokens>
            <token id="18" string="him" />
          </tokens>
        </chunking>
        <chunking id="4" string="These counts" type="NP">
          <tokens>
            <token id="1" string="These" />
            <token id="2" string="counts" />
          </tokens>
        </chunking>
        <chunking id="5" string="Ray Buckey" type="NP">
          <tokens>
            <token id="8" string="Ray" />
            <token id="9" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="6" string="a conspiracy count that accused both him and his mother" type="NP">
          <tokens>
            <token id="12" string="a" />
            <token id="13" string="conspiracy" />
            <token id="14" string="count" />
            <token id="15" string="that" />
            <token id="16" string="accused" />
            <token id="17" string="both" />
            <token id="18" string="him" />
            <token id="19" string="and" />
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
        <chunking id="7" string="accused both him and his mother" type="VP">
          <tokens>
            <token id="16" string="accused" />
            <token id="17" string="both" />
            <token id="18" string="him" />
            <token id="19" string="and" />
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
        <chunking id="8" string="12 molestation charges against Ray Buckey" type="NP">
          <tokens>
            <token id="4" string="12" />
            <token id="5" string="molestation" />
            <token id="6" string="charges" />
            <token id="7" string="against" />
            <token id="8" string="Ray" />
            <token id="9" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="9" string="12 molestation charges" type="NP">
          <tokens>
            <token id="4" string="12" />
            <token id="5" string="molestation" />
            <token id="6" string="charges" />
          </tokens>
        </chunking>
        <chunking id="10" string="his mother" type="NP">
          <tokens>
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
        <chunking id="11" string="that accused both him and his mother" type="SBAR">
          <tokens>
            <token id="15" string="that" />
            <token id="16" string="accused" />
            <token id="17" string="both" />
            <token id="18" string="him" />
            <token id="19" string="and" />
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
        <chunking id="12" string="a conspiracy count" type="NP">
          <tokens>
            <token id="12" string="a" />
            <token id="13" string="conspiracy" />
            <token id="14" string="count" />
          </tokens>
        </chunking>
        <chunking id="13" string="12 molestation charges against Ray Buckey , and a conspiracy count that accused both him and his mother" type="NP">
          <tokens>
            <token id="4" string="12" />
            <token id="5" string="molestation" />
            <token id="6" string="charges" />
            <token id="7" string="against" />
            <token id="8" string="Ray" />
            <token id="9" string="Buckey" />
            <token id="10" string="," />
            <token id="11" string="and" />
            <token id="12" string="a" />
            <token id="13" string="conspiracy" />
            <token id="14" string="count" />
            <token id="15" string="that" />
            <token id="16" string="accused" />
            <token id="17" string="both" />
            <token id="18" string="him" />
            <token id="19" string="and" />
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">counts</governor>
          <dependent id="1">These</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">involve</governor>
          <dependent id="2">counts</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">involve</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="6">charges</governor>
          <dependent id="4">12</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">charges</governor>
          <dependent id="5">molestation</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">involve</governor>
          <dependent id="6">charges</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">Buckey</governor>
          <dependent id="7">against</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">Buckey</governor>
          <dependent id="8">Ray</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">charges</governor>
          <dependent id="9">Buckey</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="6">charges</governor>
          <dependent id="11">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">count</governor>
          <dependent id="12">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">count</governor>
          <dependent id="13">conspiracy</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="6">charges</governor>
          <dependent id="14">count</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">accused</governor>
          <dependent id="15">that</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="14">count</governor>
          <dependent id="16">accused</dependent>
        </dependency>
        <dependency type="cc:preconj">
          <governor id="18">him</governor>
          <dependent id="17">both</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="16">accused</governor>
          <dependent id="18">him</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="18">him</governor>
          <dependent id="19">and</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="21">mother</governor>
          <dependent id="20">his</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="18">him</governor>
          <dependent id="21">mother</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="12" type="NUMBER" score="0.0">
          <tokens>
            <token id="4" string="12" />
          </tokens>
        </entity>
        <entity id="2" string="Ray Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Ray" />
            <token id="9" string="Buckey" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="64" has_coreference="true">
      <content>The conspiracy count against Peggy McMartin Buckey was dismissed outright Thursday.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="conspiracy" lemma="conspiracy" stem="conspiraci" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="count" lemma="count" stem="count" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="against" lemma="against" stem="against" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="Peggy" lemma="Peggy" stem="peggi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="6" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="7" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="8" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="dismissed" lemma="dismiss" stem="dismiss" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="outright" lemma="outright" stem="outright" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="Thursday" lemma="Thursday" stem="thursdai" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT The) (NN conspiracy) (NN count)) (PP (IN against) (NP (NNP Peggy) (NNP McMartin) (NNP Buckey)))) (VP (VBD was) (VP (VBN dismissed) (ADVP (RB outright)) (NP-TMP (NNP Thursday)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Peggy McMartin Buckey" type="NP">
          <tokens>
            <token id="5" string="Peggy" />
            <token id="6" string="McMartin" />
            <token id="7" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="2" string="was dismissed outright Thursday" type="VP">
          <tokens>
            <token id="8" string="was" />
            <token id="9" string="dismissed" />
            <token id="10" string="outright" />
            <token id="11" string="Thursday" />
          </tokens>
        </chunking>
        <chunking id="3" string="dismissed outright Thursday" type="VP">
          <tokens>
            <token id="9" string="dismissed" />
            <token id="10" string="outright" />
            <token id="11" string="Thursday" />
          </tokens>
        </chunking>
        <chunking id="4" string="The conspiracy count against Peggy McMartin Buckey" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="conspiracy" />
            <token id="3" string="count" />
            <token id="4" string="against" />
            <token id="5" string="Peggy" />
            <token id="6" string="McMartin" />
            <token id="7" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="5" string="The conspiracy count" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="conspiracy" />
            <token id="3" string="count" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">count</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">count</governor>
          <dependent id="2">conspiracy</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="9">dismissed</governor>
          <dependent id="3">count</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">Buckey</governor>
          <dependent id="4">against</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">Buckey</governor>
          <dependent id="5">Peggy</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">Buckey</governor>
          <dependent id="6">McMartin</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">count</governor>
          <dependent id="7">Buckey</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="9">dismissed</governor>
          <dependent id="8">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="9">dismissed</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="9">dismissed</governor>
          <dependent id="10">outright</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="9">dismissed</governor>
          <dependent id="11">Thursday</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Peggy McMartin Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="Peggy" />
            <token id="6" string="McMartin" />
            <token id="7" string="Buckey" />
          </tokens>
        </entity>
        <entity id="2" string="Thursday" type="DATE" score="0.0">
          <tokens>
            <token id="11" string="Thursday" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="65" has_coreference="true">
      <content>&amp;quot;We ultimately must respect the jury&amp;apost;s decision,&amp;quot; Rubin said, &amp;quot;even though I personally disagree with it.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="We" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="ultimately" lemma="ultimately" stem="ultim" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="must" lemma="must" stem="must" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="respect" lemma="respect" stem="respect" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="7" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="8" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="9" string="decision" lemma="decision" stem="decis" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="Rubin" lemma="Rubin" stem="rubin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="13" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="even" lemma="even" stem="even" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="though" lemma="though" stem="though" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="personally" lemma="personally" stem="person" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="disagree" lemma="disagree" stem="disagre" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (PRP We)) (ADVP (RB ultimately)) (VP (MD must) (VP (VB respect) (NP (NP (DT the) (NN jury) (POS 's)) (NN decision)))) (PRN (, ,) ('' '') (S (NP (NNP Rubin)) (VP (VBD said))) (, ,) (`` ``)) (SBAR (RB even) (IN though) (S (NP (PRP I)) (ADVP (RB personally)) (VP (VBP disagree) (PP (IN with) (NP (PRP it)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="must respect the jury 's decision" type="VP">
          <tokens>
            <token id="4" string="must" />
            <token id="5" string="respect" />
            <token id="6" string="the" />
            <token id="7" string="jury" />
            <token id="8" string="'s" />
            <token id="9" string="decision" />
          </tokens>
        </chunking>
        <chunking id="2" string="respect the jury 's decision" type="VP">
          <tokens>
            <token id="5" string="respect" />
            <token id="6" string="the" />
            <token id="7" string="jury" />
            <token id="8" string="'s" />
            <token id="9" string="decision" />
          </tokens>
        </chunking>
        <chunking id="3" string="the jury 's decision" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="jury" />
            <token id="8" string="'s" />
            <token id="9" string="decision" />
          </tokens>
        </chunking>
        <chunking id="4" string="even though I personally disagree with it" type="SBAR">
          <tokens>
            <token id="16" string="even" />
            <token id="17" string="though" />
            <token id="18" string="I" />
            <token id="19" string="personally" />
            <token id="20" string="disagree" />
            <token id="21" string="with" />
            <token id="22" string="it" />
          </tokens>
        </chunking>
        <chunking id="5" string="disagree with it" type="VP">
          <tokens>
            <token id="20" string="disagree" />
            <token id="21" string="with" />
            <token id="22" string="it" />
          </tokens>
        </chunking>
        <chunking id="6" string="I" type="NP">
          <tokens>
            <token id="18" string="I" />
          </tokens>
        </chunking>
        <chunking id="7" string="Rubin" type="NP">
          <tokens>
            <token id="12" string="Rubin" />
          </tokens>
        </chunking>
        <chunking id="8" string="it" type="NP">
          <tokens>
            <token id="22" string="it" />
          </tokens>
        </chunking>
        <chunking id="9" string="We" type="NP">
          <tokens>
            <token id="2" string="We" />
          </tokens>
        </chunking>
        <chunking id="10" string="said" type="VP">
          <tokens>
            <token id="13" string="said" />
          </tokens>
        </chunking>
        <chunking id="11" string="the jury 's" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="jury" />
            <token id="8" string="'s" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="5">respect</governor>
          <dependent id="2">We</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">respect</governor>
          <dependent id="3">ultimately</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="5">respect</governor>
          <dependent id="4">must</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">respect</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">jury</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="9">decision</governor>
          <dependent id="7">jury</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">jury</governor>
          <dependent id="8">'s</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">respect</governor>
          <dependent id="9">decision</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="13">said</governor>
          <dependent id="12">Rubin</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="5">respect</governor>
          <dependent id="13">said</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="20">disagree</governor>
          <dependent id="16">even</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="20">disagree</governor>
          <dependent id="17">though</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="20">disagree</governor>
          <dependent id="18">I</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="20">disagree</governor>
          <dependent id="19">personally</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="5">respect</governor>
          <dependent id="20">disagree</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">it</governor>
          <dependent id="21">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">disagree</governor>
          <dependent id="22">it</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Rubin" type="PERSON" score="0.0">
          <tokens>
            <token id="12" string="Rubin" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="66" has_coreference="true">
      <content>&amp;quot;The system worked well for them,&amp;quot; she said in reference to the Buckeys.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="system" lemma="system" stem="system" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="4" string="worked" lemma="work" stem="work" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="well" lemma="well" stem="well" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="them" lemma="they" stem="them" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="reference" lemma="reference" stem="refer" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="Buckeys" lemma="Buckeys" stem="buckei" pos="NNPS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (NP (DT The) (NN system)) (VP (VBD worked) (ADVP (RB well)) (PP (IN for) (NP (PRP them))))) (, ,) ('' '') (NP (PRP she)) (VP (VBD said) (PP (IN in) (NP (NN reference))) (PP (TO to) (NP (DT the) (NNPS Buckeys)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="reference" type="NP">
          <tokens>
            <token id="13" string="reference" />
          </tokens>
        </chunking>
        <chunking id="2" string="the Buckeys" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="Buckeys" />
          </tokens>
        </chunking>
        <chunking id="3" string="worked well for them" type="VP">
          <tokens>
            <token id="4" string="worked" />
            <token id="5" string="well" />
            <token id="6" string="for" />
            <token id="7" string="them" />
          </tokens>
        </chunking>
        <chunking id="4" string="said in reference to the Buckeys" type="VP">
          <tokens>
            <token id="11" string="said" />
            <token id="12" string="in" />
            <token id="13" string="reference" />
            <token id="14" string="to" />
            <token id="15" string="the" />
            <token id="16" string="Buckeys" />
          </tokens>
        </chunking>
        <chunking id="5" string="The system" type="NP">
          <tokens>
            <token id="2" string="The" />
            <token id="3" string="system" />
          </tokens>
        </chunking>
        <chunking id="6" string="them" type="NP">
          <tokens>
            <token id="7" string="them" />
          </tokens>
        </chunking>
        <chunking id="7" string="she" type="NP">
          <tokens>
            <token id="10" string="she" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">system</governor>
          <dependent id="2">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">worked</governor>
          <dependent id="3">system</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="11">said</governor>
          <dependent id="4">worked</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">worked</governor>
          <dependent id="5">well</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">them</governor>
          <dependent id="6">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">worked</governor>
          <dependent id="7">them</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">said</governor>
          <dependent id="10">she</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="11">said</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">reference</governor>
          <dependent id="12">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">said</governor>
          <dependent id="13">reference</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">Buckeys</governor>
          <dependent id="14">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="16">Buckeys</governor>
          <dependent id="15">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">said</governor>
          <dependent id="16">Buckeys</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="67" has_coreference="true">
      <content>&amp;quot;They were lucky.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="They" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="lucky" lemma="lucky" stem="lucki" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (PRP They)) (VP (VBD were) (ADJP (JJ lucky))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="They" type="NP">
          <tokens>
            <token id="2" string="They" />
          </tokens>
        </chunking>
        <chunking id="2" string="were lucky" type="VP">
          <tokens>
            <token id="3" string="were" />
            <token id="4" string="lucky" />
          </tokens>
        </chunking>
        <chunking id="3" string="lucky" type="ADJP">
          <tokens>
            <token id="4" string="lucky" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">lucky</governor>
          <dependent id="2">They</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="4">lucky</governor>
          <dependent id="3">were</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">lucky</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="68" has_coreference="true">
      <content>I just hope to God that years from now we don&amp;apost;t hear about Ray Buckey molesting children.&amp;quot;</content>
      <tokens>
        <token id="1" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="just" lemma="just" stem="just" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="hope" lemma="hope" stem="hope" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="God" lemma="God" stem="god" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="8" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="9" string="now" lemma="now" stem="now" pos="RB" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="10" string="we" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="do" lemma="do" stem="do" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="n't" lemma="not" stem="n't" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="hear" lemma="hear" stem="hear" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="16" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="17" string="molesting" lemma="molest" stem="molest" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="19" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP I)) (ADVP (RB just)) (VP (VBP hope) (PP (TO to) (NP (NNP God))) (SBAR (IN that) (S (NP (NNS years)) (PP (IN from) (NP (RB now))) (NP (PRP we)) (VP (VBP do) (RB n't) (VP (VB hear) (PP (IN about) (NP (NP (NNP Ray) (NNP Buckey)) (VP (VBG molesting) (NP (NNS children)))))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="hope to God that years from now we do n't hear about Ray Buckey molesting children" type="VP">
          <tokens>
            <token id="3" string="hope" />
            <token id="4" string="to" />
            <token id="5" string="God" />
            <token id="6" string="that" />
            <token id="7" string="years" />
            <token id="8" string="from" />
            <token id="9" string="now" />
            <token id="10" string="we" />
            <token id="11" string="do" />
            <token id="12" string="n't" />
            <token id="13" string="hear" />
            <token id="14" string="about" />
            <token id="15" string="Ray" />
            <token id="16" string="Buckey" />
            <token id="17" string="molesting" />
            <token id="18" string="children" />
          </tokens>
        </chunking>
        <chunking id="2" string="do n't hear about Ray Buckey molesting children" type="VP">
          <tokens>
            <token id="11" string="do" />
            <token id="12" string="n't" />
            <token id="13" string="hear" />
            <token id="14" string="about" />
            <token id="15" string="Ray" />
            <token id="16" string="Buckey" />
            <token id="17" string="molesting" />
            <token id="18" string="children" />
          </tokens>
        </chunking>
        <chunking id="3" string="I" type="NP">
          <tokens>
            <token id="1" string="I" />
          </tokens>
        </chunking>
        <chunking id="4" string="years" type="NP">
          <tokens>
            <token id="7" string="years" />
          </tokens>
        </chunking>
        <chunking id="5" string="we" type="NP">
          <tokens>
            <token id="10" string="we" />
          </tokens>
        </chunking>
        <chunking id="6" string="Ray Buckey" type="NP">
          <tokens>
            <token id="15" string="Ray" />
            <token id="16" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="7" string="that years from now we do n't hear about Ray Buckey molesting children" type="SBAR">
          <tokens>
            <token id="6" string="that" />
            <token id="7" string="years" />
            <token id="8" string="from" />
            <token id="9" string="now" />
            <token id="10" string="we" />
            <token id="11" string="do" />
            <token id="12" string="n't" />
            <token id="13" string="hear" />
            <token id="14" string="about" />
            <token id="15" string="Ray" />
            <token id="16" string="Buckey" />
            <token id="17" string="molesting" />
            <token id="18" string="children" />
          </tokens>
        </chunking>
        <chunking id="8" string="now" type="NP">
          <tokens>
            <token id="9" string="now" />
          </tokens>
        </chunking>
        <chunking id="9" string="hear about Ray Buckey molesting children" type="VP">
          <tokens>
            <token id="13" string="hear" />
            <token id="14" string="about" />
            <token id="15" string="Ray" />
            <token id="16" string="Buckey" />
            <token id="17" string="molesting" />
            <token id="18" string="children" />
          </tokens>
        </chunking>
        <chunking id="10" string="children" type="NP">
          <tokens>
            <token id="18" string="children" />
          </tokens>
        </chunking>
        <chunking id="11" string="God" type="NP">
          <tokens>
            <token id="5" string="God" />
          </tokens>
        </chunking>
        <chunking id="12" string="molesting children" type="VP">
          <tokens>
            <token id="17" string="molesting" />
            <token id="18" string="children" />
          </tokens>
        </chunking>
        <chunking id="13" string="Ray Buckey molesting children" type="NP">
          <tokens>
            <token id="15" string="Ray" />
            <token id="16" string="Buckey" />
            <token id="17" string="molesting" />
            <token id="18" string="children" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">hope</governor>
          <dependent id="1">I</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="3">hope</governor>
          <dependent id="2">just</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">hope</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">God</governor>
          <dependent id="4">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">hope</governor>
          <dependent id="5">God</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="13">hear</governor>
          <dependent id="6">that</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="13">hear</governor>
          <dependent id="7">years</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">now</governor>
          <dependent id="8">from</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">hear</governor>
          <dependent id="9">now</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="13">hear</governor>
          <dependent id="10">we</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="13">hear</governor>
          <dependent id="11">do</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="13">hear</governor>
          <dependent id="12">n't</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">hope</governor>
          <dependent id="13">hear</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">Buckey</governor>
          <dependent id="14">about</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="16">Buckey</governor>
          <dependent id="15">Ray</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">hear</governor>
          <dependent id="16">Buckey</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="16">Buckey</governor>
          <dependent id="17">molesting</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="17">molesting</governor>
          <dependent id="18">children</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="years from now" type="DATE" score="0.0">
          <tokens>
            <token id="7" string="years" />
            <token id="8" string="from" />
            <token id="9" string="now" />
          </tokens>
        </entity>
        <entity id="2" string="Ray Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="15" string="Ray" />
            <token id="16" string="Buckey" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="69" has_coreference="false">
      <content>Los Angeles County Dist.</content>
      <tokens>
        <token id="1" string="Los" lemma="Los" stem="lo" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="2" string="Angeles" lemma="Angeles" stem="angele" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="3" string="County" lemma="County" stem="counti" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="4" string="Dist" lemma="Dist" stem="dist" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="5" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NNP Los) (NNP Angeles) (NNP County) (NNP Dist) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Los Angeles County Dist ." type="NP">
          <tokens>
            <token id="1" string="Los" />
            <token id="2" string="Angeles" />
            <token id="3" string="County" />
            <token id="4" string="Dist" />
            <token id="5" string="." />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="4">Dist</governor>
          <dependent id="1">Los</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">Dist</governor>
          <dependent id="2">Angeles</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">Dist</governor>
          <dependent id="3">County</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">Dist</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Los Angeles County Dist" type="LOCATION" score="0.0">
          <tokens>
            <token id="1" string="Los" />
            <token id="2" string="Angeles" />
            <token id="3" string="County" />
            <token id="4" string="Dist" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="70" has_coreference="true">
      <content>Atty. Ira Reiner blamed the criminal justice system for the length of the trial, and said he backs two state court-reform initiatives.</content>
      <tokens>
        <token id="1" string="Atty." lemma="Atty." stem="atty." pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="2" string="Ira" lemma="Ira" stem="ira" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="Reiner" lemma="Reiner" stem="reiner" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="4" string="blamed" lemma="blame" stem="blame" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="criminal" lemma="criminal" stem="crimin" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="justice" lemma="justice" stem="justic" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="system" lemma="system" stem="system" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="length" lemma="length" stem="length" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="trial" lemma="trial" stem="trial" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="15" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="backs" lemma="back" stem="back" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="two" lemma="two" stem="two" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="21" string="state" lemma="state" stem="state" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="court-reform" lemma="court-reform" stem="court-reform" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="initiatives" lemma="initiative" stem="initi" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Atty.) (NNP Ira) (NNP Reiner)) (VP (VP (VBD blamed) (NP (DT the) (JJ criminal) (NN justice) (NN system)) (PP (IN for) (NP (NP (DT the) (NN length)) (PP (IN of) (NP (DT the) (NN trial)))))) (, ,) (CC and) (VP (VBD said) (SBAR (S (NP (PRP he)) (VP (VBZ backs) (NP (CD two) (NN state) (NN court-reform) (NNS initiatives))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="blamed the criminal justice system for the length of the trial" type="VP">
          <tokens>
            <token id="4" string="blamed" />
            <token id="5" string="the" />
            <token id="6" string="criminal" />
            <token id="7" string="justice" />
            <token id="8" string="system" />
            <token id="9" string="for" />
            <token id="10" string="the" />
            <token id="11" string="length" />
            <token id="12" string="of" />
            <token id="13" string="the" />
            <token id="14" string="trial" />
          </tokens>
        </chunking>
        <chunking id="2" string="Atty. Ira Reiner" type="NP">
          <tokens>
            <token id="1" string="Atty." />
            <token id="2" string="Ira" />
            <token id="3" string="Reiner" />
          </tokens>
        </chunking>
        <chunking id="3" string="backs two state court-reform initiatives" type="VP">
          <tokens>
            <token id="19" string="backs" />
            <token id="20" string="two" />
            <token id="21" string="state" />
            <token id="22" string="court-reform" />
            <token id="23" string="initiatives" />
          </tokens>
        </chunking>
        <chunking id="4" string="the trial" type="NP">
          <tokens>
            <token id="13" string="the" />
            <token id="14" string="trial" />
          </tokens>
        </chunking>
        <chunking id="5" string="the length" type="NP">
          <tokens>
            <token id="10" string="the" />
            <token id="11" string="length" />
          </tokens>
        </chunking>
        <chunking id="6" string="he backs two state court-reform initiatives" type="SBAR">
          <tokens>
            <token id="18" string="he" />
            <token id="19" string="backs" />
            <token id="20" string="two" />
            <token id="21" string="state" />
            <token id="22" string="court-reform" />
            <token id="23" string="initiatives" />
          </tokens>
        </chunking>
        <chunking id="7" string="the length of the trial" type="NP">
          <tokens>
            <token id="10" string="the" />
            <token id="11" string="length" />
            <token id="12" string="of" />
            <token id="13" string="the" />
            <token id="14" string="trial" />
          </tokens>
        </chunking>
        <chunking id="8" string="blamed the criminal justice system for the length of the trial , and said he backs two state court-reform initiatives" type="VP">
          <tokens>
            <token id="4" string="blamed" />
            <token id="5" string="the" />
            <token id="6" string="criminal" />
            <token id="7" string="justice" />
            <token id="8" string="system" />
            <token id="9" string="for" />
            <token id="10" string="the" />
            <token id="11" string="length" />
            <token id="12" string="of" />
            <token id="13" string="the" />
            <token id="14" string="trial" />
            <token id="15" string="," />
            <token id="16" string="and" />
            <token id="17" string="said" />
            <token id="18" string="he" />
            <token id="19" string="backs" />
            <token id="20" string="two" />
            <token id="21" string="state" />
            <token id="22" string="court-reform" />
            <token id="23" string="initiatives" />
          </tokens>
        </chunking>
        <chunking id="9" string="two state court-reform initiatives" type="NP">
          <tokens>
            <token id="20" string="two" />
            <token id="21" string="state" />
            <token id="22" string="court-reform" />
            <token id="23" string="initiatives" />
          </tokens>
        </chunking>
        <chunking id="10" string="the criminal justice system" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="criminal" />
            <token id="7" string="justice" />
            <token id="8" string="system" />
          </tokens>
        </chunking>
        <chunking id="11" string="said he backs two state court-reform initiatives" type="VP">
          <tokens>
            <token id="17" string="said" />
            <token id="18" string="he" />
            <token id="19" string="backs" />
            <token id="20" string="two" />
            <token id="21" string="state" />
            <token id="22" string="court-reform" />
            <token id="23" string="initiatives" />
          </tokens>
        </chunking>
        <chunking id="12" string="he" type="NP">
          <tokens>
            <token id="18" string="he" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="3">Reiner</governor>
          <dependent id="1">Atty.</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">Reiner</governor>
          <dependent id="2">Ira</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">blamed</governor>
          <dependent id="3">Reiner</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">blamed</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">system</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">system</governor>
          <dependent id="6">criminal</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">system</governor>
          <dependent id="7">justice</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">blamed</governor>
          <dependent id="8">system</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">length</governor>
          <dependent id="9">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">length</governor>
          <dependent id="10">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">blamed</governor>
          <dependent id="11">length</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">trial</governor>
          <dependent id="12">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">trial</governor>
          <dependent id="13">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">length</governor>
          <dependent id="14">trial</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">blamed</governor>
          <dependent id="16">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">blamed</governor>
          <dependent id="17">said</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">backs</governor>
          <dependent id="18">he</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="17">said</governor>
          <dependent id="19">backs</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="23">initiatives</governor>
          <dependent id="20">two</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="23">initiatives</governor>
          <dependent id="21">state</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="23">initiatives</governor>
          <dependent id="22">court-reform</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="19">backs</governor>
          <dependent id="23">initiatives</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="two" type="NUMBER" score="0.0">
          <tokens>
            <token id="20" string="two" />
          </tokens>
        </entity>
        <entity id="2" string="Ira Reiner" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Ira" />
            <token id="3" string="Reiner" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="71" has_coreference="true">
      <content>&amp;quot;Too much time and too much money has gone into this case.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="Too" lemma="too" stem="too" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="much" lemma="much" stem="much" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="time" lemma="time" stem="time" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="too" lemma="too" stem="too" pos="RB" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="much" lemma="much" stem="much" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="money" lemma="money" stem="monei" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="gone" lemma="go" stem="gone" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="into" lemma="into" stem="into" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (NP (ADJP (RB Too) (JJ much)) (NN time)) (CC and) (NP (ADJP (RB too) (JJ much)) (NN money))) (VP (VBZ has) (VP (VBN gone) (PP (IN into) (NP (DT this) (NN case))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Too much time" type="NP">
          <tokens>
            <token id="2" string="Too" />
            <token id="3" string="much" />
            <token id="4" string="time" />
          </tokens>
        </chunking>
        <chunking id="2" string="has gone into this case" type="VP">
          <tokens>
            <token id="9" string="has" />
            <token id="10" string="gone" />
            <token id="11" string="into" />
            <token id="12" string="this" />
            <token id="13" string="case" />
          </tokens>
        </chunking>
        <chunking id="3" string="too much" type="ADJP">
          <tokens>
            <token id="6" string="too" />
            <token id="7" string="much" />
          </tokens>
        </chunking>
        <chunking id="4" string="gone into this case" type="VP">
          <tokens>
            <token id="10" string="gone" />
            <token id="11" string="into" />
            <token id="12" string="this" />
            <token id="13" string="case" />
          </tokens>
        </chunking>
        <chunking id="5" string="this case" type="NP">
          <tokens>
            <token id="12" string="this" />
            <token id="13" string="case" />
          </tokens>
        </chunking>
        <chunking id="6" string="too much money" type="NP">
          <tokens>
            <token id="6" string="too" />
            <token id="7" string="much" />
            <token id="8" string="money" />
          </tokens>
        </chunking>
        <chunking id="7" string="Too much time and too much money" type="NP">
          <tokens>
            <token id="2" string="Too" />
            <token id="3" string="much" />
            <token id="4" string="time" />
            <token id="5" string="and" />
            <token id="6" string="too" />
            <token id="7" string="much" />
            <token id="8" string="money" />
          </tokens>
        </chunking>
        <chunking id="8" string="Too much" type="ADJP">
          <tokens>
            <token id="2" string="Too" />
            <token id="3" string="much" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="3">much</governor>
          <dependent id="2">Too</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="4">time</governor>
          <dependent id="3">much</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">gone</governor>
          <dependent id="4">time</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">time</governor>
          <dependent id="5">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="7">much</governor>
          <dependent id="6">too</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">money</governor>
          <dependent id="7">much</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">time</governor>
          <dependent id="8">money</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="10">gone</governor>
          <dependent id="9">has</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="10">gone</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">case</governor>
          <dependent id="11">into</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">case</governor>
          <dependent id="12">this</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">gone</governor>
          <dependent id="13">case</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="72" has_coreference="false">
      <content>This is insane.</content>
      <tokens>
        <token id="1" string="This" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="insane" lemma="insane" stem="insan" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT This)) (VP (VBZ is) (ADJP (JJ insane))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="insane" type="ADJP">
          <tokens>
            <token id="3" string="insane" />
          </tokens>
        </chunking>
        <chunking id="2" string="This" type="NP">
          <tokens>
            <token id="1" string="This" />
          </tokens>
        </chunking>
        <chunking id="3" string="is insane" type="VP">
          <tokens>
            <token id="2" string="is" />
            <token id="3" string="insane" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">insane</governor>
          <dependent id="1">This</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="3">insane</governor>
          <dependent id="2">is</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">insane</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="73" has_coreference="true">
      <content>The very idea a case can go to trial for 2 1/2 years and there can be a rational result is preposterous.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="very" lemma="very" stem="veri" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="idea" lemma="idea" stem="idea" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="can" lemma="can" stem="can" pos="MD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="go" lemma="go" stem="go" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="trial" lemma="trial" stem="trial" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="2 1/2" lemma="2 1/2" stem="2 1/2" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="true" />
        <token id="12" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="13" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="there" lemma="there" stem="there" pos="EX" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="can" lemma="can" stem="can" pos="MD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="be" lemma="be" stem="be" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="rational" lemma="rational" stem="ration" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="result" lemma="result" stem="result" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="preposterous" lemma="preposterous" stem="preposter" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT The) (JJ very) (NN idea)) (SBAR (S (S (NP (DT a) (NN case)) (VP (MD can) (VP (VB go) (PP (TO to) (NP (NP (NN trial)) (PP (IN for) (NP (CD 2 1/2) (NNS years)))))))) (CC and) (S (NP (EX there)) (VP (MD can) (VP (VB be) (NP (DT a) (JJ rational) (NN result)))))))) (VP (VBZ is) (ADJP (JJ preposterous))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a rational result" type="NP">
          <tokens>
            <token id="17" string="a" />
            <token id="18" string="rational" />
            <token id="19" string="result" />
          </tokens>
        </chunking>
        <chunking id="2" string="be a rational result" type="VP">
          <tokens>
            <token id="16" string="be" />
            <token id="17" string="a" />
            <token id="18" string="rational" />
            <token id="19" string="result" />
          </tokens>
        </chunking>
        <chunking id="3" string="The very idea a case can go to trial for 2 1/2 years and there can be a rational result" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="very" />
            <token id="3" string="idea" />
            <token id="4" string="a" />
            <token id="5" string="case" />
            <token id="6" string="can" />
            <token id="7" string="go" />
            <token id="8" string="to" />
            <token id="9" string="trial" />
            <token id="10" string="for" />
            <token id="11" string="2 1/2" />
            <token id="12" string="years" />
            <token id="13" string="and" />
            <token id="14" string="there" />
            <token id="15" string="can" />
            <token id="16" string="be" />
            <token id="17" string="a" />
            <token id="18" string="rational" />
            <token id="19" string="result" />
          </tokens>
        </chunking>
        <chunking id="4" string="can go to trial for 2 1/2 years" type="VP">
          <tokens>
            <token id="6" string="can" />
            <token id="7" string="go" />
            <token id="8" string="to" />
            <token id="9" string="trial" />
            <token id="10" string="for" />
            <token id="11" string="2 1/2" />
            <token id="12" string="years" />
          </tokens>
        </chunking>
        <chunking id="5" string="trial" type="NP">
          <tokens>
            <token id="9" string="trial" />
          </tokens>
        </chunking>
        <chunking id="6" string="there" type="NP">
          <tokens>
            <token id="14" string="there" />
          </tokens>
        </chunking>
        <chunking id="7" string="can be a rational result" type="VP">
          <tokens>
            <token id="15" string="can" />
            <token id="16" string="be" />
            <token id="17" string="a" />
            <token id="18" string="rational" />
            <token id="19" string="result" />
          </tokens>
        </chunking>
        <chunking id="8" string="is preposterous" type="VP">
          <tokens>
            <token id="20" string="is" />
            <token id="21" string="preposterous" />
          </tokens>
        </chunking>
        <chunking id="9" string="2 1/2 years" type="NP">
          <tokens>
            <token id="11" string="2 1/2" />
            <token id="12" string="years" />
          </tokens>
        </chunking>
        <chunking id="10" string="The very idea" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="very" />
            <token id="3" string="idea" />
          </tokens>
        </chunking>
        <chunking id="11" string="a case can go to trial for 2 1/2 years and there can be a rational result" type="SBAR">
          <tokens>
            <token id="4" string="a" />
            <token id="5" string="case" />
            <token id="6" string="can" />
            <token id="7" string="go" />
            <token id="8" string="to" />
            <token id="9" string="trial" />
            <token id="10" string="for" />
            <token id="11" string="2 1/2" />
            <token id="12" string="years" />
            <token id="13" string="and" />
            <token id="14" string="there" />
            <token id="15" string="can" />
            <token id="16" string="be" />
            <token id="17" string="a" />
            <token id="18" string="rational" />
            <token id="19" string="result" />
          </tokens>
        </chunking>
        <chunking id="12" string="a case" type="NP">
          <tokens>
            <token id="4" string="a" />
            <token id="5" string="case" />
          </tokens>
        </chunking>
        <chunking id="13" string="trial for 2 1/2 years" type="NP">
          <tokens>
            <token id="9" string="trial" />
            <token id="10" string="for" />
            <token id="11" string="2 1/2" />
            <token id="12" string="years" />
          </tokens>
        </chunking>
        <chunking id="14" string="preposterous" type="ADJP">
          <tokens>
            <token id="21" string="preposterous" />
          </tokens>
        </chunking>
        <chunking id="15" string="go to trial for 2 1/2 years" type="VP">
          <tokens>
            <token id="7" string="go" />
            <token id="8" string="to" />
            <token id="9" string="trial" />
            <token id="10" string="for" />
            <token id="11" string="2 1/2" />
            <token id="12" string="years" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">idea</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">idea</governor>
          <dependent id="2">very</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="20">is</governor>
          <dependent id="3">idea</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">case</governor>
          <dependent id="4">a</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">go</governor>
          <dependent id="5">case</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="7">go</governor>
          <dependent id="6">can</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="3">idea</governor>
          <dependent id="7">go</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">trial</governor>
          <dependent id="8">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">go</governor>
          <dependent id="9">trial</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">years</governor>
          <dependent id="10">for</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="12">years</governor>
          <dependent id="11">2 1/2</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">trial</governor>
          <dependent id="12">years</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">go</governor>
          <dependent id="13">and</dependent>
        </dependency>
        <dependency type="expl">
          <governor id="19">result</governor>
          <dependent id="14">there</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="19">result</governor>
          <dependent id="15">can</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="19">result</governor>
          <dependent id="16">be</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">result</governor>
          <dependent id="17">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">result</governor>
          <dependent id="18">rational</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">go</governor>
          <dependent id="19">result</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="20">is</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="20">is</governor>
          <dependent id="21">preposterous</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="2 1/2" type="NUMBER" score="0.0">
          <tokens>
            <token id="11" string="2 1/2" />
          </tokens>
        </entity>
        <entity id="2" string="years" type="DURATION" score="0.0">
          <tokens>
            <token id="12" string="years" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="74" has_coreference="true">
      <content>The McMartin case is the very worst example that we have, period.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="6" string="very" lemma="very" stem="veri" pos="RB" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="worst" lemma="worst" stem="worst" pos="JJS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="example" lemma="example" stem="exampl" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="we" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="have" lemma="have" stem="have" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="period" lemma="period" stem="period" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (DT The) (NNP McMartin) (NN case)) (VP (VBZ is) (NP (DT the) (ADJP (RB very) (JJS worst)) (NN example)) (SBAR (IN that) (S (NP (PRP we)) (VP (VBP have)))))) (, ,) (FRAG (NP (NN period))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the very worst example" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="very" />
            <token id="7" string="worst" />
            <token id="8" string="example" />
          </tokens>
        </chunking>
        <chunking id="2" string="is the very worst example that we have" type="VP">
          <tokens>
            <token id="4" string="is" />
            <token id="5" string="the" />
            <token id="6" string="very" />
            <token id="7" string="worst" />
            <token id="8" string="example" />
            <token id="9" string="that" />
            <token id="10" string="we" />
            <token id="11" string="have" />
          </tokens>
        </chunking>
        <chunking id="3" string="period" type="NP">
          <tokens>
            <token id="13" string="period" />
          </tokens>
        </chunking>
        <chunking id="4" string="The McMartin case" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="McMartin" />
            <token id="3" string="case" />
          </tokens>
        </chunking>
        <chunking id="5" string="that we have" type="SBAR">
          <tokens>
            <token id="9" string="that" />
            <token id="10" string="we" />
            <token id="11" string="have" />
          </tokens>
        </chunking>
        <chunking id="6" string="have" type="VP">
          <tokens>
            <token id="11" string="have" />
          </tokens>
        </chunking>
        <chunking id="7" string="we" type="NP">
          <tokens>
            <token id="10" string="we" />
          </tokens>
        </chunking>
        <chunking id="8" string="very worst" type="ADJP">
          <tokens>
            <token id="6" string="very" />
            <token id="7" string="worst" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">case</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">case</governor>
          <dependent id="2">McMartin</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">example</governor>
          <dependent id="3">case</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="8">example</governor>
          <dependent id="4">is</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">example</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="7">worst</governor>
          <dependent id="6">very</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">example</governor>
          <dependent id="7">worst</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="8">example</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="11">have</governor>
          <dependent id="9">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">have</governor>
          <dependent id="10">we</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="8">example</governor>
          <dependent id="11">have</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="8">example</governor>
          <dependent id="13">period</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="McMartin" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="McMartin" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="75" has_coreference="true">
      <content>But it does not stand alone.&amp;quot;</content>
      <tokens>
        <token id="1" string="But" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="does" lemma="do" stem="doe" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="stand" lemma="stand" stem="stand" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="alone" lemma="alone" stem="alon" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (CC But) (NP (PRP it)) (VP (VBZ does) (RB not) (VP (VB stand) (ADVP (RB alone)))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="does not stand alone" type="VP">
          <tokens>
            <token id="3" string="does" />
            <token id="4" string="not" />
            <token id="5" string="stand" />
            <token id="6" string="alone" />
          </tokens>
        </chunking>
        <chunking id="2" string="stand alone" type="VP">
          <tokens>
            <token id="5" string="stand" />
            <token id="6" string="alone" />
          </tokens>
        </chunking>
        <chunking id="3" string="it" type="NP">
          <tokens>
            <token id="2" string="it" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="cc">
          <governor id="5">stand</governor>
          <dependent id="1">But</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">stand</governor>
          <dependent id="2">it</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="5">stand</governor>
          <dependent id="3">does</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="5">stand</governor>
          <dependent id="4">not</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">stand</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">stand</governor>
          <dependent id="6">alone</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="76" has_coreference="true">
      <content>Defense attorneys Dean Gits and Danny Davis were beaming after the verdict announcement.</content>
      <tokens>
        <token id="1" string="Defense" lemma="Defense" stem="defens" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="attorneys" lemma="attorney" stem="attornei" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="Dean" lemma="Dean" stem="dean" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="4" string="Gits" lemma="Gits" stem="git" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="5" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Danny" lemma="Danny" stem="danni" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="7" string="Davis" lemma="Davis" stem="davi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="8" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="beaming" lemma="beam" stem="beam" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="after" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="verdict" lemma="verdict" stem="verdict" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="announcement" lemma="announcement" stem="announc" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Defense) (NNS attorneys)) (NP (NNP Dean) (NNP Gits) (CC and) (NNP Danny) (NNP Davis)) (VP (VBD were) (NP (VBG beaming)) (PP (IN after) (NP (DT the) (NN verdict) (NN announcement)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="beaming" type="NP">
          <tokens>
            <token id="9" string="beaming" />
          </tokens>
        </chunking>
        <chunking id="2" string="the verdict announcement" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="verdict" />
            <token id="13" string="announcement" />
          </tokens>
        </chunking>
        <chunking id="3" string="Dean Gits and Danny Davis" type="NP">
          <tokens>
            <token id="3" string="Dean" />
            <token id="4" string="Gits" />
            <token id="5" string="and" />
            <token id="6" string="Danny" />
            <token id="7" string="Davis" />
          </tokens>
        </chunking>
        <chunking id="4" string="were beaming after the verdict announcement" type="VP">
          <tokens>
            <token id="8" string="were" />
            <token id="9" string="beaming" />
            <token id="10" string="after" />
            <token id="11" string="the" />
            <token id="12" string="verdict" />
            <token id="13" string="announcement" />
          </tokens>
        </chunking>
        <chunking id="5" string="Defense attorneys" type="NP">
          <tokens>
            <token id="1" string="Defense" />
            <token id="2" string="attorneys" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">attorneys</governor>
          <dependent id="1">Defense</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">beaming</governor>
          <dependent id="2">attorneys</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">Gits</governor>
          <dependent id="3">Dean</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">beaming</governor>
          <dependent id="4">Gits</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">Gits</governor>
          <dependent id="5">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">Davis</governor>
          <dependent id="6">Danny</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">Gits</governor>
          <dependent id="7">Davis</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="9">beaming</governor>
          <dependent id="8">were</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="9">beaming</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">announcement</governor>
          <dependent id="10">after</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">announcement</governor>
          <dependent id="11">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="13">announcement</governor>
          <dependent id="12">verdict</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">beaming</governor>
          <dependent id="13">announcement</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Dean Gits" type="PERSON" score="0.0">
          <tokens>
            <token id="3" string="Dean" />
            <token id="4" string="Gits" />
          </tokens>
        </entity>
        <entity id="2" string="Danny Davis" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Danny" />
            <token id="7" string="Davis" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="77" has_coreference="true">
      <content>&amp;quot;I feel great,&amp;quot; Gits said, indicating he had found hope that the decision would go the defense way last week when the jury informed the judge it was reaching deadlock on a conspiracy count.</content>
      <tokens>
        <token id="1" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="feel" lemma="feel" stem="feel" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="great" lemma="great" stem="great" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="Gits" lemma="Gits" stem="git" pos="NNPS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="indicating" lemma="indicate" stem="indic" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="found" lemma="find" stem="found" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="hope" lemma="hope" stem="hope" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="decision" lemma="decision" stem="decis" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="18" string="would" lemma="would" stem="would" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="go" lemma="go" stem="go" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="defense" lemma="defense" stem="defens" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="way" lemma="way" stem="wai" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="last" lemma="last" stem="last" pos="JJ" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="24" string="week" lemma="week" stem="week" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="25" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="informed" lemma="inform" stem="inform" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="judge" lemma="judge" stem="judg" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="32" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="reaching" lemma="reach" stem="reach" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="deadlock" lemma="deadlock" stem="deadlock" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="conspiracy" lemma="conspiracy" stem="conspiraci" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="38" string="count" lemma="count" stem="count" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (NP (PRP I)) (VP (VBP feel) (ADJP (JJ great)))) (, ,) ('' '') (NP (NNPS Gits)) (VP (VBD said) (, ,) (S (VP (VBG indicating) (SBAR (S (NP (PRP he)) (VP (VBD had) (VP (VBN found) (NP (NN hope)) (SBAR (IN that) (S (NP (DT the) (NN decision)) (VP (MD would) (VP (VB go) (NP (DT the) (NN defense) (NN way)) (NP-TMP (JJ last) (NN week)) (SBAR (WHADVP (WRB when)) (S (NP (DT the) (NN jury)) (VP (VBD informed) (NP (NP (DT the) (NN judge)) (SBAR (S (NP (PRP it)) (VP (VBD was) (VP (VBG reaching) (NP (NN deadlock)) (PP (IN on) (NP (DT a) (NN conspiracy) (NN count)))))))))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Gits" type="NP">
          <tokens>
            <token id="7" string="Gits" />
          </tokens>
        </chunking>
        <chunking id="2" string="feel great" type="VP">
          <tokens>
            <token id="3" string="feel" />
            <token id="4" string="great" />
          </tokens>
        </chunking>
        <chunking id="3" string="was reaching deadlock on a conspiracy count" type="VP">
          <tokens>
            <token id="32" string="was" />
            <token id="33" string="reaching" />
            <token id="34" string="deadlock" />
            <token id="35" string="on" />
            <token id="36" string="a" />
            <token id="37" string="conspiracy" />
            <token id="38" string="count" />
          </tokens>
        </chunking>
        <chunking id="4" string="it" type="NP">
          <tokens>
            <token id="31" string="it" />
          </tokens>
        </chunking>
        <chunking id="5" string="great" type="ADJP">
          <tokens>
            <token id="4" string="great" />
          </tokens>
        </chunking>
        <chunking id="6" string="informed the judge it was reaching deadlock on a conspiracy count" type="VP">
          <tokens>
            <token id="28" string="informed" />
            <token id="29" string="the" />
            <token id="30" string="judge" />
            <token id="31" string="it" />
            <token id="32" string="was" />
            <token id="33" string="reaching" />
            <token id="34" string="deadlock" />
            <token id="35" string="on" />
            <token id="36" string="a" />
            <token id="37" string="conspiracy" />
            <token id="38" string="count" />
          </tokens>
        </chunking>
        <chunking id="7" string="go the defense way last week when the jury informed the judge it was reaching deadlock on a conspiracy count" type="VP">
          <tokens>
            <token id="19" string="go" />
            <token id="20" string="the" />
            <token id="21" string="defense" />
            <token id="22" string="way" />
            <token id="23" string="last" />
            <token id="24" string="week" />
            <token id="25" string="when" />
            <token id="26" string="the" />
            <token id="27" string="jury" />
            <token id="28" string="informed" />
            <token id="29" string="the" />
            <token id="30" string="judge" />
            <token id="31" string="it" />
            <token id="32" string="was" />
            <token id="33" string="reaching" />
            <token id="34" string="deadlock" />
            <token id="35" string="on" />
            <token id="36" string="a" />
            <token id="37" string="conspiracy" />
            <token id="38" string="count" />
          </tokens>
        </chunking>
        <chunking id="8" string="said , indicating he had found hope that the decision would go the defense way last week when the jury informed the judge it was reaching deadlock on a conspiracy count" type="VP">
          <tokens>
            <token id="8" string="said" />
            <token id="9" string="," />
            <token id="10" string="indicating" />
            <token id="11" string="he" />
            <token id="12" string="had" />
            <token id="13" string="found" />
            <token id="14" string="hope" />
            <token id="15" string="that" />
            <token id="16" string="the" />
            <token id="17" string="decision" />
            <token id="18" string="would" />
            <token id="19" string="go" />
            <token id="20" string="the" />
            <token id="21" string="defense" />
            <token id="22" string="way" />
            <token id="23" string="last" />
            <token id="24" string="week" />
            <token id="25" string="when" />
            <token id="26" string="the" />
            <token id="27" string="jury" />
            <token id="28" string="informed" />
            <token id="29" string="the" />
            <token id="30" string="judge" />
            <token id="31" string="it" />
            <token id="32" string="was" />
            <token id="33" string="reaching" />
            <token id="34" string="deadlock" />
            <token id="35" string="on" />
            <token id="36" string="a" />
            <token id="37" string="conspiracy" />
            <token id="38" string="count" />
          </tokens>
        </chunking>
        <chunking id="9" string="he had found hope that the decision would go the defense way last week when the jury informed the judge it was reaching deadlock on a conspiracy count" type="SBAR">
          <tokens>
            <token id="11" string="he" />
            <token id="12" string="had" />
            <token id="13" string="found" />
            <token id="14" string="hope" />
            <token id="15" string="that" />
            <token id="16" string="the" />
            <token id="17" string="decision" />
            <token id="18" string="would" />
            <token id="19" string="go" />
            <token id="20" string="the" />
            <token id="21" string="defense" />
            <token id="22" string="way" />
            <token id="23" string="last" />
            <token id="24" string="week" />
            <token id="25" string="when" />
            <token id="26" string="the" />
            <token id="27" string="jury" />
            <token id="28" string="informed" />
            <token id="29" string="the" />
            <token id="30" string="judge" />
            <token id="31" string="it" />
            <token id="32" string="was" />
            <token id="33" string="reaching" />
            <token id="34" string="deadlock" />
            <token id="35" string="on" />
            <token id="36" string="a" />
            <token id="37" string="conspiracy" />
            <token id="38" string="count" />
          </tokens>
        </chunking>
        <chunking id="10" string="found hope that the decision would go the defense way last week when the jury informed the judge it was reaching deadlock on a conspiracy count" type="VP">
          <tokens>
            <token id="13" string="found" />
            <token id="14" string="hope" />
            <token id="15" string="that" />
            <token id="16" string="the" />
            <token id="17" string="decision" />
            <token id="18" string="would" />
            <token id="19" string="go" />
            <token id="20" string="the" />
            <token id="21" string="defense" />
            <token id="22" string="way" />
            <token id="23" string="last" />
            <token id="24" string="week" />
            <token id="25" string="when" />
            <token id="26" string="the" />
            <token id="27" string="jury" />
            <token id="28" string="informed" />
            <token id="29" string="the" />
            <token id="30" string="judge" />
            <token id="31" string="it" />
            <token id="32" string="was" />
            <token id="33" string="reaching" />
            <token id="34" string="deadlock" />
            <token id="35" string="on" />
            <token id="36" string="a" />
            <token id="37" string="conspiracy" />
            <token id="38" string="count" />
          </tokens>
        </chunking>
        <chunking id="11" string="the judge it was reaching deadlock on a conspiracy count" type="NP">
          <tokens>
            <token id="29" string="the" />
            <token id="30" string="judge" />
            <token id="31" string="it" />
            <token id="32" string="was" />
            <token id="33" string="reaching" />
            <token id="34" string="deadlock" />
            <token id="35" string="on" />
            <token id="36" string="a" />
            <token id="37" string="conspiracy" />
            <token id="38" string="count" />
          </tokens>
        </chunking>
        <chunking id="12" string="would go the defense way last week when the jury informed the judge it was reaching deadlock on a conspiracy count" type="VP">
          <tokens>
            <token id="18" string="would" />
            <token id="19" string="go" />
            <token id="20" string="the" />
            <token id="21" string="defense" />
            <token id="22" string="way" />
            <token id="23" string="last" />
            <token id="24" string="week" />
            <token id="25" string="when" />
            <token id="26" string="the" />
            <token id="27" string="jury" />
            <token id="28" string="informed" />
            <token id="29" string="the" />
            <token id="30" string="judge" />
            <token id="31" string="it" />
            <token id="32" string="was" />
            <token id="33" string="reaching" />
            <token id="34" string="deadlock" />
            <token id="35" string="on" />
            <token id="36" string="a" />
            <token id="37" string="conspiracy" />
            <token id="38" string="count" />
          </tokens>
        </chunking>
        <chunking id="13" string="that the decision would go the defense way last week when the jury informed the judge it was reaching deadlock on a conspiracy count" type="SBAR">
          <tokens>
            <token id="15" string="that" />
            <token id="16" string="the" />
            <token id="17" string="decision" />
            <token id="18" string="would" />
            <token id="19" string="go" />
            <token id="20" string="the" />
            <token id="21" string="defense" />
            <token id="22" string="way" />
            <token id="23" string="last" />
            <token id="24" string="week" />
            <token id="25" string="when" />
            <token id="26" string="the" />
            <token id="27" string="jury" />
            <token id="28" string="informed" />
            <token id="29" string="the" />
            <token id="30" string="judge" />
            <token id="31" string="it" />
            <token id="32" string="was" />
            <token id="33" string="reaching" />
            <token id="34" string="deadlock" />
            <token id="35" string="on" />
            <token id="36" string="a" />
            <token id="37" string="conspiracy" />
            <token id="38" string="count" />
          </tokens>
        </chunking>
        <chunking id="14" string="he" type="NP">
          <tokens>
            <token id="11" string="he" />
          </tokens>
        </chunking>
        <chunking id="15" string="the defense way" type="NP">
          <tokens>
            <token id="20" string="the" />
            <token id="21" string="defense" />
            <token id="22" string="way" />
          </tokens>
        </chunking>
        <chunking id="16" string="the jury" type="NP">
          <tokens>
            <token id="26" string="the" />
            <token id="27" string="jury" />
          </tokens>
        </chunking>
        <chunking id="17" string="indicating he had found hope that the decision would go the defense way last week when the jury informed the judge it was reaching deadlock on a conspiracy count" type="VP">
          <tokens>
            <token id="10" string="indicating" />
            <token id="11" string="he" />
            <token id="12" string="had" />
            <token id="13" string="found" />
            <token id="14" string="hope" />
            <token id="15" string="that" />
            <token id="16" string="the" />
            <token id="17" string="decision" />
            <token id="18" string="would" />
            <token id="19" string="go" />
            <token id="20" string="the" />
            <token id="21" string="defense" />
            <token id="22" string="way" />
            <token id="23" string="last" />
            <token id="24" string="week" />
            <token id="25" string="when" />
            <token id="26" string="the" />
            <token id="27" string="jury" />
            <token id="28" string="informed" />
            <token id="29" string="the" />
            <token id="30" string="judge" />
            <token id="31" string="it" />
            <token id="32" string="was" />
            <token id="33" string="reaching" />
            <token id="34" string="deadlock" />
            <token id="35" string="on" />
            <token id="36" string="a" />
            <token id="37" string="conspiracy" />
            <token id="38" string="count" />
          </tokens>
        </chunking>
        <chunking id="18" string="it was reaching deadlock on a conspiracy count" type="SBAR">
          <tokens>
            <token id="31" string="it" />
            <token id="32" string="was" />
            <token id="33" string="reaching" />
            <token id="34" string="deadlock" />
            <token id="35" string="on" />
            <token id="36" string="a" />
            <token id="37" string="conspiracy" />
            <token id="38" string="count" />
          </tokens>
        </chunking>
        <chunking id="19" string="when the jury informed the judge it was reaching deadlock on a conspiracy count" type="SBAR">
          <tokens>
            <token id="25" string="when" />
            <token id="26" string="the" />
            <token id="27" string="jury" />
            <token id="28" string="informed" />
            <token id="29" string="the" />
            <token id="30" string="judge" />
            <token id="31" string="it" />
            <token id="32" string="was" />
            <token id="33" string="reaching" />
            <token id="34" string="deadlock" />
            <token id="35" string="on" />
            <token id="36" string="a" />
            <token id="37" string="conspiracy" />
            <token id="38" string="count" />
          </tokens>
        </chunking>
        <chunking id="20" string="I" type="NP">
          <tokens>
            <token id="2" string="I" />
          </tokens>
        </chunking>
        <chunking id="21" string="had found hope that the decision would go the defense way last week when the jury informed the judge it was reaching deadlock on a conspiracy count" type="VP">
          <tokens>
            <token id="12" string="had" />
            <token id="13" string="found" />
            <token id="14" string="hope" />
            <token id="15" string="that" />
            <token id="16" string="the" />
            <token id="17" string="decision" />
            <token id="18" string="would" />
            <token id="19" string="go" />
            <token id="20" string="the" />
            <token id="21" string="defense" />
            <token id="22" string="way" />
            <token id="23" string="last" />
            <token id="24" string="week" />
            <token id="25" string="when" />
            <token id="26" string="the" />
            <token id="27" string="jury" />
            <token id="28" string="informed" />
            <token id="29" string="the" />
            <token id="30" string="judge" />
            <token id="31" string="it" />
            <token id="32" string="was" />
            <token id="33" string="reaching" />
            <token id="34" string="deadlock" />
            <token id="35" string="on" />
            <token id="36" string="a" />
            <token id="37" string="conspiracy" />
            <token id="38" string="count" />
          </tokens>
        </chunking>
        <chunking id="22" string="deadlock" type="NP">
          <tokens>
            <token id="34" string="deadlock" />
          </tokens>
        </chunking>
        <chunking id="23" string="hope" type="NP">
          <tokens>
            <token id="14" string="hope" />
          </tokens>
        </chunking>
        <chunking id="24" string="the decision" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="decision" />
          </tokens>
        </chunking>
        <chunking id="25" string="when" type="WHADVP">
          <tokens>
            <token id="25" string="when" />
          </tokens>
        </chunking>
        <chunking id="26" string="reaching deadlock on a conspiracy count" type="VP">
          <tokens>
            <token id="33" string="reaching" />
            <token id="34" string="deadlock" />
            <token id="35" string="on" />
            <token id="36" string="a" />
            <token id="37" string="conspiracy" />
            <token id="38" string="count" />
          </tokens>
        </chunking>
        <chunking id="27" string="a conspiracy count" type="NP">
          <tokens>
            <token id="36" string="a" />
            <token id="37" string="conspiracy" />
            <token id="38" string="count" />
          </tokens>
        </chunking>
        <chunking id="28" string="the judge" type="NP">
          <tokens>
            <token id="29" string="the" />
            <token id="30" string="judge" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">feel</governor>
          <dependent id="2">I</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="8">said</governor>
          <dependent id="3">feel</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="3">feel</governor>
          <dependent id="4">great</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">said</governor>
          <dependent id="7">Gits</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="8">said</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="8">said</governor>
          <dependent id="10">indicating</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="13">found</governor>
          <dependent id="11">he</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="13">found</governor>
          <dependent id="12">had</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="10">indicating</governor>
          <dependent id="13">found</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="13">found</governor>
          <dependent id="14">hope</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="19">go</governor>
          <dependent id="15">that</dependent>
        </dependency>
        <dependency type="det">
          <governor id="17">decision</governor>
          <dependent id="16">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">go</governor>
          <dependent id="17">decision</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="19">go</governor>
          <dependent id="18">would</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="13">found</governor>
          <dependent id="19">go</dependent>
        </dependency>
        <dependency type="det">
          <governor id="22">way</governor>
          <dependent id="20">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="22">way</governor>
          <dependent id="21">defense</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="19">go</governor>
          <dependent id="22">way</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="24">week</governor>
          <dependent id="23">last</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="19">go</governor>
          <dependent id="24">week</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="28">informed</governor>
          <dependent id="25">when</dependent>
        </dependency>
        <dependency type="det">
          <governor id="27">jury</governor>
          <dependent id="26">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="28">informed</governor>
          <dependent id="27">jury</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="19">go</governor>
          <dependent id="28">informed</dependent>
        </dependency>
        <dependency type="det">
          <governor id="30">judge</governor>
          <dependent id="29">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="28">informed</governor>
          <dependent id="30">judge</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="33">reaching</governor>
          <dependent id="31">it</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="33">reaching</governor>
          <dependent id="32">was</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="30">judge</governor>
          <dependent id="33">reaching</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="33">reaching</governor>
          <dependent id="34">deadlock</dependent>
        </dependency>
        <dependency type="case">
          <governor id="38">count</governor>
          <dependent id="35">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="38">count</governor>
          <dependent id="36">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="38">count</governor>
          <dependent id="37">conspiracy</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="33">reaching</governor>
          <dependent id="38">count</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="last week" type="DATE" score="0.0">
          <tokens>
            <token id="23" string="last" />
            <token id="24" string="week" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="78" has_coreference="true">
      <content>With the acquittal, Pounders freed more than $3 million worth of property that had been posted by Buckey supporters as bail last year so that he could be released after five years in jail.</content>
      <tokens>
        <token id="1" string="With" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="acquittal" lemma="acquittal" stem="acquitt" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="Pounders" lemma="Pounders" stem="pounder" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="6" string="freed" lemma="free" stem="freed" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="more" lemma="more" stem="more" pos="JJR" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="than" lemma="than" stem="than" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="$" lemma="$" stem="$" pos="$" type="Symbol" isStopWord="true" ner="MONEY" is_referenced="false" is_refers="false" />
        <token id="10" string="3" lemma="3" stem="3" pos="CD" type="Number" isStopWord="false" ner="MONEY" is_referenced="false" is_refers="false" />
        <token id="11" string="million" lemma="million" stem="million" pos="CD" type="Word" isStopWord="false" ner="MONEY" is_referenced="false" is_refers="false" />
        <token id="12" string="worth" lemma="worth" stem="worth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="property" lemma="property" stem="properti" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="posted" lemma="post" stem="post" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="21" string="supporters" lemma="supporter" stem="support" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="bail" lemma="bail" stem="bail" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="last" lemma="last" stem="last" pos="JJ" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="25" string="year" lemma="year" stem="year" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="26" string="so" lemma="so" stem="so" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="29" string="could" lemma="could" stem="could" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="be" lemma="be" stem="be" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="released" lemma="release" stem="releas" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="after" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="five" lemma="five" stem="five" pos="CD" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="34" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="35" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="36" string="jail" lemma="jail" stem="jail" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="37" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (IN With) (NP (DT the) (NN acquittal))) (, ,) (NP (NNP Pounders)) (VP (VBD freed) (NP (NP (QP (JJR more) (IN than) ($ $) (CD 3) (CD million)) (NN worth)) (PP (IN of) (NP (NN property))) (SBAR (WHNP (WDT that)) (S (VP (VBD had) (VP (VBN been) (VP (VBN posted) (PP (IN by) (NP (NP (NNP Buckey) (NNS supporters)) (PP (IN as) (NP (NN bail))))) (NP-TMP (JJ last) (NN year)) (SBAR (IN so) (IN that) (S (NP (PRP he)) (VP (MD could) (VP (VB be) (VP (VBN released) (PP (IN after) (NP (NP (CD five) (NNS years)) (PP (IN in) (NP (NN jail))))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="more than $ 3 million worth" type="NP">
          <tokens>
            <token id="7" string="more" />
            <token id="8" string="than" />
            <token id="9" string="$" />
            <token id="10" string="3" />
            <token id="11" string="million" />
            <token id="12" string="worth" />
          </tokens>
        </chunking>
        <chunking id="2" string="the acquittal" type="NP">
          <tokens>
            <token id="2" string="the" />
            <token id="3" string="acquittal" />
          </tokens>
        </chunking>
        <chunking id="3" string="freed more than $ 3 million worth of property that had been posted by Buckey supporters as bail last year so that he could be released after five years in jail" type="VP">
          <tokens>
            <token id="6" string="freed" />
            <token id="7" string="more" />
            <token id="8" string="than" />
            <token id="9" string="$" />
            <token id="10" string="3" />
            <token id="11" string="million" />
            <token id="12" string="worth" />
            <token id="13" string="of" />
            <token id="14" string="property" />
            <token id="15" string="that" />
            <token id="16" string="had" />
            <token id="17" string="been" />
            <token id="18" string="posted" />
            <token id="19" string="by" />
            <token id="20" string="Buckey" />
            <token id="21" string="supporters" />
            <token id="22" string="as" />
            <token id="23" string="bail" />
            <token id="24" string="last" />
            <token id="25" string="year" />
            <token id="26" string="so" />
            <token id="27" string="that" />
            <token id="28" string="he" />
            <token id="29" string="could" />
            <token id="30" string="be" />
            <token id="31" string="released" />
            <token id="32" string="after" />
            <token id="33" string="five" />
            <token id="34" string="years" />
            <token id="35" string="in" />
            <token id="36" string="jail" />
          </tokens>
        </chunking>
        <chunking id="4" string="jail" type="NP">
          <tokens>
            <token id="36" string="jail" />
          </tokens>
        </chunking>
        <chunking id="5" string="more than $ 3 million worth of property that had been posted by Buckey supporters as bail last year so that he could be released after five years in jail" type="NP">
          <tokens>
            <token id="7" string="more" />
            <token id="8" string="than" />
            <token id="9" string="$" />
            <token id="10" string="3" />
            <token id="11" string="million" />
            <token id="12" string="worth" />
            <token id="13" string="of" />
            <token id="14" string="property" />
            <token id="15" string="that" />
            <token id="16" string="had" />
            <token id="17" string="been" />
            <token id="18" string="posted" />
            <token id="19" string="by" />
            <token id="20" string="Buckey" />
            <token id="21" string="supporters" />
            <token id="22" string="as" />
            <token id="23" string="bail" />
            <token id="24" string="last" />
            <token id="25" string="year" />
            <token id="26" string="so" />
            <token id="27" string="that" />
            <token id="28" string="he" />
            <token id="29" string="could" />
            <token id="30" string="be" />
            <token id="31" string="released" />
            <token id="32" string="after" />
            <token id="33" string="five" />
            <token id="34" string="years" />
            <token id="35" string="in" />
            <token id="36" string="jail" />
          </tokens>
        </chunking>
        <chunking id="6" string="be released after five years in jail" type="VP">
          <tokens>
            <token id="30" string="be" />
            <token id="31" string="released" />
            <token id="32" string="after" />
            <token id="33" string="five" />
            <token id="34" string="years" />
            <token id="35" string="in" />
            <token id="36" string="jail" />
          </tokens>
        </chunking>
        <chunking id="7" string="Pounders" type="NP">
          <tokens>
            <token id="5" string="Pounders" />
          </tokens>
        </chunking>
        <chunking id="8" string="posted by Buckey supporters as bail last year so that he could be released after five years in jail" type="VP">
          <tokens>
            <token id="18" string="posted" />
            <token id="19" string="by" />
            <token id="20" string="Buckey" />
            <token id="21" string="supporters" />
            <token id="22" string="as" />
            <token id="23" string="bail" />
            <token id="24" string="last" />
            <token id="25" string="year" />
            <token id="26" string="so" />
            <token id="27" string="that" />
            <token id="28" string="he" />
            <token id="29" string="could" />
            <token id="30" string="be" />
            <token id="31" string="released" />
            <token id="32" string="after" />
            <token id="33" string="five" />
            <token id="34" string="years" />
            <token id="35" string="in" />
            <token id="36" string="jail" />
          </tokens>
        </chunking>
        <chunking id="9" string="Buckey supporters as bail" type="NP">
          <tokens>
            <token id="20" string="Buckey" />
            <token id="21" string="supporters" />
            <token id="22" string="as" />
            <token id="23" string="bail" />
          </tokens>
        </chunking>
        <chunking id="10" string="Buckey supporters" type="NP">
          <tokens>
            <token id="20" string="Buckey" />
            <token id="21" string="supporters" />
          </tokens>
        </chunking>
        <chunking id="11" string="bail" type="NP">
          <tokens>
            <token id="23" string="bail" />
          </tokens>
        </chunking>
        <chunking id="12" string="five years" type="NP">
          <tokens>
            <token id="33" string="five" />
            <token id="34" string="years" />
          </tokens>
        </chunking>
        <chunking id="13" string="been posted by Buckey supporters as bail last year so that he could be released after five years in jail" type="VP">
          <tokens>
            <token id="17" string="been" />
            <token id="18" string="posted" />
            <token id="19" string="by" />
            <token id="20" string="Buckey" />
            <token id="21" string="supporters" />
            <token id="22" string="as" />
            <token id="23" string="bail" />
            <token id="24" string="last" />
            <token id="25" string="year" />
            <token id="26" string="so" />
            <token id="27" string="that" />
            <token id="28" string="he" />
            <token id="29" string="could" />
            <token id="30" string="be" />
            <token id="31" string="released" />
            <token id="32" string="after" />
            <token id="33" string="five" />
            <token id="34" string="years" />
            <token id="35" string="in" />
            <token id="36" string="jail" />
          </tokens>
        </chunking>
        <chunking id="14" string="five years in jail" type="NP">
          <tokens>
            <token id="33" string="five" />
            <token id="34" string="years" />
            <token id="35" string="in" />
            <token id="36" string="jail" />
          </tokens>
        </chunking>
        <chunking id="15" string="so that he could be released after five years in jail" type="SBAR">
          <tokens>
            <token id="26" string="so" />
            <token id="27" string="that" />
            <token id="28" string="he" />
            <token id="29" string="could" />
            <token id="30" string="be" />
            <token id="31" string="released" />
            <token id="32" string="after" />
            <token id="33" string="five" />
            <token id="34" string="years" />
            <token id="35" string="in" />
            <token id="36" string="jail" />
          </tokens>
        </chunking>
        <chunking id="16" string="had been posted by Buckey supporters as bail last year so that he could be released after five years in jail" type="VP">
          <tokens>
            <token id="16" string="had" />
            <token id="17" string="been" />
            <token id="18" string="posted" />
            <token id="19" string="by" />
            <token id="20" string="Buckey" />
            <token id="21" string="supporters" />
            <token id="22" string="as" />
            <token id="23" string="bail" />
            <token id="24" string="last" />
            <token id="25" string="year" />
            <token id="26" string="so" />
            <token id="27" string="that" />
            <token id="28" string="he" />
            <token id="29" string="could" />
            <token id="30" string="be" />
            <token id="31" string="released" />
            <token id="32" string="after" />
            <token id="33" string="five" />
            <token id="34" string="years" />
            <token id="35" string="in" />
            <token id="36" string="jail" />
          </tokens>
        </chunking>
        <chunking id="17" string="property" type="NP">
          <tokens>
            <token id="14" string="property" />
          </tokens>
        </chunking>
        <chunking id="18" string="he" type="NP">
          <tokens>
            <token id="28" string="he" />
          </tokens>
        </chunking>
        <chunking id="19" string="could be released after five years in jail" type="VP">
          <tokens>
            <token id="29" string="could" />
            <token id="30" string="be" />
            <token id="31" string="released" />
            <token id="32" string="after" />
            <token id="33" string="five" />
            <token id="34" string="years" />
            <token id="35" string="in" />
            <token id="36" string="jail" />
          </tokens>
        </chunking>
        <chunking id="20" string="that had been posted by Buckey supporters as bail last year so that he could be released after five years in jail" type="SBAR">
          <tokens>
            <token id="15" string="that" />
            <token id="16" string="had" />
            <token id="17" string="been" />
            <token id="18" string="posted" />
            <token id="19" string="by" />
            <token id="20" string="Buckey" />
            <token id="21" string="supporters" />
            <token id="22" string="as" />
            <token id="23" string="bail" />
            <token id="24" string="last" />
            <token id="25" string="year" />
            <token id="26" string="so" />
            <token id="27" string="that" />
            <token id="28" string="he" />
            <token id="29" string="could" />
            <token id="30" string="be" />
            <token id="31" string="released" />
            <token id="32" string="after" />
            <token id="33" string="five" />
            <token id="34" string="years" />
            <token id="35" string="in" />
            <token id="36" string="jail" />
          </tokens>
        </chunking>
        <chunking id="21" string="released after five years in jail" type="VP">
          <tokens>
            <token id="31" string="released" />
            <token id="32" string="after" />
            <token id="33" string="five" />
            <token id="34" string="years" />
            <token id="35" string="in" />
            <token id="36" string="jail" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="case">
          <governor id="3">acquittal</governor>
          <dependent id="1">With</dependent>
        </dependency>
        <dependency type="det">
          <governor id="3">acquittal</governor>
          <dependent id="2">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">freed</governor>
          <dependent id="3">acquittal</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">freed</governor>
          <dependent id="5">Pounders</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">freed</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="9">$</governor>
          <dependent id="7">more</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="7">more</governor>
          <dependent id="8">than</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="12">worth</governor>
          <dependent id="9">$</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">million</governor>
          <dependent id="10">3</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="9">$</governor>
          <dependent id="11">million</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">freed</governor>
          <dependent id="12">worth</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">property</governor>
          <dependent id="13">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">worth</governor>
          <dependent id="14">property</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="18">posted</governor>
          <dependent id="15">that</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="18">posted</governor>
          <dependent id="16">had</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="18">posted</governor>
          <dependent id="17">been</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="12">worth</governor>
          <dependent id="18">posted</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">supporters</governor>
          <dependent id="19">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="21">supporters</governor>
          <dependent id="20">Buckey</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="18">posted</governor>
          <dependent id="21">supporters</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">bail</governor>
          <dependent id="22">as</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="21">supporters</governor>
          <dependent id="23">bail</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="25">year</governor>
          <dependent id="24">last</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="18">posted</governor>
          <dependent id="25">year</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="31">released</governor>
          <dependent id="26">so</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="26">so</governor>
          <dependent id="27">that</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="31">released</governor>
          <dependent id="28">he</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="31">released</governor>
          <dependent id="29">could</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="31">released</governor>
          <dependent id="30">be</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="18">posted</governor>
          <dependent id="31">released</dependent>
        </dependency>
        <dependency type="case">
          <governor id="34">years</governor>
          <dependent id="32">after</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="34">years</governor>
          <dependent id="33">five</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="31">released</governor>
          <dependent id="34">years</dependent>
        </dependency>
        <dependency type="case">
          <governor id="36">jail</governor>
          <dependent id="35">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="34">years</governor>
          <dependent id="36">jail</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="20" string="Buckey" />
          </tokens>
        </entity>
        <entity id="2" string="$ 3 million" type="MONEY" score="0.0">
          <tokens>
            <token id="9" string="$" />
            <token id="10" string="3" />
            <token id="11" string="million" />
          </tokens>
        </entity>
        <entity id="3" string="Pounders" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="Pounders" />
          </tokens>
        </entity>
        <entity id="4" string="last year" type="DATE" score="0.0">
          <tokens>
            <token id="24" string="last" />
            <token id="25" string="year" />
          </tokens>
        </entity>
        <entity id="5" string="five years" type="DURATION" score="0.0">
          <tokens>
            <token id="33" string="five" />
            <token id="34" string="years" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="79" has_coreference="true">
      <content>Buckey&amp;apost;s mother had been free on a smaller bail amount earlier after spending nearly two years in jail.</content>
      <tokens>
        <token id="1" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="free" lemma="free" stem="free" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="smaller" lemma="smaller" stem="smaller" pos="JJR" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="bail" lemma="bail" stem="bail" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="amount" lemma="amount" stem="amount" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="earlier" lemma="earlier" stem="earlier" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="after" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="spending" lemma="spend" stem="spend" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="nearly" lemma="nearly" stem="nearli" pos="RB" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="16" string="two" lemma="two" stem="two" pos="CD" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="17" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="18" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="19" string="jail" lemma="jail" stem="jail" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="20" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Buckey) (POS 's)) (NN mother)) (VP (VBD had) (VP (VBN been) (ADJP (JJ free) (PP (IN on) (NP (DT a) (JJR smaller) (NN bail) (NN amount)))) (ADVP (RB earlier) (PP (IN after) (S (VP (VBG spending) (NP (NP (QP (RB nearly) (CD two)) (NNS years)) (PP (IN in) (NP (NN jail)))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Buckey 's mother" type="NP">
          <tokens>
            <token id="1" string="Buckey" />
            <token id="2" string="'s" />
            <token id="3" string="mother" />
          </tokens>
        </chunking>
        <chunking id="2" string="nearly two years in jail" type="NP">
          <tokens>
            <token id="15" string="nearly" />
            <token id="16" string="two" />
            <token id="17" string="years" />
            <token id="18" string="in" />
            <token id="19" string="jail" />
          </tokens>
        </chunking>
        <chunking id="3" string="had been free on a smaller bail amount earlier after spending nearly two years in jail" type="VP">
          <tokens>
            <token id="4" string="had" />
            <token id="5" string="been" />
            <token id="6" string="free" />
            <token id="7" string="on" />
            <token id="8" string="a" />
            <token id="9" string="smaller" />
            <token id="10" string="bail" />
            <token id="11" string="amount" />
            <token id="12" string="earlier" />
            <token id="13" string="after" />
            <token id="14" string="spending" />
            <token id="15" string="nearly" />
            <token id="16" string="two" />
            <token id="17" string="years" />
            <token id="18" string="in" />
            <token id="19" string="jail" />
          </tokens>
        </chunking>
        <chunking id="4" string="a smaller bail amount" type="NP">
          <tokens>
            <token id="8" string="a" />
            <token id="9" string="smaller" />
            <token id="10" string="bail" />
            <token id="11" string="amount" />
          </tokens>
        </chunking>
        <chunking id="5" string="Buckey 's" type="NP">
          <tokens>
            <token id="1" string="Buckey" />
            <token id="2" string="'s" />
          </tokens>
        </chunking>
        <chunking id="6" string="nearly two years" type="NP">
          <tokens>
            <token id="15" string="nearly" />
            <token id="16" string="two" />
            <token id="17" string="years" />
          </tokens>
        </chunking>
        <chunking id="7" string="jail" type="NP">
          <tokens>
            <token id="19" string="jail" />
          </tokens>
        </chunking>
        <chunking id="8" string="spending nearly two years in jail" type="VP">
          <tokens>
            <token id="14" string="spending" />
            <token id="15" string="nearly" />
            <token id="16" string="two" />
            <token id="17" string="years" />
            <token id="18" string="in" />
            <token id="19" string="jail" />
          </tokens>
        </chunking>
        <chunking id="9" string="free on a smaller bail amount" type="ADJP">
          <tokens>
            <token id="6" string="free" />
            <token id="7" string="on" />
            <token id="8" string="a" />
            <token id="9" string="smaller" />
            <token id="10" string="bail" />
            <token id="11" string="amount" />
          </tokens>
        </chunking>
        <chunking id="10" string="been free on a smaller bail amount earlier after spending nearly two years in jail" type="VP">
          <tokens>
            <token id="5" string="been" />
            <token id="6" string="free" />
            <token id="7" string="on" />
            <token id="8" string="a" />
            <token id="9" string="smaller" />
            <token id="10" string="bail" />
            <token id="11" string="amount" />
            <token id="12" string="earlier" />
            <token id="13" string="after" />
            <token id="14" string="spending" />
            <token id="15" string="nearly" />
            <token id="16" string="two" />
            <token id="17" string="years" />
            <token id="18" string="in" />
            <token id="19" string="jail" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="3">mother</governor>
          <dependent id="1">Buckey</dependent>
        </dependency>
        <dependency type="case">
          <governor id="1">Buckey</governor>
          <dependent id="2">'s</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">free</governor>
          <dependent id="3">mother</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="6">free</governor>
          <dependent id="4">had</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="6">free</governor>
          <dependent id="5">been</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">free</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">amount</governor>
          <dependent id="7">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">amount</governor>
          <dependent id="8">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">amount</governor>
          <dependent id="9">smaller</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">amount</governor>
          <dependent id="10">bail</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">free</governor>
          <dependent id="11">amount</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="6">free</governor>
          <dependent id="12">earlier</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="14">spending</governor>
          <dependent id="13">after</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="12">earlier</governor>
          <dependent id="14">spending</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="16">two</governor>
          <dependent id="15">nearly</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="17">years</governor>
          <dependent id="16">two</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="14">spending</governor>
          <dependent id="17">years</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">jail</governor>
          <dependent id="18">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">years</governor>
          <dependent id="19">jail</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Buckey" />
          </tokens>
        </entity>
        <entity id="2" string="nearly two years" type="DURATION" score="0.0">
          <tokens>
            <token id="15" string="nearly" />
            <token id="16" string="two" />
            <token id="17" string="years" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="80" has_coreference="true">
      <content>The McMartin case -- which included allegations not only of rape, sodomy, oral copulation, and other sex crimes, but also of pornographic photography sessions, &amp;quot;naked games,&amp;quot; field trips away from the school for illicit purposes, animal mutilation, threats and satanic-like ritual and sacrifice -- began in the fall of 1983.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="which" lemma="which" stem="which" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="included" lemma="include" stem="includ" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="allegations" lemma="allegation" stem="alleg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="only" lemma="only" stem="onli" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="rape" lemma="rape" stem="rape" pos="NN" type="Word" isStopWord="false" ner="CRIMINAL_CHARGE" is_referenced="false" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="sodomy" lemma="sodomy" stem="sodomi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="oral" lemma="oral" stem="oral" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="copulation" lemma="copulation" stem="copul" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="other" lemma="other" stem="other" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="sex" lemma="sex" stem="sex" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="crimes" lemma="crime" stem="crime" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="pornographic" lemma="pornographic" stem="pornograph" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="photography" lemma="photography" stem="photographi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="sessions" lemma="session" stem="session" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="naked" lemma="naked" stem="nake" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="games" lemma="game" stem="game" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="field" lemma="field" stem="field" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="36" string="trips" lemma="trip" stem="trip" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="37" string="away" lemma="away" stem="awai" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="38" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="39" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="40" string="school" lemma="school" stem="school" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="41" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="42" string="illicit" lemma="illicit" stem="illicit" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="43" string="purposes" lemma="purpose" stem="purpos" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="44" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="45" string="animal" lemma="animal" stem="anim" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="46" string="mutilation" lemma="mutilation" stem="mutil" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="47" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="48" string="threats" lemma="threat" stem="threat" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="49" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="50" string="satanic-like" lemma="satanic-like" stem="satanic-lik" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="51" string="ritual" lemma="ritual" stem="ritual" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="52" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="53" string="sacrifice" lemma="sacrifice" stem="sacrific" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="54" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="55" string="began" lemma="begin" stem="began" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="56" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="57" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="58" string="fall" lemma="fall" stem="fall" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="59" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="60" string="1983" lemma="1983" stem="1983" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="61" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT The) (NNP McMartin) (NN case)) (PRN (: --) (SBAR (WHNP (WDT which)) (S (VP (VBD included) (NP (NP (NNS allegations)) (CONJP (RB not) (RB only) (IN of)) (NP (UCP (NP (NN rape)) (, ,) (NP (NN sodomy) (, ,) (JJ oral) (NN copulation)) (, ,) (CC and) (NP (JJ other) (NN sex) (NNS crimes)) (, ,) (CC but) (ADVP (RB also)) (PP (IN of) (NP (NP (JJ pornographic) (NN photography) (NNS sessions)) (, ,) (NP (`` ``) (JJ naked) (NNS games) (, ,) ('' ''))))) (NN field) (NNS trips))) (PP (RB away) (IN from) (NP (NP (DT the) (NN school)) (PP (IN for) (NP (NP (JJ illicit) (NNS purposes)) (, ,) (NP (NN animal) (NN mutilation)) (, ,) (NP (NNS threats)) (CC and) (NP (JJ satanic-like) (NN ritual) (CC and) (NN sacrifice))))))))) (: --))) (VP (VBD began) (PP (IN in) (NP (NP (DT the) (NN fall)) (PP (IN of) (NP (CD 1983)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="rape , sodomy , oral copulation , and other sex crimes , but also of pornographic photography sessions , `` naked games , '' field trips" type="NP">
          <tokens>
            <token id="11" string="rape" />
            <token id="12" string="," />
            <token id="13" string="sodomy" />
            <token id="14" string="," />
            <token id="15" string="oral" />
            <token id="16" string="copulation" />
            <token id="17" string="," />
            <token id="18" string="and" />
            <token id="19" string="other" />
            <token id="20" string="sex" />
            <token id="21" string="crimes" />
            <token id="22" string="," />
            <token id="23" string="but" />
            <token id="24" string="also" />
            <token id="25" string="of" />
            <token id="26" string="pornographic" />
            <token id="27" string="photography" />
            <token id="28" string="sessions" />
            <token id="29" string="," />
            <token id="30" string="&quot;" />
            <token id="31" string="naked" />
            <token id="32" string="games" />
            <token id="33" string="," />
            <token id="34" string="&quot;" />
            <token id="35" string="field" />
            <token id="36" string="trips" />
          </tokens>
        </chunking>
        <chunking id="2" string="began in the fall of 1983" type="VP">
          <tokens>
            <token id="55" string="began" />
            <token id="56" string="in" />
            <token id="57" string="the" />
            <token id="58" string="fall" />
            <token id="59" string="of" />
            <token id="60" string="1983" />
          </tokens>
        </chunking>
        <chunking id="3" string="included allegations not only of rape , sodomy , oral copulation , and other sex crimes , but also of pornographic photography sessions , `` naked games , '' field trips away from the school for illicit purposes , animal mutilation , threats and satanic-like ritual and sacrifice" type="VP">
          <tokens>
            <token id="6" string="included" />
            <token id="7" string="allegations" />
            <token id="8" string="not" />
            <token id="9" string="only" />
            <token id="10" string="of" />
            <token id="11" string="rape" />
            <token id="12" string="," />
            <token id="13" string="sodomy" />
            <token id="14" string="," />
            <token id="15" string="oral" />
            <token id="16" string="copulation" />
            <token id="17" string="," />
            <token id="18" string="and" />
            <token id="19" string="other" />
            <token id="20" string="sex" />
            <token id="21" string="crimes" />
            <token id="22" string="," />
            <token id="23" string="but" />
            <token id="24" string="also" />
            <token id="25" string="of" />
            <token id="26" string="pornographic" />
            <token id="27" string="photography" />
            <token id="28" string="sessions" />
            <token id="29" string="," />
            <token id="30" string="&quot;" />
            <token id="31" string="naked" />
            <token id="32" string="games" />
            <token id="33" string="," />
            <token id="34" string="&quot;" />
            <token id="35" string="field" />
            <token id="36" string="trips" />
            <token id="37" string="away" />
            <token id="38" string="from" />
            <token id="39" string="the" />
            <token id="40" string="school" />
            <token id="41" string="for" />
            <token id="42" string="illicit" />
            <token id="43" string="purposes" />
            <token id="44" string="," />
            <token id="45" string="animal" />
            <token id="46" string="mutilation" />
            <token id="47" string="," />
            <token id="48" string="threats" />
            <token id="49" string="and" />
            <token id="50" string="satanic-like" />
            <token id="51" string="ritual" />
            <token id="52" string="and" />
            <token id="53" string="sacrifice" />
          </tokens>
        </chunking>
        <chunking id="4" string="other sex crimes" type="NP">
          <tokens>
            <token id="19" string="other" />
            <token id="20" string="sex" />
            <token id="21" string="crimes" />
          </tokens>
        </chunking>
        <chunking id="5" string="animal mutilation" type="NP">
          <tokens>
            <token id="45" string="animal" />
            <token id="46" string="mutilation" />
          </tokens>
        </chunking>
        <chunking id="6" string="the school for illicit purposes , animal mutilation , threats and satanic-like ritual and sacrifice" type="NP">
          <tokens>
            <token id="39" string="the" />
            <token id="40" string="school" />
            <token id="41" string="for" />
            <token id="42" string="illicit" />
            <token id="43" string="purposes" />
            <token id="44" string="," />
            <token id="45" string="animal" />
            <token id="46" string="mutilation" />
            <token id="47" string="," />
            <token id="48" string="threats" />
            <token id="49" string="and" />
            <token id="50" string="satanic-like" />
            <token id="51" string="ritual" />
            <token id="52" string="and" />
            <token id="53" string="sacrifice" />
          </tokens>
        </chunking>
        <chunking id="7" string="illicit purposes" type="NP">
          <tokens>
            <token id="42" string="illicit" />
            <token id="43" string="purposes" />
          </tokens>
        </chunking>
        <chunking id="8" string="pornographic photography sessions , `` naked games , ''" type="NP">
          <tokens>
            <token id="26" string="pornographic" />
            <token id="27" string="photography" />
            <token id="28" string="sessions" />
            <token id="29" string="," />
            <token id="30" string="&quot;" />
            <token id="31" string="naked" />
            <token id="32" string="games" />
            <token id="33" string="," />
            <token id="34" string="&quot;" />
          </tokens>
        </chunking>
        <chunking id="9" string="sodomy , oral copulation" type="NP">
          <tokens>
            <token id="13" string="sodomy" />
            <token id="14" string="," />
            <token id="15" string="oral" />
            <token id="16" string="copulation" />
          </tokens>
        </chunking>
        <chunking id="10" string="`` naked games , ''" type="NP">
          <tokens>
            <token id="30" string="&quot;" />
            <token id="31" string="naked" />
            <token id="32" string="games" />
            <token id="33" string="," />
            <token id="34" string="&quot;" />
          </tokens>
        </chunking>
        <chunking id="11" string="the school" type="NP">
          <tokens>
            <token id="39" string="the" />
            <token id="40" string="school" />
          </tokens>
        </chunking>
        <chunking id="12" string="The McMartin case -- which included allegations not only of rape , sodomy , oral copulation , and other sex crimes , but also of pornographic photography sessions , `` naked games , '' field trips away from the school for illicit purposes , animal mutilation , threats and satanic-like ritual and sacrifice --" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="McMartin" />
            <token id="3" string="case" />
            <token id="4" string="--" />
            <token id="5" string="which" />
            <token id="6" string="included" />
            <token id="7" string="allegations" />
            <token id="8" string="not" />
            <token id="9" string="only" />
            <token id="10" string="of" />
            <token id="11" string="rape" />
            <token id="12" string="," />
            <token id="13" string="sodomy" />
            <token id="14" string="," />
            <token id="15" string="oral" />
            <token id="16" string="copulation" />
            <token id="17" string="," />
            <token id="18" string="and" />
            <token id="19" string="other" />
            <token id="20" string="sex" />
            <token id="21" string="crimes" />
            <token id="22" string="," />
            <token id="23" string="but" />
            <token id="24" string="also" />
            <token id="25" string="of" />
            <token id="26" string="pornographic" />
            <token id="27" string="photography" />
            <token id="28" string="sessions" />
            <token id="29" string="," />
            <token id="30" string="&quot;" />
            <token id="31" string="naked" />
            <token id="32" string="games" />
            <token id="33" string="," />
            <token id="34" string="&quot;" />
            <token id="35" string="field" />
            <token id="36" string="trips" />
            <token id="37" string="away" />
            <token id="38" string="from" />
            <token id="39" string="the" />
            <token id="40" string="school" />
            <token id="41" string="for" />
            <token id="42" string="illicit" />
            <token id="43" string="purposes" />
            <token id="44" string="," />
            <token id="45" string="animal" />
            <token id="46" string="mutilation" />
            <token id="47" string="," />
            <token id="48" string="threats" />
            <token id="49" string="and" />
            <token id="50" string="satanic-like" />
            <token id="51" string="ritual" />
            <token id="52" string="and" />
            <token id="53" string="sacrifice" />
            <token id="54" string="--" />
          </tokens>
        </chunking>
        <chunking id="13" string="allegations" type="NP">
          <tokens>
            <token id="7" string="allegations" />
          </tokens>
        </chunking>
        <chunking id="14" string="pornographic photography sessions" type="NP">
          <tokens>
            <token id="26" string="pornographic" />
            <token id="27" string="photography" />
            <token id="28" string="sessions" />
          </tokens>
        </chunking>
        <chunking id="15" string="illicit purposes , animal mutilation , threats and satanic-like ritual and sacrifice" type="NP">
          <tokens>
            <token id="42" string="illicit" />
            <token id="43" string="purposes" />
            <token id="44" string="," />
            <token id="45" string="animal" />
            <token id="46" string="mutilation" />
            <token id="47" string="," />
            <token id="48" string="threats" />
            <token id="49" string="and" />
            <token id="50" string="satanic-like" />
            <token id="51" string="ritual" />
            <token id="52" string="and" />
            <token id="53" string="sacrifice" />
          </tokens>
        </chunking>
        <chunking id="16" string="the fall" type="NP">
          <tokens>
            <token id="57" string="the" />
            <token id="58" string="fall" />
          </tokens>
        </chunking>
        <chunking id="17" string="1983" type="NP">
          <tokens>
            <token id="60" string="1983" />
          </tokens>
        </chunking>
        <chunking id="18" string="The McMartin case" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="McMartin" />
            <token id="3" string="case" />
          </tokens>
        </chunking>
        <chunking id="19" string="rape" type="NP">
          <tokens>
            <token id="11" string="rape" />
          </tokens>
        </chunking>
        <chunking id="20" string="which included allegations not only of rape , sodomy , oral copulation , and other sex crimes , but also of pornographic photography sessions , `` naked games , '' field trips away from the school for illicit purposes , animal mutilation , threats and satanic-like ritual and sacrifice" type="SBAR">
          <tokens>
            <token id="5" string="which" />
            <token id="6" string="included" />
            <token id="7" string="allegations" />
            <token id="8" string="not" />
            <token id="9" string="only" />
            <token id="10" string="of" />
            <token id="11" string="rape" />
            <token id="12" string="," />
            <token id="13" string="sodomy" />
            <token id="14" string="," />
            <token id="15" string="oral" />
            <token id="16" string="copulation" />
            <token id="17" string="," />
            <token id="18" string="and" />
            <token id="19" string="other" />
            <token id="20" string="sex" />
            <token id="21" string="crimes" />
            <token id="22" string="," />
            <token id="23" string="but" />
            <token id="24" string="also" />
            <token id="25" string="of" />
            <token id="26" string="pornographic" />
            <token id="27" string="photography" />
            <token id="28" string="sessions" />
            <token id="29" string="," />
            <token id="30" string="&quot;" />
            <token id="31" string="naked" />
            <token id="32" string="games" />
            <token id="33" string="," />
            <token id="34" string="&quot;" />
            <token id="35" string="field" />
            <token id="36" string="trips" />
            <token id="37" string="away" />
            <token id="38" string="from" />
            <token id="39" string="the" />
            <token id="40" string="school" />
            <token id="41" string="for" />
            <token id="42" string="illicit" />
            <token id="43" string="purposes" />
            <token id="44" string="," />
            <token id="45" string="animal" />
            <token id="46" string="mutilation" />
            <token id="47" string="," />
            <token id="48" string="threats" />
            <token id="49" string="and" />
            <token id="50" string="satanic-like" />
            <token id="51" string="ritual" />
            <token id="52" string="and" />
            <token id="53" string="sacrifice" />
          </tokens>
        </chunking>
        <chunking id="21" string="threats" type="NP">
          <tokens>
            <token id="48" string="threats" />
          </tokens>
        </chunking>
        <chunking id="22" string="satanic-like ritual and sacrifice" type="NP">
          <tokens>
            <token id="50" string="satanic-like" />
            <token id="51" string="ritual" />
            <token id="52" string="and" />
            <token id="53" string="sacrifice" />
          </tokens>
        </chunking>
        <chunking id="23" string="the fall of 1983" type="NP">
          <tokens>
            <token id="57" string="the" />
            <token id="58" string="fall" />
            <token id="59" string="of" />
            <token id="60" string="1983" />
          </tokens>
        </chunking>
        <chunking id="24" string="allegations not only of rape , sodomy , oral copulation , and other sex crimes , but also of pornographic photography sessions , `` naked games , '' field trips" type="NP">
          <tokens>
            <token id="7" string="allegations" />
            <token id="8" string="not" />
            <token id="9" string="only" />
            <token id="10" string="of" />
            <token id="11" string="rape" />
            <token id="12" string="," />
            <token id="13" string="sodomy" />
            <token id="14" string="," />
            <token id="15" string="oral" />
            <token id="16" string="copulation" />
            <token id="17" string="," />
            <token id="18" string="and" />
            <token id="19" string="other" />
            <token id="20" string="sex" />
            <token id="21" string="crimes" />
            <token id="22" string="," />
            <token id="23" string="but" />
            <token id="24" string="also" />
            <token id="25" string="of" />
            <token id="26" string="pornographic" />
            <token id="27" string="photography" />
            <token id="28" string="sessions" />
            <token id="29" string="," />
            <token id="30" string="&quot;" />
            <token id="31" string="naked" />
            <token id="32" string="games" />
            <token id="33" string="," />
            <token id="34" string="&quot;" />
            <token id="35" string="field" />
            <token id="36" string="trips" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">case</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">case</governor>
          <dependent id="2">McMartin</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="55">began</governor>
          <dependent id="3">case</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">included</governor>
          <dependent id="5">which</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="3">case</governor>
          <dependent id="6">included</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">included</governor>
          <dependent id="7">allegations</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="9">only</governor>
          <dependent id="8">not</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="7">allegations</governor>
          <dependent id="9">only</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="9">only</governor>
          <dependent id="10">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="36">trips</governor>
          <dependent id="11">rape</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="16">copulation</governor>
          <dependent id="13">sodomy</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="16">copulation</governor>
          <dependent id="15">oral</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="11">rape</governor>
          <dependent id="16">copulation</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="11">rape</governor>
          <dependent id="18">and</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="21">crimes</governor>
          <dependent id="19">other</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="21">crimes</governor>
          <dependent id="20">sex</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="11">rape</governor>
          <dependent id="21">crimes</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="11">rape</governor>
          <dependent id="23">but</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="11">rape</governor>
          <dependent id="24">also</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">sessions</governor>
          <dependent id="25">of</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="28">sessions</governor>
          <dependent id="26">pornographic</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="28">sessions</governor>
          <dependent id="27">photography</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="24">also</governor>
          <dependent id="28">sessions</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="32">games</governor>
          <dependent id="31">naked</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="28">sessions</governor>
          <dependent id="32">games</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="36">trips</governor>
          <dependent id="35">field</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">allegations</governor>
          <dependent id="36">trips</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="40">school</governor>
          <dependent id="37">away</dependent>
        </dependency>
        <dependency type="case">
          <governor id="40">school</governor>
          <dependent id="38">from</dependent>
        </dependency>
        <dependency type="det">
          <governor id="40">school</governor>
          <dependent id="39">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">included</governor>
          <dependent id="40">school</dependent>
        </dependency>
        <dependency type="case">
          <governor id="43">purposes</governor>
          <dependent id="41">for</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="43">purposes</governor>
          <dependent id="42">illicit</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="40">school</governor>
          <dependent id="43">purposes</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="46">mutilation</governor>
          <dependent id="45">animal</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="43">purposes</governor>
          <dependent id="46">mutilation</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="43">purposes</governor>
          <dependent id="48">threats</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="43">purposes</governor>
          <dependent id="49">and</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="51">ritual</governor>
          <dependent id="50">satanic-like</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="43">purposes</governor>
          <dependent id="51">ritual</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="51">ritual</governor>
          <dependent id="52">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="51">ritual</governor>
          <dependent id="53">sacrifice</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="55">began</dependent>
        </dependency>
        <dependency type="case">
          <governor id="58">fall</governor>
          <dependent id="56">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="58">fall</governor>
          <dependent id="57">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="55">began</governor>
          <dependent id="58">fall</dependent>
        </dependency>
        <dependency type="case">
          <governor id="60">1983</governor>
          <dependent id="59">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="58">fall</governor>
          <dependent id="60">1983</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="McMartin" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="McMartin" />
          </tokens>
        </entity>
        <entity id="2" string="rape" type="CRIMINAL_CHARGE" score="0.0">
          <tokens>
            <token id="11" string="rape" />
          </tokens>
        </entity>
        <entity id="3" string="the fall of 1983" type="DATE" score="0.0">
          <tokens>
            <token id="57" string="the" />
            <token id="58" string="fall" />
            <token id="59" string="of" />
            <token id="60" string="1983" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="81" has_coreference="true">
      <content>The mother of a 2 1/2-year-old complained to Manhattan Beach police that her son had been sodomized by &amp;quot;Mr. Ray,&amp;quot; and subsequent physical examinations indicated he had been sexually abused.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="2 1/2" lemma="2 1/2" stem="2 1/2" pos="CD" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="6" string="-" lemma="-" stem="-" pos=":" type="Symbol" isStopWord="true" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="7" string="year-old" lemma="year-old" stem="year-old" pos="JJ" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="true" />
        <token id="8" string="complained" lemma="complain" stem="complain" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="Manhattan" lemma="Manhattan" stem="manhattan" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="11" string="Beach" lemma="Beach" stem="beach" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="12" string="police" lemma="police" stem="polic" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="13" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="son" lemma="son" stem="son" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="16" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="sodomized" lemma="sodomize" stem="sodom" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="Mr." lemma="Mr." stem="mr." pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="22" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="23" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="subsequent" lemma="subsequent" stem="subsequ" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="physical" lemma="physical" stem="physic" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="examinations" lemma="examination" stem="examin" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="indicated" lemma="indicate" stem="indic" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="31" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="sexually" lemma="sexually" stem="sexual" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="abused" lemma="abused" stem="abus" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NP (DT The) (NN mother)) (PP (IN of) (NP (DT a) (CD 2 1/2)))) (: -) (S (S (NP (JJ year-old)) (VP (VBD complained) (PP (TO to) (NP (NNP Manhattan) (NNP Beach) (NN police))) (SBAR (WHNP (WDT that)) (S (NP (PRP$ her) (NN son)) (VP (VBD had) (VP (VBN been) (VP (VBN sodomized) (PP (IN by) (`` ``) (NP (NNP Mr.) (NNP Ray)))))))))) (, ,) ('' '') (CC and) (S (NP (JJ subsequent) (JJ physical) (NNS examinations)) (VP (VBD indicated) (SBAR (S (NP (PRP he)) (VP (VBD had) (VP (VBN been) (VP (ADVP (RB sexually)) (JJ abused))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Manhattan Beach police" type="NP">
          <tokens>
            <token id="10" string="Manhattan" />
            <token id="11" string="Beach" />
            <token id="12" string="police" />
          </tokens>
        </chunking>
        <chunking id="2" string="he had been sexually abused" type="SBAR">
          <tokens>
            <token id="30" string="he" />
            <token id="31" string="had" />
            <token id="32" string="been" />
            <token id="33" string="sexually" />
            <token id="34" string="abused" />
          </tokens>
        </chunking>
        <chunking id="3" string="The mother" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="mother" />
          </tokens>
        </chunking>
        <chunking id="4" string="a 2 1/2" type="NP">
          <tokens>
            <token id="4" string="a" />
            <token id="5" string="2 1/2" />
          </tokens>
        </chunking>
        <chunking id="5" string="complained to Manhattan Beach police that her son had been sodomized by `` Mr. Ray" type="VP">
          <tokens>
            <token id="8" string="complained" />
            <token id="9" string="to" />
            <token id="10" string="Manhattan" />
            <token id="11" string="Beach" />
            <token id="12" string="police" />
            <token id="13" string="that" />
            <token id="14" string="her" />
            <token id="15" string="son" />
            <token id="16" string="had" />
            <token id="17" string="been" />
            <token id="18" string="sodomized" />
            <token id="19" string="by" />
            <token id="20" string="&quot;" />
            <token id="21" string="Mr." />
            <token id="22" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="6" string="subsequent physical examinations" type="NP">
          <tokens>
            <token id="26" string="subsequent" />
            <token id="27" string="physical" />
            <token id="28" string="examinations" />
          </tokens>
        </chunking>
        <chunking id="7" string="had been sodomized by `` Mr. Ray" type="VP">
          <tokens>
            <token id="16" string="had" />
            <token id="17" string="been" />
            <token id="18" string="sodomized" />
            <token id="19" string="by" />
            <token id="20" string="&quot;" />
            <token id="21" string="Mr." />
            <token id="22" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="8" string="The mother of a 2 1/2" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="mother" />
            <token id="3" string="of" />
            <token id="4" string="a" />
            <token id="5" string="2 1/2" />
          </tokens>
        </chunking>
        <chunking id="9" string="The mother of a 2 1/2 - year-old complained to Manhattan Beach police that her son had been sodomized by `` Mr. Ray , '' and subsequent physical examinations indicated he had been sexually abused ." type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="mother" />
            <token id="3" string="of" />
            <token id="4" string="a" />
            <token id="5" string="2 1/2" />
            <token id="6" string="-" />
            <token id="7" string="year-old" />
            <token id="8" string="complained" />
            <token id="9" string="to" />
            <token id="10" string="Manhattan" />
            <token id="11" string="Beach" />
            <token id="12" string="police" />
            <token id="13" string="that" />
            <token id="14" string="her" />
            <token id="15" string="son" />
            <token id="16" string="had" />
            <token id="17" string="been" />
            <token id="18" string="sodomized" />
            <token id="19" string="by" />
            <token id="20" string="&quot;" />
            <token id="21" string="Mr." />
            <token id="22" string="Ray" />
            <token id="23" string="," />
            <token id="24" string="&quot;" />
            <token id="25" string="and" />
            <token id="26" string="subsequent" />
            <token id="27" string="physical" />
            <token id="28" string="examinations" />
            <token id="29" string="indicated" />
            <token id="30" string="he" />
            <token id="31" string="had" />
            <token id="32" string="been" />
            <token id="33" string="sexually" />
            <token id="34" string="abused" />
            <token id="35" string="." />
          </tokens>
        </chunking>
        <chunking id="10" string="year-old" type="NP">
          <tokens>
            <token id="7" string="year-old" />
          </tokens>
        </chunking>
        <chunking id="11" string="sexually abused" type="VP">
          <tokens>
            <token id="33" string="sexually" />
            <token id="34" string="abused" />
          </tokens>
        </chunking>
        <chunking id="12" string="Mr. Ray" type="NP">
          <tokens>
            <token id="21" string="Mr." />
            <token id="22" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="13" string="had been sexually abused" type="VP">
          <tokens>
            <token id="31" string="had" />
            <token id="32" string="been" />
            <token id="33" string="sexually" />
            <token id="34" string="abused" />
          </tokens>
        </chunking>
        <chunking id="14" string="indicated he had been sexually abused" type="VP">
          <tokens>
            <token id="29" string="indicated" />
            <token id="30" string="he" />
            <token id="31" string="had" />
            <token id="32" string="been" />
            <token id="33" string="sexually" />
            <token id="34" string="abused" />
          </tokens>
        </chunking>
        <chunking id="15" string="that her son had been sodomized by `` Mr. Ray" type="SBAR">
          <tokens>
            <token id="13" string="that" />
            <token id="14" string="her" />
            <token id="15" string="son" />
            <token id="16" string="had" />
            <token id="17" string="been" />
            <token id="18" string="sodomized" />
            <token id="19" string="by" />
            <token id="20" string="&quot;" />
            <token id="21" string="Mr." />
            <token id="22" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="16" string="been sexually abused" type="VP">
          <tokens>
            <token id="32" string="been" />
            <token id="33" string="sexually" />
            <token id="34" string="abused" />
          </tokens>
        </chunking>
        <chunking id="17" string="been sodomized by `` Mr. Ray" type="VP">
          <tokens>
            <token id="17" string="been" />
            <token id="18" string="sodomized" />
            <token id="19" string="by" />
            <token id="20" string="&quot;" />
            <token id="21" string="Mr." />
            <token id="22" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="18" string="he" type="NP">
          <tokens>
            <token id="30" string="he" />
          </tokens>
        </chunking>
        <chunking id="19" string="her son" type="NP">
          <tokens>
            <token id="14" string="her" />
            <token id="15" string="son" />
          </tokens>
        </chunking>
        <chunking id="20" string="sodomized by `` Mr. Ray" type="VP">
          <tokens>
            <token id="18" string="sodomized" />
            <token id="19" string="by" />
            <token id="20" string="&quot;" />
            <token id="21" string="Mr." />
            <token id="22" string="Ray" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">mother</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">mother</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">2 1/2</governor>
          <dependent id="3">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">2 1/2</governor>
          <dependent id="4">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">mother</governor>
          <dependent id="5">2 1/2</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">complained</governor>
          <dependent id="7">year-old</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="2">mother</governor>
          <dependent id="8">complained</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">police</governor>
          <dependent id="9">to</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">police</governor>
          <dependent id="10">Manhattan</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">police</governor>
          <dependent id="11">Beach</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">complained</governor>
          <dependent id="12">police</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="18">sodomized</governor>
          <dependent id="13">that</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="15">son</governor>
          <dependent id="14">her</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="18">sodomized</governor>
          <dependent id="15">son</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="18">sodomized</governor>
          <dependent id="16">had</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="18">sodomized</governor>
          <dependent id="17">been</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="8">complained</governor>
          <dependent id="18">sodomized</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">Ray</governor>
          <dependent id="19">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="22">Ray</governor>
          <dependent id="21">Mr.</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="18">sodomized</governor>
          <dependent id="22">Ray</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="8">complained</governor>
          <dependent id="25">and</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="28">examinations</governor>
          <dependent id="26">subsequent</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="28">examinations</governor>
          <dependent id="27">physical</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="29">indicated</governor>
          <dependent id="28">examinations</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="8">complained</governor>
          <dependent id="29">indicated</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="34">abused</governor>
          <dependent id="30">he</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="34">abused</governor>
          <dependent id="31">had</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="34">abused</governor>
          <dependent id="32">been</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="34">abused</governor>
          <dependent id="33">sexually</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="29">indicated</governor>
          <dependent id="34">abused</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="2 1/2 - year-old" type="DURATION" score="0.0">
          <tokens>
            <token id="5" string="2 1/2" />
            <token id="6" string="-" />
            <token id="7" string="year-old" />
          </tokens>
        </entity>
        <entity id="2" string="Ray" type="PERSON" score="0.0">
          <tokens>
            <token id="22" string="Ray" />
          </tokens>
        </entity>
        <entity id="3" string="Manhattan Beach" type="LOCATION" score="0.0">
          <tokens>
            <token id="10" string="Manhattan" />
            <token id="11" string="Beach" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="82" has_coreference="true">
      <content>Buckey was arrested in September of that year, but was released as the investigation continued and widened to include six other teachers at the Manhattan Beach nursery school, among them his grandmother, dozens of &amp;quot;uncharged suspects,&amp;quot; and eight other South Bay nursery schools.</content>
      <tokens>
        <token id="1" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="arrested" lemma="arrest" stem="arrest" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="September" lemma="September" stem="septemb" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="6" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="that" lemma="that" stem="that" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="year" lemma="year" stem="year" pos="NN" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="true" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="released" lemma="release" stem="releas" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="investigation" lemma="investigation" stem="investig" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="continued" lemma="continue" stem="continu" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="widened" lemma="widen" stem="widen" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="include" lemma="include" stem="includ" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="six" lemma="six" stem="six" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="22" string="other" lemma="other" stem="other" pos="JJ" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="23" string="teachers" lemma="teacher" stem="teacher" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="24" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="26" string="Manhattan" lemma="Manhattan" stem="manhattan" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="27" string="Beach" lemma="Beach" stem="beach" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="28" string="nursery" lemma="nursery" stem="nurseri" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="29" string="school" lemma="school" stem="school" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="30" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="among" lemma="among" stem="among" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="them" lemma="they" stem="them" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="34" string="grandmother" lemma="grandmother" stem="grandmoth" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="35" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="36" string="dozens" lemma="dozen" stem="dozen" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="37" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="38" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="39" string="uncharged" lemma="uncharged" stem="uncharg" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="40" string="suspects" lemma="suspect" stem="suspect" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="41" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="42" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="43" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="44" string="eight" lemma="eight" stem="eight" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="45" string="other" lemma="other" stem="other" pos="JJ" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="46" string="South" lemma="South" stem="south" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="47" string="Bay" lemma="Bay" stem="bai" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="48" string="nursery" lemma="nursery" stem="nurseri" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="49" string="schools" lemma="school" stem="school" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="50" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (NNP Buckey)) (VP (VBD was) (VP (VBN arrested) (PP (IN in) (NP (NP (NNP September)) (PP (IN of) (NP (DT that) (NN year)))))))) (, ,) (CC but) (SINV (VP (VBD was) (VP (VBN released) (SBAR (IN as) (S (NP (DT the) (NN investigation)) (VP (VP (VBD continued)) (CC and) (VP (VBD widened) (S (VP (TO to) (VP (VB include) (NP (CD six) (JJ other) (NNS teachers)) (PP (IN at) (NP (DT the) (NNP Manhattan) (NNP Beach) (NN nursery) (NN school))) (, ,) (PP (IN among) (NP (PRP them)))))))))))) (NP (NP (PRP$ his) (NN grandmother)) (, ,) (NP (NP (NNS dozens)) (PP (IN of) (NP (NP (`` ``) (JJ uncharged) (NNS suspects) (, ,) ('' '')) (CC and) (NP (CD eight) (JJ other) (NNP South) (NNP Bay) (NN nursery) (NNS schools))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="continued and widened to include six other teachers at the Manhattan Beach nursery school , among them" type="VP">
          <tokens>
            <token id="16" string="continued" />
            <token id="17" string="and" />
            <token id="18" string="widened" />
            <token id="19" string="to" />
            <token id="20" string="include" />
            <token id="21" string="six" />
            <token id="22" string="other" />
            <token id="23" string="teachers" />
            <token id="24" string="at" />
            <token id="25" string="the" />
            <token id="26" string="Manhattan" />
            <token id="27" string="Beach" />
            <token id="28" string="nursery" />
            <token id="29" string="school" />
            <token id="30" string="," />
            <token id="31" string="among" />
            <token id="32" string="them" />
          </tokens>
        </chunking>
        <chunking id="2" string="the Manhattan Beach nursery school" type="NP">
          <tokens>
            <token id="25" string="the" />
            <token id="26" string="Manhattan" />
            <token id="27" string="Beach" />
            <token id="28" string="nursery" />
            <token id="29" string="school" />
          </tokens>
        </chunking>
        <chunking id="3" string="to include six other teachers at the Manhattan Beach nursery school , among them" type="VP">
          <tokens>
            <token id="19" string="to" />
            <token id="20" string="include" />
            <token id="21" string="six" />
            <token id="22" string="other" />
            <token id="23" string="teachers" />
            <token id="24" string="at" />
            <token id="25" string="the" />
            <token id="26" string="Manhattan" />
            <token id="27" string="Beach" />
            <token id="28" string="nursery" />
            <token id="29" string="school" />
            <token id="30" string="," />
            <token id="31" string="among" />
            <token id="32" string="them" />
          </tokens>
        </chunking>
        <chunking id="4" string="his grandmother , dozens of `` uncharged suspects , '' and eight other South Bay nursery schools" type="NP">
          <tokens>
            <token id="33" string="his" />
            <token id="34" string="grandmother" />
            <token id="35" string="," />
            <token id="36" string="dozens" />
            <token id="37" string="of" />
            <token id="38" string="&quot;" />
            <token id="39" string="uncharged" />
            <token id="40" string="suspects" />
            <token id="41" string="," />
            <token id="42" string="&quot;" />
            <token id="43" string="and" />
            <token id="44" string="eight" />
            <token id="45" string="other" />
            <token id="46" string="South" />
            <token id="47" string="Bay" />
            <token id="48" string="nursery" />
            <token id="49" string="schools" />
          </tokens>
        </chunking>
        <chunking id="5" string="six other teachers" type="NP">
          <tokens>
            <token id="21" string="six" />
            <token id="22" string="other" />
            <token id="23" string="teachers" />
          </tokens>
        </chunking>
        <chunking id="6" string="continued" type="VP">
          <tokens>
            <token id="16" string="continued" />
          </tokens>
        </chunking>
        <chunking id="7" string="widened to include six other teachers at the Manhattan Beach nursery school , among them" type="VP">
          <tokens>
            <token id="18" string="widened" />
            <token id="19" string="to" />
            <token id="20" string="include" />
            <token id="21" string="six" />
            <token id="22" string="other" />
            <token id="23" string="teachers" />
            <token id="24" string="at" />
            <token id="25" string="the" />
            <token id="26" string="Manhattan" />
            <token id="27" string="Beach" />
            <token id="28" string="nursery" />
            <token id="29" string="school" />
            <token id="30" string="," />
            <token id="31" string="among" />
            <token id="32" string="them" />
          </tokens>
        </chunking>
        <chunking id="8" string="them" type="NP">
          <tokens>
            <token id="32" string="them" />
          </tokens>
        </chunking>
        <chunking id="9" string="Buckey" type="NP">
          <tokens>
            <token id="1" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="10" string="as the investigation continued and widened to include six other teachers at the Manhattan Beach nursery school , among them" type="SBAR">
          <tokens>
            <token id="13" string="as" />
            <token id="14" string="the" />
            <token id="15" string="investigation" />
            <token id="16" string="continued" />
            <token id="17" string="and" />
            <token id="18" string="widened" />
            <token id="19" string="to" />
            <token id="20" string="include" />
            <token id="21" string="six" />
            <token id="22" string="other" />
            <token id="23" string="teachers" />
            <token id="24" string="at" />
            <token id="25" string="the" />
            <token id="26" string="Manhattan" />
            <token id="27" string="Beach" />
            <token id="28" string="nursery" />
            <token id="29" string="school" />
            <token id="30" string="," />
            <token id="31" string="among" />
            <token id="32" string="them" />
          </tokens>
        </chunking>
        <chunking id="11" string="that year" type="NP">
          <tokens>
            <token id="7" string="that" />
            <token id="8" string="year" />
          </tokens>
        </chunking>
        <chunking id="12" string="arrested in September of that year" type="VP">
          <tokens>
            <token id="3" string="arrested" />
            <token id="4" string="in" />
            <token id="5" string="September" />
            <token id="6" string="of" />
            <token id="7" string="that" />
            <token id="8" string="year" />
          </tokens>
        </chunking>
        <chunking id="13" string="dozens of `` uncharged suspects , '' and eight other South Bay nursery schools" type="NP">
          <tokens>
            <token id="36" string="dozens" />
            <token id="37" string="of" />
            <token id="38" string="&quot;" />
            <token id="39" string="uncharged" />
            <token id="40" string="suspects" />
            <token id="41" string="," />
            <token id="42" string="&quot;" />
            <token id="43" string="and" />
            <token id="44" string="eight" />
            <token id="45" string="other" />
            <token id="46" string="South" />
            <token id="47" string="Bay" />
            <token id="48" string="nursery" />
            <token id="49" string="schools" />
          </tokens>
        </chunking>
        <chunking id="14" string="dozens" type="NP">
          <tokens>
            <token id="36" string="dozens" />
          </tokens>
        </chunking>
        <chunking id="15" string="September of that year" type="NP">
          <tokens>
            <token id="5" string="September" />
            <token id="6" string="of" />
            <token id="7" string="that" />
            <token id="8" string="year" />
          </tokens>
        </chunking>
        <chunking id="16" string="September" type="NP">
          <tokens>
            <token id="5" string="September" />
          </tokens>
        </chunking>
        <chunking id="17" string="released as the investigation continued and widened to include six other teachers at the Manhattan Beach nursery school , among them" type="VP">
          <tokens>
            <token id="12" string="released" />
            <token id="13" string="as" />
            <token id="14" string="the" />
            <token id="15" string="investigation" />
            <token id="16" string="continued" />
            <token id="17" string="and" />
            <token id="18" string="widened" />
            <token id="19" string="to" />
            <token id="20" string="include" />
            <token id="21" string="six" />
            <token id="22" string="other" />
            <token id="23" string="teachers" />
            <token id="24" string="at" />
            <token id="25" string="the" />
            <token id="26" string="Manhattan" />
            <token id="27" string="Beach" />
            <token id="28" string="nursery" />
            <token id="29" string="school" />
            <token id="30" string="," />
            <token id="31" string="among" />
            <token id="32" string="them" />
          </tokens>
        </chunking>
        <chunking id="18" string="include six other teachers at the Manhattan Beach nursery school , among them" type="VP">
          <tokens>
            <token id="20" string="include" />
            <token id="21" string="six" />
            <token id="22" string="other" />
            <token id="23" string="teachers" />
            <token id="24" string="at" />
            <token id="25" string="the" />
            <token id="26" string="Manhattan" />
            <token id="27" string="Beach" />
            <token id="28" string="nursery" />
            <token id="29" string="school" />
            <token id="30" string="," />
            <token id="31" string="among" />
            <token id="32" string="them" />
          </tokens>
        </chunking>
        <chunking id="19" string="the investigation" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="investigation" />
          </tokens>
        </chunking>
        <chunking id="20" string="eight other South Bay nursery schools" type="NP">
          <tokens>
            <token id="44" string="eight" />
            <token id="45" string="other" />
            <token id="46" string="South" />
            <token id="47" string="Bay" />
            <token id="48" string="nursery" />
            <token id="49" string="schools" />
          </tokens>
        </chunking>
        <chunking id="21" string="his grandmother" type="NP">
          <tokens>
            <token id="33" string="his" />
            <token id="34" string="grandmother" />
          </tokens>
        </chunking>
        <chunking id="22" string="`` uncharged suspects , '' and eight other South Bay nursery schools" type="NP">
          <tokens>
            <token id="38" string="&quot;" />
            <token id="39" string="uncharged" />
            <token id="40" string="suspects" />
            <token id="41" string="," />
            <token id="42" string="&quot;" />
            <token id="43" string="and" />
            <token id="44" string="eight" />
            <token id="45" string="other" />
            <token id="46" string="South" />
            <token id="47" string="Bay" />
            <token id="48" string="nursery" />
            <token id="49" string="schools" />
          </tokens>
        </chunking>
        <chunking id="23" string="was arrested in September of that year" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="arrested" />
            <token id="4" string="in" />
            <token id="5" string="September" />
            <token id="6" string="of" />
            <token id="7" string="that" />
            <token id="8" string="year" />
          </tokens>
        </chunking>
        <chunking id="24" string="`` uncharged suspects , ''" type="NP">
          <tokens>
            <token id="38" string="&quot;" />
            <token id="39" string="uncharged" />
            <token id="40" string="suspects" />
            <token id="41" string="," />
            <token id="42" string="&quot;" />
          </tokens>
        </chunking>
        <chunking id="25" string="was released as the investigation continued and widened to include six other teachers at the Manhattan Beach nursery school , among them" type="VP">
          <tokens>
            <token id="11" string="was" />
            <token id="12" string="released" />
            <token id="13" string="as" />
            <token id="14" string="the" />
            <token id="15" string="investigation" />
            <token id="16" string="continued" />
            <token id="17" string="and" />
            <token id="18" string="widened" />
            <token id="19" string="to" />
            <token id="20" string="include" />
            <token id="21" string="six" />
            <token id="22" string="other" />
            <token id="23" string="teachers" />
            <token id="24" string="at" />
            <token id="25" string="the" />
            <token id="26" string="Manhattan" />
            <token id="27" string="Beach" />
            <token id="28" string="nursery" />
            <token id="29" string="school" />
            <token id="30" string="," />
            <token id="31" string="among" />
            <token id="32" string="them" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="3">arrested</governor>
          <dependent id="1">Buckey</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="3">arrested</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">arrested</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">September</governor>
          <dependent id="4">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">arrested</governor>
          <dependent id="5">September</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">year</governor>
          <dependent id="6">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">year</governor>
          <dependent id="7">that</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">September</governor>
          <dependent id="8">year</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="3">arrested</governor>
          <dependent id="10">but</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="12">released</governor>
          <dependent id="11">was</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="3">arrested</governor>
          <dependent id="12">released</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="16">continued</governor>
          <dependent id="13">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">investigation</governor>
          <dependent id="14">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">continued</governor>
          <dependent id="15">investigation</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="12">released</governor>
          <dependent id="16">continued</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="16">continued</governor>
          <dependent id="17">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="16">continued</governor>
          <dependent id="18">widened</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="20">include</governor>
          <dependent id="19">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="18">widened</governor>
          <dependent id="20">include</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="23">teachers</governor>
          <dependent id="21">six</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="23">teachers</governor>
          <dependent id="22">other</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="20">include</governor>
          <dependent id="23">teachers</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">school</governor>
          <dependent id="24">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="29">school</governor>
          <dependent id="25">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="29">school</governor>
          <dependent id="26">Manhattan</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="29">school</governor>
          <dependent id="27">Beach</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="29">school</governor>
          <dependent id="28">nursery</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">include</governor>
          <dependent id="29">school</dependent>
        </dependency>
        <dependency type="case">
          <governor id="32">them</governor>
          <dependent id="31">among</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">include</governor>
          <dependent id="32">them</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="34">grandmother</governor>
          <dependent id="33">his</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="12">released</governor>
          <dependent id="34">grandmother</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="34">grandmother</governor>
          <dependent id="36">dozens</dependent>
        </dependency>
        <dependency type="case">
          <governor id="40">suspects</governor>
          <dependent id="37">of</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="40">suspects</governor>
          <dependent id="39">uncharged</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="36">dozens</governor>
          <dependent id="40">suspects</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="40">suspects</governor>
          <dependent id="43">and</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="49">schools</governor>
          <dependent id="44">eight</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="49">schools</governor>
          <dependent id="45">other</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="49">schools</governor>
          <dependent id="46">South</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="49">schools</governor>
          <dependent id="47">Bay</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="49">schools</governor>
          <dependent id="48">nursery</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="40">suspects</governor>
          <dependent id="49">schools</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="six" type="NUMBER" score="0.0">
          <tokens>
            <token id="21" string="six" />
          </tokens>
        </entity>
        <entity id="2" string="South Bay" type="LOCATION" score="0.0">
          <tokens>
            <token id="46" string="South" />
            <token id="47" string="Bay" />
          </tokens>
        </entity>
        <entity id="3" string="Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Buckey" />
          </tokens>
        </entity>
        <entity id="4" string="year" type="DURATION" score="0.0">
          <tokens>
            <token id="8" string="year" />
          </tokens>
        </entity>
        <entity id="5" string="September" type="DATE" score="0.0">
          <tokens>
            <token id="5" string="September" />
          </tokens>
        </entity>
        <entity id="6" string="eight" type="NUMBER" score="0.0">
          <tokens>
            <token id="44" string="eight" />
          </tokens>
        </entity>
        <entity id="7" string="Manhattan Beach" type="LOCATION" score="0.0">
          <tokens>
            <token id="26" string="Manhattan" />
            <token id="27" string="Beach" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="83" has_coreference="true">
      <content>The Manhattan Beach Police Department sent a form letter to hundreds of McMartin school parents asking for information, and a wave of hysteria swept through the affluent beach town.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="Manhattan" lemma="Manhattan" stem="manhattan" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="3" string="Beach" lemma="Beach" stem="beach" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="4" string="Police" lemma="Police" stem="polic" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="5" string="Department" lemma="Department" stem="depart" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="6" string="sent" lemma="send" stem="sent" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="form" lemma="form" stem="form" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="letter" lemma="letter" stem="letter" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="hundreds" lemma="hundred" stem="hundr" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="school" lemma="school" stem="school" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="parents" lemma="parent" stem="parent" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="asking" lemma="ask" stem="ask" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="information" lemma="information" stem="inform" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="wave" lemma="wave" stem="wave" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="hysteria" lemma="hysteria" stem="hysteria" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="swept" lemma="sweep" stem="swept" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="through" lemma="through" stem="through" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="affluent" lemma="affluent" stem="affluent" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="beach" lemma="beach" stem="beach" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="town" lemma="town" stem="town" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (DT The) (NNP Manhattan) (NNP Beach) (NNP Police) (NNP Department)) (VP (VBD sent) (NP (DT a) (NN form) (NN letter)) (PP (TO to) (NP (NP (NNS hundreds)) (PP (IN of) (NP (NNP McMartin) (NN school) (NNS parents))))) (S (VP (VBG asking) (PP (IN for) (NP (NN information))))))) (, ,) (CC and) (S (NP (NP (DT a) (NN wave)) (PP (IN of) (NP (NN hysteria)))) (VP (VBD swept) (PP (IN through) (NP (DT the) (JJ affluent) (NN beach) (NN town))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="McMartin school parents" type="NP">
          <tokens>
            <token id="13" string="McMartin" />
            <token id="14" string="school" />
            <token id="15" string="parents" />
          </tokens>
        </chunking>
        <chunking id="2" string="a form letter" type="NP">
          <tokens>
            <token id="7" string="a" />
            <token id="8" string="form" />
            <token id="9" string="letter" />
          </tokens>
        </chunking>
        <chunking id="3" string="the affluent beach town" type="NP">
          <tokens>
            <token id="27" string="the" />
            <token id="28" string="affluent" />
            <token id="29" string="beach" />
            <token id="30" string="town" />
          </tokens>
        </chunking>
        <chunking id="4" string="hundreds of McMartin school parents" type="NP">
          <tokens>
            <token id="11" string="hundreds" />
            <token id="12" string="of" />
            <token id="13" string="McMartin" />
            <token id="14" string="school" />
            <token id="15" string="parents" />
          </tokens>
        </chunking>
        <chunking id="5" string="sent a form letter to hundreds of McMartin school parents asking for information" type="VP">
          <tokens>
            <token id="6" string="sent" />
            <token id="7" string="a" />
            <token id="8" string="form" />
            <token id="9" string="letter" />
            <token id="10" string="to" />
            <token id="11" string="hundreds" />
            <token id="12" string="of" />
            <token id="13" string="McMartin" />
            <token id="14" string="school" />
            <token id="15" string="parents" />
            <token id="16" string="asking" />
            <token id="17" string="for" />
            <token id="18" string="information" />
          </tokens>
        </chunking>
        <chunking id="6" string="hysteria" type="NP">
          <tokens>
            <token id="24" string="hysteria" />
          </tokens>
        </chunking>
        <chunking id="7" string="swept through the affluent beach town" type="VP">
          <tokens>
            <token id="25" string="swept" />
            <token id="26" string="through" />
            <token id="27" string="the" />
            <token id="28" string="affluent" />
            <token id="29" string="beach" />
            <token id="30" string="town" />
          </tokens>
        </chunking>
        <chunking id="8" string="a wave" type="NP">
          <tokens>
            <token id="21" string="a" />
            <token id="22" string="wave" />
          </tokens>
        </chunking>
        <chunking id="9" string="hundreds" type="NP">
          <tokens>
            <token id="11" string="hundreds" />
          </tokens>
        </chunking>
        <chunking id="10" string="asking for information" type="VP">
          <tokens>
            <token id="16" string="asking" />
            <token id="17" string="for" />
            <token id="18" string="information" />
          </tokens>
        </chunking>
        <chunking id="11" string="a wave of hysteria" type="NP">
          <tokens>
            <token id="21" string="a" />
            <token id="22" string="wave" />
            <token id="23" string="of" />
            <token id="24" string="hysteria" />
          </tokens>
        </chunking>
        <chunking id="12" string="information" type="NP">
          <tokens>
            <token id="18" string="information" />
          </tokens>
        </chunking>
        <chunking id="13" string="The Manhattan Beach Police Department" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Manhattan" />
            <token id="3" string="Beach" />
            <token id="4" string="Police" />
            <token id="5" string="Department" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="5">Department</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">Department</governor>
          <dependent id="2">Manhattan</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">Department</governor>
          <dependent id="3">Beach</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">Department</governor>
          <dependent id="4">Police</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">sent</governor>
          <dependent id="5">Department</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">sent</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">letter</governor>
          <dependent id="7">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">letter</governor>
          <dependent id="8">form</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">sent</governor>
          <dependent id="9">letter</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">hundreds</governor>
          <dependent id="10">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">sent</governor>
          <dependent id="11">hundreds</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">parents</governor>
          <dependent id="12">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">parents</governor>
          <dependent id="13">McMartin</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">parents</governor>
          <dependent id="14">school</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">hundreds</governor>
          <dependent id="15">parents</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="6">sent</governor>
          <dependent id="16">asking</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">information</governor>
          <dependent id="17">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">asking</governor>
          <dependent id="18">information</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="6">sent</governor>
          <dependent id="20">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="22">wave</governor>
          <dependent id="21">a</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="25">swept</governor>
          <dependent id="22">wave</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">hysteria</governor>
          <dependent id="23">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="22">wave</governor>
          <dependent id="24">hysteria</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="6">sent</governor>
          <dependent id="25">swept</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">town</governor>
          <dependent id="26">through</dependent>
        </dependency>
        <dependency type="det">
          <governor id="30">town</governor>
          <dependent id="27">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="30">town</governor>
          <dependent id="28">affluent</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="30">town</governor>
          <dependent id="29">beach</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="25">swept</governor>
          <dependent id="30">town</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Manhattan Beach Police Department" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="2" string="Manhattan" />
            <token id="3" string="Beach" />
            <token id="4" string="Police" />
            <token id="5" string="Department" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="84" has_coreference="true">
      <content>Many parents took their youngsters to Children&amp;apost;s Institute International, a Los Angeles child-abuse diagnostic and treatment center, for evaluation; the majority were told that their children had been victimized.</content>
      <tokens>
        <token id="1" string="Many" lemma="many" stem="mani" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="2" string="parents" lemma="parent" stem="parent" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="took" lemma="take" stem="took" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="youngsters" lemma="youngster" stem="youngster" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="Children" lemma="Children" stem="children" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="8" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="9" string="Institute" lemma="Institute" stem="institut" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="10" string="International" lemma="International" stem="internat" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="Los" lemma="Los" stem="lo" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="14" string="Angeles" lemma="Angeles" stem="angele" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="15" string="child-abuse" lemma="child-abuse" stem="child-abus" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="diagnostic" lemma="diagnostic" stem="diagnost" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="treatment" lemma="treatment" stem="treatment" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="center" lemma="center" stem="center" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="evaluation" lemma="evaluation" stem="evalu" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string=";" lemma=";" stem=";" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="majority" lemma="majority" stem="major" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="told" lemma="tell" stem="told" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="victimized" lemma="victimize" stem="victim" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (JJ Many) (NNS parents)) (VP (VBD took) (NP (PRP$ their) (NNS youngsters)) (PP (TO to) (NP (NP (NNP Children) (POS 's)) (NNP Institute) (NNP International))) (, ,) (NP (NP (DT a) (NNP Los) (NNP Angeles)) (ADJP (JJ child-abuse) (NP-TMP (NP (JJ diagnostic) (CC and) (NN treatment) (NN center)) (, ,) (PP (IN for) (NP (NN evaluation)))))))) (: ;) (S (NP (DT the) (NN majority)) (VP (VBD were) (VP (VBN told) (SBAR (IN that) (S (NP (PRP$ their) (NNS children)) (VP (VBD had) (VP (VBN been) (VP (VBN victimized))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a Los Angeles" type="NP">
          <tokens>
            <token id="12" string="a" />
            <token id="13" string="Los" />
            <token id="14" string="Angeles" />
          </tokens>
        </chunking>
        <chunking id="2" string="were told that their children had been victimized" type="VP">
          <tokens>
            <token id="26" string="were" />
            <token id="27" string="told" />
            <token id="28" string="that" />
            <token id="29" string="their" />
            <token id="30" string="children" />
            <token id="31" string="had" />
            <token id="32" string="been" />
            <token id="33" string="victimized" />
          </tokens>
        </chunking>
        <chunking id="3" string="their youngsters" type="NP">
          <tokens>
            <token id="4" string="their" />
            <token id="5" string="youngsters" />
          </tokens>
        </chunking>
        <chunking id="4" string="that their children had been victimized" type="SBAR">
          <tokens>
            <token id="28" string="that" />
            <token id="29" string="their" />
            <token id="30" string="children" />
            <token id="31" string="had" />
            <token id="32" string="been" />
            <token id="33" string="victimized" />
          </tokens>
        </chunking>
        <chunking id="5" string="diagnostic and treatment center" type="NP">
          <tokens>
            <token id="16" string="diagnostic" />
            <token id="17" string="and" />
            <token id="18" string="treatment" />
            <token id="19" string="center" />
          </tokens>
        </chunking>
        <chunking id="6" string="been victimized" type="VP">
          <tokens>
            <token id="32" string="been" />
            <token id="33" string="victimized" />
          </tokens>
        </chunking>
        <chunking id="7" string="victimized" type="VP">
          <tokens>
            <token id="33" string="victimized" />
          </tokens>
        </chunking>
        <chunking id="8" string="a Los Angeles child-abuse diagnostic and treatment center , for evaluation" type="NP">
          <tokens>
            <token id="12" string="a" />
            <token id="13" string="Los" />
            <token id="14" string="Angeles" />
            <token id="15" string="child-abuse" />
            <token id="16" string="diagnostic" />
            <token id="17" string="and" />
            <token id="18" string="treatment" />
            <token id="19" string="center" />
            <token id="20" string="," />
            <token id="21" string="for" />
            <token id="22" string="evaluation" />
          </tokens>
        </chunking>
        <chunking id="9" string="their children" type="NP">
          <tokens>
            <token id="29" string="their" />
            <token id="30" string="children" />
          </tokens>
        </chunking>
        <chunking id="10" string="Children 's Institute International" type="NP">
          <tokens>
            <token id="7" string="Children" />
            <token id="8" string="'s" />
            <token id="9" string="Institute" />
            <token id="10" string="International" />
          </tokens>
        </chunking>
        <chunking id="11" string="told that their children had been victimized" type="VP">
          <tokens>
            <token id="27" string="told" />
            <token id="28" string="that" />
            <token id="29" string="their" />
            <token id="30" string="children" />
            <token id="31" string="had" />
            <token id="32" string="been" />
            <token id="33" string="victimized" />
          </tokens>
        </chunking>
        <chunking id="12" string="had been victimized" type="VP">
          <tokens>
            <token id="31" string="had" />
            <token id="32" string="been" />
            <token id="33" string="victimized" />
          </tokens>
        </chunking>
        <chunking id="13" string="Children 's" type="NP">
          <tokens>
            <token id="7" string="Children" />
            <token id="8" string="'s" />
          </tokens>
        </chunking>
        <chunking id="14" string="evaluation" type="NP">
          <tokens>
            <token id="22" string="evaluation" />
          </tokens>
        </chunking>
        <chunking id="15" string="child-abuse diagnostic and treatment center , for evaluation" type="ADJP">
          <tokens>
            <token id="15" string="child-abuse" />
            <token id="16" string="diagnostic" />
            <token id="17" string="and" />
            <token id="18" string="treatment" />
            <token id="19" string="center" />
            <token id="20" string="," />
            <token id="21" string="for" />
            <token id="22" string="evaluation" />
          </tokens>
        </chunking>
        <chunking id="16" string="took their youngsters to Children 's Institute International , a Los Angeles child-abuse diagnostic and treatment center , for evaluation" type="VP">
          <tokens>
            <token id="3" string="took" />
            <token id="4" string="their" />
            <token id="5" string="youngsters" />
            <token id="6" string="to" />
            <token id="7" string="Children" />
            <token id="8" string="'s" />
            <token id="9" string="Institute" />
            <token id="10" string="International" />
            <token id="11" string="," />
            <token id="12" string="a" />
            <token id="13" string="Los" />
            <token id="14" string="Angeles" />
            <token id="15" string="child-abuse" />
            <token id="16" string="diagnostic" />
            <token id="17" string="and" />
            <token id="18" string="treatment" />
            <token id="19" string="center" />
            <token id="20" string="," />
            <token id="21" string="for" />
            <token id="22" string="evaluation" />
          </tokens>
        </chunking>
        <chunking id="17" string="the majority" type="NP">
          <tokens>
            <token id="24" string="the" />
            <token id="25" string="majority" />
          </tokens>
        </chunking>
        <chunking id="18" string="Many parents" type="NP">
          <tokens>
            <token id="1" string="Many" />
            <token id="2" string="parents" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="amod">
          <governor id="2">parents</governor>
          <dependent id="1">Many</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">took</governor>
          <dependent id="2">parents</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">took</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="5">youngsters</governor>
          <dependent id="4">their</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">took</governor>
          <dependent id="5">youngsters</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">International</governor>
          <dependent id="6">to</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="10">International</governor>
          <dependent id="7">Children</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">Children</governor>
          <dependent id="8">'s</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">International</governor>
          <dependent id="9">Institute</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">took</governor>
          <dependent id="10">International</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">Angeles</governor>
          <dependent id="12">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">Angeles</governor>
          <dependent id="13">Los</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">took</governor>
          <dependent id="14">Angeles</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="14">Angeles</governor>
          <dependent id="15">child-abuse</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">center</governor>
          <dependent id="16">diagnostic</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="16">diagnostic</governor>
          <dependent id="17">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="16">diagnostic</governor>
          <dependent id="18">treatment</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="15">child-abuse</governor>
          <dependent id="19">center</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">evaluation</governor>
          <dependent id="21">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">center</governor>
          <dependent id="22">evaluation</dependent>
        </dependency>
        <dependency type="det">
          <governor id="25">majority</governor>
          <dependent id="24">the</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="27">told</governor>
          <dependent id="25">majority</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="27">told</governor>
          <dependent id="26">were</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="3">took</governor>
          <dependent id="27">told</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="33">victimized</governor>
          <dependent id="28">that</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="30">children</governor>
          <dependent id="29">their</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="33">victimized</governor>
          <dependent id="30">children</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="33">victimized</governor>
          <dependent id="31">had</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="33">victimized</governor>
          <dependent id="32">been</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="27">told</governor>
          <dependent id="33">victimized</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Children 's Institute International" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="7" string="Children" />
            <token id="8" string="'s" />
            <token id="9" string="Institute" />
            <token id="10" string="International" />
          </tokens>
        </entity>
        <entity id="2" string="Los Angeles" type="LOCATION" score="0.0">
          <tokens>
            <token id="13" string="Los" />
            <token id="14" string="Angeles" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="85" has_coreference="true">
      <content>In March, 1984, Ray Buckey was rearrested, along with his sister, Peggy Ann Buckey, his mother, Peggy McMartin Buckey, his grandmother, Virginia McMartin, and teachers Betty Raidor, Babette Spitler and Mary Ann Jackson.</content>
      <tokens>
        <token id="1" string="In" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="March" lemma="March" stem="march" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1984" lemma="1984" stem="1984" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="7" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="8" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="rearrested" lemma="rearrest" stem="rearrest" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="along" lemma="along" stem="along" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="14" string="sister" lemma="sister" stem="sister" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="16" string="Peggy" lemma="Peggy" stem="peggi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="17" string="Ann" lemma="Ann" stem="ann" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="18" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="20" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="21" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="22" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="23" string="Peggy" lemma="Peggy" stem="peggi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="24" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="25" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="26" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="27" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="28" string="grandmother" lemma="grandmother" stem="grandmoth" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="29" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="30" string="Virginia" lemma="Virginia" stem="virginia" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="31" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="32" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="33" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="34" string="teachers" lemma="teacher" stem="teacher" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="35" string="Betty" lemma="Betty" stem="betti" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="36" string="Raidor" lemma="Raidor" stem="raidor" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="37" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="Babette" lemma="Babette" stem="babett" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="39" string="Spitler" lemma="Spitler" stem="spitler" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="40" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="41" string="Mary" lemma="Mary" stem="mari" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="42" string="Ann" lemma="Ann" stem="ann" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="43" string="Jackson" lemma="Jackson" stem="jackson" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="44" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (IN In) (NP (NP (NNP March)) (, ,) (NP (CD 1984)))) (, ,) (NP (NNP Ray) (NNP Buckey)) (VP (VBD was) (VP (VBN rearrested) (PRN (, ,) (ADVP (IN along) (PP (IN with) (NP (NP (PRP$ his) (NN sister)) (, ,) (NP (NNP Peggy) (NNP Ann) (NNP Buckey)) (, ,) (NP (PRP$ his) (NN mother)) (, ,) (NP (NNP Peggy) (NNP McMartin) (NNP Buckey)) (, ,) (NP (PRP$ his) (NN grandmother)) (, ,) (NP (NNP Virginia) (NNP McMartin)) (, ,) (CC and) (NP (NNS teachers))))) (NP (NP (NNP Betty) (NNP Raidor)) (, ,) (NP (NNP Babette) (NNP Spitler)) (CC and) (NP (NNP Mary) (NNP Ann) (NNP Jackson)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="his sister" type="NP">
          <tokens>
            <token id="13" string="his" />
            <token id="14" string="sister" />
          </tokens>
        </chunking>
        <chunking id="2" string="Peggy McMartin Buckey" type="NP">
          <tokens>
            <token id="23" string="Peggy" />
            <token id="24" string="McMartin" />
            <token id="25" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="3" string="Mary Ann Jackson" type="NP">
          <tokens>
            <token id="41" string="Mary" />
            <token id="42" string="Ann" />
            <token id="43" string="Jackson" />
          </tokens>
        </chunking>
        <chunking id="4" string="March , 1984" type="NP">
          <tokens>
            <token id="2" string="March" />
            <token id="3" string="," />
            <token id="4" string="1984" />
          </tokens>
        </chunking>
        <chunking id="5" string="Virginia McMartin" type="NP">
          <tokens>
            <token id="30" string="Virginia" />
            <token id="31" string="McMartin" />
          </tokens>
        </chunking>
        <chunking id="6" string="March" type="NP">
          <tokens>
            <token id="2" string="March" />
          </tokens>
        </chunking>
        <chunking id="7" string="Betty Raidor , Babette Spitler and Mary Ann Jackson" type="NP">
          <tokens>
            <token id="35" string="Betty" />
            <token id="36" string="Raidor" />
            <token id="37" string="," />
            <token id="38" string="Babette" />
            <token id="39" string="Spitler" />
            <token id="40" string="and" />
            <token id="41" string="Mary" />
            <token id="42" string="Ann" />
            <token id="43" string="Jackson" />
          </tokens>
        </chunking>
        <chunking id="8" string="Ray Buckey" type="NP">
          <tokens>
            <token id="6" string="Ray" />
            <token id="7" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="9" string="Betty Raidor" type="NP">
          <tokens>
            <token id="35" string="Betty" />
            <token id="36" string="Raidor" />
          </tokens>
        </chunking>
        <chunking id="10" string="was rearrested , along with his sister , Peggy Ann Buckey , his mother , Peggy McMartin Buckey , his grandmother , Virginia McMartin , and teachers Betty Raidor , Babette Spitler and Mary Ann Jackson" type="VP">
          <tokens>
            <token id="8" string="was" />
            <token id="9" string="rearrested" />
            <token id="10" string="," />
            <token id="11" string="along" />
            <token id="12" string="with" />
            <token id="13" string="his" />
            <token id="14" string="sister" />
            <token id="15" string="," />
            <token id="16" string="Peggy" />
            <token id="17" string="Ann" />
            <token id="18" string="Buckey" />
            <token id="19" string="," />
            <token id="20" string="his" />
            <token id="21" string="mother" />
            <token id="22" string="," />
            <token id="23" string="Peggy" />
            <token id="24" string="McMartin" />
            <token id="25" string="Buckey" />
            <token id="26" string="," />
            <token id="27" string="his" />
            <token id="28" string="grandmother" />
            <token id="29" string="," />
            <token id="30" string="Virginia" />
            <token id="31" string="McMartin" />
            <token id="32" string="," />
            <token id="33" string="and" />
            <token id="34" string="teachers" />
            <token id="35" string="Betty" />
            <token id="36" string="Raidor" />
            <token id="37" string="," />
            <token id="38" string="Babette" />
            <token id="39" string="Spitler" />
            <token id="40" string="and" />
            <token id="41" string="Mary" />
            <token id="42" string="Ann" />
            <token id="43" string="Jackson" />
          </tokens>
        </chunking>
        <chunking id="11" string="1984" type="NP">
          <tokens>
            <token id="4" string="1984" />
          </tokens>
        </chunking>
        <chunking id="12" string="his sister , Peggy Ann Buckey , his mother , Peggy McMartin Buckey , his grandmother , Virginia McMartin , and teachers" type="NP">
          <tokens>
            <token id="13" string="his" />
            <token id="14" string="sister" />
            <token id="15" string="," />
            <token id="16" string="Peggy" />
            <token id="17" string="Ann" />
            <token id="18" string="Buckey" />
            <token id="19" string="," />
            <token id="20" string="his" />
            <token id="21" string="mother" />
            <token id="22" string="," />
            <token id="23" string="Peggy" />
            <token id="24" string="McMartin" />
            <token id="25" string="Buckey" />
            <token id="26" string="," />
            <token id="27" string="his" />
            <token id="28" string="grandmother" />
            <token id="29" string="," />
            <token id="30" string="Virginia" />
            <token id="31" string="McMartin" />
            <token id="32" string="," />
            <token id="33" string="and" />
            <token id="34" string="teachers" />
          </tokens>
        </chunking>
        <chunking id="13" string="his grandmother" type="NP">
          <tokens>
            <token id="27" string="his" />
            <token id="28" string="grandmother" />
          </tokens>
        </chunking>
        <chunking id="14" string="Peggy Ann Buckey" type="NP">
          <tokens>
            <token id="16" string="Peggy" />
            <token id="17" string="Ann" />
            <token id="18" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="15" string="teachers" type="NP">
          <tokens>
            <token id="34" string="teachers" />
          </tokens>
        </chunking>
        <chunking id="16" string="his mother" type="NP">
          <tokens>
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
        <chunking id="17" string="Babette Spitler" type="NP">
          <tokens>
            <token id="38" string="Babette" />
            <token id="39" string="Spitler" />
          </tokens>
        </chunking>
        <chunking id="18" string="rearrested , along with his sister , Peggy Ann Buckey , his mother , Peggy McMartin Buckey , his grandmother , Virginia McMartin , and teachers Betty Raidor , Babette Spitler and Mary Ann Jackson" type="VP">
          <tokens>
            <token id="9" string="rearrested" />
            <token id="10" string="," />
            <token id="11" string="along" />
            <token id="12" string="with" />
            <token id="13" string="his" />
            <token id="14" string="sister" />
            <token id="15" string="," />
            <token id="16" string="Peggy" />
            <token id="17" string="Ann" />
            <token id="18" string="Buckey" />
            <token id="19" string="," />
            <token id="20" string="his" />
            <token id="21" string="mother" />
            <token id="22" string="," />
            <token id="23" string="Peggy" />
            <token id="24" string="McMartin" />
            <token id="25" string="Buckey" />
            <token id="26" string="," />
            <token id="27" string="his" />
            <token id="28" string="grandmother" />
            <token id="29" string="," />
            <token id="30" string="Virginia" />
            <token id="31" string="McMartin" />
            <token id="32" string="," />
            <token id="33" string="and" />
            <token id="34" string="teachers" />
            <token id="35" string="Betty" />
            <token id="36" string="Raidor" />
            <token id="37" string="," />
            <token id="38" string="Babette" />
            <token id="39" string="Spitler" />
            <token id="40" string="and" />
            <token id="41" string="Mary" />
            <token id="42" string="Ann" />
            <token id="43" string="Jackson" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="case">
          <governor id="2">March</governor>
          <dependent id="1">In</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">rearrested</governor>
          <dependent id="2">March</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="2">March</governor>
          <dependent id="4">1984</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">Buckey</governor>
          <dependent id="6">Ray</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="9">rearrested</governor>
          <dependent id="7">Buckey</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="9">rearrested</governor>
          <dependent id="8">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="9">rearrested</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="36">Raidor</governor>
          <dependent id="11">along</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">sister</governor>
          <dependent id="12">with</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="14">sister</governor>
          <dependent id="13">his</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">along</governor>
          <dependent id="14">sister</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="18">Buckey</governor>
          <dependent id="16">Peggy</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="18">Buckey</governor>
          <dependent id="17">Ann</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">sister</governor>
          <dependent id="18">Buckey</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="21">mother</governor>
          <dependent id="20">his</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">sister</governor>
          <dependent id="21">mother</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">Buckey</governor>
          <dependent id="23">Peggy</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">Buckey</governor>
          <dependent id="24">McMartin</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">sister</governor>
          <dependent id="25">Buckey</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="28">grandmother</governor>
          <dependent id="27">his</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">sister</governor>
          <dependent id="28">grandmother</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="31">McMartin</governor>
          <dependent id="30">Virginia</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">sister</governor>
          <dependent id="31">McMartin</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="14">sister</governor>
          <dependent id="33">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">sister</governor>
          <dependent id="34">teachers</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="36">Raidor</governor>
          <dependent id="35">Betty</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="9">rearrested</governor>
          <dependent id="36">Raidor</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="39">Spitler</governor>
          <dependent id="38">Babette</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="36">Raidor</governor>
          <dependent id="39">Spitler</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="36">Raidor</governor>
          <dependent id="40">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="43">Jackson</governor>
          <dependent id="41">Mary</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="43">Jackson</governor>
          <dependent id="42">Ann</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="36">Raidor</governor>
          <dependent id="43">Jackson</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Peggy McMartin Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="23" string="Peggy" />
            <token id="24" string="McMartin" />
            <token id="25" string="Buckey" />
          </tokens>
        </entity>
        <entity id="2" string="Mary Ann Jackson" type="PERSON" score="0.0">
          <tokens>
            <token id="41" string="Mary" />
            <token id="42" string="Ann" />
            <token id="43" string="Jackson" />
          </tokens>
        </entity>
        <entity id="3" string="Peggy Ann Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="16" string="Peggy" />
            <token id="17" string="Ann" />
            <token id="18" string="Buckey" />
          </tokens>
        </entity>
        <entity id="4" string="March , 1984" type="DATE" score="0.0">
          <tokens>
            <token id="2" string="March" />
            <token id="3" string="," />
            <token id="4" string="1984" />
          </tokens>
        </entity>
        <entity id="5" string="Virginia McMartin" type="PERSON" score="0.0">
          <tokens>
            <token id="30" string="Virginia" />
            <token id="31" string="McMartin" />
          </tokens>
        </entity>
        <entity id="6" string="Babette Spitler" type="PERSON" score="0.0">
          <tokens>
            <token id="38" string="Babette" />
            <token id="39" string="Spitler" />
          </tokens>
        </entity>
        <entity id="7" string="Ray Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Ray" />
            <token id="7" string="Buckey" />
          </tokens>
        </entity>
        <entity id="8" string="Betty Raidor" type="PERSON" score="0.0">
          <tokens>
            <token id="35" string="Betty" />
            <token id="36" string="Raidor" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="86" has_coreference="true">
      <content>A county grand jury had indicted them on 115 counts; that indictment was superseded in May by a criminal complaint charging them with 208 counts involving 41 children.</content>
      <tokens>
        <token id="1" string="A" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="county" lemma="county" stem="counti" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="grand" lemma="grand" stem="grand" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="indicted" lemma="indict" stem="indict" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="them" lemma="they" stem="them" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="115" lemma="115" stem="115" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="10" string="counts" lemma="count" stem="count" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string=";" lemma=";" stem=";" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="indictment" lemma="indictment" stem="indict" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="superseded" lemma="supersede" stem="supersed" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="May" lemma="May" stem="mai" pos="NNP" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="18" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="criminal" lemma="criminal" stem="crimin" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="complaint" lemma="complaint" stem="complaint" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="charging" lemma="charge" stem="charg" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="them" lemma="they" stem="them" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="24" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="208" lemma="208" stem="208" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="26" string="counts" lemma="count" stem="count" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="involving" lemma="involve" stem="involv" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="41" lemma="41" stem="41" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="29" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT A) (NN county) (JJ grand) (NN jury)) (VP (VBD had) (VP (VBN indicted) (NP (PRP them)) (PP (IN on) (NP (NP (CD 115) (NNS counts)) (: ;) (SBAR (IN that) (S (NP (NN indictment)) (VP (VBD was) (VP (VBN superseded) (PP (IN in) (NP (NNP May))) (PP (IN by) (NP (NP (DT a) (JJ criminal) (NN complaint)) (VP (VBG charging) (NP (PRP them)) (PP (IN with) (NP (NP (CD 208) (NNS counts)) (VP (VBG involving) (NP (CD 41) (NNS children)))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="charging them with 208 counts involving 41 children" type="VP">
          <tokens>
            <token id="22" string="charging" />
            <token id="23" string="them" />
            <token id="24" string="with" />
            <token id="25" string="208" />
            <token id="26" string="counts" />
            <token id="27" string="involving" />
            <token id="28" string="41" />
            <token id="29" string="children" />
          </tokens>
        </chunking>
        <chunking id="2" string="208 counts involving 41 children" type="NP">
          <tokens>
            <token id="25" string="208" />
            <token id="26" string="counts" />
            <token id="27" string="involving" />
            <token id="28" string="41" />
            <token id="29" string="children" />
          </tokens>
        </chunking>
        <chunking id="3" string="May" type="NP">
          <tokens>
            <token id="17" string="May" />
          </tokens>
        </chunking>
        <chunking id="4" string="that indictment was superseded in May by a criminal complaint charging them with 208 counts involving 41 children" type="SBAR">
          <tokens>
            <token id="12" string="that" />
            <token id="13" string="indictment" />
            <token id="14" string="was" />
            <token id="15" string="superseded" />
            <token id="16" string="in" />
            <token id="17" string="May" />
            <token id="18" string="by" />
            <token id="19" string="a" />
            <token id="20" string="criminal" />
            <token id="21" string="complaint" />
            <token id="22" string="charging" />
            <token id="23" string="them" />
            <token id="24" string="with" />
            <token id="25" string="208" />
            <token id="26" string="counts" />
            <token id="27" string="involving" />
            <token id="28" string="41" />
            <token id="29" string="children" />
          </tokens>
        </chunking>
        <chunking id="5" string="a criminal complaint" type="NP">
          <tokens>
            <token id="19" string="a" />
            <token id="20" string="criminal" />
            <token id="21" string="complaint" />
          </tokens>
        </chunking>
        <chunking id="6" string="indicted them on 115 counts ; that indictment was superseded in May by a criminal complaint charging them with 208 counts involving 41 children" type="VP">
          <tokens>
            <token id="6" string="indicted" />
            <token id="7" string="them" />
            <token id="8" string="on" />
            <token id="9" string="115" />
            <token id="10" string="counts" />
            <token id="11" string=";" />
            <token id="12" string="that" />
            <token id="13" string="indictment" />
            <token id="14" string="was" />
            <token id="15" string="superseded" />
            <token id="16" string="in" />
            <token id="17" string="May" />
            <token id="18" string="by" />
            <token id="19" string="a" />
            <token id="20" string="criminal" />
            <token id="21" string="complaint" />
            <token id="22" string="charging" />
            <token id="23" string="them" />
            <token id="24" string="with" />
            <token id="25" string="208" />
            <token id="26" string="counts" />
            <token id="27" string="involving" />
            <token id="28" string="41" />
            <token id="29" string="children" />
          </tokens>
        </chunking>
        <chunking id="7" string="indictment" type="NP">
          <tokens>
            <token id="13" string="indictment" />
          </tokens>
        </chunking>
        <chunking id="8" string="superseded in May by a criminal complaint charging them with 208 counts involving 41 children" type="VP">
          <tokens>
            <token id="15" string="superseded" />
            <token id="16" string="in" />
            <token id="17" string="May" />
            <token id="18" string="by" />
            <token id="19" string="a" />
            <token id="20" string="criminal" />
            <token id="21" string="complaint" />
            <token id="22" string="charging" />
            <token id="23" string="them" />
            <token id="24" string="with" />
            <token id="25" string="208" />
            <token id="26" string="counts" />
            <token id="27" string="involving" />
            <token id="28" string="41" />
            <token id="29" string="children" />
          </tokens>
        </chunking>
        <chunking id="9" string="them" type="NP">
          <tokens>
            <token id="7" string="them" />
          </tokens>
        </chunking>
        <chunking id="10" string="208 counts" type="NP">
          <tokens>
            <token id="25" string="208" />
            <token id="26" string="counts" />
          </tokens>
        </chunking>
        <chunking id="11" string="A county grand jury" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="county" />
            <token id="3" string="grand" />
            <token id="4" string="jury" />
          </tokens>
        </chunking>
        <chunking id="12" string="a criminal complaint charging them with 208 counts involving 41 children" type="NP">
          <tokens>
            <token id="19" string="a" />
            <token id="20" string="criminal" />
            <token id="21" string="complaint" />
            <token id="22" string="charging" />
            <token id="23" string="them" />
            <token id="24" string="with" />
            <token id="25" string="208" />
            <token id="26" string="counts" />
            <token id="27" string="involving" />
            <token id="28" string="41" />
            <token id="29" string="children" />
          </tokens>
        </chunking>
        <chunking id="13" string="involving 41 children" type="VP">
          <tokens>
            <token id="27" string="involving" />
            <token id="28" string="41" />
            <token id="29" string="children" />
          </tokens>
        </chunking>
        <chunking id="14" string="115 counts ; that indictment was superseded in May by a criminal complaint charging them with 208 counts involving 41 children" type="NP">
          <tokens>
            <token id="9" string="115" />
            <token id="10" string="counts" />
            <token id="11" string=";" />
            <token id="12" string="that" />
            <token id="13" string="indictment" />
            <token id="14" string="was" />
            <token id="15" string="superseded" />
            <token id="16" string="in" />
            <token id="17" string="May" />
            <token id="18" string="by" />
            <token id="19" string="a" />
            <token id="20" string="criminal" />
            <token id="21" string="complaint" />
            <token id="22" string="charging" />
            <token id="23" string="them" />
            <token id="24" string="with" />
            <token id="25" string="208" />
            <token id="26" string="counts" />
            <token id="27" string="involving" />
            <token id="28" string="41" />
            <token id="29" string="children" />
          </tokens>
        </chunking>
        <chunking id="15" string="had indicted them on 115 counts ; that indictment was superseded in May by a criminal complaint charging them with 208 counts involving 41 children" type="VP">
          <tokens>
            <token id="5" string="had" />
            <token id="6" string="indicted" />
            <token id="7" string="them" />
            <token id="8" string="on" />
            <token id="9" string="115" />
            <token id="10" string="counts" />
            <token id="11" string=";" />
            <token id="12" string="that" />
            <token id="13" string="indictment" />
            <token id="14" string="was" />
            <token id="15" string="superseded" />
            <token id="16" string="in" />
            <token id="17" string="May" />
            <token id="18" string="by" />
            <token id="19" string="a" />
            <token id="20" string="criminal" />
            <token id="21" string="complaint" />
            <token id="22" string="charging" />
            <token id="23" string="them" />
            <token id="24" string="with" />
            <token id="25" string="208" />
            <token id="26" string="counts" />
            <token id="27" string="involving" />
            <token id="28" string="41" />
            <token id="29" string="children" />
          </tokens>
        </chunking>
        <chunking id="16" string="was superseded in May by a criminal complaint charging them with 208 counts involving 41 children" type="VP">
          <tokens>
            <token id="14" string="was" />
            <token id="15" string="superseded" />
            <token id="16" string="in" />
            <token id="17" string="May" />
            <token id="18" string="by" />
            <token id="19" string="a" />
            <token id="20" string="criminal" />
            <token id="21" string="complaint" />
            <token id="22" string="charging" />
            <token id="23" string="them" />
            <token id="24" string="with" />
            <token id="25" string="208" />
            <token id="26" string="counts" />
            <token id="27" string="involving" />
            <token id="28" string="41" />
            <token id="29" string="children" />
          </tokens>
        </chunking>
        <chunking id="17" string="41 children" type="NP">
          <tokens>
            <token id="28" string="41" />
            <token id="29" string="children" />
          </tokens>
        </chunking>
        <chunking id="18" string="115 counts" type="NP">
          <tokens>
            <token id="9" string="115" />
            <token id="10" string="counts" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="4">jury</governor>
          <dependent id="1">A</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">jury</governor>
          <dependent id="2">county</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="4">jury</governor>
          <dependent id="3">grand</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">indicted</governor>
          <dependent id="4">jury</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="6">indicted</governor>
          <dependent id="5">had</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">indicted</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">indicted</governor>
          <dependent id="7">them</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">counts</governor>
          <dependent id="8">on</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="10">counts</governor>
          <dependent id="9">115</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">indicted</governor>
          <dependent id="10">counts</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="15">superseded</governor>
          <dependent id="12">that</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="15">superseded</governor>
          <dependent id="13">indictment</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="15">superseded</governor>
          <dependent id="14">was</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="10">counts</governor>
          <dependent id="15">superseded</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">May</governor>
          <dependent id="16">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">superseded</governor>
          <dependent id="17">May</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">complaint</governor>
          <dependent id="18">by</dependent>
        </dependency>
        <dependency type="det">
          <governor id="21">complaint</governor>
          <dependent id="19">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="21">complaint</governor>
          <dependent id="20">criminal</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">superseded</governor>
          <dependent id="21">complaint</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="21">complaint</governor>
          <dependent id="22">charging</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="22">charging</governor>
          <dependent id="23">them</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">counts</governor>
          <dependent id="24">with</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="26">counts</governor>
          <dependent id="25">208</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="22">charging</governor>
          <dependent id="26">counts</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="26">counts</governor>
          <dependent id="27">involving</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="29">children</governor>
          <dependent id="28">41</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="27">involving</governor>
          <dependent id="29">children</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="115" type="NUMBER" score="0.0">
          <tokens>
            <token id="9" string="115" />
          </tokens>
        </entity>
        <entity id="2" string="May" type="DATE" score="0.0">
          <tokens>
            <token id="17" string="May" />
          </tokens>
        </entity>
        <entity id="3" string="208" type="NUMBER" score="0.0">
          <tokens>
            <token id="25" string="208" />
          </tokens>
        </entity>
        <entity id="4" string="41" type="NUMBER" score="0.0">
          <tokens>
            <token id="28" string="41" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="87" has_coreference="true">
      <content>The case was hastily filed, and it was flawed from the beginning.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="hastily" lemma="hastily" stem="hastili" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="filed" lemma="file" stem="file" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="flawed" lemma="flaw" stem="flaw" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="beginning" lemma="beginning" stem="begin" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (DT The) (NN case)) (VP (VBD was) (VP (ADVP (RB hastily)) (VBN filed)))) (, ,) (CC and) (S (NP (PRP it)) (VP (VBD was) (VP (VBN flawed) (PP (IN from) (NP (DT the) (NN beginning)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the beginning" type="NP">
          <tokens>
            <token id="12" string="the" />
            <token id="13" string="beginning" />
          </tokens>
        </chunking>
        <chunking id="2" string="flawed from the beginning" type="VP">
          <tokens>
            <token id="10" string="flawed" />
            <token id="11" string="from" />
            <token id="12" string="the" />
            <token id="13" string="beginning" />
          </tokens>
        </chunking>
        <chunking id="3" string="was hastily filed" type="VP">
          <tokens>
            <token id="3" string="was" />
            <token id="4" string="hastily" />
            <token id="5" string="filed" />
          </tokens>
        </chunking>
        <chunking id="4" string="hastily filed" type="VP">
          <tokens>
            <token id="4" string="hastily" />
            <token id="5" string="filed" />
          </tokens>
        </chunking>
        <chunking id="5" string="it" type="NP">
          <tokens>
            <token id="8" string="it" />
          </tokens>
        </chunking>
        <chunking id="6" string="The case" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="case" />
          </tokens>
        </chunking>
        <chunking id="7" string="was flawed from the beginning" type="VP">
          <tokens>
            <token id="9" string="was" />
            <token id="10" string="flawed" />
            <token id="11" string="from" />
            <token id="12" string="the" />
            <token id="13" string="beginning" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">case</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="5">filed</governor>
          <dependent id="2">case</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="5">filed</governor>
          <dependent id="3">was</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">filed</governor>
          <dependent id="4">hastily</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">filed</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="5">filed</governor>
          <dependent id="7">and</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="10">flawed</governor>
          <dependent id="8">it</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="10">flawed</governor>
          <dependent id="9">was</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="5">filed</governor>
          <dependent id="10">flawed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">beginning</governor>
          <dependent id="11">from</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">beginning</governor>
          <dependent id="12">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">flawed</governor>
          <dependent id="13">beginning</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="88" has_coreference="false">
      <content>Then-Dist.</content>
      <tokens>
        <token id="1" string="Then-Dist" lemma="Then-Dist" stem="then-dist" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (FRAG (NP (NNP Then-Dist)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Then-Dist" type="NP">
          <tokens>
            <token id="1" string="Then-Dist" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Then-Dist</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="89" has_coreference="true">
      <content>Atty. Robert Philibosian, running for reelection, and the media, which caught wind of the investigation, were pressing for action.</content>
      <tokens>
        <token id="1" string="Atty." lemma="Atty." stem="atty." pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="Robert" lemma="Robert" stem="robert" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="3" string="Philibosian" lemma="Philibosian" stem="philibosian" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="running" lemma="run" stem="run" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="reelection" lemma="reelection" stem="reelect" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="media" lemma="media" stem="media" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="which" lemma="which" stem="which" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="caught" lemma="catch" stem="caught" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="wind" lemma="wind" stem="wind" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="investigation" lemma="investigation" stem="investig" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="pressing" lemma="press" stem="press" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="action" lemma="action" stem="action" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Atty.) (NNP Robert) (NNP Philibosian)) (, ,) (VP (VBG running) (PP (IN for) (NP (NP (NP (NN reelection)) (, ,) (CC and) (NP (DT the) (NNS media)) (, ,)) (SBAR (WHNP (WDT which)) (S (VP (VBD caught) (NP (NP (NN wind)) (PP (IN of) (NP (DT the) (NN investigation)))))))))) (, ,)) (VP (VBD were) (VP (VBG pressing) (PP (IN for) (NP (NN action))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Atty. Robert Philibosian , running for reelection , and the media , which caught wind of the investigation ," type="NP">
          <tokens>
            <token id="1" string="Atty." />
            <token id="2" string="Robert" />
            <token id="3" string="Philibosian" />
            <token id="4" string="," />
            <token id="5" string="running" />
            <token id="6" string="for" />
            <token id="7" string="reelection" />
            <token id="8" string="," />
            <token id="9" string="and" />
            <token id="10" string="the" />
            <token id="11" string="media" />
            <token id="12" string="," />
            <token id="13" string="which" />
            <token id="14" string="caught" />
            <token id="15" string="wind" />
            <token id="16" string="of" />
            <token id="17" string="the" />
            <token id="18" string="investigation" />
            <token id="19" string="," />
          </tokens>
        </chunking>
        <chunking id="2" string="running for reelection , and the media , which caught wind of the investigation" type="VP">
          <tokens>
            <token id="5" string="running" />
            <token id="6" string="for" />
            <token id="7" string="reelection" />
            <token id="8" string="," />
            <token id="9" string="and" />
            <token id="10" string="the" />
            <token id="11" string="media" />
            <token id="12" string="," />
            <token id="13" string="which" />
            <token id="14" string="caught" />
            <token id="15" string="wind" />
            <token id="16" string="of" />
            <token id="17" string="the" />
            <token id="18" string="investigation" />
          </tokens>
        </chunking>
        <chunking id="3" string="reelection" type="NP">
          <tokens>
            <token id="7" string="reelection" />
          </tokens>
        </chunking>
        <chunking id="4" string="were pressing for action" type="VP">
          <tokens>
            <token id="20" string="were" />
            <token id="21" string="pressing" />
            <token id="22" string="for" />
            <token id="23" string="action" />
          </tokens>
        </chunking>
        <chunking id="5" string="the media" type="NP">
          <tokens>
            <token id="10" string="the" />
            <token id="11" string="media" />
          </tokens>
        </chunking>
        <chunking id="6" string="reelection , and the media , which caught wind of the investigation" type="NP">
          <tokens>
            <token id="7" string="reelection" />
            <token id="8" string="," />
            <token id="9" string="and" />
            <token id="10" string="the" />
            <token id="11" string="media" />
            <token id="12" string="," />
            <token id="13" string="which" />
            <token id="14" string="caught" />
            <token id="15" string="wind" />
            <token id="16" string="of" />
            <token id="17" string="the" />
            <token id="18" string="investigation" />
          </tokens>
        </chunking>
        <chunking id="7" string="wind of the investigation" type="NP">
          <tokens>
            <token id="15" string="wind" />
            <token id="16" string="of" />
            <token id="17" string="the" />
            <token id="18" string="investigation" />
          </tokens>
        </chunking>
        <chunking id="8" string="wind" type="NP">
          <tokens>
            <token id="15" string="wind" />
          </tokens>
        </chunking>
        <chunking id="9" string="pressing for action" type="VP">
          <tokens>
            <token id="21" string="pressing" />
            <token id="22" string="for" />
            <token id="23" string="action" />
          </tokens>
        </chunking>
        <chunking id="10" string="reelection , and the media ," type="NP">
          <tokens>
            <token id="7" string="reelection" />
            <token id="8" string="," />
            <token id="9" string="and" />
            <token id="10" string="the" />
            <token id="11" string="media" />
            <token id="12" string="," />
          </tokens>
        </chunking>
        <chunking id="11" string="the investigation" type="NP">
          <tokens>
            <token id="17" string="the" />
            <token id="18" string="investigation" />
          </tokens>
        </chunking>
        <chunking id="12" string="caught wind of the investigation" type="VP">
          <tokens>
            <token id="14" string="caught" />
            <token id="15" string="wind" />
            <token id="16" string="of" />
            <token id="17" string="the" />
            <token id="18" string="investigation" />
          </tokens>
        </chunking>
        <chunking id="13" string="Atty. Robert Philibosian" type="NP">
          <tokens>
            <token id="1" string="Atty." />
            <token id="2" string="Robert" />
            <token id="3" string="Philibosian" />
          </tokens>
        </chunking>
        <chunking id="14" string="action" type="NP">
          <tokens>
            <token id="23" string="action" />
          </tokens>
        </chunking>
        <chunking id="15" string="which caught wind of the investigation" type="SBAR">
          <tokens>
            <token id="13" string="which" />
            <token id="14" string="caught" />
            <token id="15" string="wind" />
            <token id="16" string="of" />
            <token id="17" string="the" />
            <token id="18" string="investigation" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="3">Philibosian</governor>
          <dependent id="1">Atty.</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">Philibosian</governor>
          <dependent id="2">Robert</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">pressing</governor>
          <dependent id="3">Philibosian</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="3">Philibosian</governor>
          <dependent id="5">running</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">reelection</governor>
          <dependent id="6">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">running</governor>
          <dependent id="7">reelection</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">reelection</governor>
          <dependent id="9">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">media</governor>
          <dependent id="10">the</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">reelection</governor>
          <dependent id="11">media</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">caught</governor>
          <dependent id="13">which</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="7">reelection</governor>
          <dependent id="14">caught</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="14">caught</governor>
          <dependent id="15">wind</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">investigation</governor>
          <dependent id="16">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">investigation</governor>
          <dependent id="17">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">wind</governor>
          <dependent id="18">investigation</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="21">pressing</governor>
          <dependent id="20">were</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="21">pressing</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">action</governor>
          <dependent id="22">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="21">pressing</governor>
          <dependent id="23">action</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Robert Philibosian" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Robert" />
            <token id="3" string="Philibosian" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="90" has_coreference="true">
      <content>Children&amp;apost;s Institute International, deluged with concerned parents, enlisted untrained therapists to assess the children, resulting in videotaped interviews filled with leading and suggestive questions that would later prove embarrassing to the prosecution and provide the defense with grounds for claiming that the alleged victims had been programmed to believe that they had been molested.</content>
      <tokens>
        <token id="1" string="Children" lemma="Children" stem="children" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="2" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="3" string="Institute" lemma="Institute" stem="institut" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="4" string="International" lemma="International" stem="internat" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="deluged" lemma="deluge" stem="delug" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="concerned" lemma="concerned" stem="concern" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="9" string="parents" lemma="parent" stem="parent" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="enlisted" lemma="enlist" stem="enlist" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="untrained" lemma="untrained" stem="untrain" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="13" string="therapists" lemma="therapist" stem="therapist" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="assess" lemma="assess" stem="assess" pos="VB" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="16" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="17" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="18" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="resulting" lemma="result" stem="result" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="videotaped" lemma="videotaped" stem="videotap" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="interviews" lemma="interview" stem="interview" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="filled" lemma="fill" stem="fill" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="leading" lemma="lead" stem="lead" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="suggestive" lemma="suggestive" stem="suggest" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="questions" lemma="question" stem="question" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="would" lemma="would" stem="would" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="later" lemma="later" stem="later" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="prove" lemma="prove" stem="prove" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="embarrassing" lemma="embarrassing" stem="embarrass" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="prosecution" lemma="prosecution" stem="prosecut" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="37" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="provide" lemma="provide" stem="provid" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="40" string="defense" lemma="defense" stem="defens" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="41" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="42" string="grounds" lemma="grounds" stem="ground" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="43" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="44" string="claiming" lemma="claim" stem="claim" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="45" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="46" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="47" string="alleged" lemma="allege" stem="alleg" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="48" string="victims" lemma="victim" stem="victim" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="49" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="50" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="51" string="programmed" lemma="program" stem="program" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="52" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="53" string="believe" lemma="believe" stem="believ" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="54" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="55" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="56" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="57" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="58" string="molested" lemma="molest" stem="molest" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="59" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NP (NNP Children) (POS 's)) (NNP Institute) (NNP International)) (, ,) (VP (VBN deluged) (PP (IN with) (NP (JJ concerned) (NNS parents)))) (, ,)) (VP (VBN enlisted) (NP (JJ untrained) (NNS therapists) (S (VP (TO to) (VP (VB assess) (NP (DT the) (NNS children)))))) (, ,) (S (VP (VBG resulting) (PP (IN in) (NP (NP (JJ videotaped) (NNS interviews)) (VP (VBN filled) (PP (IN with) (NP (NP (VBG leading)) (CC and) (NP (NP (JJ suggestive) (NNS questions)) (SBAR (WHNP (WDT that)) (S (VP (MD would) (ADVP (RB later)) (VP (VP (VB prove) (ADJP (JJ embarrassing) (PP (TO to) (NP (DT the) (NN prosecution))))) (CC and) (VP (VB provide) (NP (DT the) (NN defense)) (PP (IN with) (NP (NP (NNS grounds)) (PP (IN for) (S (VP (VBG claiming) (SBAR (IN that) (S (NP (DT the) (VBN alleged) (NNS victims)) (VP (VBD had) (VP (VBN been) (VP (VBN programmed) (S (VP (TO to) (VP (VB believe) (SBAR (IN that) (S (NP (PRP they)) (VP (VBD had) (VP (VBN been) (VP (VBN molested))))))))))))))))))))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the alleged victims" type="NP">
          <tokens>
            <token id="46" string="the" />
            <token id="47" string="alleged" />
            <token id="48" string="victims" />
          </tokens>
        </chunking>
        <chunking id="2" string="deluged with concerned parents" type="VP">
          <tokens>
            <token id="6" string="deluged" />
            <token id="7" string="with" />
            <token id="8" string="concerned" />
            <token id="9" string="parents" />
          </tokens>
        </chunking>
        <chunking id="3" string="to assess the children" type="VP">
          <tokens>
            <token id="14" string="to" />
            <token id="15" string="assess" />
            <token id="16" string="the" />
            <token id="17" string="children" />
          </tokens>
        </chunking>
        <chunking id="4" string="the defense" type="NP">
          <tokens>
            <token id="39" string="the" />
            <token id="40" string="defense" />
          </tokens>
        </chunking>
        <chunking id="5" string="grounds for claiming that the alleged victims had been programmed to believe that they had been molested" type="NP">
          <tokens>
            <token id="42" string="grounds" />
            <token id="43" string="for" />
            <token id="44" string="claiming" />
            <token id="45" string="that" />
            <token id="46" string="the" />
            <token id="47" string="alleged" />
            <token id="48" string="victims" />
            <token id="49" string="had" />
            <token id="50" string="been" />
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="6" string="claiming that the alleged victims had been programmed to believe that they had been molested" type="VP">
          <tokens>
            <token id="44" string="claiming" />
            <token id="45" string="that" />
            <token id="46" string="the" />
            <token id="47" string="alleged" />
            <token id="48" string="victims" />
            <token id="49" string="had" />
            <token id="50" string="been" />
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="7" string="had been molested" type="VP">
          <tokens>
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="8" string="prove embarrassing to the prosecution" type="VP">
          <tokens>
            <token id="32" string="prove" />
            <token id="33" string="embarrassing" />
            <token id="34" string="to" />
            <token id="35" string="the" />
            <token id="36" string="prosecution" />
          </tokens>
        </chunking>
        <chunking id="9" string="resulting in videotaped interviews filled with leading and suggestive questions that would later prove embarrassing to the prosecution and provide the defense with grounds for claiming that the alleged victims had been programmed to believe that they had been molested" type="VP">
          <tokens>
            <token id="19" string="resulting" />
            <token id="20" string="in" />
            <token id="21" string="videotaped" />
            <token id="22" string="interviews" />
            <token id="23" string="filled" />
            <token id="24" string="with" />
            <token id="25" string="leading" />
            <token id="26" string="and" />
            <token id="27" string="suggestive" />
            <token id="28" string="questions" />
            <token id="29" string="that" />
            <token id="30" string="would" />
            <token id="31" string="later" />
            <token id="32" string="prove" />
            <token id="33" string="embarrassing" />
            <token id="34" string="to" />
            <token id="35" string="the" />
            <token id="36" string="prosecution" />
            <token id="37" string="and" />
            <token id="38" string="provide" />
            <token id="39" string="the" />
            <token id="40" string="defense" />
            <token id="41" string="with" />
            <token id="42" string="grounds" />
            <token id="43" string="for" />
            <token id="44" string="claiming" />
            <token id="45" string="that" />
            <token id="46" string="the" />
            <token id="47" string="alleged" />
            <token id="48" string="victims" />
            <token id="49" string="had" />
            <token id="50" string="been" />
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="10" string="that the alleged victims had been programmed to believe that they had been molested" type="SBAR">
          <tokens>
            <token id="45" string="that" />
            <token id="46" string="the" />
            <token id="47" string="alleged" />
            <token id="48" string="victims" />
            <token id="49" string="had" />
            <token id="50" string="been" />
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="11" string="that would later prove embarrassing to the prosecution and provide the defense with grounds for claiming that the alleged victims had been programmed to believe that they had been molested" type="SBAR">
          <tokens>
            <token id="29" string="that" />
            <token id="30" string="would" />
            <token id="31" string="later" />
            <token id="32" string="prove" />
            <token id="33" string="embarrassing" />
            <token id="34" string="to" />
            <token id="35" string="the" />
            <token id="36" string="prosecution" />
            <token id="37" string="and" />
            <token id="38" string="provide" />
            <token id="39" string="the" />
            <token id="40" string="defense" />
            <token id="41" string="with" />
            <token id="42" string="grounds" />
            <token id="43" string="for" />
            <token id="44" string="claiming" />
            <token id="45" string="that" />
            <token id="46" string="the" />
            <token id="47" string="alleged" />
            <token id="48" string="victims" />
            <token id="49" string="had" />
            <token id="50" string="been" />
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="12" string="would later prove embarrassing to the prosecution and provide the defense with grounds for claiming that the alleged victims had been programmed to believe that they had been molested" type="VP">
          <tokens>
            <token id="30" string="would" />
            <token id="31" string="later" />
            <token id="32" string="prove" />
            <token id="33" string="embarrassing" />
            <token id="34" string="to" />
            <token id="35" string="the" />
            <token id="36" string="prosecution" />
            <token id="37" string="and" />
            <token id="38" string="provide" />
            <token id="39" string="the" />
            <token id="40" string="defense" />
            <token id="41" string="with" />
            <token id="42" string="grounds" />
            <token id="43" string="for" />
            <token id="44" string="claiming" />
            <token id="45" string="that" />
            <token id="46" string="the" />
            <token id="47" string="alleged" />
            <token id="48" string="victims" />
            <token id="49" string="had" />
            <token id="50" string="been" />
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="13" string="been programmed to believe that they had been molested" type="VP">
          <tokens>
            <token id="50" string="been" />
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="14" string="Children 's Institute International" type="NP">
          <tokens>
            <token id="1" string="Children" />
            <token id="2" string="'s" />
            <token id="3" string="Institute" />
            <token id="4" string="International" />
          </tokens>
        </chunking>
        <chunking id="15" string="Children 's Institute International , deluged with concerned parents ," type="NP">
          <tokens>
            <token id="1" string="Children" />
            <token id="2" string="'s" />
            <token id="3" string="Institute" />
            <token id="4" string="International" />
            <token id="5" string="," />
            <token id="6" string="deluged" />
            <token id="7" string="with" />
            <token id="8" string="concerned" />
            <token id="9" string="parents" />
            <token id="10" string="," />
          </tokens>
        </chunking>
        <chunking id="16" string="provide the defense with grounds for claiming that the alleged victims had been programmed to believe that they had been molested" type="VP">
          <tokens>
            <token id="38" string="provide" />
            <token id="39" string="the" />
            <token id="40" string="defense" />
            <token id="41" string="with" />
            <token id="42" string="grounds" />
            <token id="43" string="for" />
            <token id="44" string="claiming" />
            <token id="45" string="that" />
            <token id="46" string="the" />
            <token id="47" string="alleged" />
            <token id="48" string="victims" />
            <token id="49" string="had" />
            <token id="50" string="been" />
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="17" string="Children 's" type="NP">
          <tokens>
            <token id="1" string="Children" />
            <token id="2" string="'s" />
          </tokens>
        </chunking>
        <chunking id="18" string="grounds" type="NP">
          <tokens>
            <token id="42" string="grounds" />
          </tokens>
        </chunking>
        <chunking id="19" string="concerned parents" type="NP">
          <tokens>
            <token id="8" string="concerned" />
            <token id="9" string="parents" />
          </tokens>
        </chunking>
        <chunking id="20" string="the prosecution" type="NP">
          <tokens>
            <token id="35" string="the" />
            <token id="36" string="prosecution" />
          </tokens>
        </chunking>
        <chunking id="21" string="that they had been molested" type="SBAR">
          <tokens>
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="22" string="suggestive questions that would later prove embarrassing to the prosecution and provide the defense with grounds for claiming that the alleged victims had been programmed to believe that they had been molested" type="NP">
          <tokens>
            <token id="27" string="suggestive" />
            <token id="28" string="questions" />
            <token id="29" string="that" />
            <token id="30" string="would" />
            <token id="31" string="later" />
            <token id="32" string="prove" />
            <token id="33" string="embarrassing" />
            <token id="34" string="to" />
            <token id="35" string="the" />
            <token id="36" string="prosecution" />
            <token id="37" string="and" />
            <token id="38" string="provide" />
            <token id="39" string="the" />
            <token id="40" string="defense" />
            <token id="41" string="with" />
            <token id="42" string="grounds" />
            <token id="43" string="for" />
            <token id="44" string="claiming" />
            <token id="45" string="that" />
            <token id="46" string="the" />
            <token id="47" string="alleged" />
            <token id="48" string="victims" />
            <token id="49" string="had" />
            <token id="50" string="been" />
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="23" string="programmed to believe that they had been molested" type="VP">
          <tokens>
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="24" string="videotaped interviews filled with leading and suggestive questions that would later prove embarrassing to the prosecution and provide the defense with grounds for claiming that the alleged victims had been programmed to believe that they had been molested" type="NP">
          <tokens>
            <token id="21" string="videotaped" />
            <token id="22" string="interviews" />
            <token id="23" string="filled" />
            <token id="24" string="with" />
            <token id="25" string="leading" />
            <token id="26" string="and" />
            <token id="27" string="suggestive" />
            <token id="28" string="questions" />
            <token id="29" string="that" />
            <token id="30" string="would" />
            <token id="31" string="later" />
            <token id="32" string="prove" />
            <token id="33" string="embarrassing" />
            <token id="34" string="to" />
            <token id="35" string="the" />
            <token id="36" string="prosecution" />
            <token id="37" string="and" />
            <token id="38" string="provide" />
            <token id="39" string="the" />
            <token id="40" string="defense" />
            <token id="41" string="with" />
            <token id="42" string="grounds" />
            <token id="43" string="for" />
            <token id="44" string="claiming" />
            <token id="45" string="that" />
            <token id="46" string="the" />
            <token id="47" string="alleged" />
            <token id="48" string="victims" />
            <token id="49" string="had" />
            <token id="50" string="been" />
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="25" string="untrained therapists to assess the children" type="NP">
          <tokens>
            <token id="12" string="untrained" />
            <token id="13" string="therapists" />
            <token id="14" string="to" />
            <token id="15" string="assess" />
            <token id="16" string="the" />
            <token id="17" string="children" />
          </tokens>
        </chunking>
        <chunking id="26" string="leading" type="NP">
          <tokens>
            <token id="25" string="leading" />
          </tokens>
        </chunking>
        <chunking id="27" string="suggestive questions" type="NP">
          <tokens>
            <token id="27" string="suggestive" />
            <token id="28" string="questions" />
          </tokens>
        </chunking>
        <chunking id="28" string="believe that they had been molested" type="VP">
          <tokens>
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="29" string="enlisted untrained therapists to assess the children , resulting in videotaped interviews filled with leading and suggestive questions that would later prove embarrassing to the prosecution and provide the defense with grounds for claiming that the alleged victims had been programmed to believe that they had been molested" type="VP">
          <tokens>
            <token id="11" string="enlisted" />
            <token id="12" string="untrained" />
            <token id="13" string="therapists" />
            <token id="14" string="to" />
            <token id="15" string="assess" />
            <token id="16" string="the" />
            <token id="17" string="children" />
            <token id="18" string="," />
            <token id="19" string="resulting" />
            <token id="20" string="in" />
            <token id="21" string="videotaped" />
            <token id="22" string="interviews" />
            <token id="23" string="filled" />
            <token id="24" string="with" />
            <token id="25" string="leading" />
            <token id="26" string="and" />
            <token id="27" string="suggestive" />
            <token id="28" string="questions" />
            <token id="29" string="that" />
            <token id="30" string="would" />
            <token id="31" string="later" />
            <token id="32" string="prove" />
            <token id="33" string="embarrassing" />
            <token id="34" string="to" />
            <token id="35" string="the" />
            <token id="36" string="prosecution" />
            <token id="37" string="and" />
            <token id="38" string="provide" />
            <token id="39" string="the" />
            <token id="40" string="defense" />
            <token id="41" string="with" />
            <token id="42" string="grounds" />
            <token id="43" string="for" />
            <token id="44" string="claiming" />
            <token id="45" string="that" />
            <token id="46" string="the" />
            <token id="47" string="alleged" />
            <token id="48" string="victims" />
            <token id="49" string="had" />
            <token id="50" string="been" />
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="30" string="to believe that they had been molested" type="VP">
          <tokens>
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="31" string="had been programmed to believe that they had been molested" type="VP">
          <tokens>
            <token id="49" string="had" />
            <token id="50" string="been" />
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="32" string="prove embarrassing to the prosecution and provide the defense with grounds for claiming that the alleged victims had been programmed to believe that they had been molested" type="VP">
          <tokens>
            <token id="32" string="prove" />
            <token id="33" string="embarrassing" />
            <token id="34" string="to" />
            <token id="35" string="the" />
            <token id="36" string="prosecution" />
            <token id="37" string="and" />
            <token id="38" string="provide" />
            <token id="39" string="the" />
            <token id="40" string="defense" />
            <token id="41" string="with" />
            <token id="42" string="grounds" />
            <token id="43" string="for" />
            <token id="44" string="claiming" />
            <token id="45" string="that" />
            <token id="46" string="the" />
            <token id="47" string="alleged" />
            <token id="48" string="victims" />
            <token id="49" string="had" />
            <token id="50" string="been" />
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="33" string="they" type="NP">
          <tokens>
            <token id="55" string="they" />
          </tokens>
        </chunking>
        <chunking id="34" string="the children" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="children" />
          </tokens>
        </chunking>
        <chunking id="35" string="been molested" type="VP">
          <tokens>
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="36" string="assess the children" type="VP">
          <tokens>
            <token id="15" string="assess" />
            <token id="16" string="the" />
            <token id="17" string="children" />
          </tokens>
        </chunking>
        <chunking id="37" string="videotaped interviews" type="NP">
          <tokens>
            <token id="21" string="videotaped" />
            <token id="22" string="interviews" />
          </tokens>
        </chunking>
        <chunking id="38" string="filled with leading and suggestive questions that would later prove embarrassing to the prosecution and provide the defense with grounds for claiming that the alleged victims had been programmed to believe that they had been molested" type="VP">
          <tokens>
            <token id="23" string="filled" />
            <token id="24" string="with" />
            <token id="25" string="leading" />
            <token id="26" string="and" />
            <token id="27" string="suggestive" />
            <token id="28" string="questions" />
            <token id="29" string="that" />
            <token id="30" string="would" />
            <token id="31" string="later" />
            <token id="32" string="prove" />
            <token id="33" string="embarrassing" />
            <token id="34" string="to" />
            <token id="35" string="the" />
            <token id="36" string="prosecution" />
            <token id="37" string="and" />
            <token id="38" string="provide" />
            <token id="39" string="the" />
            <token id="40" string="defense" />
            <token id="41" string="with" />
            <token id="42" string="grounds" />
            <token id="43" string="for" />
            <token id="44" string="claiming" />
            <token id="45" string="that" />
            <token id="46" string="the" />
            <token id="47" string="alleged" />
            <token id="48" string="victims" />
            <token id="49" string="had" />
            <token id="50" string="been" />
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="39" string="leading and suggestive questions that would later prove embarrassing to the prosecution and provide the defense with grounds for claiming that the alleged victims had been programmed to believe that they had been molested" type="NP">
          <tokens>
            <token id="25" string="leading" />
            <token id="26" string="and" />
            <token id="27" string="suggestive" />
            <token id="28" string="questions" />
            <token id="29" string="that" />
            <token id="30" string="would" />
            <token id="31" string="later" />
            <token id="32" string="prove" />
            <token id="33" string="embarrassing" />
            <token id="34" string="to" />
            <token id="35" string="the" />
            <token id="36" string="prosecution" />
            <token id="37" string="and" />
            <token id="38" string="provide" />
            <token id="39" string="the" />
            <token id="40" string="defense" />
            <token id="41" string="with" />
            <token id="42" string="grounds" />
            <token id="43" string="for" />
            <token id="44" string="claiming" />
            <token id="45" string="that" />
            <token id="46" string="the" />
            <token id="47" string="alleged" />
            <token id="48" string="victims" />
            <token id="49" string="had" />
            <token id="50" string="been" />
            <token id="51" string="programmed" />
            <token id="52" string="to" />
            <token id="53" string="believe" />
            <token id="54" string="that" />
            <token id="55" string="they" />
            <token id="56" string="had" />
            <token id="57" string="been" />
            <token id="58" string="molested" />
          </tokens>
        </chunking>
        <chunking id="40" string="embarrassing to the prosecution" type="ADJP">
          <tokens>
            <token id="33" string="embarrassing" />
            <token id="34" string="to" />
            <token id="35" string="the" />
            <token id="36" string="prosecution" />
          </tokens>
        </chunking>
        <chunking id="41" string="molested" type="VP">
          <tokens>
            <token id="58" string="molested" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="4">International</governor>
          <dependent id="1">Children</dependent>
        </dependency>
        <dependency type="case">
          <governor id="1">Children</governor>
          <dependent id="2">'s</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">International</governor>
          <dependent id="3">Institute</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">enlisted</governor>
          <dependent id="4">International</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="4">International</governor>
          <dependent id="6">deluged</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">parents</governor>
          <dependent id="7">with</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">parents</governor>
          <dependent id="8">concerned</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">deluged</governor>
          <dependent id="9">parents</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="11">enlisted</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">therapists</governor>
          <dependent id="12">untrained</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">enlisted</governor>
          <dependent id="13">therapists</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="15">assess</governor>
          <dependent id="14">to</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="13">therapists</governor>
          <dependent id="15">assess</dependent>
        </dependency>
        <dependency type="det">
          <governor id="17">children</governor>
          <dependent id="16">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">assess</governor>
          <dependent id="17">children</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="11">enlisted</governor>
          <dependent id="19">resulting</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">interviews</governor>
          <dependent id="20">in</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="22">interviews</governor>
          <dependent id="21">videotaped</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">resulting</governor>
          <dependent id="22">interviews</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="22">interviews</governor>
          <dependent id="23">filled</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">leading</governor>
          <dependent id="24">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">filled</governor>
          <dependent id="25">leading</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="25">leading</governor>
          <dependent id="26">and</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="28">questions</governor>
          <dependent id="27">suggestive</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="25">leading</governor>
          <dependent id="28">questions</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="32">prove</governor>
          <dependent id="29">that</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="32">prove</governor>
          <dependent id="30">would</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="32">prove</governor>
          <dependent id="31">later</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="28">questions</governor>
          <dependent id="32">prove</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="32">prove</governor>
          <dependent id="33">embarrassing</dependent>
        </dependency>
        <dependency type="case">
          <governor id="36">prosecution</governor>
          <dependent id="34">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="36">prosecution</governor>
          <dependent id="35">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="33">embarrassing</governor>
          <dependent id="36">prosecution</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="32">prove</governor>
          <dependent id="37">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="32">prove</governor>
          <dependent id="38">provide</dependent>
        </dependency>
        <dependency type="det">
          <governor id="40">defense</governor>
          <dependent id="39">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="38">provide</governor>
          <dependent id="40">defense</dependent>
        </dependency>
        <dependency type="case">
          <governor id="42">grounds</governor>
          <dependent id="41">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="38">provide</governor>
          <dependent id="42">grounds</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="44">claiming</governor>
          <dependent id="43">for</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="42">grounds</governor>
          <dependent id="44">claiming</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="51">programmed</governor>
          <dependent id="45">that</dependent>
        </dependency>
        <dependency type="det">
          <governor id="48">victims</governor>
          <dependent id="46">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="48">victims</governor>
          <dependent id="47">alleged</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="51">programmed</governor>
          <dependent id="48">victims</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="51">programmed</governor>
          <dependent id="49">had</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="51">programmed</governor>
          <dependent id="50">been</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="44">claiming</governor>
          <dependent id="51">programmed</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="53">believe</governor>
          <dependent id="52">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="51">programmed</governor>
          <dependent id="53">believe</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="58">molested</governor>
          <dependent id="54">that</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="58">molested</governor>
          <dependent id="55">they</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="58">molested</governor>
          <dependent id="56">had</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="58">molested</governor>
          <dependent id="57">been</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="53">believe</governor>
          <dependent id="58">molested</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Children 's Institute International" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="1" string="Children" />
            <token id="2" string="'s" />
            <token id="3" string="Institute" />
            <token id="4" string="International" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="91" has_coreference="false">
      <content>Hysteria grew.</content>
      <tokens>
        <token id="1" string="Hysteria" lemma="hysteria" stem="hysteria" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="grew" lemma="grow" stem="grew" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NN Hysteria)) (VP (VBD grew)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Hysteria" type="NP">
          <tokens>
            <token id="1" string="Hysteria" />
          </tokens>
        </chunking>
        <chunking id="2" string="grew" type="VP">
          <tokens>
            <token id="2" string="grew" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">grew</governor>
          <dependent id="1">Hysteria</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">grew</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="92" has_coreference="true">
      <content>Hundreds of South Bay children had fallen victim to a nationwide conspiracy and pornography ring, it was alleged, and an angry mob spray-painted &amp;quot;Ray Must Die&amp;quot; on the walls of the nursery school and began digging up an adjacent vacant lot in search of animal remains.</content>
      <tokens>
        <token id="1" string="Hundreds" lemma="hundred" stem="hundr" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="South" lemma="South" stem="south" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="4" string="Bay" lemma="Bay" stem="bai" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="5" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="fallen" lemma="fall" stem="fallen" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="victim" lemma="victim" stem="victim" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="nationwide" lemma="nationwide" stem="nationwid" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="conspiracy" lemma="conspiracy" stem="conspiraci" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="pornography" lemma="pornography" stem="pornographi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="ring" lemma="ring" stem="ring" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="alleged" lemma="allege" stem="alleg" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="angry" lemma="angry" stem="angri" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="mob" lemma="mob" stem="mob" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="spray-painted" lemma="spray-painted" stem="spray-paint" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="28" string="Must" lemma="must" stem="must" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="Die" lemma="die" stem="die" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="walls" lemma="wall" stem="wall" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="36" string="nursery" lemma="nursery" stem="nurseri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="37" string="school" lemma="school" stem="school" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="38" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="39" string="began" lemma="begin" stem="began" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="40" string="digging" lemma="digging" stem="dig" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="41" string="up" lemma="up" stem="up" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="42" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="43" string="adjacent" lemma="adjacent" stem="adjac" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="44" string="vacant" lemma="vacant" stem="vacant" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="45" string="lot" lemma="lot" stem="lot" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="46" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="47" string="search" lemma="search" stem="search" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="48" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="49" string="animal" lemma="animal" stem="anim" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="50" string="remains" lemma="remain" stem="remain" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="51" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (NP (NNS Hundreds)) (PP (IN of) (NP (NNP South) (NNP Bay) (NNS children)))) (VP (VBD had) (VP (VBN fallen) (NP (NN victim)) (PP (TO to) (NP (DT a) (JJ nationwide) (NN conspiracy) (CC and) (NN pornography) (NN ring)))))) (PRN (, ,) (S (NP (PRP it)) (VP (VBD was) (VP (VBN alleged)))) (, ,)) (CC and) (S (NP (NP (DT an) (JJ angry) (NN mob)) (ADJP (JJ spray-painted) (SBAR (`` ``) (S (NP (NNP Ray)) (VP (VP (MD Must) (VP (VB Die) ('' '') (PP (IN on) (NP (NP (DT the) (NNS walls)) (PP (IN of) (NP (DT the) (NN nursery) (NN school))))))) (CC and) (VP (VBD began) (NP (NN digging)) (PP (IN up) (NP (NP (DT an) (JJ adjacent) (JJ vacant) (NN lot)) (PP (IN in) (NP (NP (NN search)) (PP (IN of) (NP (NN animal))))))))))))) (VP (VBZ remains))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="spray-painted `` Ray Must Die '' on the walls of the nursery school and began digging up an adjacent vacant lot in search of animal" type="ADJP">
          <tokens>
            <token id="25" string="spray-painted" />
            <token id="26" string="&quot;" />
            <token id="27" string="Ray" />
            <token id="28" string="Must" />
            <token id="29" string="Die" />
            <token id="30" string="&quot;" />
            <token id="31" string="on" />
            <token id="32" string="the" />
            <token id="33" string="walls" />
            <token id="34" string="of" />
            <token id="35" string="the" />
            <token id="36" string="nursery" />
            <token id="37" string="school" />
            <token id="38" string="and" />
            <token id="39" string="began" />
            <token id="40" string="digging" />
            <token id="41" string="up" />
            <token id="42" string="an" />
            <token id="43" string="adjacent" />
            <token id="44" string="vacant" />
            <token id="45" string="lot" />
            <token id="46" string="in" />
            <token id="47" string="search" />
            <token id="48" string="of" />
            <token id="49" string="animal" />
          </tokens>
        </chunking>
        <chunking id="2" string="Hundreds of South Bay children" type="NP">
          <tokens>
            <token id="1" string="Hundreds" />
            <token id="2" string="of" />
            <token id="3" string="South" />
            <token id="4" string="Bay" />
            <token id="5" string="children" />
          </tokens>
        </chunking>
        <chunking id="3" string="Must Die '' on the walls of the nursery school and began digging up an adjacent vacant lot in search of animal" type="VP">
          <tokens>
            <token id="28" string="Must" />
            <token id="29" string="Die" />
            <token id="30" string="&quot;" />
            <token id="31" string="on" />
            <token id="32" string="the" />
            <token id="33" string="walls" />
            <token id="34" string="of" />
            <token id="35" string="the" />
            <token id="36" string="nursery" />
            <token id="37" string="school" />
            <token id="38" string="and" />
            <token id="39" string="began" />
            <token id="40" string="digging" />
            <token id="41" string="up" />
            <token id="42" string="an" />
            <token id="43" string="adjacent" />
            <token id="44" string="vacant" />
            <token id="45" string="lot" />
            <token id="46" string="in" />
            <token id="47" string="search" />
            <token id="48" string="of" />
            <token id="49" string="animal" />
          </tokens>
        </chunking>
        <chunking id="4" string="an adjacent vacant lot" type="NP">
          <tokens>
            <token id="42" string="an" />
            <token id="43" string="adjacent" />
            <token id="44" string="vacant" />
            <token id="45" string="lot" />
          </tokens>
        </chunking>
        <chunking id="5" string="it" type="NP">
          <tokens>
            <token id="17" string="it" />
          </tokens>
        </chunking>
        <chunking id="6" string="`` Ray Must Die '' on the walls of the nursery school and began digging up an adjacent vacant lot in search of animal" type="SBAR">
          <tokens>
            <token id="26" string="&quot;" />
            <token id="27" string="Ray" />
            <token id="28" string="Must" />
            <token id="29" string="Die" />
            <token id="30" string="&quot;" />
            <token id="31" string="on" />
            <token id="32" string="the" />
            <token id="33" string="walls" />
            <token id="34" string="of" />
            <token id="35" string="the" />
            <token id="36" string="nursery" />
            <token id="37" string="school" />
            <token id="38" string="and" />
            <token id="39" string="began" />
            <token id="40" string="digging" />
            <token id="41" string="up" />
            <token id="42" string="an" />
            <token id="43" string="adjacent" />
            <token id="44" string="vacant" />
            <token id="45" string="lot" />
            <token id="46" string="in" />
            <token id="47" string="search" />
            <token id="48" string="of" />
            <token id="49" string="animal" />
          </tokens>
        </chunking>
        <chunking id="7" string="Die '' on the walls of the nursery school" type="VP">
          <tokens>
            <token id="29" string="Die" />
            <token id="30" string="&quot;" />
            <token id="31" string="on" />
            <token id="32" string="the" />
            <token id="33" string="walls" />
            <token id="34" string="of" />
            <token id="35" string="the" />
            <token id="36" string="nursery" />
            <token id="37" string="school" />
          </tokens>
        </chunking>
        <chunking id="8" string="digging" type="NP">
          <tokens>
            <token id="40" string="digging" />
          </tokens>
        </chunking>
        <chunking id="9" string="began digging up an adjacent vacant lot in search of animal" type="VP">
          <tokens>
            <token id="39" string="began" />
            <token id="40" string="digging" />
            <token id="41" string="up" />
            <token id="42" string="an" />
            <token id="43" string="adjacent" />
            <token id="44" string="vacant" />
            <token id="45" string="lot" />
            <token id="46" string="in" />
            <token id="47" string="search" />
            <token id="48" string="of" />
            <token id="49" string="animal" />
          </tokens>
        </chunking>
        <chunking id="10" string="animal" type="NP">
          <tokens>
            <token id="49" string="animal" />
          </tokens>
        </chunking>
        <chunking id="11" string="an adjacent vacant lot in search of animal" type="NP">
          <tokens>
            <token id="42" string="an" />
            <token id="43" string="adjacent" />
            <token id="44" string="vacant" />
            <token id="45" string="lot" />
            <token id="46" string="in" />
            <token id="47" string="search" />
            <token id="48" string="of" />
            <token id="49" string="animal" />
          </tokens>
        </chunking>
        <chunking id="12" string="was alleged" type="VP">
          <tokens>
            <token id="18" string="was" />
            <token id="19" string="alleged" />
          </tokens>
        </chunking>
        <chunking id="13" string="alleged" type="VP">
          <tokens>
            <token id="19" string="alleged" />
          </tokens>
        </chunking>
        <chunking id="14" string="Ray" type="NP">
          <tokens>
            <token id="27" string="Ray" />
          </tokens>
        </chunking>
        <chunking id="15" string="had fallen victim to a nationwide conspiracy and pornography ring" type="VP">
          <tokens>
            <token id="6" string="had" />
            <token id="7" string="fallen" />
            <token id="8" string="victim" />
            <token id="9" string="to" />
            <token id="10" string="a" />
            <token id="11" string="nationwide" />
            <token id="12" string="conspiracy" />
            <token id="13" string="and" />
            <token id="14" string="pornography" />
            <token id="15" string="ring" />
          </tokens>
        </chunking>
        <chunking id="16" string="remains" type="VP">
          <tokens>
            <token id="50" string="remains" />
          </tokens>
        </chunking>
        <chunking id="17" string="South Bay children" type="NP">
          <tokens>
            <token id="3" string="South" />
            <token id="4" string="Bay" />
            <token id="5" string="children" />
          </tokens>
        </chunking>
        <chunking id="18" string="Must Die '' on the walls of the nursery school" type="VP">
          <tokens>
            <token id="28" string="Must" />
            <token id="29" string="Die" />
            <token id="30" string="&quot;" />
            <token id="31" string="on" />
            <token id="32" string="the" />
            <token id="33" string="walls" />
            <token id="34" string="of" />
            <token id="35" string="the" />
            <token id="36" string="nursery" />
            <token id="37" string="school" />
          </tokens>
        </chunking>
        <chunking id="19" string="a nationwide conspiracy and pornography ring" type="NP">
          <tokens>
            <token id="10" string="a" />
            <token id="11" string="nationwide" />
            <token id="12" string="conspiracy" />
            <token id="13" string="and" />
            <token id="14" string="pornography" />
            <token id="15" string="ring" />
          </tokens>
        </chunking>
        <chunking id="20" string="victim" type="NP">
          <tokens>
            <token id="8" string="victim" />
          </tokens>
        </chunking>
        <chunking id="21" string="the walls of the nursery school" type="NP">
          <tokens>
            <token id="32" string="the" />
            <token id="33" string="walls" />
            <token id="34" string="of" />
            <token id="35" string="the" />
            <token id="36" string="nursery" />
            <token id="37" string="school" />
          </tokens>
        </chunking>
        <chunking id="22" string="an angry mob" type="NP">
          <tokens>
            <token id="22" string="an" />
            <token id="23" string="angry" />
            <token id="24" string="mob" />
          </tokens>
        </chunking>
        <chunking id="23" string="search of animal" type="NP">
          <tokens>
            <token id="47" string="search" />
            <token id="48" string="of" />
            <token id="49" string="animal" />
          </tokens>
        </chunking>
        <chunking id="24" string="fallen victim to a nationwide conspiracy and pornography ring" type="VP">
          <tokens>
            <token id="7" string="fallen" />
            <token id="8" string="victim" />
            <token id="9" string="to" />
            <token id="10" string="a" />
            <token id="11" string="nationwide" />
            <token id="12" string="conspiracy" />
            <token id="13" string="and" />
            <token id="14" string="pornography" />
            <token id="15" string="ring" />
          </tokens>
        </chunking>
        <chunking id="25" string="the nursery school" type="NP">
          <tokens>
            <token id="35" string="the" />
            <token id="36" string="nursery" />
            <token id="37" string="school" />
          </tokens>
        </chunking>
        <chunking id="26" string="an angry mob spray-painted `` Ray Must Die '' on the walls of the nursery school and began digging up an adjacent vacant lot in search of animal" type="NP">
          <tokens>
            <token id="22" string="an" />
            <token id="23" string="angry" />
            <token id="24" string="mob" />
            <token id="25" string="spray-painted" />
            <token id="26" string="&quot;" />
            <token id="27" string="Ray" />
            <token id="28" string="Must" />
            <token id="29" string="Die" />
            <token id="30" string="&quot;" />
            <token id="31" string="on" />
            <token id="32" string="the" />
            <token id="33" string="walls" />
            <token id="34" string="of" />
            <token id="35" string="the" />
            <token id="36" string="nursery" />
            <token id="37" string="school" />
            <token id="38" string="and" />
            <token id="39" string="began" />
            <token id="40" string="digging" />
            <token id="41" string="up" />
            <token id="42" string="an" />
            <token id="43" string="adjacent" />
            <token id="44" string="vacant" />
            <token id="45" string="lot" />
            <token id="46" string="in" />
            <token id="47" string="search" />
            <token id="48" string="of" />
            <token id="49" string="animal" />
          </tokens>
        </chunking>
        <chunking id="27" string="Hundreds" type="NP">
          <tokens>
            <token id="1" string="Hundreds" />
          </tokens>
        </chunking>
        <chunking id="28" string="search" type="NP">
          <tokens>
            <token id="47" string="search" />
          </tokens>
        </chunking>
        <chunking id="29" string="the walls" type="NP">
          <tokens>
            <token id="32" string="the" />
            <token id="33" string="walls" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="7">fallen</governor>
          <dependent id="1">Hundreds</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">children</governor>
          <dependent id="2">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">children</governor>
          <dependent id="3">South</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">children</governor>
          <dependent id="4">Bay</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="1">Hundreds</governor>
          <dependent id="5">children</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="7">fallen</governor>
          <dependent id="6">had</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">fallen</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">fallen</governor>
          <dependent id="8">victim</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">conspiracy</governor>
          <dependent id="9">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">conspiracy</governor>
          <dependent id="10">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">conspiracy</governor>
          <dependent id="11">nationwide</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">fallen</governor>
          <dependent id="12">conspiracy</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="12">conspiracy</governor>
          <dependent id="13">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">ring</governor>
          <dependent id="14">pornography</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="12">conspiracy</governor>
          <dependent id="15">ring</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="19">alleged</governor>
          <dependent id="17">it</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="19">alleged</governor>
          <dependent id="18">was</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="7">fallen</governor>
          <dependent id="19">alleged</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">fallen</governor>
          <dependent id="21">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="24">mob</governor>
          <dependent id="22">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="24">mob</governor>
          <dependent id="23">angry</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="50">remains</governor>
          <dependent id="24">mob</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="24">mob</governor>
          <dependent id="25">spray-painted</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="29">Die</governor>
          <dependent id="27">Ray</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="29">Die</governor>
          <dependent id="28">Must</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="25">spray-painted</governor>
          <dependent id="29">Die</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">walls</governor>
          <dependent id="31">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="33">walls</governor>
          <dependent id="32">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="29">Die</governor>
          <dependent id="33">walls</dependent>
        </dependency>
        <dependency type="case">
          <governor id="37">school</governor>
          <dependent id="34">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="37">school</governor>
          <dependent id="35">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="37">school</governor>
          <dependent id="36">nursery</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="33">walls</governor>
          <dependent id="37">school</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="29">Die</governor>
          <dependent id="38">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="29">Die</governor>
          <dependent id="39">began</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="39">began</governor>
          <dependent id="40">digging</dependent>
        </dependency>
        <dependency type="case">
          <governor id="45">lot</governor>
          <dependent id="41">up</dependent>
        </dependency>
        <dependency type="det">
          <governor id="45">lot</governor>
          <dependent id="42">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="45">lot</governor>
          <dependent id="43">adjacent</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="45">lot</governor>
          <dependent id="44">vacant</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="39">began</governor>
          <dependent id="45">lot</dependent>
        </dependency>
        <dependency type="case">
          <governor id="47">search</governor>
          <dependent id="46">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="45">lot</governor>
          <dependent id="47">search</dependent>
        </dependency>
        <dependency type="case">
          <governor id="49">animal</governor>
          <dependent id="48">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="47">search</governor>
          <dependent id="49">animal</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">fallen</governor>
          <dependent id="50">remains</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="South Bay" type="LOCATION" score="0.0">
          <tokens>
            <token id="3" string="South" />
            <token id="4" string="Bay" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="93" has_coreference="true">
      <content>The 18-month preliminary hearing, which began in the fall of 1984, was marked by frequent shouting matches between three prosecutors, nine defense attorneys and Municipal Judge Aviva Bobb, and lengthy questioning of child witnesses that lasted in one instance for 16 days.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="18-month" lemma="18-month" stem="18-month" pos="JJ" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="3" string="preliminary" lemma="preliminary" stem="preliminari" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="hearing" lemma="hearing" stem="hear" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="which" lemma="which" stem="which" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="began" lemma="begin" stem="began" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="10" string="fall" lemma="fall" stem="fall" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="11" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="12" string="1984" lemma="1984" stem="1984" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="marked" lemma="mark" stem="mark" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="frequent" lemma="frequent" stem="frequent" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="shouting" lemma="shout" stem="shout" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="matches" lemma="match" stem="match" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="between" lemma="between" stem="between" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="three" lemma="three" stem="three" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="22" string="prosecutors" lemma="prosecutor" stem="prosecutor" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="23" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="nine" lemma="nine" stem="nine" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="25" string="defense" lemma="defense" stem="defens" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="attorneys" lemma="attorney" stem="attornei" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="Municipal" lemma="municipal" stem="municip" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="Judge" lemma="Judge" stem="judg" pos="NNP" type="Word" isStopWord="false" ner="TITLE" is_referenced="false" is_refers="false" />
        <token id="30" string="Aviva" lemma="Aviva" stem="aviva" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="31" string="Bobb" lemma="Bobb" stem="bobb" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="32" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="lengthy" lemma="lengthy" stem="lengthi" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="questioning" lemma="question" stem="question" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="36" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="child" lemma="child" stem="child" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="38" string="witnesses" lemma="witness" stem="wit" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="40" string="lasted" lemma="last" stem="last" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="41" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="42" string="one" lemma="one" stem="on" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="43" string="instance" lemma="instance" stem="instanc" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="44" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="45" string="16" lemma="16" stem="16" pos="CD" type="Number" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="46" string="days" lemma="day" stem="dai" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="47" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT The) (JJ 18-month) (JJ preliminary) (NN hearing)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBD began) (PP (IN in) (NP (NP (DT the) (NN fall)) (PP (IN of) (NP (CD 1984)))))))) (, ,)) (VP (VBD was) (VP (VBN marked) (PP (IN by) (NP (NP (NP (JJ frequent)) (VP (VBG shouting) (NP (NP (NNS matches)) (PP (IN between) (NP (NP (CD three) (NNS prosecutors)) (, ,) (NP (CD nine) (NN defense) (NNS attorneys)) (CC and) (NP (JJ Municipal) (NNP Judge) (NNP Aviva) (NNP Bobb))))))) (, ,) (CC and) (NP (NP (JJ lengthy) (VBG questioning)) (PP (IN of) (NP (NN child) (NNS witnesses))) (SBAR (WHNP (WDT that)) (S (VP (VBD lasted) (PP (IN in) (NP (CD one) (NN instance))) (PP (IN for) (NP (CD 16) (NNS days))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="The 18-month preliminary hearing" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="18-month" />
            <token id="3" string="preliminary" />
            <token id="4" string="hearing" />
          </tokens>
        </chunking>
        <chunking id="2" string="was marked by frequent shouting matches between three prosecutors , nine defense attorneys and Municipal Judge Aviva Bobb , and lengthy questioning of child witnesses that lasted in one instance for 16 days" type="VP">
          <tokens>
            <token id="14" string="was" />
            <token id="15" string="marked" />
            <token id="16" string="by" />
            <token id="17" string="frequent" />
            <token id="18" string="shouting" />
            <token id="19" string="matches" />
            <token id="20" string="between" />
            <token id="21" string="three" />
            <token id="22" string="prosecutors" />
            <token id="23" string="," />
            <token id="24" string="nine" />
            <token id="25" string="defense" />
            <token id="26" string="attorneys" />
            <token id="27" string="and" />
            <token id="28" string="Municipal" />
            <token id="29" string="Judge" />
            <token id="30" string="Aviva" />
            <token id="31" string="Bobb" />
            <token id="32" string="," />
            <token id="33" string="and" />
            <token id="34" string="lengthy" />
            <token id="35" string="questioning" />
            <token id="36" string="of" />
            <token id="37" string="child" />
            <token id="38" string="witnesses" />
            <token id="39" string="that" />
            <token id="40" string="lasted" />
            <token id="41" string="in" />
            <token id="42" string="one" />
            <token id="43" string="instance" />
            <token id="44" string="for" />
            <token id="45" string="16" />
            <token id="46" string="days" />
          </tokens>
        </chunking>
        <chunking id="3" string="lasted in one instance for 16 days" type="VP">
          <tokens>
            <token id="40" string="lasted" />
            <token id="41" string="in" />
            <token id="42" string="one" />
            <token id="43" string="instance" />
            <token id="44" string="for" />
            <token id="45" string="16" />
            <token id="46" string="days" />
          </tokens>
        </chunking>
        <chunking id="4" string="began in the fall of 1984" type="VP">
          <tokens>
            <token id="7" string="began" />
            <token id="8" string="in" />
            <token id="9" string="the" />
            <token id="10" string="fall" />
            <token id="11" string="of" />
            <token id="12" string="1984" />
          </tokens>
        </chunking>
        <chunking id="5" string="one instance" type="NP">
          <tokens>
            <token id="42" string="one" />
            <token id="43" string="instance" />
          </tokens>
        </chunking>
        <chunking id="6" string="marked by frequent shouting matches between three prosecutors , nine defense attorneys and Municipal Judge Aviva Bobb , and lengthy questioning of child witnesses that lasted in one instance for 16 days" type="VP">
          <tokens>
            <token id="15" string="marked" />
            <token id="16" string="by" />
            <token id="17" string="frequent" />
            <token id="18" string="shouting" />
            <token id="19" string="matches" />
            <token id="20" string="between" />
            <token id="21" string="three" />
            <token id="22" string="prosecutors" />
            <token id="23" string="," />
            <token id="24" string="nine" />
            <token id="25" string="defense" />
            <token id="26" string="attorneys" />
            <token id="27" string="and" />
            <token id="28" string="Municipal" />
            <token id="29" string="Judge" />
            <token id="30" string="Aviva" />
            <token id="31" string="Bobb" />
            <token id="32" string="," />
            <token id="33" string="and" />
            <token id="34" string="lengthy" />
            <token id="35" string="questioning" />
            <token id="36" string="of" />
            <token id="37" string="child" />
            <token id="38" string="witnesses" />
            <token id="39" string="that" />
            <token id="40" string="lasted" />
            <token id="41" string="in" />
            <token id="42" string="one" />
            <token id="43" string="instance" />
            <token id="44" string="for" />
            <token id="45" string="16" />
            <token id="46" string="days" />
          </tokens>
        </chunking>
        <chunking id="7" string="matches" type="NP">
          <tokens>
            <token id="19" string="matches" />
          </tokens>
        </chunking>
        <chunking id="8" string="three prosecutors , nine defense attorneys and Municipal Judge Aviva Bobb" type="NP">
          <tokens>
            <token id="21" string="three" />
            <token id="22" string="prosecutors" />
            <token id="23" string="," />
            <token id="24" string="nine" />
            <token id="25" string="defense" />
            <token id="26" string="attorneys" />
            <token id="27" string="and" />
            <token id="28" string="Municipal" />
            <token id="29" string="Judge" />
            <token id="30" string="Aviva" />
            <token id="31" string="Bobb" />
          </tokens>
        </chunking>
        <chunking id="9" string="three prosecutors" type="NP">
          <tokens>
            <token id="21" string="three" />
            <token id="22" string="prosecutors" />
          </tokens>
        </chunking>
        <chunking id="10" string="nine defense attorneys" type="NP">
          <tokens>
            <token id="24" string="nine" />
            <token id="25" string="defense" />
            <token id="26" string="attorneys" />
          </tokens>
        </chunking>
        <chunking id="11" string="1984" type="NP">
          <tokens>
            <token id="12" string="1984" />
          </tokens>
        </chunking>
        <chunking id="12" string="shouting matches between three prosecutors , nine defense attorneys and Municipal Judge Aviva Bobb" type="VP">
          <tokens>
            <token id="18" string="shouting" />
            <token id="19" string="matches" />
            <token id="20" string="between" />
            <token id="21" string="three" />
            <token id="22" string="prosecutors" />
            <token id="23" string="," />
            <token id="24" string="nine" />
            <token id="25" string="defense" />
            <token id="26" string="attorneys" />
            <token id="27" string="and" />
            <token id="28" string="Municipal" />
            <token id="29" string="Judge" />
            <token id="30" string="Aviva" />
            <token id="31" string="Bobb" />
          </tokens>
        </chunking>
        <chunking id="13" string="The 18-month preliminary hearing , which began in the fall of 1984 ," type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="18-month" />
            <token id="3" string="preliminary" />
            <token id="4" string="hearing" />
            <token id="5" string="," />
            <token id="6" string="which" />
            <token id="7" string="began" />
            <token id="8" string="in" />
            <token id="9" string="the" />
            <token id="10" string="fall" />
            <token id="11" string="of" />
            <token id="12" string="1984" />
            <token id="13" string="," />
          </tokens>
        </chunking>
        <chunking id="14" string="lengthy questioning of child witnesses that lasted in one instance for 16 days" type="NP">
          <tokens>
            <token id="34" string="lengthy" />
            <token id="35" string="questioning" />
            <token id="36" string="of" />
            <token id="37" string="child" />
            <token id="38" string="witnesses" />
            <token id="39" string="that" />
            <token id="40" string="lasted" />
            <token id="41" string="in" />
            <token id="42" string="one" />
            <token id="43" string="instance" />
            <token id="44" string="for" />
            <token id="45" string="16" />
            <token id="46" string="days" />
          </tokens>
        </chunking>
        <chunking id="15" string="Municipal Judge Aviva Bobb" type="NP">
          <tokens>
            <token id="28" string="Municipal" />
            <token id="29" string="Judge" />
            <token id="30" string="Aviva" />
            <token id="31" string="Bobb" />
          </tokens>
        </chunking>
        <chunking id="16" string="child witnesses" type="NP">
          <tokens>
            <token id="37" string="child" />
            <token id="38" string="witnesses" />
          </tokens>
        </chunking>
        <chunking id="17" string="that lasted in one instance for 16 days" type="SBAR">
          <tokens>
            <token id="39" string="that" />
            <token id="40" string="lasted" />
            <token id="41" string="in" />
            <token id="42" string="one" />
            <token id="43" string="instance" />
            <token id="44" string="for" />
            <token id="45" string="16" />
            <token id="46" string="days" />
          </tokens>
        </chunking>
        <chunking id="18" string="the fall of 1984" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="fall" />
            <token id="11" string="of" />
            <token id="12" string="1984" />
          </tokens>
        </chunking>
        <chunking id="19" string="matches between three prosecutors , nine defense attorneys and Municipal Judge Aviva Bobb" type="NP">
          <tokens>
            <token id="19" string="matches" />
            <token id="20" string="between" />
            <token id="21" string="three" />
            <token id="22" string="prosecutors" />
            <token id="23" string="," />
            <token id="24" string="nine" />
            <token id="25" string="defense" />
            <token id="26" string="attorneys" />
            <token id="27" string="and" />
            <token id="28" string="Municipal" />
            <token id="29" string="Judge" />
            <token id="30" string="Aviva" />
            <token id="31" string="Bobb" />
          </tokens>
        </chunking>
        <chunking id="20" string="which began in the fall of 1984" type="SBAR">
          <tokens>
            <token id="6" string="which" />
            <token id="7" string="began" />
            <token id="8" string="in" />
            <token id="9" string="the" />
            <token id="10" string="fall" />
            <token id="11" string="of" />
            <token id="12" string="1984" />
          </tokens>
        </chunking>
        <chunking id="21" string="frequent" type="NP">
          <tokens>
            <token id="17" string="frequent" />
          </tokens>
        </chunking>
        <chunking id="22" string="lengthy questioning" type="NP">
          <tokens>
            <token id="34" string="lengthy" />
            <token id="35" string="questioning" />
          </tokens>
        </chunking>
        <chunking id="23" string="the fall" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="fall" />
          </tokens>
        </chunking>
        <chunking id="24" string="frequent shouting matches between three prosecutors , nine defense attorneys and Municipal Judge Aviva Bobb" type="NP">
          <tokens>
            <token id="17" string="frequent" />
            <token id="18" string="shouting" />
            <token id="19" string="matches" />
            <token id="20" string="between" />
            <token id="21" string="three" />
            <token id="22" string="prosecutors" />
            <token id="23" string="," />
            <token id="24" string="nine" />
            <token id="25" string="defense" />
            <token id="26" string="attorneys" />
            <token id="27" string="and" />
            <token id="28" string="Municipal" />
            <token id="29" string="Judge" />
            <token id="30" string="Aviva" />
            <token id="31" string="Bobb" />
          </tokens>
        </chunking>
        <chunking id="25" string="16 days" type="NP">
          <tokens>
            <token id="45" string="16" />
            <token id="46" string="days" />
          </tokens>
        </chunking>
        <chunking id="26" string="frequent shouting matches between three prosecutors , nine defense attorneys and Municipal Judge Aviva Bobb , and lengthy questioning of child witnesses that lasted in one instance for 16 days" type="NP">
          <tokens>
            <token id="17" string="frequent" />
            <token id="18" string="shouting" />
            <token id="19" string="matches" />
            <token id="20" string="between" />
            <token id="21" string="three" />
            <token id="22" string="prosecutors" />
            <token id="23" string="," />
            <token id="24" string="nine" />
            <token id="25" string="defense" />
            <token id="26" string="attorneys" />
            <token id="27" string="and" />
            <token id="28" string="Municipal" />
            <token id="29" string="Judge" />
            <token id="30" string="Aviva" />
            <token id="31" string="Bobb" />
            <token id="32" string="," />
            <token id="33" string="and" />
            <token id="34" string="lengthy" />
            <token id="35" string="questioning" />
            <token id="36" string="of" />
            <token id="37" string="child" />
            <token id="38" string="witnesses" />
            <token id="39" string="that" />
            <token id="40" string="lasted" />
            <token id="41" string="in" />
            <token id="42" string="one" />
            <token id="43" string="instance" />
            <token id="44" string="for" />
            <token id="45" string="16" />
            <token id="46" string="days" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="4">hearing</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="4">hearing</governor>
          <dependent id="2">18-month</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="4">hearing</governor>
          <dependent id="3">preliminary</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="15">marked</governor>
          <dependent id="4">hearing</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">began</governor>
          <dependent id="6">which</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="4">hearing</governor>
          <dependent id="7">began</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">fall</governor>
          <dependent id="8">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">fall</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">began</governor>
          <dependent id="10">fall</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">1984</governor>
          <dependent id="11">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">fall</governor>
          <dependent id="12">1984</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="15">marked</governor>
          <dependent id="14">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="15">marked</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">frequent</governor>
          <dependent id="16">by</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">marked</governor>
          <dependent id="17">frequent</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="17">frequent</governor>
          <dependent id="18">shouting</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="18">shouting</governor>
          <dependent id="19">matches</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">prosecutors</governor>
          <dependent id="20">between</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="22">prosecutors</governor>
          <dependent id="21">three</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">matches</governor>
          <dependent id="22">prosecutors</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="26">attorneys</governor>
          <dependent id="24">nine</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="26">attorneys</governor>
          <dependent id="25">defense</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="22">prosecutors</governor>
          <dependent id="26">attorneys</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="22">prosecutors</governor>
          <dependent id="27">and</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="31">Bobb</governor>
          <dependent id="28">Municipal</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="31">Bobb</governor>
          <dependent id="29">Judge</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="31">Bobb</governor>
          <dependent id="30">Aviva</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="22">prosecutors</governor>
          <dependent id="31">Bobb</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="17">frequent</governor>
          <dependent id="33">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="17">frequent</governor>
          <dependent id="34">lengthy</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="34">lengthy</governor>
          <dependent id="35">questioning</dependent>
        </dependency>
        <dependency type="case">
          <governor id="38">witnesses</governor>
          <dependent id="36">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="38">witnesses</governor>
          <dependent id="37">child</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="34">lengthy</governor>
          <dependent id="38">witnesses</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="40">lasted</governor>
          <dependent id="39">that</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="34">lengthy</governor>
          <dependent id="40">lasted</dependent>
        </dependency>
        <dependency type="case">
          <governor id="43">instance</governor>
          <dependent id="41">in</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="43">instance</governor>
          <dependent id="42">one</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="40">lasted</governor>
          <dependent id="43">instance</dependent>
        </dependency>
        <dependency type="case">
          <governor id="46">days</governor>
          <dependent id="44">for</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="46">days</governor>
          <dependent id="45">16</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="40">lasted</governor>
          <dependent id="46">days</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="nine" type="NUMBER" score="0.0">
          <tokens>
            <token id="24" string="nine" />
          </tokens>
        </entity>
        <entity id="2" string="one" type="NUMBER" score="0.0">
          <tokens>
            <token id="42" string="one" />
          </tokens>
        </entity>
        <entity id="3" string="Aviva Bobb" type="PERSON" score="0.0">
          <tokens>
            <token id="30" string="Aviva" />
            <token id="31" string="Bobb" />
          </tokens>
        </entity>
        <entity id="4" string="16 days" type="DURATION" score="0.0">
          <tokens>
            <token id="45" string="16" />
            <token id="46" string="days" />
          </tokens>
        </entity>
        <entity id="5" string="18-month" type="DURATION" score="0.0">
          <tokens>
            <token id="2" string="18-month" />
          </tokens>
        </entity>
        <entity id="6" string="the fall of 1984" type="DATE" score="0.0">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="fall" />
            <token id="11" string="of" />
            <token id="12" string="1984" />
          </tokens>
        </entity>
        <entity id="7" string="three" type="NUMBER" score="0.0">
          <tokens>
            <token id="21" string="three" />
          </tokens>
        </entity>
        <entity id="8" string="Judge" type="TITLE" score="0.0">
          <tokens>
            <token id="29" string="Judge" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="94" has_coreference="true">
      <content>Los Angeles was riveted by the daily images of youngsters clambering atop a booster chair in their Sunday best to face their alleged molesters.</content>
      <tokens>
        <token id="1" string="Los" lemma="Los" stem="lo" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="2" string="Angeles" lemma="Angeles" stem="angele" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="3" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="riveted" lemma="rivet" stem="rivet" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="daily" lemma="daily" stem="daili" pos="JJ" type="Word" isStopWord="false" ner="SET" is_referenced="false" is_refers="false" />
        <token id="8" string="images" lemma="image" stem="imag" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="youngsters" lemma="youngster" stem="youngster" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="clambering" lemma="clamber" stem="clamber" pos="VBG" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="atop" lemma="atop" stem="atop" pos="IN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="13" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="booster" lemma="booster" stem="booster" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="chair" lemma="chair" stem="chair" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="16" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="17" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="18" string="Sunday" lemma="Sunday" stem="sundai" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="19" string="best" lemma="best" stem="best" pos="JJS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="20" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="21" string="face" lemma="face" stem="face" pos="VB" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="22" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="23" string="alleged" lemma="allege" stem="alleg" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="24" string="molesters" lemma="molester" stem="molest" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="25" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Los) (NNP Angeles)) (VP (VBD was) (VP (VBN riveted) (PP (IN by) (NP (NP (DT the) (JJ daily) (NNS images)) (PP (IN of) (NP (NP (NNS youngsters)) (VP (VBG clambering) (PP (IN atop) (NP (NP (DT a) (NN booster) (NN chair)) (PP (IN in) (NP (PRP$ their) (NNP Sunday))))) (ADVP (JJS best)) (S (VP (TO to) (VP (VB face) (NP (PRP$ their) (VBN alleged) (NNS molesters)))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="face their alleged molesters" type="VP">
          <tokens>
            <token id="21" string="face" />
            <token id="22" string="their" />
            <token id="23" string="alleged" />
            <token id="24" string="molesters" />
          </tokens>
        </chunking>
        <chunking id="2" string="a booster chair" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="booster" />
            <token id="15" string="chair" />
          </tokens>
        </chunking>
        <chunking id="3" string="was riveted by the daily images of youngsters clambering atop a booster chair in their Sunday best to face their alleged molesters" type="VP">
          <tokens>
            <token id="3" string="was" />
            <token id="4" string="riveted" />
            <token id="5" string="by" />
            <token id="6" string="the" />
            <token id="7" string="daily" />
            <token id="8" string="images" />
            <token id="9" string="of" />
            <token id="10" string="youngsters" />
            <token id="11" string="clambering" />
            <token id="12" string="atop" />
            <token id="13" string="a" />
            <token id="14" string="booster" />
            <token id="15" string="chair" />
            <token id="16" string="in" />
            <token id="17" string="their" />
            <token id="18" string="Sunday" />
            <token id="19" string="best" />
            <token id="20" string="to" />
            <token id="21" string="face" />
            <token id="22" string="their" />
            <token id="23" string="alleged" />
            <token id="24" string="molesters" />
          </tokens>
        </chunking>
        <chunking id="4" string="youngsters clambering atop a booster chair in their Sunday best to face their alleged molesters" type="NP">
          <tokens>
            <token id="10" string="youngsters" />
            <token id="11" string="clambering" />
            <token id="12" string="atop" />
            <token id="13" string="a" />
            <token id="14" string="booster" />
            <token id="15" string="chair" />
            <token id="16" string="in" />
            <token id="17" string="their" />
            <token id="18" string="Sunday" />
            <token id="19" string="best" />
            <token id="20" string="to" />
            <token id="21" string="face" />
            <token id="22" string="their" />
            <token id="23" string="alleged" />
            <token id="24" string="molesters" />
          </tokens>
        </chunking>
        <chunking id="5" string="youngsters" type="NP">
          <tokens>
            <token id="10" string="youngsters" />
          </tokens>
        </chunking>
        <chunking id="6" string="riveted by the daily images of youngsters clambering atop a booster chair in their Sunday best to face their alleged molesters" type="VP">
          <tokens>
            <token id="4" string="riveted" />
            <token id="5" string="by" />
            <token id="6" string="the" />
            <token id="7" string="daily" />
            <token id="8" string="images" />
            <token id="9" string="of" />
            <token id="10" string="youngsters" />
            <token id="11" string="clambering" />
            <token id="12" string="atop" />
            <token id="13" string="a" />
            <token id="14" string="booster" />
            <token id="15" string="chair" />
            <token id="16" string="in" />
            <token id="17" string="their" />
            <token id="18" string="Sunday" />
            <token id="19" string="best" />
            <token id="20" string="to" />
            <token id="21" string="face" />
            <token id="22" string="their" />
            <token id="23" string="alleged" />
            <token id="24" string="molesters" />
          </tokens>
        </chunking>
        <chunking id="7" string="clambering atop a booster chair in their Sunday best to face their alleged molesters" type="VP">
          <tokens>
            <token id="11" string="clambering" />
            <token id="12" string="atop" />
            <token id="13" string="a" />
            <token id="14" string="booster" />
            <token id="15" string="chair" />
            <token id="16" string="in" />
            <token id="17" string="their" />
            <token id="18" string="Sunday" />
            <token id="19" string="best" />
            <token id="20" string="to" />
            <token id="21" string="face" />
            <token id="22" string="their" />
            <token id="23" string="alleged" />
            <token id="24" string="molesters" />
          </tokens>
        </chunking>
        <chunking id="8" string="their alleged molesters" type="NP">
          <tokens>
            <token id="22" string="their" />
            <token id="23" string="alleged" />
            <token id="24" string="molesters" />
          </tokens>
        </chunking>
        <chunking id="9" string="their Sunday" type="NP">
          <tokens>
            <token id="17" string="their" />
            <token id="18" string="Sunday" />
          </tokens>
        </chunking>
        <chunking id="10" string="a booster chair in their Sunday" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="booster" />
            <token id="15" string="chair" />
            <token id="16" string="in" />
            <token id="17" string="their" />
            <token id="18" string="Sunday" />
          </tokens>
        </chunking>
        <chunking id="11" string="the daily images" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="daily" />
            <token id="8" string="images" />
          </tokens>
        </chunking>
        <chunking id="12" string="Los Angeles" type="NP">
          <tokens>
            <token id="1" string="Los" />
            <token id="2" string="Angeles" />
          </tokens>
        </chunking>
        <chunking id="13" string="the daily images of youngsters clambering atop a booster chair in their Sunday best to face their alleged molesters" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="daily" />
            <token id="8" string="images" />
            <token id="9" string="of" />
            <token id="10" string="youngsters" />
            <token id="11" string="clambering" />
            <token id="12" string="atop" />
            <token id="13" string="a" />
            <token id="14" string="booster" />
            <token id="15" string="chair" />
            <token id="16" string="in" />
            <token id="17" string="their" />
            <token id="18" string="Sunday" />
            <token id="19" string="best" />
            <token id="20" string="to" />
            <token id="21" string="face" />
            <token id="22" string="their" />
            <token id="23" string="alleged" />
            <token id="24" string="molesters" />
          </tokens>
        </chunking>
        <chunking id="14" string="to face their alleged molesters" type="VP">
          <tokens>
            <token id="20" string="to" />
            <token id="21" string="face" />
            <token id="22" string="their" />
            <token id="23" string="alleged" />
            <token id="24" string="molesters" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">Angeles</governor>
          <dependent id="1">Los</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="4">riveted</governor>
          <dependent id="2">Angeles</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="4">riveted</governor>
          <dependent id="3">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">riveted</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">images</governor>
          <dependent id="5">by</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">images</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">images</governor>
          <dependent id="7">daily</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">riveted</governor>
          <dependent id="8">images</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">youngsters</governor>
          <dependent id="9">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">images</governor>
          <dependent id="10">youngsters</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="10">youngsters</governor>
          <dependent id="11">clambering</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">chair</governor>
          <dependent id="12">atop</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">chair</governor>
          <dependent id="13">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">chair</governor>
          <dependent id="14">booster</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">clambering</governor>
          <dependent id="15">chair</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">Sunday</governor>
          <dependent id="16">in</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="18">Sunday</governor>
          <dependent id="17">their</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">chair</governor>
          <dependent id="18">Sunday</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">clambering</governor>
          <dependent id="19">best</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="21">face</governor>
          <dependent id="20">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="11">clambering</governor>
          <dependent id="21">face</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="24">molesters</governor>
          <dependent id="22">their</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="24">molesters</governor>
          <dependent id="23">alleged</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="21">face</governor>
          <dependent id="24">molesters</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="daily" type="SET" score="0.0">
          <tokens>
            <token id="7" string="daily" />
          </tokens>
        </entity>
        <entity id="2" string="Sunday" type="DATE" score="0.0">
          <tokens>
            <token id="18" string="Sunday" />
          </tokens>
        </entity>
        <entity id="3" string="Los Angeles" type="LOCATION" score="0.0">
          <tokens>
            <token id="1" string="Los" />
            <token id="2" string="Angeles" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="95" has_coreference="true">
      <content>They munched cookies, swigged root beer, and sometimes sobbed as they told of their experiences at McMartin.</content>
      <tokens>
        <token id="1" string="They" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="munched" lemma="munch" stem="munch" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="cookies" lemma="cookie" stem="cooki" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="swigged" lemma="swig" stem="swig" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="root" lemma="root" stem="root" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="beer" lemma="beer" stem="beer" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="sometimes" lemma="sometimes" stem="sometim" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="sobbed" lemma="sob" stem="sob" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="told" lemma="tell" stem="told" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="experiences" lemma="experience" stem="experi" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="20" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP They)) (VP (VP (VBD munched) (NP (NNS cookies))) (, ,) (VP (VBD swigged) (NP (NN root) (NN beer))) (, ,) (CC and) (ADVP (RB sometimes)) (VP (VBD sobbed) (SBAR (IN as) (S (NP (PRP they)) (VP (VBD told) (PP (IN of) (NP (PRP$ their) (NNS experiences))) (PP (IN at) (NP (NNP McMartin)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="They" type="NP">
          <tokens>
            <token id="1" string="They" />
          </tokens>
        </chunking>
        <chunking id="2" string="they" type="NP">
          <tokens>
            <token id="13" string="they" />
          </tokens>
        </chunking>
        <chunking id="3" string="munched cookies" type="VP">
          <tokens>
            <token id="2" string="munched" />
            <token id="3" string="cookies" />
          </tokens>
        </chunking>
        <chunking id="4" string="McMartin" type="NP">
          <tokens>
            <token id="19" string="McMartin" />
          </tokens>
        </chunking>
        <chunking id="5" string="told of their experiences at McMartin" type="VP">
          <tokens>
            <token id="14" string="told" />
            <token id="15" string="of" />
            <token id="16" string="their" />
            <token id="17" string="experiences" />
            <token id="18" string="at" />
            <token id="19" string="McMartin" />
          </tokens>
        </chunking>
        <chunking id="6" string="their experiences" type="NP">
          <tokens>
            <token id="16" string="their" />
            <token id="17" string="experiences" />
          </tokens>
        </chunking>
        <chunking id="7" string="sobbed as they told of their experiences at McMartin" type="VP">
          <tokens>
            <token id="11" string="sobbed" />
            <token id="12" string="as" />
            <token id="13" string="they" />
            <token id="14" string="told" />
            <token id="15" string="of" />
            <token id="16" string="their" />
            <token id="17" string="experiences" />
            <token id="18" string="at" />
            <token id="19" string="McMartin" />
          </tokens>
        </chunking>
        <chunking id="8" string="as they told of their experiences at McMartin" type="SBAR">
          <tokens>
            <token id="12" string="as" />
            <token id="13" string="they" />
            <token id="14" string="told" />
            <token id="15" string="of" />
            <token id="16" string="their" />
            <token id="17" string="experiences" />
            <token id="18" string="at" />
            <token id="19" string="McMartin" />
          </tokens>
        </chunking>
        <chunking id="9" string="swigged root beer" type="VP">
          <tokens>
            <token id="5" string="swigged" />
            <token id="6" string="root" />
            <token id="7" string="beer" />
          </tokens>
        </chunking>
        <chunking id="10" string="munched cookies , swigged root beer , and sometimes sobbed as they told of their experiences at McMartin" type="VP">
          <tokens>
            <token id="2" string="munched" />
            <token id="3" string="cookies" />
            <token id="4" string="," />
            <token id="5" string="swigged" />
            <token id="6" string="root" />
            <token id="7" string="beer" />
            <token id="8" string="," />
            <token id="9" string="and" />
            <token id="10" string="sometimes" />
            <token id="11" string="sobbed" />
            <token id="12" string="as" />
            <token id="13" string="they" />
            <token id="14" string="told" />
            <token id="15" string="of" />
            <token id="16" string="their" />
            <token id="17" string="experiences" />
            <token id="18" string="at" />
            <token id="19" string="McMartin" />
          </tokens>
        </chunking>
        <chunking id="11" string="cookies" type="NP">
          <tokens>
            <token id="3" string="cookies" />
          </tokens>
        </chunking>
        <chunking id="12" string="root beer" type="NP">
          <tokens>
            <token id="6" string="root" />
            <token id="7" string="beer" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">munched</governor>
          <dependent id="1">They</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">munched</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">munched</governor>
          <dependent id="3">cookies</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="2">munched</governor>
          <dependent id="5">swigged</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">beer</governor>
          <dependent id="6">root</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">swigged</governor>
          <dependent id="7">beer</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="2">munched</governor>
          <dependent id="9">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">sobbed</governor>
          <dependent id="10">sometimes</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="2">munched</governor>
          <dependent id="11">sobbed</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="14">told</governor>
          <dependent id="12">as</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">told</governor>
          <dependent id="13">they</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="11">sobbed</governor>
          <dependent id="14">told</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">experiences</governor>
          <dependent id="15">of</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="17">experiences</governor>
          <dependent id="16">their</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">told</governor>
          <dependent id="17">experiences</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">McMartin</governor>
          <dependent id="18">at</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">told</governor>
          <dependent id="19">McMartin</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="McMartin" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="19" string="McMartin" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="96" has_coreference="true">
      <content>Perhaps the most poignant moment came when an 8-year-old girl told in a wispy voice of having been raped, photographed, tied up and placed in a dark closet by her teachers five years earlier.</content>
      <tokens>
        <token id="1" string="Perhaps" lemma="perhaps" stem="perhap" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="most" lemma="most" stem="most" pos="RBS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="poignant" lemma="poignant" stem="poignant" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="moment" lemma="moment" stem="moment" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="came" lemma="come" stem="came" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="8-year-old" lemma="8-year-old" stem="8-year-old" pos="JJ" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="10" string="girl" lemma="girl" stem="girl" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="told" lemma="tell" stem="told" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="wispy" lemma="wispy" stem="wispi" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="voice" lemma="voice" stem="voic" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="having" lemma="have" stem="have" pos="VBG" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="raped" lemma="rape" stem="rape" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="photographed" lemma="photograph" stem="photograph" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="tied" lemma="tie" stem="ti" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="up" lemma="up" stem="up" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="placed" lemma="place" stem="place" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="dark" lemma="dark" stem="dark" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="closet" lemma="closet" stem="closet" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="33" string="teachers" lemma="teacher" stem="teacher" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="34" string="five" lemma="five" stem="five" pos="CD" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="35" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="36" string="earlier" lemma="earlier" stem="earlier" pos="RBR" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="37" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (ADVP (RB Perhaps)) (NP (DT the) (RBS most) (JJ poignant) (NN moment)) (VP (VBD came) (SBAR (WHADVP (WRB when)) (S (NP (DT an) (JJ 8-year-old) (NN girl)) (VP (VBD told) (PP (IN in) (NP (NP (DT a) (JJ wispy) (NN voice)) (PP (IN of) (S (VP (VBG having) (VP (VP (VBN been) (VP (VBN raped))) (, ,) (VP (VBN photographed)) (, ,) (VP (VBN tied) (ADVP (RB up))) (CC and) (VP (VBN placed) (PP (IN in) (NP (DT a) (JJ dark) (NN closet))) (PP (IN by) (NP (PRP$ her) (NNS teachers)) (ADVP (NP (CD five) (NNS years)) (RBR earlier)))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="an 8-year-old girl" type="NP">
          <tokens>
            <token id="8" string="an" />
            <token id="9" string="8-year-old" />
            <token id="10" string="girl" />
          </tokens>
        </chunking>
        <chunking id="2" string="been raped , photographed , tied up and placed in a dark closet by her teachers five years earlier" type="VP">
          <tokens>
            <token id="18" string="been" />
            <token id="19" string="raped" />
            <token id="20" string="," />
            <token id="21" string="photographed" />
            <token id="22" string="," />
            <token id="23" string="tied" />
            <token id="24" string="up" />
            <token id="25" string="and" />
            <token id="26" string="placed" />
            <token id="27" string="in" />
            <token id="28" string="a" />
            <token id="29" string="dark" />
            <token id="30" string="closet" />
            <token id="31" string="by" />
            <token id="32" string="her" />
            <token id="33" string="teachers" />
            <token id="34" string="five" />
            <token id="35" string="years" />
            <token id="36" string="earlier" />
          </tokens>
        </chunking>
        <chunking id="3" string="photographed" type="VP">
          <tokens>
            <token id="21" string="photographed" />
          </tokens>
        </chunking>
        <chunking id="4" string="having been raped , photographed , tied up and placed in a dark closet by her teachers five years earlier" type="VP">
          <tokens>
            <token id="17" string="having" />
            <token id="18" string="been" />
            <token id="19" string="raped" />
            <token id="20" string="," />
            <token id="21" string="photographed" />
            <token id="22" string="," />
            <token id="23" string="tied" />
            <token id="24" string="up" />
            <token id="25" string="and" />
            <token id="26" string="placed" />
            <token id="27" string="in" />
            <token id="28" string="a" />
            <token id="29" string="dark" />
            <token id="30" string="closet" />
            <token id="31" string="by" />
            <token id="32" string="her" />
            <token id="33" string="teachers" />
            <token id="34" string="five" />
            <token id="35" string="years" />
            <token id="36" string="earlier" />
          </tokens>
        </chunking>
        <chunking id="5" string="raped" type="VP">
          <tokens>
            <token id="19" string="raped" />
          </tokens>
        </chunking>
        <chunking id="6" string="came when an 8-year-old girl told in a wispy voice of having been raped , photographed , tied up and placed in a dark closet by her teachers five years earlier" type="VP">
          <tokens>
            <token id="6" string="came" />
            <token id="7" string="when" />
            <token id="8" string="an" />
            <token id="9" string="8-year-old" />
            <token id="10" string="girl" />
            <token id="11" string="told" />
            <token id="12" string="in" />
            <token id="13" string="a" />
            <token id="14" string="wispy" />
            <token id="15" string="voice" />
            <token id="16" string="of" />
            <token id="17" string="having" />
            <token id="18" string="been" />
            <token id="19" string="raped" />
            <token id="20" string="," />
            <token id="21" string="photographed" />
            <token id="22" string="," />
            <token id="23" string="tied" />
            <token id="24" string="up" />
            <token id="25" string="and" />
            <token id="26" string="placed" />
            <token id="27" string="in" />
            <token id="28" string="a" />
            <token id="29" string="dark" />
            <token id="30" string="closet" />
            <token id="31" string="by" />
            <token id="32" string="her" />
            <token id="33" string="teachers" />
            <token id="34" string="five" />
            <token id="35" string="years" />
            <token id="36" string="earlier" />
          </tokens>
        </chunking>
        <chunking id="7" string="placed in a dark closet by her teachers five years earlier" type="VP">
          <tokens>
            <token id="26" string="placed" />
            <token id="27" string="in" />
            <token id="28" string="a" />
            <token id="29" string="dark" />
            <token id="30" string="closet" />
            <token id="31" string="by" />
            <token id="32" string="her" />
            <token id="33" string="teachers" />
            <token id="34" string="five" />
            <token id="35" string="years" />
            <token id="36" string="earlier" />
          </tokens>
        </chunking>
        <chunking id="8" string="when" type="WHADVP">
          <tokens>
            <token id="7" string="when" />
          </tokens>
        </chunking>
        <chunking id="9" string="five years" type="NP">
          <tokens>
            <token id="34" string="five" />
            <token id="35" string="years" />
          </tokens>
        </chunking>
        <chunking id="10" string="told in a wispy voice of having been raped , photographed , tied up and placed in a dark closet by her teachers five years earlier" type="VP">
          <tokens>
            <token id="11" string="told" />
            <token id="12" string="in" />
            <token id="13" string="a" />
            <token id="14" string="wispy" />
            <token id="15" string="voice" />
            <token id="16" string="of" />
            <token id="17" string="having" />
            <token id="18" string="been" />
            <token id="19" string="raped" />
            <token id="20" string="," />
            <token id="21" string="photographed" />
            <token id="22" string="," />
            <token id="23" string="tied" />
            <token id="24" string="up" />
            <token id="25" string="and" />
            <token id="26" string="placed" />
            <token id="27" string="in" />
            <token id="28" string="a" />
            <token id="29" string="dark" />
            <token id="30" string="closet" />
            <token id="31" string="by" />
            <token id="32" string="her" />
            <token id="33" string="teachers" />
            <token id="34" string="five" />
            <token id="35" string="years" />
            <token id="36" string="earlier" />
          </tokens>
        </chunking>
        <chunking id="11" string="when an 8-year-old girl told in a wispy voice of having been raped , photographed , tied up and placed in a dark closet by her teachers five years earlier" type="SBAR">
          <tokens>
            <token id="7" string="when" />
            <token id="8" string="an" />
            <token id="9" string="8-year-old" />
            <token id="10" string="girl" />
            <token id="11" string="told" />
            <token id="12" string="in" />
            <token id="13" string="a" />
            <token id="14" string="wispy" />
            <token id="15" string="voice" />
            <token id="16" string="of" />
            <token id="17" string="having" />
            <token id="18" string="been" />
            <token id="19" string="raped" />
            <token id="20" string="," />
            <token id="21" string="photographed" />
            <token id="22" string="," />
            <token id="23" string="tied" />
            <token id="24" string="up" />
            <token id="25" string="and" />
            <token id="26" string="placed" />
            <token id="27" string="in" />
            <token id="28" string="a" />
            <token id="29" string="dark" />
            <token id="30" string="closet" />
            <token id="31" string="by" />
            <token id="32" string="her" />
            <token id="33" string="teachers" />
            <token id="34" string="five" />
            <token id="35" string="years" />
            <token id="36" string="earlier" />
          </tokens>
        </chunking>
        <chunking id="12" string="a wispy voice of having been raped , photographed , tied up and placed in a dark closet by her teachers five years earlier" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="wispy" />
            <token id="15" string="voice" />
            <token id="16" string="of" />
            <token id="17" string="having" />
            <token id="18" string="been" />
            <token id="19" string="raped" />
            <token id="20" string="," />
            <token id="21" string="photographed" />
            <token id="22" string="," />
            <token id="23" string="tied" />
            <token id="24" string="up" />
            <token id="25" string="and" />
            <token id="26" string="placed" />
            <token id="27" string="in" />
            <token id="28" string="a" />
            <token id="29" string="dark" />
            <token id="30" string="closet" />
            <token id="31" string="by" />
            <token id="32" string="her" />
            <token id="33" string="teachers" />
            <token id="34" string="five" />
            <token id="35" string="years" />
            <token id="36" string="earlier" />
          </tokens>
        </chunking>
        <chunking id="13" string="her teachers" type="NP">
          <tokens>
            <token id="32" string="her" />
            <token id="33" string="teachers" />
          </tokens>
        </chunking>
        <chunking id="14" string="a wispy voice" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="wispy" />
            <token id="15" string="voice" />
          </tokens>
        </chunking>
        <chunking id="15" string="the most poignant moment" type="NP">
          <tokens>
            <token id="2" string="the" />
            <token id="3" string="most" />
            <token id="4" string="poignant" />
            <token id="5" string="moment" />
          </tokens>
        </chunking>
        <chunking id="16" string="tied up" type="VP">
          <tokens>
            <token id="23" string="tied" />
            <token id="24" string="up" />
          </tokens>
        </chunking>
        <chunking id="17" string="been raped" type="VP">
          <tokens>
            <token id="18" string="been" />
            <token id="19" string="raped" />
          </tokens>
        </chunking>
        <chunking id="18" string="a dark closet" type="NP">
          <tokens>
            <token id="28" string="a" />
            <token id="29" string="dark" />
            <token id="30" string="closet" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="6">came</governor>
          <dependent id="1">Perhaps</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">moment</governor>
          <dependent id="2">the</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">moment</governor>
          <dependent id="3">most</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">moment</governor>
          <dependent id="4">poignant</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">came</governor>
          <dependent id="5">moment</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">came</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">told</governor>
          <dependent id="7">when</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">girl</governor>
          <dependent id="8">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="10">girl</governor>
          <dependent id="9">8-year-old</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">told</governor>
          <dependent id="10">girl</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="6">came</governor>
          <dependent id="11">told</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">voice</governor>
          <dependent id="12">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">voice</governor>
          <dependent id="13">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">voice</governor>
          <dependent id="14">wispy</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">told</governor>
          <dependent id="15">voice</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="19">raped</governor>
          <dependent id="16">of</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="19">raped</governor>
          <dependent id="17">having</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="19">raped</governor>
          <dependent id="18">been</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="15">voice</governor>
          <dependent id="19">raped</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="19">raped</governor>
          <dependent id="21">photographed</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="19">raped</governor>
          <dependent id="23">tied</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="23">tied</governor>
          <dependent id="24">up</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="19">raped</governor>
          <dependent id="25">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="19">raped</governor>
          <dependent id="26">placed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">closet</governor>
          <dependent id="27">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="30">closet</governor>
          <dependent id="28">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="30">closet</governor>
          <dependent id="29">dark</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">placed</governor>
          <dependent id="30">closet</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">teachers</governor>
          <dependent id="31">by</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="33">teachers</governor>
          <dependent id="32">her</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">placed</governor>
          <dependent id="33">teachers</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="35">years</governor>
          <dependent id="34">five</dependent>
        </dependency>
        <dependency type="nmod:npmod">
          <governor id="36">earlier</governor>
          <dependent id="35">years</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="33">teachers</governor>
          <dependent id="36">earlier</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="8-year-old" type="DURATION" score="0.0">
          <tokens>
            <token id="9" string="8-year-old" />
          </tokens>
        </entity>
        <entity id="2" string="five years earlier" type="DATE" score="0.0">
          <tokens>
            <token id="34" string="five" />
            <token id="35" string="years" />
            <token id="36" string="earlier" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="97" has_coreference="true">
      <content>In the end, Bobb ordered all seven defendants to stand trial on 135 charges, telling each, &amp;quot;The court believes there is sufficient cause to believe you are guilty.&amp;quot;</content>
      <tokens>
        <token id="1" string="In" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="end" lemma="end" stem="end" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="Bobb" lemma="Bobb" stem="bobb" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="6" string="ordered" lemma="order" stem="order" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="seven" lemma="seven" stem="seven" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="9" string="defendants" lemma="defendant" stem="defend" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="stand" lemma="stand" stem="stand" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="trial" lemma="trial" stem="trial" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="135" lemma="135" stem="135" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="15" string="charges" lemma="charge" stem="charg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="telling" lemma="tell" stem="tell" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="each" lemma="each" stem="each" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="court" lemma="court" stem="court" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="believes" lemma="believe" stem="believ" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="there" lemma="there" stem="there" pos="EX" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="sufficient" lemma="sufficient" stem="suffici" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="cause" lemma="cause" stem="caus" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="believe" lemma="believe" stem="believ" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="you" lemma="you" stem="you" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="are" lemma="be" stem="ar" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="guilty" lemma="guilty" stem="guilti" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (IN In) (NP (DT the) (NN end))) (PRN (, ,) (S (NP (NNP Bobb)) (VP (VBD ordered) (S (NP (DT all) (CD seven) (NNS defendants)) (VP (TO to) (VP (VB stand) (NP (NN trial)) (PP (IN on) (NP (CD 135) (NNS charges)))))) (, ,) (S (VP (VBG telling) (NP (DT each)))))) (, ,)) (`` ``) (NP (DT The) (NN court)) (VP (VBZ believes) (SBAR (S (NP (EX there)) (VP (VBZ is) (NP (JJ sufficient) (NN cause) (S (VP (TO to) (VP (VB believe) (SBAR (S (NP (PRP you)) (VP (VBP are) (ADJP (JJ guilty))))))))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="the end" type="NP">
          <tokens>
            <token id="2" string="the" />
            <token id="3" string="end" />
          </tokens>
        </chunking>
        <chunking id="2" string="ordered all seven defendants to stand trial on 135 charges , telling each" type="VP">
          <tokens>
            <token id="6" string="ordered" />
            <token id="7" string="all" />
            <token id="8" string="seven" />
            <token id="9" string="defendants" />
            <token id="10" string="to" />
            <token id="11" string="stand" />
            <token id="12" string="trial" />
            <token id="13" string="on" />
            <token id="14" string="135" />
            <token id="15" string="charges" />
            <token id="16" string="," />
            <token id="17" string="telling" />
            <token id="18" string="each" />
          </tokens>
        </chunking>
        <chunking id="3" string="to stand trial on 135 charges" type="VP">
          <tokens>
            <token id="10" string="to" />
            <token id="11" string="stand" />
            <token id="12" string="trial" />
            <token id="13" string="on" />
            <token id="14" string="135" />
            <token id="15" string="charges" />
          </tokens>
        </chunking>
        <chunking id="4" string="believe you are guilty" type="VP">
          <tokens>
            <token id="29" string="believe" />
            <token id="30" string="you" />
            <token id="31" string="are" />
            <token id="32" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="5" string="trial" type="NP">
          <tokens>
            <token id="12" string="trial" />
          </tokens>
        </chunking>
        <chunking id="6" string="believes there is sufficient cause to believe you are guilty" type="VP">
          <tokens>
            <token id="23" string="believes" />
            <token id="24" string="there" />
            <token id="25" string="is" />
            <token id="26" string="sufficient" />
            <token id="27" string="cause" />
            <token id="28" string="to" />
            <token id="29" string="believe" />
            <token id="30" string="you" />
            <token id="31" string="are" />
            <token id="32" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="7" string="is sufficient cause to believe you are guilty" type="VP">
          <tokens>
            <token id="25" string="is" />
            <token id="26" string="sufficient" />
            <token id="27" string="cause" />
            <token id="28" string="to" />
            <token id="29" string="believe" />
            <token id="30" string="you" />
            <token id="31" string="are" />
            <token id="32" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="8" string="each" type="NP">
          <tokens>
            <token id="18" string="each" />
          </tokens>
        </chunking>
        <chunking id="9" string="there is sufficient cause to believe you are guilty" type="SBAR">
          <tokens>
            <token id="24" string="there" />
            <token id="25" string="is" />
            <token id="26" string="sufficient" />
            <token id="27" string="cause" />
            <token id="28" string="to" />
            <token id="29" string="believe" />
            <token id="30" string="you" />
            <token id="31" string="are" />
            <token id="32" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="10" string="there" type="NP">
          <tokens>
            <token id="24" string="there" />
          </tokens>
        </chunking>
        <chunking id="11" string="guilty" type="ADJP">
          <tokens>
            <token id="32" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="12" string="stand trial on 135 charges" type="VP">
          <tokens>
            <token id="11" string="stand" />
            <token id="12" string="trial" />
            <token id="13" string="on" />
            <token id="14" string="135" />
            <token id="15" string="charges" />
          </tokens>
        </chunking>
        <chunking id="13" string="are guilty" type="VP">
          <tokens>
            <token id="31" string="are" />
            <token id="32" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="14" string="Bobb" type="NP">
          <tokens>
            <token id="5" string="Bobb" />
          </tokens>
        </chunking>
        <chunking id="15" string="135 charges" type="NP">
          <tokens>
            <token id="14" string="135" />
            <token id="15" string="charges" />
          </tokens>
        </chunking>
        <chunking id="16" string="to believe you are guilty" type="VP">
          <tokens>
            <token id="28" string="to" />
            <token id="29" string="believe" />
            <token id="30" string="you" />
            <token id="31" string="are" />
            <token id="32" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="17" string="sufficient cause to believe you are guilty" type="NP">
          <tokens>
            <token id="26" string="sufficient" />
            <token id="27" string="cause" />
            <token id="28" string="to" />
            <token id="29" string="believe" />
            <token id="30" string="you" />
            <token id="31" string="are" />
            <token id="32" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="18" string="all seven defendants" type="NP">
          <tokens>
            <token id="7" string="all" />
            <token id="8" string="seven" />
            <token id="9" string="defendants" />
          </tokens>
        </chunking>
        <chunking id="19" string="you are guilty" type="SBAR">
          <tokens>
            <token id="30" string="you" />
            <token id="31" string="are" />
            <token id="32" string="guilty" />
          </tokens>
        </chunking>
        <chunking id="20" string="telling each" type="VP">
          <tokens>
            <token id="17" string="telling" />
            <token id="18" string="each" />
          </tokens>
        </chunking>
        <chunking id="21" string="The court" type="NP">
          <tokens>
            <token id="21" string="The" />
            <token id="22" string="court" />
          </tokens>
        </chunking>
        <chunking id="22" string="you" type="NP">
          <tokens>
            <token id="30" string="you" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="case">
          <governor id="3">end</governor>
          <dependent id="1">In</dependent>
        </dependency>
        <dependency type="det">
          <governor id="3">end</governor>
          <dependent id="2">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">believes</governor>
          <dependent id="3">end</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">ordered</governor>
          <dependent id="5">Bobb</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="23">believes</governor>
          <dependent id="6">ordered</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">defendants</governor>
          <dependent id="7">all</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="9">defendants</governor>
          <dependent id="8">seven</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">ordered</governor>
          <dependent id="9">defendants</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="11">stand</governor>
          <dependent id="10">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="6">ordered</governor>
          <dependent id="11">stand</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">stand</governor>
          <dependent id="12">trial</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">charges</governor>
          <dependent id="13">on</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="15">charges</governor>
          <dependent id="14">135</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">stand</governor>
          <dependent id="15">charges</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="6">ordered</governor>
          <dependent id="17">telling</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="17">telling</governor>
          <dependent id="18">each</dependent>
        </dependency>
        <dependency type="det">
          <governor id="22">court</governor>
          <dependent id="21">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="23">believes</governor>
          <dependent id="22">court</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="23">believes</dependent>
        </dependency>
        <dependency type="expl">
          <governor id="25">is</governor>
          <dependent id="24">there</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="23">believes</governor>
          <dependent id="25">is</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="27">cause</governor>
          <dependent id="26">sufficient</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="25">is</governor>
          <dependent id="27">cause</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="29">believe</governor>
          <dependent id="28">to</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="27">cause</governor>
          <dependent id="29">believe</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="32">guilty</governor>
          <dependent id="30">you</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="32">guilty</governor>
          <dependent id="31">are</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="29">believe</governor>
          <dependent id="32">guilty</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="135" type="NUMBER" score="0.0">
          <tokens>
            <token id="14" string="135" />
          </tokens>
        </entity>
        <entity id="2" string="Bobb" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="Bobb" />
          </tokens>
        </entity>
        <entity id="3" string="seven" type="NUMBER" score="0.0">
          <tokens>
            <token id="8" string="seven" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="98" has_coreference="true">
      <content>The remaining charges had been dropped after only 14 of the scheduled child witnesses came forward to testify.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="remaining" lemma="remain" stem="remain" pos="VBG" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="charges" lemma="charge" stem="charg" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="dropped" lemma="drop" stem="drop" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="after" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="only" lemma="only" stem="onli" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="14" lemma="14" stem="14" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="10" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="scheduled" lemma="schedule" stem="schedul" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="child" lemma="child" stem="child" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="witnesses" lemma="witness" stem="wit" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="came" lemma="come" stem="came" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="forward" lemma="forward" stem="forward" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="testify" lemma="testify" stem="testifi" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (VBG remaining) (NNS charges)) (VP (VBD had) (VP (VBN been) (VP (VBN dropped) (SBAR (IN after) (S (NP (NP (QP (RB only) (CD 14))) (PP (IN of) (NP (DT the) (VBN scheduled) (NN child) (NNS witnesses)))) (VP (VBD came) (ADVP (RB forward)) (S (VP (TO to) (VP (VB testify)))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="The remaining charges" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="remaining" />
            <token id="3" string="charges" />
          </tokens>
        </chunking>
        <chunking id="2" string="came forward to testify" type="VP">
          <tokens>
            <token id="15" string="came" />
            <token id="16" string="forward" />
            <token id="17" string="to" />
            <token id="18" string="testify" />
          </tokens>
        </chunking>
        <chunking id="3" string="only 14" type="NP">
          <tokens>
            <token id="8" string="only" />
            <token id="9" string="14" />
          </tokens>
        </chunking>
        <chunking id="4" string="to testify" type="VP">
          <tokens>
            <token id="17" string="to" />
            <token id="18" string="testify" />
          </tokens>
        </chunking>
        <chunking id="5" string="the scheduled child witnesses" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="scheduled" />
            <token id="13" string="child" />
            <token id="14" string="witnesses" />
          </tokens>
        </chunking>
        <chunking id="6" string="had been dropped after only 14 of the scheduled child witnesses came forward to testify" type="VP">
          <tokens>
            <token id="4" string="had" />
            <token id="5" string="been" />
            <token id="6" string="dropped" />
            <token id="7" string="after" />
            <token id="8" string="only" />
            <token id="9" string="14" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="scheduled" />
            <token id="13" string="child" />
            <token id="14" string="witnesses" />
            <token id="15" string="came" />
            <token id="16" string="forward" />
            <token id="17" string="to" />
            <token id="18" string="testify" />
          </tokens>
        </chunking>
        <chunking id="7" string="been dropped after only 14 of the scheduled child witnesses came forward to testify" type="VP">
          <tokens>
            <token id="5" string="been" />
            <token id="6" string="dropped" />
            <token id="7" string="after" />
            <token id="8" string="only" />
            <token id="9" string="14" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="scheduled" />
            <token id="13" string="child" />
            <token id="14" string="witnesses" />
            <token id="15" string="came" />
            <token id="16" string="forward" />
            <token id="17" string="to" />
            <token id="18" string="testify" />
          </tokens>
        </chunking>
        <chunking id="8" string="only 14 of the scheduled child witnesses" type="NP">
          <tokens>
            <token id="8" string="only" />
            <token id="9" string="14" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="scheduled" />
            <token id="13" string="child" />
            <token id="14" string="witnesses" />
          </tokens>
        </chunking>
        <chunking id="9" string="after only 14 of the scheduled child witnesses came forward to testify" type="SBAR">
          <tokens>
            <token id="7" string="after" />
            <token id="8" string="only" />
            <token id="9" string="14" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="scheduled" />
            <token id="13" string="child" />
            <token id="14" string="witnesses" />
            <token id="15" string="came" />
            <token id="16" string="forward" />
            <token id="17" string="to" />
            <token id="18" string="testify" />
          </tokens>
        </chunking>
        <chunking id="10" string="testify" type="VP">
          <tokens>
            <token id="18" string="testify" />
          </tokens>
        </chunking>
        <chunking id="11" string="dropped after only 14 of the scheduled child witnesses came forward to testify" type="VP">
          <tokens>
            <token id="6" string="dropped" />
            <token id="7" string="after" />
            <token id="8" string="only" />
            <token id="9" string="14" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="scheduled" />
            <token id="13" string="child" />
            <token id="14" string="witnesses" />
            <token id="15" string="came" />
            <token id="16" string="forward" />
            <token id="17" string="to" />
            <token id="18" string="testify" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">charges</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">charges</governor>
          <dependent id="2">remaining</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="6">dropped</governor>
          <dependent id="3">charges</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="6">dropped</governor>
          <dependent id="4">had</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="6">dropped</governor>
          <dependent id="5">been</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">dropped</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="15">came</governor>
          <dependent id="7">after</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="9">14</governor>
          <dependent id="8">only</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">came</governor>
          <dependent id="9">14</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">witnesses</governor>
          <dependent id="10">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">witnesses</governor>
          <dependent id="11">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="14">witnesses</governor>
          <dependent id="12">scheduled</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">witnesses</governor>
          <dependent id="13">child</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">14</governor>
          <dependent id="14">witnesses</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="6">dropped</governor>
          <dependent id="15">came</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="15">came</governor>
          <dependent id="16">forward</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="18">testify</governor>
          <dependent id="17">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="15">came</governor>
          <dependent id="18">testify</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="14" type="NUMBER" score="0.0">
          <tokens>
            <token id="9" string="14" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="99" has_coreference="true">
      <content>One child was allowed to testify by closed-circuit television, under a new state law passed especially for the McMartin case.</content>
      <tokens>
        <token id="1" string="One" lemma="one" stem="one" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="2" string="child" lemma="child" stem="child" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="allowed" lemma="allow" stem="allow" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="testify" lemma="testify" stem="testifi" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="closed-circuit" lemma="closed-circuit" stem="closed-circuit" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="television" lemma="television" stem="televis" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="under" lemma="under" stem="under" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="new" lemma="new" stem="new" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string="state" lemma="state" stem="state" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="law" lemma="law" stem="law" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="16" string="passed" lemma="pass" stem="pass" pos="VBD" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="17" string="especially" lemma="especially" stem="especi" pos="RB" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="18" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="19" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="20" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="21" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="22" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (CD One) (NN child)) (VP (VBD was) (VP (VBN allowed) (S (VP (TO to) (VP (VB testify) (PP (IN by) (NP (JJ closed-circuit) (NN television)))))) (, ,) (PP (IN under) (NP (NP (DT a) (JJ new) (NN state) (NN law)) (VP (VBD passed) (ADVP (RB especially)) (PP (IN for) (NP (DT the) (NNP McMartin) (NN case)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="closed-circuit television" type="NP">
          <tokens>
            <token id="8" string="closed-circuit" />
            <token id="9" string="television" />
          </tokens>
        </chunking>
        <chunking id="2" string="One child" type="NP">
          <tokens>
            <token id="1" string="One" />
            <token id="2" string="child" />
          </tokens>
        </chunking>
        <chunking id="3" string="allowed to testify by closed-circuit television , under a new state law passed especially for the McMartin case" type="VP">
          <tokens>
            <token id="4" string="allowed" />
            <token id="5" string="to" />
            <token id="6" string="testify" />
            <token id="7" string="by" />
            <token id="8" string="closed-circuit" />
            <token id="9" string="television" />
            <token id="10" string="," />
            <token id="11" string="under" />
            <token id="12" string="a" />
            <token id="13" string="new" />
            <token id="14" string="state" />
            <token id="15" string="law" />
            <token id="16" string="passed" />
            <token id="17" string="especially" />
            <token id="18" string="for" />
            <token id="19" string="the" />
            <token id="20" string="McMartin" />
            <token id="21" string="case" />
          </tokens>
        </chunking>
        <chunking id="4" string="a new state law" type="NP">
          <tokens>
            <token id="12" string="a" />
            <token id="13" string="new" />
            <token id="14" string="state" />
            <token id="15" string="law" />
          </tokens>
        </chunking>
        <chunking id="5" string="a new state law passed especially for the McMartin case" type="NP">
          <tokens>
            <token id="12" string="a" />
            <token id="13" string="new" />
            <token id="14" string="state" />
            <token id="15" string="law" />
            <token id="16" string="passed" />
            <token id="17" string="especially" />
            <token id="18" string="for" />
            <token id="19" string="the" />
            <token id="20" string="McMartin" />
            <token id="21" string="case" />
          </tokens>
        </chunking>
        <chunking id="6" string="passed especially for the McMartin case" type="VP">
          <tokens>
            <token id="16" string="passed" />
            <token id="17" string="especially" />
            <token id="18" string="for" />
            <token id="19" string="the" />
            <token id="20" string="McMartin" />
            <token id="21" string="case" />
          </tokens>
        </chunking>
        <chunking id="7" string="was allowed to testify by closed-circuit television , under a new state law passed especially for the McMartin case" type="VP">
          <tokens>
            <token id="3" string="was" />
            <token id="4" string="allowed" />
            <token id="5" string="to" />
            <token id="6" string="testify" />
            <token id="7" string="by" />
            <token id="8" string="closed-circuit" />
            <token id="9" string="television" />
            <token id="10" string="," />
            <token id="11" string="under" />
            <token id="12" string="a" />
            <token id="13" string="new" />
            <token id="14" string="state" />
            <token id="15" string="law" />
            <token id="16" string="passed" />
            <token id="17" string="especially" />
            <token id="18" string="for" />
            <token id="19" string="the" />
            <token id="20" string="McMartin" />
            <token id="21" string="case" />
          </tokens>
        </chunking>
        <chunking id="8" string="the McMartin case" type="NP">
          <tokens>
            <token id="19" string="the" />
            <token id="20" string="McMartin" />
            <token id="21" string="case" />
          </tokens>
        </chunking>
        <chunking id="9" string="to testify by closed-circuit television" type="VP">
          <tokens>
            <token id="5" string="to" />
            <token id="6" string="testify" />
            <token id="7" string="by" />
            <token id="8" string="closed-circuit" />
            <token id="9" string="television" />
          </tokens>
        </chunking>
        <chunking id="10" string="testify by closed-circuit television" type="VP">
          <tokens>
            <token id="6" string="testify" />
            <token id="7" string="by" />
            <token id="8" string="closed-circuit" />
            <token id="9" string="television" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nummod">
          <governor id="2">child</governor>
          <dependent id="1">One</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="4">allowed</governor>
          <dependent id="2">child</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="4">allowed</governor>
          <dependent id="3">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">allowed</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="6">testify</governor>
          <dependent id="5">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="4">allowed</governor>
          <dependent id="6">testify</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">television</governor>
          <dependent id="7">by</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">television</governor>
          <dependent id="8">closed-circuit</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">testify</governor>
          <dependent id="9">television</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">law</governor>
          <dependent id="11">under</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">law</governor>
          <dependent id="12">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">law</governor>
          <dependent id="13">new</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">law</governor>
          <dependent id="14">state</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">allowed</governor>
          <dependent id="15">law</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="15">law</governor>
          <dependent id="16">passed</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="16">passed</governor>
          <dependent id="17">especially</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">case</governor>
          <dependent id="18">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="21">case</governor>
          <dependent id="19">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="21">case</governor>
          <dependent id="20">McMartin</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">passed</governor>
          <dependent id="21">case</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="One" type="NUMBER" score="0.0">
          <tokens>
            <token id="1" string="One" />
          </tokens>
        </entity>
        <entity id="2" string="McMartin" type="PERSON" score="0.0">
          <tokens>
            <token id="20" string="McMartin" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="100" has_coreference="true">
      <content>But a week later, in January, 1986, the new district attorney, Ira Reiner, who had inherited the case from Philibosian, dropped charges against five of the seven defendants, citing &amp;quot;incredibly weak evidence&amp;quot; and deciding to proceed only against Ray Buckey and his mother.</content>
      <tokens>
        <token id="1" string="But" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="week" lemma="week" stem="week" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="later" lemma="later" stem="later" pos="RB" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="January" lemma="January" stem="januari" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="9" string="1986" lemma="1986" stem="1986" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="new" lemma="new" stem="new" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="district" lemma="district" stem="district" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="attorney" lemma="attorney" stem="attornei" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="Ira" lemma="Ira" stem="ira" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="17" string="Reiner" lemma="Reiner" stem="reiner" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="18" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="inherited" lemma="inherit" stem="inherit" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="Philibosian" lemma="Philibosian" stem="philibosian" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="26" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="dropped" lemma="drop" stem="drop" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="charges" lemma="charge" stem="charg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="29" string="against" lemma="against" stem="against" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="five" lemma="five" stem="five" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="31" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="32" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="33" string="seven" lemma="seven" stem="seven" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="34" string="defendants" lemma="defendant" stem="defend" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="35" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="citing" lemma="cite" stem="cite" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="37" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="incredibly" lemma="incredibly" stem="incredibli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="weak" lemma="weak" stem="weak" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="40" string="evidence" lemma="evidence" stem="evid" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="41" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="42" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="43" string="deciding" lemma="decide" stem="decid" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="44" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="45" string="proceed" lemma="proceed" stem="proce" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="46" string="only" lemma="only" stem="onli" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="47" string="against" lemma="against" stem="against" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="48" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="49" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="50" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="51" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="52" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="53" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (CC But) (NP (NP-TMP (DT a) (NN week) (RB later)) (, ,) (PP (IN in) (NP (NP (NNP January)) (, ,) (NP (CD 1986)) (, ,) (NP (DT the) (JJ new) (NN district) (NN attorney)) (, ,) (NP (NNP Ira) (NNP Reiner)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBD had) (VP (VBN inherited) (NP (DT the) (NN case)) (PP (IN from) (NP (NNP Philibosian))))))))) (, ,)) (VP (VBD dropped) (NP (NNS charges)) (PP (IN against) (NP (NP (CD five)) (PP (IN of) (NP (DT the) (CD seven) (NNS defendants))))) (, ,) (S (VP (VP (VBG citing) (NP (`` ``) (ADJP (RB incredibly) (JJ weak)) (NN evidence) ('' ''))) (CC and) (VP (VBG deciding) (S (VP (TO to) (VP (VB proceed) (ADVP (RB only)) (PP (IN against) (NP (NP (NNP Ray) (NNP Buckey)) (CC and) (NP (PRP$ his) (NN mother))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="proceed only against Ray Buckey and his mother" type="VP">
          <tokens>
            <token id="45" string="proceed" />
            <token id="46" string="only" />
            <token id="47" string="against" />
            <token id="48" string="Ray" />
            <token id="49" string="Buckey" />
            <token id="50" string="and" />
            <token id="51" string="his" />
            <token id="52" string="mother" />
          </tokens>
        </chunking>
        <chunking id="2" string="the case" type="NP">
          <tokens>
            <token id="22" string="the" />
            <token id="23" string="case" />
          </tokens>
        </chunking>
        <chunking id="3" string="citing `` incredibly weak evidence ''" type="VP">
          <tokens>
            <token id="36" string="citing" />
            <token id="37" string="&quot;" />
            <token id="38" string="incredibly" />
            <token id="39" string="weak" />
            <token id="40" string="evidence" />
            <token id="41" string="&quot;" />
          </tokens>
        </chunking>
        <chunking id="4" string="who had inherited the case from Philibosian" type="SBAR">
          <tokens>
            <token id="19" string="who" />
            <token id="20" string="had" />
            <token id="21" string="inherited" />
            <token id="22" string="the" />
            <token id="23" string="case" />
            <token id="24" string="from" />
            <token id="25" string="Philibosian" />
          </tokens>
        </chunking>
        <chunking id="5" string="1986" type="NP">
          <tokens>
            <token id="9" string="1986" />
          </tokens>
        </chunking>
        <chunking id="6" string="citing `` incredibly weak evidence '' and deciding to proceed only against Ray Buckey and his mother" type="VP">
          <tokens>
            <token id="36" string="citing" />
            <token id="37" string="&quot;" />
            <token id="38" string="incredibly" />
            <token id="39" string="weak" />
            <token id="40" string="evidence" />
            <token id="41" string="&quot;" />
            <token id="42" string="and" />
            <token id="43" string="deciding" />
            <token id="44" string="to" />
            <token id="45" string="proceed" />
            <token id="46" string="only" />
            <token id="47" string="against" />
            <token id="48" string="Ray" />
            <token id="49" string="Buckey" />
            <token id="50" string="and" />
            <token id="51" string="his" />
            <token id="52" string="mother" />
          </tokens>
        </chunking>
        <chunking id="7" string="the new district attorney" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="new" />
            <token id="13" string="district" />
            <token id="14" string="attorney" />
          </tokens>
        </chunking>
        <chunking id="8" string="charges" type="NP">
          <tokens>
            <token id="28" string="charges" />
          </tokens>
        </chunking>
        <chunking id="9" string="to proceed only against Ray Buckey and his mother" type="VP">
          <tokens>
            <token id="44" string="to" />
            <token id="45" string="proceed" />
            <token id="46" string="only" />
            <token id="47" string="against" />
            <token id="48" string="Ray" />
            <token id="49" string="Buckey" />
            <token id="50" string="and" />
            <token id="51" string="his" />
            <token id="52" string="mother" />
          </tokens>
        </chunking>
        <chunking id="10" string="`` incredibly weak evidence ''" type="NP">
          <tokens>
            <token id="37" string="&quot;" />
            <token id="38" string="incredibly" />
            <token id="39" string="weak" />
            <token id="40" string="evidence" />
            <token id="41" string="&quot;" />
          </tokens>
        </chunking>
        <chunking id="11" string="his mother" type="NP">
          <tokens>
            <token id="51" string="his" />
            <token id="52" string="mother" />
          </tokens>
        </chunking>
        <chunking id="12" string="a week later , in January , 1986 , the new district attorney , Ira Reiner , who had inherited the case from Philibosian ," type="NP">
          <tokens>
            <token id="2" string="a" />
            <token id="3" string="week" />
            <token id="4" string="later" />
            <token id="5" string="," />
            <token id="6" string="in" />
            <token id="7" string="January" />
            <token id="8" string="," />
            <token id="9" string="1986" />
            <token id="10" string="," />
            <token id="11" string="the" />
            <token id="12" string="new" />
            <token id="13" string="district" />
            <token id="14" string="attorney" />
            <token id="15" string="," />
            <token id="16" string="Ira" />
            <token id="17" string="Reiner" />
            <token id="18" string="," />
            <token id="19" string="who" />
            <token id="20" string="had" />
            <token id="21" string="inherited" />
            <token id="22" string="the" />
            <token id="23" string="case" />
            <token id="24" string="from" />
            <token id="25" string="Philibosian" />
            <token id="26" string="," />
          </tokens>
        </chunking>
        <chunking id="13" string="January , 1986 , the new district attorney , Ira Reiner , who had inherited the case from Philibosian" type="NP">
          <tokens>
            <token id="7" string="January" />
            <token id="8" string="," />
            <token id="9" string="1986" />
            <token id="10" string="," />
            <token id="11" string="the" />
            <token id="12" string="new" />
            <token id="13" string="district" />
            <token id="14" string="attorney" />
            <token id="15" string="," />
            <token id="16" string="Ira" />
            <token id="17" string="Reiner" />
            <token id="18" string="," />
            <token id="19" string="who" />
            <token id="20" string="had" />
            <token id="21" string="inherited" />
            <token id="22" string="the" />
            <token id="23" string="case" />
            <token id="24" string="from" />
            <token id="25" string="Philibosian" />
          </tokens>
        </chunking>
        <chunking id="14" string="Ira Reiner" type="NP">
          <tokens>
            <token id="16" string="Ira" />
            <token id="17" string="Reiner" />
          </tokens>
        </chunking>
        <chunking id="15" string="had inherited the case from Philibosian" type="VP">
          <tokens>
            <token id="20" string="had" />
            <token id="21" string="inherited" />
            <token id="22" string="the" />
            <token id="23" string="case" />
            <token id="24" string="from" />
            <token id="25" string="Philibosian" />
          </tokens>
        </chunking>
        <chunking id="16" string="Philibosian" type="NP">
          <tokens>
            <token id="25" string="Philibosian" />
          </tokens>
        </chunking>
        <chunking id="17" string="the seven defendants" type="NP">
          <tokens>
            <token id="32" string="the" />
            <token id="33" string="seven" />
            <token id="34" string="defendants" />
          </tokens>
        </chunking>
        <chunking id="18" string="inherited the case from Philibosian" type="VP">
          <tokens>
            <token id="21" string="inherited" />
            <token id="22" string="the" />
            <token id="23" string="case" />
            <token id="24" string="from" />
            <token id="25" string="Philibosian" />
          </tokens>
        </chunking>
        <chunking id="19" string="deciding to proceed only against Ray Buckey and his mother" type="VP">
          <tokens>
            <token id="43" string="deciding" />
            <token id="44" string="to" />
            <token id="45" string="proceed" />
            <token id="46" string="only" />
            <token id="47" string="against" />
            <token id="48" string="Ray" />
            <token id="49" string="Buckey" />
            <token id="50" string="and" />
            <token id="51" string="his" />
            <token id="52" string="mother" />
          </tokens>
        </chunking>
        <chunking id="20" string="dropped charges against five of the seven defendants , citing `` incredibly weak evidence '' and deciding to proceed only against Ray Buckey and his mother" type="VP">
          <tokens>
            <token id="27" string="dropped" />
            <token id="28" string="charges" />
            <token id="29" string="against" />
            <token id="30" string="five" />
            <token id="31" string="of" />
            <token id="32" string="the" />
            <token id="33" string="seven" />
            <token id="34" string="defendants" />
            <token id="35" string="," />
            <token id="36" string="citing" />
            <token id="37" string="&quot;" />
            <token id="38" string="incredibly" />
            <token id="39" string="weak" />
            <token id="40" string="evidence" />
            <token id="41" string="&quot;" />
            <token id="42" string="and" />
            <token id="43" string="deciding" />
            <token id="44" string="to" />
            <token id="45" string="proceed" />
            <token id="46" string="only" />
            <token id="47" string="against" />
            <token id="48" string="Ray" />
            <token id="49" string="Buckey" />
            <token id="50" string="and" />
            <token id="51" string="his" />
            <token id="52" string="mother" />
          </tokens>
        </chunking>
        <chunking id="21" string="incredibly weak" type="ADJP">
          <tokens>
            <token id="38" string="incredibly" />
            <token id="39" string="weak" />
          </tokens>
        </chunking>
        <chunking id="22" string="Ray Buckey and his mother" type="NP">
          <tokens>
            <token id="48" string="Ray" />
            <token id="49" string="Buckey" />
            <token id="50" string="and" />
            <token id="51" string="his" />
            <token id="52" string="mother" />
          </tokens>
        </chunking>
        <chunking id="23" string="Ray Buckey" type="NP">
          <tokens>
            <token id="48" string="Ray" />
            <token id="49" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="24" string="five of the seven defendants" type="NP">
          <tokens>
            <token id="30" string="five" />
            <token id="31" string="of" />
            <token id="32" string="the" />
            <token id="33" string="seven" />
            <token id="34" string="defendants" />
          </tokens>
        </chunking>
        <chunking id="25" string="January" type="NP">
          <tokens>
            <token id="7" string="January" />
          </tokens>
        </chunking>
        <chunking id="26" string="five" type="NP">
          <tokens>
            <token id="30" string="five" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="cc">
          <governor id="27">dropped</governor>
          <dependent id="1">But</dependent>
        </dependency>
        <dependency type="det">
          <governor id="3">week</governor>
          <dependent id="2">a</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="27">dropped</governor>
          <dependent id="3">week</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="3">week</governor>
          <dependent id="4">later</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">January</governor>
          <dependent id="6">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">week</governor>
          <dependent id="7">January</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">January</governor>
          <dependent id="9">1986</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">attorney</governor>
          <dependent id="11">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="14">attorney</governor>
          <dependent id="12">new</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">attorney</governor>
          <dependent id="13">district</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="7">January</governor>
          <dependent id="14">attorney</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="17">Reiner</governor>
          <dependent id="16">Ira</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="7">January</governor>
          <dependent id="17">Reiner</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">inherited</governor>
          <dependent id="19">who</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="21">inherited</governor>
          <dependent id="20">had</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="7">January</governor>
          <dependent id="21">inherited</dependent>
        </dependency>
        <dependency type="det">
          <governor id="23">case</governor>
          <dependent id="22">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="21">inherited</governor>
          <dependent id="23">case</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">Philibosian</governor>
          <dependent id="24">from</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="21">inherited</governor>
          <dependent id="25">Philibosian</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="27">dropped</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="27">dropped</governor>
          <dependent id="28">charges</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">five</governor>
          <dependent id="29">against</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">dropped</governor>
          <dependent id="30">five</dependent>
        </dependency>
        <dependency type="case">
          <governor id="34">defendants</governor>
          <dependent id="31">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="34">defendants</governor>
          <dependent id="32">the</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="34">defendants</governor>
          <dependent id="33">seven</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="30">five</governor>
          <dependent id="34">defendants</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="27">dropped</governor>
          <dependent id="36">citing</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="39">weak</governor>
          <dependent id="38">incredibly</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="40">evidence</governor>
          <dependent id="39">weak</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="36">citing</governor>
          <dependent id="40">evidence</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="36">citing</governor>
          <dependent id="42">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="36">citing</governor>
          <dependent id="43">deciding</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="45">proceed</governor>
          <dependent id="44">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="43">deciding</governor>
          <dependent id="45">proceed</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="45">proceed</governor>
          <dependent id="46">only</dependent>
        </dependency>
        <dependency type="case">
          <governor id="49">Buckey</governor>
          <dependent id="47">against</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="49">Buckey</governor>
          <dependent id="48">Ray</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="45">proceed</governor>
          <dependent id="49">Buckey</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="49">Buckey</governor>
          <dependent id="50">and</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="52">mother</governor>
          <dependent id="51">his</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="49">Buckey</governor>
          <dependent id="52">mother</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="January , 1986" type="DATE" score="0.0">
          <tokens>
            <token id="7" string="January" />
            <token id="8" string="," />
            <token id="9" string="1986" />
          </tokens>
        </entity>
        <entity id="2" string="Philibosian" type="PERSON" score="0.0">
          <tokens>
            <token id="25" string="Philibosian" />
          </tokens>
        </entity>
        <entity id="3" string="seven" type="NUMBER" score="0.0">
          <tokens>
            <token id="33" string="seven" />
          </tokens>
        </entity>
        <entity id="4" string="a week later" type="DATE" score="0.0">
          <tokens>
            <token id="2" string="a" />
            <token id="3" string="week" />
            <token id="4" string="later" />
          </tokens>
        </entity>
        <entity id="5" string="Ray Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="48" string="Ray" />
            <token id="49" string="Buckey" />
          </tokens>
        </entity>
        <entity id="6" string="Ira Reiner" type="PERSON" score="0.0">
          <tokens>
            <token id="16" string="Ira" />
            <token id="17" string="Reiner" />
          </tokens>
        </entity>
        <entity id="7" string="five" type="NUMBER" score="0.0">
          <tokens>
            <token id="30" string="five" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="101" has_coreference="true">
      <content>The trial itself began with jury selection in April, 1987.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="trial" lemma="trial" stem="trial" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="itself" lemma="itself" stem="itself" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="began" lemma="begin" stem="began" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="selection" lemma="selection" stem="select" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="April" lemma="April" stem="april" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="11" string="1987" lemma="1987" stem="1987" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN trial)) (ADVP (PRP itself)) (VP (VBD began) (PP (IN with) (NP (NN jury) (NN selection))) (PP (IN in) (NP (NP (NNP April)) (, ,) (NP (CD 1987))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="April , 1987" type="NP">
          <tokens>
            <token id="9" string="April" />
            <token id="10" string="," />
            <token id="11" string="1987" />
          </tokens>
        </chunking>
        <chunking id="2" string="The trial" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="trial" />
          </tokens>
        </chunking>
        <chunking id="3" string="1987" type="NP">
          <tokens>
            <token id="11" string="1987" />
          </tokens>
        </chunking>
        <chunking id="4" string="began with jury selection in April , 1987" type="VP">
          <tokens>
            <token id="4" string="began" />
            <token id="5" string="with" />
            <token id="6" string="jury" />
            <token id="7" string="selection" />
            <token id="8" string="in" />
            <token id="9" string="April" />
            <token id="10" string="," />
            <token id="11" string="1987" />
          </tokens>
        </chunking>
        <chunking id="5" string="April" type="NP">
          <tokens>
            <token id="9" string="April" />
          </tokens>
        </chunking>
        <chunking id="6" string="jury selection" type="NP">
          <tokens>
            <token id="6" string="jury" />
            <token id="7" string="selection" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">trial</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">began</governor>
          <dependent id="2">trial</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">began</governor>
          <dependent id="3">itself</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">began</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">selection</governor>
          <dependent id="5">with</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">selection</governor>
          <dependent id="6">jury</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">began</governor>
          <dependent id="7">selection</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">April</governor>
          <dependent id="8">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">began</governor>
          <dependent id="9">April</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">April</governor>
          <dependent id="11">1987</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="April , 1987" type="DATE" score="0.0">
          <tokens>
            <token id="9" string="April" />
            <token id="10" string="," />
            <token id="11" string="1987" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="102" has_coreference="false">
      <content>Testimony began in July.</content>
      <tokens>
        <token id="1" string="Testimony" lemma="testimony" stem="testimoni" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="began" lemma="begin" stem="began" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="July" lemma="July" stem="juli" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NN Testimony)) (VP (VBD began) (PP (IN in) (NP (NNP July)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="July" type="NP">
          <tokens>
            <token id="4" string="July" />
          </tokens>
        </chunking>
        <chunking id="2" string="Testimony" type="NP">
          <tokens>
            <token id="1" string="Testimony" />
          </tokens>
        </chunking>
        <chunking id="3" string="began in July" type="VP">
          <tokens>
            <token id="2" string="began" />
            <token id="3" string="in" />
            <token id="4" string="July" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">began</governor>
          <dependent id="1">Testimony</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">began</dependent>
        </dependency>
        <dependency type="case">
          <governor id="4">July</governor>
          <dependent id="3">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">began</governor>
          <dependent id="4">July</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="July" type="DATE" score="0.0">
          <tokens>
            <token id="4" string="July" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="103" has_coreference="true">
      <content>The scope of the trial was narrowed again when several parents decided not to allow their children to testify.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="scope" lemma="scope" stem="scope" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="trial" lemma="trial" stem="trial" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="6" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="narrowed" lemma="narrow" stem="narrow" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="again" lemma="again" stem="again" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="several" lemma="several" stem="sever" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="parents" lemma="parent" stem="parent" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="decided" lemma="decide" stem="decid" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="allow" lemma="allow" stem="allow" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="testify" lemma="testify" stem="testifi" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT The) (NN scope)) (PP (IN of) (NP (DT the) (NN trial)))) (VP (VBD was) (VP (VBN narrowed) (ADVP (RB again)) (SBAR (WHADVP (WRB when)) (S (NP (JJ several) (NNS parents)) (VP (VBD decided) (S (RB not) (VP (TO to) (VP (VB allow) (NP (PRP$ their) (NNS children)) (S (VP (TO to) (VP (VB testify)))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the trial" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="trial" />
          </tokens>
        </chunking>
        <chunking id="2" string="The scope of the trial" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="scope" />
            <token id="3" string="of" />
            <token id="4" string="the" />
            <token id="5" string="trial" />
          </tokens>
        </chunking>
        <chunking id="3" string="narrowed again when several parents decided not to allow their children to testify" type="VP">
          <tokens>
            <token id="7" string="narrowed" />
            <token id="8" string="again" />
            <token id="9" string="when" />
            <token id="10" string="several" />
            <token id="11" string="parents" />
            <token id="12" string="decided" />
            <token id="13" string="not" />
            <token id="14" string="to" />
            <token id="15" string="allow" />
            <token id="16" string="their" />
            <token id="17" string="children" />
            <token id="18" string="to" />
            <token id="19" string="testify" />
          </tokens>
        </chunking>
        <chunking id="4" string="The scope" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="scope" />
          </tokens>
        </chunking>
        <chunking id="5" string="when" type="WHADVP">
          <tokens>
            <token id="9" string="when" />
          </tokens>
        </chunking>
        <chunking id="6" string="their children" type="NP">
          <tokens>
            <token id="16" string="their" />
            <token id="17" string="children" />
          </tokens>
        </chunking>
        <chunking id="7" string="was narrowed again when several parents decided not to allow their children to testify" type="VP">
          <tokens>
            <token id="6" string="was" />
            <token id="7" string="narrowed" />
            <token id="8" string="again" />
            <token id="9" string="when" />
            <token id="10" string="several" />
            <token id="11" string="parents" />
            <token id="12" string="decided" />
            <token id="13" string="not" />
            <token id="14" string="to" />
            <token id="15" string="allow" />
            <token id="16" string="their" />
            <token id="17" string="children" />
            <token id="18" string="to" />
            <token id="19" string="testify" />
          </tokens>
        </chunking>
        <chunking id="8" string="allow their children to testify" type="VP">
          <tokens>
            <token id="15" string="allow" />
            <token id="16" string="their" />
            <token id="17" string="children" />
            <token id="18" string="to" />
            <token id="19" string="testify" />
          </tokens>
        </chunking>
        <chunking id="9" string="to testify" type="VP">
          <tokens>
            <token id="18" string="to" />
            <token id="19" string="testify" />
          </tokens>
        </chunking>
        <chunking id="10" string="several parents" type="NP">
          <tokens>
            <token id="10" string="several" />
            <token id="11" string="parents" />
          </tokens>
        </chunking>
        <chunking id="11" string="when several parents decided not to allow their children to testify" type="SBAR">
          <tokens>
            <token id="9" string="when" />
            <token id="10" string="several" />
            <token id="11" string="parents" />
            <token id="12" string="decided" />
            <token id="13" string="not" />
            <token id="14" string="to" />
            <token id="15" string="allow" />
            <token id="16" string="their" />
            <token id="17" string="children" />
            <token id="18" string="to" />
            <token id="19" string="testify" />
          </tokens>
        </chunking>
        <chunking id="12" string="decided not to allow their children to testify" type="VP">
          <tokens>
            <token id="12" string="decided" />
            <token id="13" string="not" />
            <token id="14" string="to" />
            <token id="15" string="allow" />
            <token id="16" string="their" />
            <token id="17" string="children" />
            <token id="18" string="to" />
            <token id="19" string="testify" />
          </tokens>
        </chunking>
        <chunking id="13" string="to allow their children to testify" type="VP">
          <tokens>
            <token id="14" string="to" />
            <token id="15" string="allow" />
            <token id="16" string="their" />
            <token id="17" string="children" />
            <token id="18" string="to" />
            <token id="19" string="testify" />
          </tokens>
        </chunking>
        <chunking id="14" string="testify" type="VP">
          <tokens>
            <token id="19" string="testify" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">scope</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="7">narrowed</governor>
          <dependent id="2">scope</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">trial</governor>
          <dependent id="3">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">trial</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">scope</governor>
          <dependent id="5">trial</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="7">narrowed</governor>
          <dependent id="6">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">narrowed</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="7">narrowed</governor>
          <dependent id="8">again</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="12">decided</governor>
          <dependent id="9">when</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">parents</governor>
          <dependent id="10">several</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">decided</governor>
          <dependent id="11">parents</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="7">narrowed</governor>
          <dependent id="12">decided</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="15">allow</governor>
          <dependent id="13">not</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="15">allow</governor>
          <dependent id="14">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="12">decided</governor>
          <dependent id="15">allow</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="17">children</governor>
          <dependent id="16">their</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">allow</governor>
          <dependent id="17">children</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="19">testify</governor>
          <dependent id="18">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="15">allow</governor>
          <dependent id="19">testify</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="104" has_coreference="true">
      <content>Eventually nine of 11 named victims took the witness stand, all sticking to their earlier accounts, but for minor inconsistencies.</content>
      <tokens>
        <token id="1" string="Eventually" lemma="eventually" stem="eventual" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="nine" lemma="nine" stem="nine" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="3" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="11" lemma="11" stem="11" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="5" string="named" lemma="name" stem="name" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="victims" lemma="victim" stem="victim" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="took" lemma="take" stem="took" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="witness" lemma="witness" stem="wit" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="stand" lemma="stand" stem="stand" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="sticking" lemma="stick" stem="stick" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="16" string="earlier" lemma="earlier" stem="earlier" pos="JJR" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="accounts" lemma="account" stem="account" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="minor" lemma="minor" stem="minor" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="inconsistencies" lemma="inconsistency" stem="inconsist" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (ADVP (RB Eventually)) (NP (NP (CD nine)) (PP (IN of) (NP (CD 11) (VBN named) (NNS victims)))) (VP (VBD took) (NP (NP (DT the) (NN witness) (NN stand)) (, ,) (NP (NP (DT all)) (VP (VBG sticking) (PP (TO to) (NP (PRP$ their) (JJR earlier) (NNS accounts)))))) (, ,) (CC but) (PP (IN for) (NP (JJ minor) (NNS inconsistencies)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="all" type="NP">
          <tokens>
            <token id="12" string="all" />
          </tokens>
        </chunking>
        <chunking id="2" string="nine" type="NP">
          <tokens>
            <token id="2" string="nine" />
          </tokens>
        </chunking>
        <chunking id="3" string="11 named victims" type="NP">
          <tokens>
            <token id="4" string="11" />
            <token id="5" string="named" />
            <token id="6" string="victims" />
          </tokens>
        </chunking>
        <chunking id="4" string="nine of 11 named victims" type="NP">
          <tokens>
            <token id="2" string="nine" />
            <token id="3" string="of" />
            <token id="4" string="11" />
            <token id="5" string="named" />
            <token id="6" string="victims" />
          </tokens>
        </chunking>
        <chunking id="5" string="their earlier accounts" type="NP">
          <tokens>
            <token id="15" string="their" />
            <token id="16" string="earlier" />
            <token id="17" string="accounts" />
          </tokens>
        </chunking>
        <chunking id="6" string="took the witness stand , all sticking to their earlier accounts , but for minor inconsistencies" type="VP">
          <tokens>
            <token id="7" string="took" />
            <token id="8" string="the" />
            <token id="9" string="witness" />
            <token id="10" string="stand" />
            <token id="11" string="," />
            <token id="12" string="all" />
            <token id="13" string="sticking" />
            <token id="14" string="to" />
            <token id="15" string="their" />
            <token id="16" string="earlier" />
            <token id="17" string="accounts" />
            <token id="18" string="," />
            <token id="19" string="but" />
            <token id="20" string="for" />
            <token id="21" string="minor" />
            <token id="22" string="inconsistencies" />
          </tokens>
        </chunking>
        <chunking id="7" string="all sticking to their earlier accounts" type="NP">
          <tokens>
            <token id="12" string="all" />
            <token id="13" string="sticking" />
            <token id="14" string="to" />
            <token id="15" string="their" />
            <token id="16" string="earlier" />
            <token id="17" string="accounts" />
          </tokens>
        </chunking>
        <chunking id="8" string="sticking to their earlier accounts" type="VP">
          <tokens>
            <token id="13" string="sticking" />
            <token id="14" string="to" />
            <token id="15" string="their" />
            <token id="16" string="earlier" />
            <token id="17" string="accounts" />
          </tokens>
        </chunking>
        <chunking id="9" string="the witness stand" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="witness" />
            <token id="10" string="stand" />
          </tokens>
        </chunking>
        <chunking id="10" string="the witness stand , all sticking to their earlier accounts" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="witness" />
            <token id="10" string="stand" />
            <token id="11" string="," />
            <token id="12" string="all" />
            <token id="13" string="sticking" />
            <token id="14" string="to" />
            <token id="15" string="their" />
            <token id="16" string="earlier" />
            <token id="17" string="accounts" />
          </tokens>
        </chunking>
        <chunking id="11" string="minor inconsistencies" type="NP">
          <tokens>
            <token id="21" string="minor" />
            <token id="22" string="inconsistencies" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="7">took</governor>
          <dependent id="1">Eventually</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">took</governor>
          <dependent id="2">nine</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">victims</governor>
          <dependent id="3">of</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="6">victims</governor>
          <dependent id="4">11</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">victims</governor>
          <dependent id="5">named</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">nine</governor>
          <dependent id="6">victims</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">took</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">stand</governor>
          <dependent id="8">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">stand</governor>
          <dependent id="9">witness</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">took</governor>
          <dependent id="10">stand</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="10">stand</governor>
          <dependent id="12">all</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="12">all</governor>
          <dependent id="13">sticking</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">accounts</governor>
          <dependent id="14">to</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="17">accounts</governor>
          <dependent id="15">their</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="17">accounts</governor>
          <dependent id="16">earlier</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">sticking</governor>
          <dependent id="17">accounts</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">took</governor>
          <dependent id="19">but</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">inconsistencies</governor>
          <dependent id="20">for</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="22">inconsistencies</governor>
          <dependent id="21">minor</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">took</governor>
          <dependent id="22">inconsistencies</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="nine" type="NUMBER" score="0.0">
          <tokens>
            <token id="2" string="nine" />
          </tokens>
        </entity>
        <entity id="2" string="11" type="NUMBER" score="0.0">
          <tokens>
            <token id="4" string="11" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="105" has_coreference="true">
      <content>Toddlers at the time of their alleged abuse and now nearing adolescence, they described in graphic detail the abuses they say they suffered at the hands of the Buckeys.</content>
      <tokens>
        <token id="1" string="Toddlers" lemma="toddler" stem="toddler" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="2" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="4" string="time" lemma="time" stem="time" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="5" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="6" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="7" string="alleged" lemma="allege" stem="alleg" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="abuse" lemma="abuse" stem="abus" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="now" lemma="now" stem="now" pos="RB" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="11" string="nearing" lemma="near" stem="near" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="adolescence" lemma="adolescence" stem="adolesc" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="described" lemma="describe" stem="describ" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="graphic" lemma="graphic" stem="graphic" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="detail" lemma="detail" stem="detail" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="abuses" lemma="abuse" stem="abus" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="say" lemma="say" stem="sai" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="suffered" lemma="suffer" stem="suffer" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="hands" lemma="hand" stem="hand" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="Buckeys" lemma="Buckeys" stem="buckei" pos="NNPS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (S (NP (NP (NNS Toddlers)) (PP (IN at) (NP (NP (DT the) (NN time)) (PP (IN of) (NP (PRP$ their) (VBN alleged) (NN abuse))))))) (CC and) (S (VP (ADVP (RB now)) (VBG nearing) (NP (NN adolescence))))) (, ,) (NP (PRP they)) (VP (VBD described) (PP (IN in) (NP (JJ graphic) (NN detail))) (NP (NP (DT the) (NNS abuses)) (SBAR (S (NP (PRP they)) (VP (VBP say) (SBAR (S (NP (PRP they)) (VP (VBD suffered) (PP (IN at) (NP (NP (DT the) (NNS hands)) (PP (IN of) (NP (DT the) (NNPS Buckeys))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the hands" type="NP">
          <tokens>
            <token id="26" string="the" />
            <token id="27" string="hands" />
          </tokens>
        </chunking>
        <chunking id="2" string="they say they suffered at the hands of the Buckeys" type="SBAR">
          <tokens>
            <token id="21" string="they" />
            <token id="22" string="say" />
            <token id="23" string="they" />
            <token id="24" string="suffered" />
            <token id="25" string="at" />
            <token id="26" string="the" />
            <token id="27" string="hands" />
            <token id="28" string="of" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
          </tokens>
        </chunking>
        <chunking id="3" string="suffered at the hands of the Buckeys" type="VP">
          <tokens>
            <token id="24" string="suffered" />
            <token id="25" string="at" />
            <token id="26" string="the" />
            <token id="27" string="hands" />
            <token id="28" string="of" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
          </tokens>
        </chunking>
        <chunking id="4" string="adolescence" type="NP">
          <tokens>
            <token id="12" string="adolescence" />
          </tokens>
        </chunking>
        <chunking id="5" string="graphic detail" type="NP">
          <tokens>
            <token id="17" string="graphic" />
            <token id="18" string="detail" />
          </tokens>
        </chunking>
        <chunking id="6" string="Toddlers at the time of their alleged abuse" type="NP">
          <tokens>
            <token id="1" string="Toddlers" />
            <token id="2" string="at" />
            <token id="3" string="the" />
            <token id="4" string="time" />
            <token id="5" string="of" />
            <token id="6" string="their" />
            <token id="7" string="alleged" />
            <token id="8" string="abuse" />
          </tokens>
        </chunking>
        <chunking id="7" string="now nearing adolescence" type="VP">
          <tokens>
            <token id="10" string="now" />
            <token id="11" string="nearing" />
            <token id="12" string="adolescence" />
          </tokens>
        </chunking>
        <chunking id="8" string="the hands of the Buckeys" type="NP">
          <tokens>
            <token id="26" string="the" />
            <token id="27" string="hands" />
            <token id="28" string="of" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
          </tokens>
        </chunking>
        <chunking id="9" string="say they suffered at the hands of the Buckeys" type="VP">
          <tokens>
            <token id="22" string="say" />
            <token id="23" string="they" />
            <token id="24" string="suffered" />
            <token id="25" string="at" />
            <token id="26" string="the" />
            <token id="27" string="hands" />
            <token id="28" string="of" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
          </tokens>
        </chunking>
        <chunking id="10" string="the abuses" type="NP">
          <tokens>
            <token id="19" string="the" />
            <token id="20" string="abuses" />
          </tokens>
        </chunking>
        <chunking id="11" string="they" type="NP">
          <tokens>
            <token id="14" string="they" />
          </tokens>
        </chunking>
        <chunking id="12" string="they suffered at the hands of the Buckeys" type="SBAR">
          <tokens>
            <token id="23" string="they" />
            <token id="24" string="suffered" />
            <token id="25" string="at" />
            <token id="26" string="the" />
            <token id="27" string="hands" />
            <token id="28" string="of" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
          </tokens>
        </chunking>
        <chunking id="13" string="the time of their alleged abuse" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="time" />
            <token id="5" string="of" />
            <token id="6" string="their" />
            <token id="7" string="alleged" />
            <token id="8" string="abuse" />
          </tokens>
        </chunking>
        <chunking id="14" string="described in graphic detail the abuses they say they suffered at the hands of the Buckeys" type="VP">
          <tokens>
            <token id="15" string="described" />
            <token id="16" string="in" />
            <token id="17" string="graphic" />
            <token id="18" string="detail" />
            <token id="19" string="the" />
            <token id="20" string="abuses" />
            <token id="21" string="they" />
            <token id="22" string="say" />
            <token id="23" string="they" />
            <token id="24" string="suffered" />
            <token id="25" string="at" />
            <token id="26" string="the" />
            <token id="27" string="hands" />
            <token id="28" string="of" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
          </tokens>
        </chunking>
        <chunking id="15" string="the abuses they say they suffered at the hands of the Buckeys" type="NP">
          <tokens>
            <token id="19" string="the" />
            <token id="20" string="abuses" />
            <token id="21" string="they" />
            <token id="22" string="say" />
            <token id="23" string="they" />
            <token id="24" string="suffered" />
            <token id="25" string="at" />
            <token id="26" string="the" />
            <token id="27" string="hands" />
            <token id="28" string="of" />
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
          </tokens>
        </chunking>
        <chunking id="16" string="the Buckeys" type="NP">
          <tokens>
            <token id="29" string="the" />
            <token id="30" string="Buckeys" />
          </tokens>
        </chunking>
        <chunking id="17" string="the time" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="time" />
          </tokens>
        </chunking>
        <chunking id="18" string="Toddlers" type="NP">
          <tokens>
            <token id="1" string="Toddlers" />
          </tokens>
        </chunking>
        <chunking id="19" string="their alleged abuse" type="NP">
          <tokens>
            <token id="6" string="their" />
            <token id="7" string="alleged" />
            <token id="8" string="abuse" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="ccomp">
          <governor id="15">described</governor>
          <dependent id="1">Toddlers</dependent>
        </dependency>
        <dependency type="case">
          <governor id="4">time</governor>
          <dependent id="2">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">time</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="1">Toddlers</governor>
          <dependent id="4">time</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">abuse</governor>
          <dependent id="5">of</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="8">abuse</governor>
          <dependent id="6">their</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">abuse</governor>
          <dependent id="7">alleged</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">time</governor>
          <dependent id="8">abuse</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="1">Toddlers</governor>
          <dependent id="9">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">nearing</governor>
          <dependent id="10">now</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="1">Toddlers</governor>
          <dependent id="11">nearing</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">nearing</governor>
          <dependent id="12">adolescence</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">described</governor>
          <dependent id="14">they</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="15">described</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">detail</governor>
          <dependent id="16">in</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="18">detail</governor>
          <dependent id="17">graphic</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">described</governor>
          <dependent id="18">detail</dependent>
        </dependency>
        <dependency type="det">
          <governor id="20">abuses</governor>
          <dependent id="19">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">described</governor>
          <dependent id="20">abuses</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">say</governor>
          <dependent id="21">they</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="20">abuses</governor>
          <dependent id="22">say</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="24">suffered</governor>
          <dependent id="23">they</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="22">say</governor>
          <dependent id="24">suffered</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">hands</governor>
          <dependent id="25">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="27">hands</governor>
          <dependent id="26">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="24">suffered</governor>
          <dependent id="27">hands</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">Buckeys</governor>
          <dependent id="28">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="30">Buckeys</governor>
          <dependent id="29">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">hands</governor>
          <dependent id="30">Buckeys</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="now" type="DATE" score="0.0">
          <tokens>
            <token id="10" string="now" />
          </tokens>
        </entity>
        <entity id="2" string="time" type="DATE" score="0.0">
          <tokens>
            <token id="4" string="time" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="106" has_coreference="true">
      <content>Their testimony was often raw and unsettling.</content>
      <tokens>
        <token id="1" string="Their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="testimony" lemma="testimony" stem="testimoni" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="often" lemma="often" stem="often" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="raw" lemma="raw" stem="raw" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="unsettling" lemma="unsettling" stem="unsettl" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP$ Their) (NN testimony)) (VP (VBD was) (ADVP (RB often)) (ADJP (JJ raw) (CC and) (JJ unsettling))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Their testimony" type="NP">
          <tokens>
            <token id="1" string="Their" />
            <token id="2" string="testimony" />
          </tokens>
        </chunking>
        <chunking id="2" string="was often raw and unsettling" type="VP">
          <tokens>
            <token id="3" string="was" />
            <token id="4" string="often" />
            <token id="5" string="raw" />
            <token id="6" string="and" />
            <token id="7" string="unsettling" />
          </tokens>
        </chunking>
        <chunking id="3" string="raw and unsettling" type="ADJP">
          <tokens>
            <token id="5" string="raw" />
            <token id="6" string="and" />
            <token id="7" string="unsettling" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="2">testimony</governor>
          <dependent id="1">Their</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">raw</governor>
          <dependent id="2">testimony</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="5">raw</governor>
          <dependent id="3">was</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">raw</governor>
          <dependent id="4">often</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">raw</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="5">raw</governor>
          <dependent id="6">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="5">raw</governor>
          <dependent id="7">unsettling</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="107" has_coreference="true">
      <content>The children&amp;apost;s allegations were supported by medical evidence testified to by physicians.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="2" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="3" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="4" string="allegations" lemma="allegation" stem="alleg" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="supported" lemma="support" stem="support" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="medical" lemma="medical" stem="medic" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="evidence" lemma="evidence" stem="evid" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="testified" lemma="testify" stem="testifi" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="physicians" lemma="physician" stem="physician" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT The) (NNS children) (POS 's)) (NNS allegations)) (VP (VBD were) (VP (VBN supported) (PP (IN by) (NP (NP (JJ medical) (NN evidence)) (VP (VBD testified) (S (VP (TO to))) (PP (IN by) (NP (NNS physicians)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="were supported by medical evidence testified to by physicians" type="VP">
          <tokens>
            <token id="5" string="were" />
            <token id="6" string="supported" />
            <token id="7" string="by" />
            <token id="8" string="medical" />
            <token id="9" string="evidence" />
            <token id="10" string="testified" />
            <token id="11" string="to" />
            <token id="12" string="by" />
            <token id="13" string="physicians" />
          </tokens>
        </chunking>
        <chunking id="2" string="supported by medical evidence testified to by physicians" type="VP">
          <tokens>
            <token id="6" string="supported" />
            <token id="7" string="by" />
            <token id="8" string="medical" />
            <token id="9" string="evidence" />
            <token id="10" string="testified" />
            <token id="11" string="to" />
            <token id="12" string="by" />
            <token id="13" string="physicians" />
          </tokens>
        </chunking>
        <chunking id="3" string="medical evidence" type="NP">
          <tokens>
            <token id="8" string="medical" />
            <token id="9" string="evidence" />
          </tokens>
        </chunking>
        <chunking id="4" string="The children 's allegations" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="children" />
            <token id="3" string="'s" />
            <token id="4" string="allegations" />
          </tokens>
        </chunking>
        <chunking id="5" string="The children 's" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="children" />
            <token id="3" string="'s" />
          </tokens>
        </chunking>
        <chunking id="6" string="physicians" type="NP">
          <tokens>
            <token id="13" string="physicians" />
          </tokens>
        </chunking>
        <chunking id="7" string="testified to by physicians" type="VP">
          <tokens>
            <token id="10" string="testified" />
            <token id="11" string="to" />
            <token id="12" string="by" />
            <token id="13" string="physicians" />
          </tokens>
        </chunking>
        <chunking id="8" string="to" type="VP">
          <tokens>
            <token id="11" string="to" />
          </tokens>
        </chunking>
        <chunking id="9" string="medical evidence testified to by physicians" type="NP">
          <tokens>
            <token id="8" string="medical" />
            <token id="9" string="evidence" />
            <token id="10" string="testified" />
            <token id="11" string="to" />
            <token id="12" string="by" />
            <token id="13" string="physicians" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">children</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="4">allegations</governor>
          <dependent id="2">children</dependent>
        </dependency>
        <dependency type="case">
          <governor id="2">children</governor>
          <dependent id="3">'s</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="6">supported</governor>
          <dependent id="4">allegations</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="6">supported</governor>
          <dependent id="5">were</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">supported</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">evidence</governor>
          <dependent id="7">by</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">evidence</governor>
          <dependent id="8">medical</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">supported</governor>
          <dependent id="9">evidence</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="9">evidence</governor>
          <dependent id="10">testified</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="10">testified</governor>
          <dependent id="11">to</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">physicians</governor>
          <dependent id="12">by</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">testified</governor>
          <dependent id="13">physicians</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="108" has_coreference="true">
      <content>But both Buckey and his mother also took the stand and staunchly maintained their innocence.</content>
      <tokens>
        <token id="1" string="But" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="both" lemma="both" stem="both" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="3" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="4" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="6" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="took" lemma="take" stem="took" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="stand" lemma="stand" stem="stand" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="11" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="staunchly" lemma="staunchly" stem="staunchli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="maintained" lemma="maintain" stem="maintain" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="innocence" lemma="innocence" stem="innoc" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (CC But) (NP (NP (DT both) (NNP Buckey)) (CC and) (NP (PRP$ his) (NN mother))) (ADVP (RB also)) (VP (VP (VBD took) (NP (DT the) (NN stand))) (CC and) (VP (ADVP (RB staunchly)) (VBD maintained) (NP (PRP$ their) (NN innocence)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the stand" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="stand" />
          </tokens>
        </chunking>
        <chunking id="2" string="staunchly maintained their innocence" type="VP">
          <tokens>
            <token id="12" string="staunchly" />
            <token id="13" string="maintained" />
            <token id="14" string="their" />
            <token id="15" string="innocence" />
          </tokens>
        </chunking>
        <chunking id="3" string="took the stand and staunchly maintained their innocence" type="VP">
          <tokens>
            <token id="8" string="took" />
            <token id="9" string="the" />
            <token id="10" string="stand" />
            <token id="11" string="and" />
            <token id="12" string="staunchly" />
            <token id="13" string="maintained" />
            <token id="14" string="their" />
            <token id="15" string="innocence" />
          </tokens>
        </chunking>
        <chunking id="4" string="their innocence" type="NP">
          <tokens>
            <token id="14" string="their" />
            <token id="15" string="innocence" />
          </tokens>
        </chunking>
        <chunking id="5" string="his mother" type="NP">
          <tokens>
            <token id="5" string="his" />
            <token id="6" string="mother" />
          </tokens>
        </chunking>
        <chunking id="6" string="both Buckey" type="NP">
          <tokens>
            <token id="2" string="both" />
            <token id="3" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="7" string="both Buckey and his mother" type="NP">
          <tokens>
            <token id="2" string="both" />
            <token id="3" string="Buckey" />
            <token id="4" string="and" />
            <token id="5" string="his" />
            <token id="6" string="mother" />
          </tokens>
        </chunking>
        <chunking id="8" string="took the stand" type="VP">
          <tokens>
            <token id="8" string="took" />
            <token id="9" string="the" />
            <token id="10" string="stand" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="cc">
          <governor id="8">took</governor>
          <dependent id="1">But</dependent>
        </dependency>
        <dependency type="det">
          <governor id="3">Buckey</governor>
          <dependent id="2">both</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">took</governor>
          <dependent id="3">Buckey</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="3">Buckey</governor>
          <dependent id="4">and</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="6">mother</governor>
          <dependent id="5">his</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="3">Buckey</governor>
          <dependent id="6">mother</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="8">took</governor>
          <dependent id="7">also</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="8">took</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">stand</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="8">took</governor>
          <dependent id="10">stand</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="8">took</governor>
          <dependent id="11">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="13">maintained</governor>
          <dependent id="12">staunchly</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="8">took</governor>
          <dependent id="13">maintained</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="15">innocence</governor>
          <dependent id="14">their</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="13">maintained</governor>
          <dependent id="15">innocence</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="3" string="Buckey" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="109" has_coreference="true">
      <content>And fellow teachers, including former defendants, testified that no improprieties in the small school could have occurred without their knowledge.</content>
      <tokens>
        <token id="1" string="And" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="fellow" lemma="fellow" stem="fellow" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="teachers" lemma="teacher" stem="teacher" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="including" lemma="include" stem="includ" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="former" lemma="former" stem="former" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="defendants" lemma="defendant" stem="defend" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="testified" lemma="testify" stem="testifi" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="no" lemma="no" stem="no" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="improprieties" lemma="impropriety" stem="improprieti" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="small" lemma="small" stem="small" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="16" string="school" lemma="school" stem="school" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="17" string="could" lemma="could" stem="could" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="have" lemma="have" stem="have" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="occurred" lemma="occur" stem="occur" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="without" lemma="without" stem="without" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="knowledge" lemma="knowledge" stem="knowledg" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (CC And) (NP (NP (JJ fellow) (NNS teachers)) (, ,) (PP (VBG including) (NP (JJ former) (NNS defendants))) (, ,)) (VP (VBD testified) (SBAR (IN that) (S (NP (NP (DT no) (NNS improprieties)) (PP (IN in) (NP (DT the) (JJ small) (NN school)))) (VP (MD could) (VP (VB have) (VP (VBN occurred) (PP (IN without) (NP (PRP$ their) (NN knowledge))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="testified that no improprieties in the small school could have occurred without their knowledge" type="VP">
          <tokens>
            <token id="9" string="testified" />
            <token id="10" string="that" />
            <token id="11" string="no" />
            <token id="12" string="improprieties" />
            <token id="13" string="in" />
            <token id="14" string="the" />
            <token id="15" string="small" />
            <token id="16" string="school" />
            <token id="17" string="could" />
            <token id="18" string="have" />
            <token id="19" string="occurred" />
            <token id="20" string="without" />
            <token id="21" string="their" />
            <token id="22" string="knowledge" />
          </tokens>
        </chunking>
        <chunking id="2" string="fellow teachers" type="NP">
          <tokens>
            <token id="2" string="fellow" />
            <token id="3" string="teachers" />
          </tokens>
        </chunking>
        <chunking id="3" string="fellow teachers , including former defendants ," type="NP">
          <tokens>
            <token id="2" string="fellow" />
            <token id="3" string="teachers" />
            <token id="4" string="," />
            <token id="5" string="including" />
            <token id="6" string="former" />
            <token id="7" string="defendants" />
            <token id="8" string="," />
          </tokens>
        </chunking>
        <chunking id="4" string="could have occurred without their knowledge" type="VP">
          <tokens>
            <token id="17" string="could" />
            <token id="18" string="have" />
            <token id="19" string="occurred" />
            <token id="20" string="without" />
            <token id="21" string="their" />
            <token id="22" string="knowledge" />
          </tokens>
        </chunking>
        <chunking id="5" string="no improprieties in the small school" type="NP">
          <tokens>
            <token id="11" string="no" />
            <token id="12" string="improprieties" />
            <token id="13" string="in" />
            <token id="14" string="the" />
            <token id="15" string="small" />
            <token id="16" string="school" />
          </tokens>
        </chunking>
        <chunking id="6" string="former defendants" type="NP">
          <tokens>
            <token id="6" string="former" />
            <token id="7" string="defendants" />
          </tokens>
        </chunking>
        <chunking id="7" string="that no improprieties in the small school could have occurred without their knowledge" type="SBAR">
          <tokens>
            <token id="10" string="that" />
            <token id="11" string="no" />
            <token id="12" string="improprieties" />
            <token id="13" string="in" />
            <token id="14" string="the" />
            <token id="15" string="small" />
            <token id="16" string="school" />
            <token id="17" string="could" />
            <token id="18" string="have" />
            <token id="19" string="occurred" />
            <token id="20" string="without" />
            <token id="21" string="their" />
            <token id="22" string="knowledge" />
          </tokens>
        </chunking>
        <chunking id="8" string="have occurred without their knowledge" type="VP">
          <tokens>
            <token id="18" string="have" />
            <token id="19" string="occurred" />
            <token id="20" string="without" />
            <token id="21" string="their" />
            <token id="22" string="knowledge" />
          </tokens>
        </chunking>
        <chunking id="9" string="their knowledge" type="NP">
          <tokens>
            <token id="21" string="their" />
            <token id="22" string="knowledge" />
          </tokens>
        </chunking>
        <chunking id="10" string="occurred without their knowledge" type="VP">
          <tokens>
            <token id="19" string="occurred" />
            <token id="20" string="without" />
            <token id="21" string="their" />
            <token id="22" string="knowledge" />
          </tokens>
        </chunking>
        <chunking id="11" string="no improprieties" type="NP">
          <tokens>
            <token id="11" string="no" />
            <token id="12" string="improprieties" />
          </tokens>
        </chunking>
        <chunking id="12" string="the small school" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="small" />
            <token id="16" string="school" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="cc">
          <governor id="9">testified</governor>
          <dependent id="1">And</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">teachers</governor>
          <dependent id="2">fellow</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">testified</governor>
          <dependent id="3">teachers</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">defendants</governor>
          <dependent id="5">including</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">defendants</governor>
          <dependent id="6">former</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">teachers</governor>
          <dependent id="7">defendants</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="9">testified</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="19">occurred</governor>
          <dependent id="10">that</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="12">improprieties</governor>
          <dependent id="11">no</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">occurred</governor>
          <dependent id="12">improprieties</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">school</governor>
          <dependent id="13">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="16">school</governor>
          <dependent id="14">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="16">school</governor>
          <dependent id="15">small</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">improprieties</governor>
          <dependent id="16">school</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="19">occurred</governor>
          <dependent id="17">could</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="19">occurred</governor>
          <dependent id="18">have</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="9">testified</governor>
          <dependent id="19">occurred</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">knowledge</governor>
          <dependent id="20">without</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="22">knowledge</governor>
          <dependent id="21">their</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">occurred</governor>
          <dependent id="22">knowledge</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="110" has_coreference="true">
      <content>The prosecution sought to show that the children repeatedly had been abused sexually, but silenced for years by threats, and also that their child-like efforts to tell their parents that something was wrong had been ignored or misinterpreted.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="prosecution" lemma="prosecution" stem="prosecut" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="sought" lemma="seek" stem="sought" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="show" lemma="show" stem="show" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="9" string="repeatedly" lemma="repeatedly" stem="repeatedli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="abused" lemma="abused" stem="abus" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="sexually" lemma="sexually" stem="sexual" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="silenced" lemma="silence" stem="silenc" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="true" />
        <token id="19" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="threats" lemma="threat" stem="threat" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="26" string="child-like" lemma="child-like" stem="child-lik" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="efforts" lemma="effort" stem="effort" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="tell" lemma="tell" stem="tell" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="31" string="parents" lemma="parent" stem="parent" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="32" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="something" lemma="something" stem="someth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="wrong" lemma="wrong" stem="wrong" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="36" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="ignored" lemma="ignore" stem="ignor" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="or" lemma="or" stem="or" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="40" string="misinterpreted" lemma="misinterpret" stem="misinterpret" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="41" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN prosecution)) (VP (VP (VBD sought) (S (VP (TO to) (VP (VB show) (SBAR (IN that) (S (NP (DT the) (NNS children)) (ADVP (RB repeatedly)) (VP (VBD had) (VP (VBN been) (VP (JJ abused)))))) (ADVP (RB sexually)))))) (, ,) (CC but) (VP (VBD silenced) (PP (IN for) (NP (NNS years))) (PP (IN by) (NP (NNS threats)))) (, ,) (CC and) (ADVP (RB also)) (SBAR (IN that) (S (NP (PRP$ their) (JJ child-like) (NNS efforts) (S (VP (TO to) (VP (VB tell) (NP (PRP$ their) (NNS parents)) (SBAR (WHNP (WDT that)) (S (NP (NN something)) (VP (VBD was) (ADJP (JJ wrong))))))))) (VP (VBD had) (VP (VBN been) (VP (VBN ignored) (CC or) (VBN misinterpreted))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="been abused" type="VP">
          <tokens>
            <token id="11" string="been" />
            <token id="12" string="abused" />
          </tokens>
        </chunking>
        <chunking id="2" string="that something was wrong" type="SBAR">
          <tokens>
            <token id="32" string="that" />
            <token id="33" string="something" />
            <token id="34" string="was" />
            <token id="35" string="wrong" />
          </tokens>
        </chunking>
        <chunking id="3" string="silenced for years by threats" type="VP">
          <tokens>
            <token id="16" string="silenced" />
            <token id="17" string="for" />
            <token id="18" string="years" />
            <token id="19" string="by" />
            <token id="20" string="threats" />
          </tokens>
        </chunking>
        <chunking id="4" string="to show that the children repeatedly had been abused sexually" type="VP">
          <tokens>
            <token id="4" string="to" />
            <token id="5" string="show" />
            <token id="6" string="that" />
            <token id="7" string="the" />
            <token id="8" string="children" />
            <token id="9" string="repeatedly" />
            <token id="10" string="had" />
            <token id="11" string="been" />
            <token id="12" string="abused" />
            <token id="13" string="sexually" />
          </tokens>
        </chunking>
        <chunking id="5" string="been ignored or misinterpreted" type="VP">
          <tokens>
            <token id="37" string="been" />
            <token id="38" string="ignored" />
            <token id="39" string="or" />
            <token id="40" string="misinterpreted" />
          </tokens>
        </chunking>
        <chunking id="6" string="something" type="NP">
          <tokens>
            <token id="33" string="something" />
          </tokens>
        </chunking>
        <chunking id="7" string="wrong" type="ADJP">
          <tokens>
            <token id="35" string="wrong" />
          </tokens>
        </chunking>
        <chunking id="8" string="The prosecution" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="prosecution" />
          </tokens>
        </chunking>
        <chunking id="9" string="sought to show that the children repeatedly had been abused sexually , but silenced for years by threats , and also that their child-like efforts to tell their parents that something was wrong had been ignored or misinterpreted" type="VP">
          <tokens>
            <token id="3" string="sought" />
            <token id="4" string="to" />
            <token id="5" string="show" />
            <token id="6" string="that" />
            <token id="7" string="the" />
            <token id="8" string="children" />
            <token id="9" string="repeatedly" />
            <token id="10" string="had" />
            <token id="11" string="been" />
            <token id="12" string="abused" />
            <token id="13" string="sexually" />
            <token id="14" string="," />
            <token id="15" string="but" />
            <token id="16" string="silenced" />
            <token id="17" string="for" />
            <token id="18" string="years" />
            <token id="19" string="by" />
            <token id="20" string="threats" />
            <token id="21" string="," />
            <token id="22" string="and" />
            <token id="23" string="also" />
            <token id="24" string="that" />
            <token id="25" string="their" />
            <token id="26" string="child-like" />
            <token id="27" string="efforts" />
            <token id="28" string="to" />
            <token id="29" string="tell" />
            <token id="30" string="their" />
            <token id="31" string="parents" />
            <token id="32" string="that" />
            <token id="33" string="something" />
            <token id="34" string="was" />
            <token id="35" string="wrong" />
            <token id="36" string="had" />
            <token id="37" string="been" />
            <token id="38" string="ignored" />
            <token id="39" string="or" />
            <token id="40" string="misinterpreted" />
          </tokens>
        </chunking>
        <chunking id="10" string="to tell their parents that something was wrong" type="VP">
          <tokens>
            <token id="28" string="to" />
            <token id="29" string="tell" />
            <token id="30" string="their" />
            <token id="31" string="parents" />
            <token id="32" string="that" />
            <token id="33" string="something" />
            <token id="34" string="was" />
            <token id="35" string="wrong" />
          </tokens>
        </chunking>
        <chunking id="11" string="tell their parents that something was wrong" type="VP">
          <tokens>
            <token id="29" string="tell" />
            <token id="30" string="their" />
            <token id="31" string="parents" />
            <token id="32" string="that" />
            <token id="33" string="something" />
            <token id="34" string="was" />
            <token id="35" string="wrong" />
          </tokens>
        </chunking>
        <chunking id="12" string="had been ignored or misinterpreted" type="VP">
          <tokens>
            <token id="36" string="had" />
            <token id="37" string="been" />
            <token id="38" string="ignored" />
            <token id="39" string="or" />
            <token id="40" string="misinterpreted" />
          </tokens>
        </chunking>
        <chunking id="13" string="their parents" type="NP">
          <tokens>
            <token id="30" string="their" />
            <token id="31" string="parents" />
          </tokens>
        </chunking>
        <chunking id="14" string="had been abused" type="VP">
          <tokens>
            <token id="10" string="had" />
            <token id="11" string="been" />
            <token id="12" string="abused" />
          </tokens>
        </chunking>
        <chunking id="15" string="abused" type="VP">
          <tokens>
            <token id="12" string="abused" />
          </tokens>
        </chunking>
        <chunking id="16" string="show that the children repeatedly had been abused sexually" type="VP">
          <tokens>
            <token id="5" string="show" />
            <token id="6" string="that" />
            <token id="7" string="the" />
            <token id="8" string="children" />
            <token id="9" string="repeatedly" />
            <token id="10" string="had" />
            <token id="11" string="been" />
            <token id="12" string="abused" />
            <token id="13" string="sexually" />
          </tokens>
        </chunking>
        <chunking id="17" string="that their child-like efforts to tell their parents that something was wrong had been ignored or misinterpreted" type="SBAR">
          <tokens>
            <token id="24" string="that" />
            <token id="25" string="their" />
            <token id="26" string="child-like" />
            <token id="27" string="efforts" />
            <token id="28" string="to" />
            <token id="29" string="tell" />
            <token id="30" string="their" />
            <token id="31" string="parents" />
            <token id="32" string="that" />
            <token id="33" string="something" />
            <token id="34" string="was" />
            <token id="35" string="wrong" />
            <token id="36" string="had" />
            <token id="37" string="been" />
            <token id="38" string="ignored" />
            <token id="39" string="or" />
            <token id="40" string="misinterpreted" />
          </tokens>
        </chunking>
        <chunking id="18" string="years" type="NP">
          <tokens>
            <token id="18" string="years" />
          </tokens>
        </chunking>
        <chunking id="19" string="ignored or misinterpreted" type="VP">
          <tokens>
            <token id="38" string="ignored" />
            <token id="39" string="or" />
            <token id="40" string="misinterpreted" />
          </tokens>
        </chunking>
        <chunking id="20" string="the children" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="children" />
          </tokens>
        </chunking>
        <chunking id="21" string="their child-like efforts to tell their parents that something was wrong" type="NP">
          <tokens>
            <token id="25" string="their" />
            <token id="26" string="child-like" />
            <token id="27" string="efforts" />
            <token id="28" string="to" />
            <token id="29" string="tell" />
            <token id="30" string="their" />
            <token id="31" string="parents" />
            <token id="32" string="that" />
            <token id="33" string="something" />
            <token id="34" string="was" />
            <token id="35" string="wrong" />
          </tokens>
        </chunking>
        <chunking id="22" string="sought to show that the children repeatedly had been abused sexually" type="VP">
          <tokens>
            <token id="3" string="sought" />
            <token id="4" string="to" />
            <token id="5" string="show" />
            <token id="6" string="that" />
            <token id="7" string="the" />
            <token id="8" string="children" />
            <token id="9" string="repeatedly" />
            <token id="10" string="had" />
            <token id="11" string="been" />
            <token id="12" string="abused" />
            <token id="13" string="sexually" />
          </tokens>
        </chunking>
        <chunking id="23" string="threats" type="NP">
          <tokens>
            <token id="20" string="threats" />
          </tokens>
        </chunking>
        <chunking id="24" string="that the children repeatedly had been abused" type="SBAR">
          <tokens>
            <token id="6" string="that" />
            <token id="7" string="the" />
            <token id="8" string="children" />
            <token id="9" string="repeatedly" />
            <token id="10" string="had" />
            <token id="11" string="been" />
            <token id="12" string="abused" />
          </tokens>
        </chunking>
        <chunking id="25" string="was wrong" type="VP">
          <tokens>
            <token id="34" string="was" />
            <token id="35" string="wrong" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">prosecution</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">sought</governor>
          <dependent id="2">prosecution</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">sought</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="5">show</governor>
          <dependent id="4">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="3">sought</governor>
          <dependent id="5">show</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="12">abused</governor>
          <dependent id="6">that</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">children</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">abused</governor>
          <dependent id="8">children</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="12">abused</governor>
          <dependent id="9">repeatedly</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="12">abused</governor>
          <dependent id="10">had</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="12">abused</governor>
          <dependent id="11">been</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="5">show</governor>
          <dependent id="12">abused</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">show</governor>
          <dependent id="13">sexually</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="3">sought</governor>
          <dependent id="15">but</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="3">sought</governor>
          <dependent id="16">silenced</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">years</governor>
          <dependent id="17">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">silenced</governor>
          <dependent id="18">years</dependent>
        </dependency>
        <dependency type="case">
          <governor id="20">threats</governor>
          <dependent id="19">by</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">silenced</governor>
          <dependent id="20">threats</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="3">sought</governor>
          <dependent id="22">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="38">ignored</governor>
          <dependent id="23">also</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="38">ignored</governor>
          <dependent id="24">that</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="27">efforts</governor>
          <dependent id="25">their</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="27">efforts</governor>
          <dependent id="26">child-like</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="38">ignored</governor>
          <dependent id="27">efforts</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="29">tell</governor>
          <dependent id="28">to</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="27">efforts</governor>
          <dependent id="29">tell</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="31">parents</governor>
          <dependent id="30">their</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="29">tell</governor>
          <dependent id="31">parents</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="35">wrong</governor>
          <dependent id="32">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="35">wrong</governor>
          <dependent id="33">something</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="35">wrong</governor>
          <dependent id="34">was</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="29">tell</governor>
          <dependent id="35">wrong</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="38">ignored</governor>
          <dependent id="36">had</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="38">ignored</governor>
          <dependent id="37">been</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="3">sought</governor>
          <dependent id="38">ignored</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="38">ignored</governor>
          <dependent id="39">or</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="38">ignored</governor>
          <dependent id="40">misinterpreted</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="years" type="DURATION" score="0.0">
          <tokens>
            <token id="18" string="years" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="111" has_coreference="true">
      <content>The defense contended that nothing untoward happened at the school, but that the children were programmed by therapists to believe that something bad had happened to them.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="defense" lemma="defense" stem="defens" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="contended" lemma="contend" stem="contend" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="nothing" lemma="nothing" stem="noth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="untoward" lemma="untoward" stem="untoward" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="happened" lemma="happen" stem="happen" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="school" lemma="school" stem="school" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="16" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="programmed" lemma="program" stem="program" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="therapists" lemma="therapist" stem="therapist" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="20" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="believe" lemma="believe" stem="believ" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="something" lemma="something" stem="someth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="bad" lemma="bad" stem="bad" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="happened" lemma="happen" stem="happen" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="them" lemma="they" stem="them" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="29" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN defense)) (VP (VBD contended) (SBAR (SBAR (IN that) (S (NP (NN nothing) (JJ untoward)) (VP (VBD happened) (PP (IN at) (NP (DT the) (NN school)))))) (, ,) (CC but) (SBAR (IN that) (S (NP (DT the) (NNS children)) (VP (VBD were) (VP (VBN programmed) (PP (IN by) (NP (NNS therapists))) (S (VP (TO to) (VP (VB believe) (SBAR (IN that) (S (NP (NN something) (JJ bad)) (VP (VBD had) (VP (VBN happened) (PP (TO to) (NP (PRP them)))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="nothing untoward" type="NP">
          <tokens>
            <token id="5" string="nothing" />
            <token id="6" string="untoward" />
          </tokens>
        </chunking>
        <chunking id="2" string="had happened to them" type="VP">
          <tokens>
            <token id="25" string="had" />
            <token id="26" string="happened" />
            <token id="27" string="to" />
            <token id="28" string="them" />
          </tokens>
        </chunking>
        <chunking id="3" string="that something bad had happened to them" type="SBAR">
          <tokens>
            <token id="22" string="that" />
            <token id="23" string="something" />
            <token id="24" string="bad" />
            <token id="25" string="had" />
            <token id="26" string="happened" />
            <token id="27" string="to" />
            <token id="28" string="them" />
          </tokens>
        </chunking>
        <chunking id="4" string="that nothing untoward happened at the school" type="SBAR">
          <tokens>
            <token id="4" string="that" />
            <token id="5" string="nothing" />
            <token id="6" string="untoward" />
            <token id="7" string="happened" />
            <token id="8" string="at" />
            <token id="9" string="the" />
            <token id="10" string="school" />
          </tokens>
        </chunking>
        <chunking id="5" string="happened at the school" type="VP">
          <tokens>
            <token id="7" string="happened" />
            <token id="8" string="at" />
            <token id="9" string="the" />
            <token id="10" string="school" />
          </tokens>
        </chunking>
        <chunking id="6" string="something bad" type="NP">
          <tokens>
            <token id="23" string="something" />
            <token id="24" string="bad" />
          </tokens>
        </chunking>
        <chunking id="7" string="The defense" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="defense" />
          </tokens>
        </chunking>
        <chunking id="8" string="therapists" type="NP">
          <tokens>
            <token id="19" string="therapists" />
          </tokens>
        </chunking>
        <chunking id="9" string="that the children were programmed by therapists to believe that something bad had happened to them" type="SBAR">
          <tokens>
            <token id="13" string="that" />
            <token id="14" string="the" />
            <token id="15" string="children" />
            <token id="16" string="were" />
            <token id="17" string="programmed" />
            <token id="18" string="by" />
            <token id="19" string="therapists" />
            <token id="20" string="to" />
            <token id="21" string="believe" />
            <token id="22" string="that" />
            <token id="23" string="something" />
            <token id="24" string="bad" />
            <token id="25" string="had" />
            <token id="26" string="happened" />
            <token id="27" string="to" />
            <token id="28" string="them" />
          </tokens>
        </chunking>
        <chunking id="10" string="happened to them" type="VP">
          <tokens>
            <token id="26" string="happened" />
            <token id="27" string="to" />
            <token id="28" string="them" />
          </tokens>
        </chunking>
        <chunking id="11" string="them" type="NP">
          <tokens>
            <token id="28" string="them" />
          </tokens>
        </chunking>
        <chunking id="12" string="to believe that something bad had happened to them" type="VP">
          <tokens>
            <token id="20" string="to" />
            <token id="21" string="believe" />
            <token id="22" string="that" />
            <token id="23" string="something" />
            <token id="24" string="bad" />
            <token id="25" string="had" />
            <token id="26" string="happened" />
            <token id="27" string="to" />
            <token id="28" string="them" />
          </tokens>
        </chunking>
        <chunking id="13" string="that nothing untoward happened at the school , but that the children were programmed by therapists to believe that something bad had happened to them" type="SBAR">
          <tokens>
            <token id="4" string="that" />
            <token id="5" string="nothing" />
            <token id="6" string="untoward" />
            <token id="7" string="happened" />
            <token id="8" string="at" />
            <token id="9" string="the" />
            <token id="10" string="school" />
            <token id="11" string="," />
            <token id="12" string="but" />
            <token id="13" string="that" />
            <token id="14" string="the" />
            <token id="15" string="children" />
            <token id="16" string="were" />
            <token id="17" string="programmed" />
            <token id="18" string="by" />
            <token id="19" string="therapists" />
            <token id="20" string="to" />
            <token id="21" string="believe" />
            <token id="22" string="that" />
            <token id="23" string="something" />
            <token id="24" string="bad" />
            <token id="25" string="had" />
            <token id="26" string="happened" />
            <token id="27" string="to" />
            <token id="28" string="them" />
          </tokens>
        </chunking>
        <chunking id="14" string="the school" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="school" />
          </tokens>
        </chunking>
        <chunking id="15" string="were programmed by therapists to believe that something bad had happened to them" type="VP">
          <tokens>
            <token id="16" string="were" />
            <token id="17" string="programmed" />
            <token id="18" string="by" />
            <token id="19" string="therapists" />
            <token id="20" string="to" />
            <token id="21" string="believe" />
            <token id="22" string="that" />
            <token id="23" string="something" />
            <token id="24" string="bad" />
            <token id="25" string="had" />
            <token id="26" string="happened" />
            <token id="27" string="to" />
            <token id="28" string="them" />
          </tokens>
        </chunking>
        <chunking id="16" string="the children" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="children" />
          </tokens>
        </chunking>
        <chunking id="17" string="believe that something bad had happened to them" type="VP">
          <tokens>
            <token id="21" string="believe" />
            <token id="22" string="that" />
            <token id="23" string="something" />
            <token id="24" string="bad" />
            <token id="25" string="had" />
            <token id="26" string="happened" />
            <token id="27" string="to" />
            <token id="28" string="them" />
          </tokens>
        </chunking>
        <chunking id="18" string="contended that nothing untoward happened at the school , but that the children were programmed by therapists to believe that something bad had happened to them" type="VP">
          <tokens>
            <token id="3" string="contended" />
            <token id="4" string="that" />
            <token id="5" string="nothing" />
            <token id="6" string="untoward" />
            <token id="7" string="happened" />
            <token id="8" string="at" />
            <token id="9" string="the" />
            <token id="10" string="school" />
            <token id="11" string="," />
            <token id="12" string="but" />
            <token id="13" string="that" />
            <token id="14" string="the" />
            <token id="15" string="children" />
            <token id="16" string="were" />
            <token id="17" string="programmed" />
            <token id="18" string="by" />
            <token id="19" string="therapists" />
            <token id="20" string="to" />
            <token id="21" string="believe" />
            <token id="22" string="that" />
            <token id="23" string="something" />
            <token id="24" string="bad" />
            <token id="25" string="had" />
            <token id="26" string="happened" />
            <token id="27" string="to" />
            <token id="28" string="them" />
          </tokens>
        </chunking>
        <chunking id="19" string="programmed by therapists to believe that something bad had happened to them" type="VP">
          <tokens>
            <token id="17" string="programmed" />
            <token id="18" string="by" />
            <token id="19" string="therapists" />
            <token id="20" string="to" />
            <token id="21" string="believe" />
            <token id="22" string="that" />
            <token id="23" string="something" />
            <token id="24" string="bad" />
            <token id="25" string="had" />
            <token id="26" string="happened" />
            <token id="27" string="to" />
            <token id="28" string="them" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">defense</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">contended</governor>
          <dependent id="2">defense</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">contended</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="7">happened</governor>
          <dependent id="4">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">happened</governor>
          <dependent id="5">nothing</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">nothing</governor>
          <dependent id="6">untoward</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">contended</governor>
          <dependent id="7">happened</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">school</governor>
          <dependent id="8">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">school</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">happened</governor>
          <dependent id="10">school</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">happened</governor>
          <dependent id="12">but</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="17">programmed</governor>
          <dependent id="13">that</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">children</governor>
          <dependent id="14">the</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="17">programmed</governor>
          <dependent id="15">children</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="17">programmed</governor>
          <dependent id="16">were</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">happened</governor>
          <dependent id="17">programmed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">therapists</governor>
          <dependent id="18">by</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">programmed</governor>
          <dependent id="19">therapists</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="21">believe</governor>
          <dependent id="20">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="17">programmed</governor>
          <dependent id="21">believe</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="26">happened</governor>
          <dependent id="22">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="26">happened</governor>
          <dependent id="23">something</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="23">something</governor>
          <dependent id="24">bad</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="26">happened</governor>
          <dependent id="25">had</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="21">believe</governor>
          <dependent id="26">happened</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">them</governor>
          <dependent id="27">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">happened</governor>
          <dependent id="28">them</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="112" has_coreference="true">
      <content>It claimed that their stories defied common sense and that the medical evidence was unreliable and perhaps fabricated.</content>
      <tokens>
        <token id="1" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="claimed" lemma="claim" stem="claim" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="stories" lemma="story" stem="stori" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="defied" lemma="defy" stem="defi" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="common" lemma="common" stem="common" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="sense" lemma="sense" stem="sens" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="medical" lemma="medical" stem="medic" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="evidence" lemma="evidence" stem="evid" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="unreliable" lemma="unreliable" stem="unreli" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="perhaps" lemma="perhaps" stem="perhap" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="fabricated" lemma="fabricate" stem="fabric" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP It)) (VP (VBD claimed) (SBAR (SBAR (IN that) (S (NP (PRP$ their) (NNS stories)) (VP (VBD defied) (NP (JJ common) (NN sense))))) (CC and) (SBAR (IN that) (S (NP (DT the) (JJ medical) (NN evidence)) (VP (VBD was) (UCP (ADJP (JJ unreliable)) (CC and) (VP (ADVP (RB perhaps)) (VBN fabricated)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="defied common sense" type="VP">
          <tokens>
            <token id="6" string="defied" />
            <token id="7" string="common" />
            <token id="8" string="sense" />
          </tokens>
        </chunking>
        <chunking id="2" string="that the medical evidence was unreliable and perhaps fabricated" type="SBAR">
          <tokens>
            <token id="10" string="that" />
            <token id="11" string="the" />
            <token id="12" string="medical" />
            <token id="13" string="evidence" />
            <token id="14" string="was" />
            <token id="15" string="unreliable" />
            <token id="16" string="and" />
            <token id="17" string="perhaps" />
            <token id="18" string="fabricated" />
          </tokens>
        </chunking>
        <chunking id="3" string="unreliable" type="ADJP">
          <tokens>
            <token id="15" string="unreliable" />
          </tokens>
        </chunking>
        <chunking id="4" string="It" type="NP">
          <tokens>
            <token id="1" string="It" />
          </tokens>
        </chunking>
        <chunking id="5" string="perhaps fabricated" type="VP">
          <tokens>
            <token id="17" string="perhaps" />
            <token id="18" string="fabricated" />
          </tokens>
        </chunking>
        <chunking id="6" string="was unreliable and perhaps fabricated" type="VP">
          <tokens>
            <token id="14" string="was" />
            <token id="15" string="unreliable" />
            <token id="16" string="and" />
            <token id="17" string="perhaps" />
            <token id="18" string="fabricated" />
          </tokens>
        </chunking>
        <chunking id="7" string="that their stories defied common sense and that the medical evidence was unreliable and perhaps fabricated" type="SBAR">
          <tokens>
            <token id="3" string="that" />
            <token id="4" string="their" />
            <token id="5" string="stories" />
            <token id="6" string="defied" />
            <token id="7" string="common" />
            <token id="8" string="sense" />
            <token id="9" string="and" />
            <token id="10" string="that" />
            <token id="11" string="the" />
            <token id="12" string="medical" />
            <token id="13" string="evidence" />
            <token id="14" string="was" />
            <token id="15" string="unreliable" />
            <token id="16" string="and" />
            <token id="17" string="perhaps" />
            <token id="18" string="fabricated" />
          </tokens>
        </chunking>
        <chunking id="8" string="common sense" type="NP">
          <tokens>
            <token id="7" string="common" />
            <token id="8" string="sense" />
          </tokens>
        </chunking>
        <chunking id="9" string="claimed that their stories defied common sense and that the medical evidence was unreliable and perhaps fabricated" type="VP">
          <tokens>
            <token id="2" string="claimed" />
            <token id="3" string="that" />
            <token id="4" string="their" />
            <token id="5" string="stories" />
            <token id="6" string="defied" />
            <token id="7" string="common" />
            <token id="8" string="sense" />
            <token id="9" string="and" />
            <token id="10" string="that" />
            <token id="11" string="the" />
            <token id="12" string="medical" />
            <token id="13" string="evidence" />
            <token id="14" string="was" />
            <token id="15" string="unreliable" />
            <token id="16" string="and" />
            <token id="17" string="perhaps" />
            <token id="18" string="fabricated" />
          </tokens>
        </chunking>
        <chunking id="10" string="that their stories defied common sense" type="SBAR">
          <tokens>
            <token id="3" string="that" />
            <token id="4" string="their" />
            <token id="5" string="stories" />
            <token id="6" string="defied" />
            <token id="7" string="common" />
            <token id="8" string="sense" />
          </tokens>
        </chunking>
        <chunking id="11" string="their stories" type="NP">
          <tokens>
            <token id="4" string="their" />
            <token id="5" string="stories" />
          </tokens>
        </chunking>
        <chunking id="12" string="the medical evidence" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="medical" />
            <token id="13" string="evidence" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">claimed</governor>
          <dependent id="1">It</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">claimed</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="6">defied</governor>
          <dependent id="3">that</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="5">stories</governor>
          <dependent id="4">their</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">defied</governor>
          <dependent id="5">stories</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="2">claimed</governor>
          <dependent id="6">defied</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">sense</governor>
          <dependent id="7">common</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">defied</governor>
          <dependent id="8">sense</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="6">defied</governor>
          <dependent id="9">and</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="15">unreliable</governor>
          <dependent id="10">that</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">evidence</governor>
          <dependent id="11">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">evidence</governor>
          <dependent id="12">medical</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="15">unreliable</governor>
          <dependent id="13">evidence</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="15">unreliable</governor>
          <dependent id="14">was</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="6">defied</governor>
          <dependent id="15">unreliable</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="15">unreliable</governor>
          <dependent id="16">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="18">fabricated</governor>
          <dependent id="17">perhaps</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="15">unreliable</governor>
          <dependent id="18">fabricated</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="113" has_coreference="false">
      <content>The 6-year-long case was marked by twists and turns and tragedies, prompting the judge to comment that it had &amp;quot;poisoned&amp;quot; everyone it had touched.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="6-year-long" lemma="6-year-long" stem="6-year-long" pos="JJ" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="3" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="marked" lemma="mark" stem="mark" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="twists" lemma="twist" stem="twist" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="turns" lemma="turn" stem="turn" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="tragedies" lemma="tragedy" stem="tragedi" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="prompting" lemma="prompt" stem="prompt" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="judge" lemma="judge" stem="judg" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="comment" lemma="comment" stem="comment" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="poisoned" lemma="poison" stem="poison" pos="VBN" type="Word" isStopWord="false" ner="CAUSE_OF_DEATH" is_referenced="false" is_refers="false" />
        <token id="23" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="everyone" lemma="everyone" stem="everyon" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="touched" lemma="touch" stem="touch" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (DT The) (JJ 6-year-long) (NN case)) (VP (VBD was) (VP (VBN marked) (PP (IN by) (NP (NNS twists) (CC and) (NNS turns)))))) (CC and) (S (NP (NNS tragedies)) (, ,) (S (VP (VBG prompting) (NP (DT the) (NN judge) (S (VP (TO to) (VP (VB comment) (SBAR (IN that) (S (NP (PRP it)) (VP (VBD had) (`` ``) (VP (VBN poisoned) ('' '') (NP (NN everyone)))))))))))) (NP (PRP it)) (VP (VBD had) (VP (VBN touched)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="touched" type="VP">
          <tokens>
            <token id="27" string="touched" />
          </tokens>
        </chunking>
        <chunking id="2" string="The 6-year-long case" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="6-year-long" />
            <token id="3" string="case" />
          </tokens>
        </chunking>
        <chunking id="3" string="marked by twists and turns" type="VP">
          <tokens>
            <token id="5" string="marked" />
            <token id="6" string="by" />
            <token id="7" string="twists" />
            <token id="8" string="and" />
            <token id="9" string="turns" />
          </tokens>
        </chunking>
        <chunking id="4" string="everyone" type="NP">
          <tokens>
            <token id="24" string="everyone" />
          </tokens>
        </chunking>
        <chunking id="5" string="was marked by twists and turns" type="VP">
          <tokens>
            <token id="4" string="was" />
            <token id="5" string="marked" />
            <token id="6" string="by" />
            <token id="7" string="twists" />
            <token id="8" string="and" />
            <token id="9" string="turns" />
          </tokens>
        </chunking>
        <chunking id="6" string="to comment that it had `` poisoned '' everyone" type="VP">
          <tokens>
            <token id="16" string="to" />
            <token id="17" string="comment" />
            <token id="18" string="that" />
            <token id="19" string="it" />
            <token id="20" string="had" />
            <token id="21" string="&quot;" />
            <token id="22" string="poisoned" />
            <token id="23" string="&quot;" />
            <token id="24" string="everyone" />
          </tokens>
        </chunking>
        <chunking id="7" string="it" type="NP">
          <tokens>
            <token id="19" string="it" />
          </tokens>
        </chunking>
        <chunking id="8" string="had `` poisoned '' everyone" type="VP">
          <tokens>
            <token id="20" string="had" />
            <token id="21" string="&quot;" />
            <token id="22" string="poisoned" />
            <token id="23" string="&quot;" />
            <token id="24" string="everyone" />
          </tokens>
        </chunking>
        <chunking id="9" string="the judge to comment that it had `` poisoned '' everyone" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="judge" />
            <token id="16" string="to" />
            <token id="17" string="comment" />
            <token id="18" string="that" />
            <token id="19" string="it" />
            <token id="20" string="had" />
            <token id="21" string="&quot;" />
            <token id="22" string="poisoned" />
            <token id="23" string="&quot;" />
            <token id="24" string="everyone" />
          </tokens>
        </chunking>
        <chunking id="10" string="that it had `` poisoned '' everyone" type="SBAR">
          <tokens>
            <token id="18" string="that" />
            <token id="19" string="it" />
            <token id="20" string="had" />
            <token id="21" string="&quot;" />
            <token id="22" string="poisoned" />
            <token id="23" string="&quot;" />
            <token id="24" string="everyone" />
          </tokens>
        </chunking>
        <chunking id="11" string="poisoned '' everyone" type="VP">
          <tokens>
            <token id="22" string="poisoned" />
            <token id="23" string="&quot;" />
            <token id="24" string="everyone" />
          </tokens>
        </chunking>
        <chunking id="12" string="had touched" type="VP">
          <tokens>
            <token id="26" string="had" />
            <token id="27" string="touched" />
          </tokens>
        </chunking>
        <chunking id="13" string="tragedies" type="NP">
          <tokens>
            <token id="11" string="tragedies" />
          </tokens>
        </chunking>
        <chunking id="14" string="comment that it had `` poisoned '' everyone" type="VP">
          <tokens>
            <token id="17" string="comment" />
            <token id="18" string="that" />
            <token id="19" string="it" />
            <token id="20" string="had" />
            <token id="21" string="&quot;" />
            <token id="22" string="poisoned" />
            <token id="23" string="&quot;" />
            <token id="24" string="everyone" />
          </tokens>
        </chunking>
        <chunking id="15" string="prompting the judge to comment that it had `` poisoned '' everyone" type="VP">
          <tokens>
            <token id="13" string="prompting" />
            <token id="14" string="the" />
            <token id="15" string="judge" />
            <token id="16" string="to" />
            <token id="17" string="comment" />
            <token id="18" string="that" />
            <token id="19" string="it" />
            <token id="20" string="had" />
            <token id="21" string="&quot;" />
            <token id="22" string="poisoned" />
            <token id="23" string="&quot;" />
            <token id="24" string="everyone" />
          </tokens>
        </chunking>
        <chunking id="16" string="twists and turns" type="NP">
          <tokens>
            <token id="7" string="twists" />
            <token id="8" string="and" />
            <token id="9" string="turns" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">case</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">case</governor>
          <dependent id="2">6-year-long</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="5">marked</governor>
          <dependent id="3">case</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="5">marked</governor>
          <dependent id="4">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">marked</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">twists</governor>
          <dependent id="6">by</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">marked</governor>
          <dependent id="7">twists</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">twists</governor>
          <dependent id="8">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">twists</governor>
          <dependent id="9">turns</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="5">marked</governor>
          <dependent id="10">and</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="27">touched</governor>
          <dependent id="11">tragedies</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="27">touched</governor>
          <dependent id="13">prompting</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">judge</governor>
          <dependent id="14">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="13">prompting</governor>
          <dependent id="15">judge</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="17">comment</governor>
          <dependent id="16">to</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="15">judge</governor>
          <dependent id="17">comment</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="22">poisoned</governor>
          <dependent id="18">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">poisoned</governor>
          <dependent id="19">it</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="22">poisoned</governor>
          <dependent id="20">had</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="17">comment</governor>
          <dependent id="22">poisoned</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="22">poisoned</governor>
          <dependent id="24">everyone</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="27">touched</governor>
          <dependent id="25">it</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="27">touched</governor>
          <dependent id="26">had</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="5">marked</governor>
          <dependent id="27">touched</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="6-year-long" type="DURATION" score="0.0">
          <tokens>
            <token id="2" string="6-year-long" />
          </tokens>
        </entity>
        <entity id="2" string="poisoned" type="CAUSE_OF_DEATH" score="0.0">
          <tokens>
            <token id="22" string="poisoned" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="114" has_coreference="true">
      <content>Day after day, it was punctuated with the unexpected and bizarre: * Ailing family matriarch Virginia McMartin, now 82, wrote poetry, conducted colorful hallway interviews and tested the judge&amp;apost;s patience with such outbursts as, &amp;quot;This awful court.</content>
      <tokens>
        <token id="1" string="Day" lemma="day" stem="dai" pos="NN" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="true" />
        <token id="2" string="after" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="day" lemma="day" stem="dai" pos="NN" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="true" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="punctuated" lemma="punctuate" stem="punctuat" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="unexpected" lemma="unexpected" stem="unexpect" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="bizarre" lemma="bizarre" stem="bizarr" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="*" lemma="*" stem="*" pos="SYM" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="Ailing" lemma="ail" stem="aile" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="family" lemma="family" stem="famili" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="17" string="matriarch" lemma="matriarch" stem="matriarch" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="18" string="Virginia" lemma="Virginia" stem="virginia" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="19" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="now" lemma="now" stem="now" pos="RB" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="22" string="82" lemma="82" stem="82" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="23" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="wrote" lemma="write" stem="wrote" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="poetry" lemma="poetry" stem="poetri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="conducted" lemma="conduct" stem="conduct" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="colorful" lemma="colorful" stem="color" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="hallway" lemma="hallway" stem="hallwai" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="interviews" lemma="interview" stem="interview" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="tested" lemma="test" stem="test" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="34" string="judge" lemma="judge" stem="judg" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="35" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="36" string="patience" lemma="patience" stem="patienc" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="37" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="such" lemma="such" stem="such" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="39" string="outbursts" lemma="outburst" stem="outburst" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="40" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="41" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="42" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="43" string="This" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="44" string="awful" lemma="awful" stem="aw" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="45" string="court" lemma="court" stem="court" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="46" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (NP (NN Day)) (IN after) (NP (NN day))) (, ,) (NP (PRP it)) (VP (VBD was) (VP (VP (VBN punctuated) (PP (IN with) (NP (DT the) (ADJP (JJ unexpected) (CC and) (JJ bizarre)))) (: :) (FRAG (X (SYM *)) (VP (S (VP (VBG Ailing) (NP (NP (NN family) (NN matriarch) (NNP Virginia) (NNP McMartin)) (, ,) (RRC (ADVP (RB now)) (NP (CD 82)))))) (, ,) (VP (VBD wrote) (NP (NN poetry)))))) (, ,) (VP (VBN conducted) (NP (JJ colorful) (NN hallway) (NNS interviews))) (CC and) (VP (VBN tested) (ADVP (NP (NP (NP (DT the) (NN judge) (POS 's)) (NN patience)) (PP (IN with) (NP (JJ such) (NNS outbursts)))) (IN as)) (, ,) (`` ``) (NP (DT This) (JJ awful) (NN court))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the judge 's" type="NP">
          <tokens>
            <token id="33" string="the" />
            <token id="34" string="judge" />
            <token id="35" string="'s" />
          </tokens>
        </chunking>
        <chunking id="2" string="wrote poetry" type="VP">
          <tokens>
            <token id="24" string="wrote" />
            <token id="25" string="poetry" />
          </tokens>
        </chunking>
        <chunking id="3" string="punctuated with the unexpected and bizarre : * Ailing family matriarch Virginia McMartin , now 82 , wrote poetry" type="VP">
          <tokens>
            <token id="7" string="punctuated" />
            <token id="8" string="with" />
            <token id="9" string="the" />
            <token id="10" string="unexpected" />
            <token id="11" string="and" />
            <token id="12" string="bizarre" />
            <token id="13" string=":" />
            <token id="14" string="*" />
            <token id="15" string="Ailing" />
            <token id="16" string="family" />
            <token id="17" string="matriarch" />
            <token id="18" string="Virginia" />
            <token id="19" string="McMartin" />
            <token id="20" string="," />
            <token id="21" string="now" />
            <token id="22" string="82" />
            <token id="23" string="," />
            <token id="24" string="wrote" />
            <token id="25" string="poetry" />
          </tokens>
        </chunking>
        <chunking id="4" string="Ailing family matriarch Virginia McMartin , now 82 , wrote poetry" type="VP">
          <tokens>
            <token id="15" string="Ailing" />
            <token id="16" string="family" />
            <token id="17" string="matriarch" />
            <token id="18" string="Virginia" />
            <token id="19" string="McMartin" />
            <token id="20" string="," />
            <token id="21" string="now" />
            <token id="22" string="82" />
            <token id="23" string="," />
            <token id="24" string="wrote" />
            <token id="25" string="poetry" />
          </tokens>
        </chunking>
        <chunking id="5" string="conducted colorful hallway interviews" type="VP">
          <tokens>
            <token id="27" string="conducted" />
            <token id="28" string="colorful" />
            <token id="29" string="hallway" />
            <token id="30" string="interviews" />
          </tokens>
        </chunking>
        <chunking id="6" string="it" type="NP">
          <tokens>
            <token id="5" string="it" />
          </tokens>
        </chunking>
        <chunking id="7" string="the judge 's patience with such outbursts" type="NP">
          <tokens>
            <token id="33" string="the" />
            <token id="34" string="judge" />
            <token id="35" string="'s" />
            <token id="36" string="patience" />
            <token id="37" string="with" />
            <token id="38" string="such" />
            <token id="39" string="outbursts" />
          </tokens>
        </chunking>
        <chunking id="8" string="the judge 's patience" type="NP">
          <tokens>
            <token id="33" string="the" />
            <token id="34" string="judge" />
            <token id="35" string="'s" />
            <token id="36" string="patience" />
          </tokens>
        </chunking>
        <chunking id="9" string="This awful court" type="NP">
          <tokens>
            <token id="43" string="This" />
            <token id="44" string="awful" />
            <token id="45" string="court" />
          </tokens>
        </chunking>
        <chunking id="10" string="family matriarch Virginia McMartin" type="NP">
          <tokens>
            <token id="16" string="family" />
            <token id="17" string="matriarch" />
            <token id="18" string="Virginia" />
            <token id="19" string="McMartin" />
          </tokens>
        </chunking>
        <chunking id="11" string="tested the judge 's patience with such outbursts as , `` This awful court" type="VP">
          <tokens>
            <token id="32" string="tested" />
            <token id="33" string="the" />
            <token id="34" string="judge" />
            <token id="35" string="'s" />
            <token id="36" string="patience" />
            <token id="37" string="with" />
            <token id="38" string="such" />
            <token id="39" string="outbursts" />
            <token id="40" string="as" />
            <token id="41" string="," />
            <token id="42" string="&quot;" />
            <token id="43" string="This" />
            <token id="44" string="awful" />
            <token id="45" string="court" />
          </tokens>
        </chunking>
        <chunking id="12" string="punctuated with the unexpected and bizarre : * Ailing family matriarch Virginia McMartin , now 82 , wrote poetry , conducted colorful hallway interviews and tested the judge 's patience with such outbursts as , `` This awful court" type="VP">
          <tokens>
            <token id="7" string="punctuated" />
            <token id="8" string="with" />
            <token id="9" string="the" />
            <token id="10" string="unexpected" />
            <token id="11" string="and" />
            <token id="12" string="bizarre" />
            <token id="13" string=":" />
            <token id="14" string="*" />
            <token id="15" string="Ailing" />
            <token id="16" string="family" />
            <token id="17" string="matriarch" />
            <token id="18" string="Virginia" />
            <token id="19" string="McMartin" />
            <token id="20" string="," />
            <token id="21" string="now" />
            <token id="22" string="82" />
            <token id="23" string="," />
            <token id="24" string="wrote" />
            <token id="25" string="poetry" />
            <token id="26" string="," />
            <token id="27" string="conducted" />
            <token id="28" string="colorful" />
            <token id="29" string="hallway" />
            <token id="30" string="interviews" />
            <token id="31" string="and" />
            <token id="32" string="tested" />
            <token id="33" string="the" />
            <token id="34" string="judge" />
            <token id="35" string="'s" />
            <token id="36" string="patience" />
            <token id="37" string="with" />
            <token id="38" string="such" />
            <token id="39" string="outbursts" />
            <token id="40" string="as" />
            <token id="41" string="," />
            <token id="42" string="&quot;" />
            <token id="43" string="This" />
            <token id="44" string="awful" />
            <token id="45" string="court" />
          </tokens>
        </chunking>
        <chunking id="13" string="such outbursts" type="NP">
          <tokens>
            <token id="38" string="such" />
            <token id="39" string="outbursts" />
          </tokens>
        </chunking>
        <chunking id="14" string="was punctuated with the unexpected and bizarre : * Ailing family matriarch Virginia McMartin , now 82 , wrote poetry , conducted colorful hallway interviews and tested the judge 's patience with such outbursts as , `` This awful court" type="VP">
          <tokens>
            <token id="6" string="was" />
            <token id="7" string="punctuated" />
            <token id="8" string="with" />
            <token id="9" string="the" />
            <token id="10" string="unexpected" />
            <token id="11" string="and" />
            <token id="12" string="bizarre" />
            <token id="13" string=":" />
            <token id="14" string="*" />
            <token id="15" string="Ailing" />
            <token id="16" string="family" />
            <token id="17" string="matriarch" />
            <token id="18" string="Virginia" />
            <token id="19" string="McMartin" />
            <token id="20" string="," />
            <token id="21" string="now" />
            <token id="22" string="82" />
            <token id="23" string="," />
            <token id="24" string="wrote" />
            <token id="25" string="poetry" />
            <token id="26" string="," />
            <token id="27" string="conducted" />
            <token id="28" string="colorful" />
            <token id="29" string="hallway" />
            <token id="30" string="interviews" />
            <token id="31" string="and" />
            <token id="32" string="tested" />
            <token id="33" string="the" />
            <token id="34" string="judge" />
            <token id="35" string="'s" />
            <token id="36" string="patience" />
            <token id="37" string="with" />
            <token id="38" string="such" />
            <token id="39" string="outbursts" />
            <token id="40" string="as" />
            <token id="41" string="," />
            <token id="42" string="&quot;" />
            <token id="43" string="This" />
            <token id="44" string="awful" />
            <token id="45" string="court" />
          </tokens>
        </chunking>
        <chunking id="15" string="unexpected and bizarre" type="ADJP">
          <tokens>
            <token id="10" string="unexpected" />
            <token id="11" string="and" />
            <token id="12" string="bizarre" />
          </tokens>
        </chunking>
        <chunking id="16" string="family matriarch Virginia McMartin , now 82" type="NP">
          <tokens>
            <token id="16" string="family" />
            <token id="17" string="matriarch" />
            <token id="18" string="Virginia" />
            <token id="19" string="McMartin" />
            <token id="20" string="," />
            <token id="21" string="now" />
            <token id="22" string="82" />
          </tokens>
        </chunking>
        <chunking id="17" string="the unexpected and bizarre" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="unexpected" />
            <token id="11" string="and" />
            <token id="12" string="bizarre" />
          </tokens>
        </chunking>
        <chunking id="18" string="82" type="NP">
          <tokens>
            <token id="22" string="82" />
          </tokens>
        </chunking>
        <chunking id="19" string="Day" type="NP">
          <tokens>
            <token id="1" string="Day" />
          </tokens>
        </chunking>
        <chunking id="20" string="day" type="NP">
          <tokens>
            <token id="3" string="day" />
          </tokens>
        </chunking>
        <chunking id="21" string="Ailing family matriarch Virginia McMartin , now 82" type="VP">
          <tokens>
            <token id="15" string="Ailing" />
            <token id="16" string="family" />
            <token id="17" string="matriarch" />
            <token id="18" string="Virginia" />
            <token id="19" string="McMartin" />
            <token id="20" string="," />
            <token id="21" string="now" />
            <token id="22" string="82" />
          </tokens>
        </chunking>
        <chunking id="22" string="poetry" type="NP">
          <tokens>
            <token id="25" string="poetry" />
          </tokens>
        </chunking>
        <chunking id="23" string="colorful hallway interviews" type="NP">
          <tokens>
            <token id="28" string="colorful" />
            <token id="29" string="hallway" />
            <token id="30" string="interviews" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod">
          <governor id="7">punctuated</governor>
          <dependent id="1">Day</dependent>
        </dependency>
        <dependency type="case">
          <governor id="1">Day</governor>
          <dependent id="2">after</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="1">Day</governor>
          <dependent id="3">day</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="7">punctuated</governor>
          <dependent id="5">it</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="7">punctuated</governor>
          <dependent id="6">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">punctuated</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">unexpected</governor>
          <dependent id="8">with</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">unexpected</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">punctuated</governor>
          <dependent id="10">unexpected</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="10">unexpected</governor>
          <dependent id="11">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="10">unexpected</governor>
          <dependent id="12">bizarre</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="24">wrote</governor>
          <dependent id="14">*</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="24">wrote</governor>
          <dependent id="15">Ailing</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">McMartin</governor>
          <dependent id="16">family</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">McMartin</governor>
          <dependent id="17">matriarch</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">McMartin</governor>
          <dependent id="18">Virginia</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">Ailing</governor>
          <dependent id="19">McMartin</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="22">82</governor>
          <dependent id="21">now</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="19">McMartin</governor>
          <dependent id="22">82</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="7">punctuated</governor>
          <dependent id="24">wrote</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="24">wrote</governor>
          <dependent id="25">poetry</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">punctuated</governor>
          <dependent id="27">conducted</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="30">interviews</governor>
          <dependent id="28">colorful</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="30">interviews</governor>
          <dependent id="29">hallway</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="27">conducted</governor>
          <dependent id="30">interviews</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">punctuated</governor>
          <dependent id="31">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">punctuated</governor>
          <dependent id="32">tested</dependent>
        </dependency>
        <dependency type="det">
          <governor id="34">judge</governor>
          <dependent id="33">the</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="36">patience</governor>
          <dependent id="34">judge</dependent>
        </dependency>
        <dependency type="case">
          <governor id="34">judge</governor>
          <dependent id="35">'s</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="32">tested</governor>
          <dependent id="36">patience</dependent>
        </dependency>
        <dependency type="case">
          <governor id="39">outbursts</governor>
          <dependent id="37">with</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="39">outbursts</governor>
          <dependent id="38">such</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="36">patience</governor>
          <dependent id="39">outbursts</dependent>
        </dependency>
        <dependency type="case">
          <governor id="36">patience</governor>
          <dependent id="40">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="45">court</governor>
          <dependent id="43">This</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="45">court</governor>
          <dependent id="44">awful</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="32">tested</governor>
          <dependent id="45">court</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="now" type="DATE" score="0.0">
          <tokens>
            <token id="21" string="now" />
          </tokens>
        </entity>
        <entity id="2" string="Virginia McMartin" type="PERSON" score="0.0">
          <tokens>
            <token id="18" string="Virginia" />
            <token id="19" string="McMartin" />
          </tokens>
        </entity>
        <entity id="3" string="82" type="NUMBER" score="0.0">
          <tokens>
            <token id="22" string="82" />
          </tokens>
        </entity>
        <entity id="4" string="Day" type="DURATION" score="0.0">
          <tokens>
            <token id="1" string="Day" />
          </tokens>
        </entity>
        <entity id="5" string="day" type="DURATION" score="0.0">
          <tokens>
            <token id="3" string="day" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="115" has_coreference="false">
      <content>These awful people.</content>
      <tokens>
        <token id="1" string="These" lemma="these" stem="these" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="awful" lemma="awful" stem="aw" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="people" lemma="people" stem="peopl" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (DT These)) (NP (JJ awful) (NNS people)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="These" type="NP">
          <tokens>
            <token id="1" string="These" />
          </tokens>
        </chunking>
        <chunking id="2" string="These awful people ." type="NP">
          <tokens>
            <token id="1" string="These" />
            <token id="2" string="awful" />
            <token id="3" string="people" />
            <token id="4" string="." />
          </tokens>
        </chunking>
        <chunking id="3" string="awful people" type="NP">
          <tokens>
            <token id="2" string="awful" />
            <token id="3" string="people" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">These</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">people</governor>
          <dependent id="2">awful</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="1">These</governor>
          <dependent id="3">people</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="116" has_coreference="false">
      <content>These awful lies.&amp;quot;</content>
      <tokens>
        <token id="1" string="These" lemma="these" stem="these" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="awful" lemma="awful" stem="aw" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="lies" lemma="lie" stem="li" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (FRAG (NP (NP (DT These)) (NP (ADJP (JJ awful)) (NNS lies))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="These" type="NP">
          <tokens>
            <token id="1" string="These" />
          </tokens>
        </chunking>
        <chunking id="2" string="awful lies" type="NP">
          <tokens>
            <token id="2" string="awful" />
            <token id="3" string="lies" />
          </tokens>
        </chunking>
        <chunking id="3" string="awful" type="ADJP">
          <tokens>
            <token id="2" string="awful" />
          </tokens>
        </chunking>
        <chunking id="4" string="These awful lies" type="NP">
          <tokens>
            <token id="1" string="These" />
            <token id="2" string="awful" />
            <token id="3" string="lies" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">These</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">lies</governor>
          <dependent id="2">awful</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="1">These</governor>
          <dependent id="3">lies</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="117" has_coreference="true">
      <content>* Therapist Kee MacFarlane, who conducted many of the interviews in which children first disclosed that they had been molested, came under fire when it was revealed not only that she had minimal previous experience in the field but also that she was involved romantically with the television newsman who broke the story.</content>
      <tokens>
        <token id="1" string="*" lemma="*" stem="*" pos="SYM" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="Therapist" lemma="Therapist" stem="therapist" pos="NNP" type="Word" isStopWord="false" ner="TITLE" is_referenced="true" is_refers="false" />
        <token id="3" string="Kee" lemma="Kee" stem="kee" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="4" string="MacFarlane" lemma="MacFarlane" stem="macfarlan" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="conducted" lemma="conduct" stem="conduct" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="many" lemma="many" stem="mani" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="interviews" lemma="interview" stem="interview" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="which" lemma="which" stem="which" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="15" string="first" lemma="first" stem="first" pos="RB" type="Word" isStopWord="false" ner="ORDINAL" is_referenced="false" is_refers="false" />
        <token id="16" string="disclosed" lemma="disclose" stem="disclos" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="molested" lemma="molest" stem="molest" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="came" lemma="come" stem="came" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="under" lemma="under" stem="under" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="fire" lemma="fire" stem="fire" pos="NN" type="Word" isStopWord="false" ner="CAUSE_OF_DEATH" is_referenced="false" is_refers="false" />
        <token id="26" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="revealed" lemma="reveal" stem="reveal" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="only" lemma="only" stem="onli" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="minimal" lemma="minimal" stem="minim" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="36" string="previous" lemma="previous" stem="previou" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="37" string="experience" lemma="experience" stem="experi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="38" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="39" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="40" string="field" lemma="field" stem="field" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="41" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="42" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="43" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="44" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="45" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="46" string="involved" lemma="involve" stem="involv" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="47" string="romantically" lemma="romantically" stem="romant" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="48" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="49" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="50" string="television" lemma="television" stem="televis" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="51" string="newsman" lemma="newsman" stem="newsman" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="52" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="53" string="broke" lemma="break" stem="broke" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="54" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="55" string="story" lemma="story" stem="stori" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="56" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (FRAG (SBAR (X (SYM *)) (S (NP (NP (NNP Therapist) (NNP Kee) (NNP MacFarlane)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBD conducted) (NP (NP (JJ many)) (PP (IN of) (NP (NP (DT the) (NNS interviews)) (SBAR (WHPP (IN in) (WHNP (WDT which))) (S (NP (NNS children)) (ADVP (RB first)) (VP (VBD disclosed) (SBAR (IN that) (S (NP (PRP they)) (VP (VBD had) (VP (VBN been) (VP (VBN molested))))))))))))))) (, ,)) (VP (VBD came) (PP (IN under) (NP (NN fire))) (SBAR (WHADVP (WRB when)) (S (NP (PRP it)) (VP (VBD was) (VP (VBN revealed) (SBAR (RB not) (RB only) (IN that) (S (NP (PRP she)) (VP (VBD had) (NP (JJ minimal) (JJ previous) (NN experience)) (PP (IN in) (NP (DT the) (NN field))) (PRN (CC but) (ADVP (RB also))) (SBAR (IN that) (S (NP (PRP she)) (VP (VBD was) (VP (VBN involved) (ADVP (RB romantically)) (PP (IN with) (NP (NP (DT the) (NN television) (NN newsman)) (SBAR (WHNP (WP who)) (S (VP (VBD broke) (NP (DT the) (NN story))))))))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the interviews" type="NP">
          <tokens>
            <token id="10" string="the" />
            <token id="11" string="interviews" />
          </tokens>
        </chunking>
        <chunking id="2" string="had been molested" type="VP">
          <tokens>
            <token id="19" string="had" />
            <token id="20" string="been" />
            <token id="21" string="molested" />
          </tokens>
        </chunking>
        <chunking id="3" string="it" type="NP">
          <tokens>
            <token id="27" string="it" />
          </tokens>
        </chunking>
        <chunking id="4" string="had minimal previous experience in the field but also that she was involved romantically with the television newsman who broke the story" type="VP">
          <tokens>
            <token id="34" string="had" />
            <token id="35" string="minimal" />
            <token id="36" string="previous" />
            <token id="37" string="experience" />
            <token id="38" string="in" />
            <token id="39" string="the" />
            <token id="40" string="field" />
            <token id="41" string="but" />
            <token id="42" string="also" />
            <token id="43" string="that" />
            <token id="44" string="she" />
            <token id="45" string="was" />
            <token id="46" string="involved" />
            <token id="47" string="romantically" />
            <token id="48" string="with" />
            <token id="49" string="the" />
            <token id="50" string="television" />
            <token id="51" string="newsman" />
            <token id="52" string="who" />
            <token id="53" string="broke" />
            <token id="54" string="the" />
            <token id="55" string="story" />
          </tokens>
        </chunking>
        <chunking id="5" string="many" type="NP">
          <tokens>
            <token id="8" string="many" />
          </tokens>
        </chunking>
        <chunking id="6" string="not only that she had minimal previous experience in the field but also that she was involved romantically with the television newsman who broke the story" type="SBAR">
          <tokens>
            <token id="30" string="not" />
            <token id="31" string="only" />
            <token id="32" string="that" />
            <token id="33" string="she" />
            <token id="34" string="had" />
            <token id="35" string="minimal" />
            <token id="36" string="previous" />
            <token id="37" string="experience" />
            <token id="38" string="in" />
            <token id="39" string="the" />
            <token id="40" string="field" />
            <token id="41" string="but" />
            <token id="42" string="also" />
            <token id="43" string="that" />
            <token id="44" string="she" />
            <token id="45" string="was" />
            <token id="46" string="involved" />
            <token id="47" string="romantically" />
            <token id="48" string="with" />
            <token id="49" string="the" />
            <token id="50" string="television" />
            <token id="51" string="newsman" />
            <token id="52" string="who" />
            <token id="53" string="broke" />
            <token id="54" string="the" />
            <token id="55" string="story" />
          </tokens>
        </chunking>
        <chunking id="7" string="broke the story" type="VP">
          <tokens>
            <token id="53" string="broke" />
            <token id="54" string="the" />
            <token id="55" string="story" />
          </tokens>
        </chunking>
        <chunking id="8" string="she" type="NP">
          <tokens>
            <token id="33" string="she" />
          </tokens>
        </chunking>
        <chunking id="9" string="the field" type="NP">
          <tokens>
            <token id="39" string="the" />
            <token id="40" string="field" />
          </tokens>
        </chunking>
        <chunking id="10" string="who broke the story" type="SBAR">
          <tokens>
            <token id="52" string="who" />
            <token id="53" string="broke" />
            <token id="54" string="the" />
            <token id="55" string="story" />
          </tokens>
        </chunking>
        <chunking id="11" string="the story" type="NP">
          <tokens>
            <token id="54" string="the" />
            <token id="55" string="story" />
          </tokens>
        </chunking>
        <chunking id="12" string="conducted many of the interviews in which children first disclosed that they had been molested" type="VP">
          <tokens>
            <token id="7" string="conducted" />
            <token id="8" string="many" />
            <token id="9" string="of" />
            <token id="10" string="the" />
            <token id="11" string="interviews" />
            <token id="12" string="in" />
            <token id="13" string="which" />
            <token id="14" string="children" />
            <token id="15" string="first" />
            <token id="16" string="disclosed" />
            <token id="17" string="that" />
            <token id="18" string="they" />
            <token id="19" string="had" />
            <token id="20" string="been" />
            <token id="21" string="molested" />
          </tokens>
        </chunking>
        <chunking id="13" string="the television newsman" type="NP">
          <tokens>
            <token id="49" string="the" />
            <token id="50" string="television" />
            <token id="51" string="newsman" />
          </tokens>
        </chunking>
        <chunking id="14" string="that they had been molested" type="SBAR">
          <tokens>
            <token id="17" string="that" />
            <token id="18" string="they" />
            <token id="19" string="had" />
            <token id="20" string="been" />
            <token id="21" string="molested" />
          </tokens>
        </chunking>
        <chunking id="15" string="was revealed not only that she had minimal previous experience in the field but also that she was involved romantically with the television newsman who broke the story" type="VP">
          <tokens>
            <token id="28" string="was" />
            <token id="29" string="revealed" />
            <token id="30" string="not" />
            <token id="31" string="only" />
            <token id="32" string="that" />
            <token id="33" string="she" />
            <token id="34" string="had" />
            <token id="35" string="minimal" />
            <token id="36" string="previous" />
            <token id="37" string="experience" />
            <token id="38" string="in" />
            <token id="39" string="the" />
            <token id="40" string="field" />
            <token id="41" string="but" />
            <token id="42" string="also" />
            <token id="43" string="that" />
            <token id="44" string="she" />
            <token id="45" string="was" />
            <token id="46" string="involved" />
            <token id="47" string="romantically" />
            <token id="48" string="with" />
            <token id="49" string="the" />
            <token id="50" string="television" />
            <token id="51" string="newsman" />
            <token id="52" string="who" />
            <token id="53" string="broke" />
            <token id="54" string="the" />
            <token id="55" string="story" />
          </tokens>
        </chunking>
        <chunking id="16" string="the interviews in which children first disclosed that they had been molested" type="NP">
          <tokens>
            <token id="10" string="the" />
            <token id="11" string="interviews" />
            <token id="12" string="in" />
            <token id="13" string="which" />
            <token id="14" string="children" />
            <token id="15" string="first" />
            <token id="16" string="disclosed" />
            <token id="17" string="that" />
            <token id="18" string="they" />
            <token id="19" string="had" />
            <token id="20" string="been" />
            <token id="21" string="molested" />
          </tokens>
        </chunking>
        <chunking id="17" string="* Therapist Kee MacFarlane , who conducted many of the interviews in which children first disclosed that they had been molested , came under fire when it was revealed not only that she had minimal previous experience in the field but also that she was involved romantically with the television newsman who broke the story" type="SBAR">
          <tokens>
            <token id="1" string="*" />
            <token id="2" string="Therapist" />
            <token id="3" string="Kee" />
            <token id="4" string="MacFarlane" />
            <token id="5" string="," />
            <token id="6" string="who" />
            <token id="7" string="conducted" />
            <token id="8" string="many" />
            <token id="9" string="of" />
            <token id="10" string="the" />
            <token id="11" string="interviews" />
            <token id="12" string="in" />
            <token id="13" string="which" />
            <token id="14" string="children" />
            <token id="15" string="first" />
            <token id="16" string="disclosed" />
            <token id="17" string="that" />
            <token id="18" string="they" />
            <token id="19" string="had" />
            <token id="20" string="been" />
            <token id="21" string="molested" />
            <token id="22" string="," />
            <token id="23" string="came" />
            <token id="24" string="under" />
            <token id="25" string="fire" />
            <token id="26" string="when" />
            <token id="27" string="it" />
            <token id="28" string="was" />
            <token id="29" string="revealed" />
            <token id="30" string="not" />
            <token id="31" string="only" />
            <token id="32" string="that" />
            <token id="33" string="she" />
            <token id="34" string="had" />
            <token id="35" string="minimal" />
            <token id="36" string="previous" />
            <token id="37" string="experience" />
            <token id="38" string="in" />
            <token id="39" string="the" />
            <token id="40" string="field" />
            <token id="41" string="but" />
            <token id="42" string="also" />
            <token id="43" string="that" />
            <token id="44" string="she" />
            <token id="45" string="was" />
            <token id="46" string="involved" />
            <token id="47" string="romantically" />
            <token id="48" string="with" />
            <token id="49" string="the" />
            <token id="50" string="television" />
            <token id="51" string="newsman" />
            <token id="52" string="who" />
            <token id="53" string="broke" />
            <token id="54" string="the" />
            <token id="55" string="story" />
          </tokens>
        </chunking>
        <chunking id="18" string="Therapist Kee MacFarlane , who conducted many of the interviews in which children first disclosed that they had been molested ," type="NP">
          <tokens>
            <token id="2" string="Therapist" />
            <token id="3" string="Kee" />
            <token id="4" string="MacFarlane" />
            <token id="5" string="," />
            <token id="6" string="who" />
            <token id="7" string="conducted" />
            <token id="8" string="many" />
            <token id="9" string="of" />
            <token id="10" string="the" />
            <token id="11" string="interviews" />
            <token id="12" string="in" />
            <token id="13" string="which" />
            <token id="14" string="children" />
            <token id="15" string="first" />
            <token id="16" string="disclosed" />
            <token id="17" string="that" />
            <token id="18" string="they" />
            <token id="19" string="had" />
            <token id="20" string="been" />
            <token id="21" string="molested" />
            <token id="22" string="," />
          </tokens>
        </chunking>
        <chunking id="19" string="disclosed that they had been molested" type="VP">
          <tokens>
            <token id="16" string="disclosed" />
            <token id="17" string="that" />
            <token id="18" string="they" />
            <token id="19" string="had" />
            <token id="20" string="been" />
            <token id="21" string="molested" />
          </tokens>
        </chunking>
        <chunking id="20" string="when it was revealed not only that she had minimal previous experience in the field but also that she was involved romantically with the television newsman who broke the story" type="SBAR">
          <tokens>
            <token id="26" string="when" />
            <token id="27" string="it" />
            <token id="28" string="was" />
            <token id="29" string="revealed" />
            <token id="30" string="not" />
            <token id="31" string="only" />
            <token id="32" string="that" />
            <token id="33" string="she" />
            <token id="34" string="had" />
            <token id="35" string="minimal" />
            <token id="36" string="previous" />
            <token id="37" string="experience" />
            <token id="38" string="in" />
            <token id="39" string="the" />
            <token id="40" string="field" />
            <token id="41" string="but" />
            <token id="42" string="also" />
            <token id="43" string="that" />
            <token id="44" string="she" />
            <token id="45" string="was" />
            <token id="46" string="involved" />
            <token id="47" string="romantically" />
            <token id="48" string="with" />
            <token id="49" string="the" />
            <token id="50" string="television" />
            <token id="51" string="newsman" />
            <token id="52" string="who" />
            <token id="53" string="broke" />
            <token id="54" string="the" />
            <token id="55" string="story" />
          </tokens>
        </chunking>
        <chunking id="21" string="when" type="WHADVP">
          <tokens>
            <token id="26" string="when" />
          </tokens>
        </chunking>
        <chunking id="22" string="they" type="NP">
          <tokens>
            <token id="18" string="they" />
          </tokens>
        </chunking>
        <chunking id="23" string="was involved romantically with the television newsman who broke the story" type="VP">
          <tokens>
            <token id="45" string="was" />
            <token id="46" string="involved" />
            <token id="47" string="romantically" />
            <token id="48" string="with" />
            <token id="49" string="the" />
            <token id="50" string="television" />
            <token id="51" string="newsman" />
            <token id="52" string="who" />
            <token id="53" string="broke" />
            <token id="54" string="the" />
            <token id="55" string="story" />
          </tokens>
        </chunking>
        <chunking id="24" string="came under fire when it was revealed not only that she had minimal previous experience in the field but also that she was involved romantically with the television newsman who broke the story" type="VP">
          <tokens>
            <token id="23" string="came" />
            <token id="24" string="under" />
            <token id="25" string="fire" />
            <token id="26" string="when" />
            <token id="27" string="it" />
            <token id="28" string="was" />
            <token id="29" string="revealed" />
            <token id="30" string="not" />
            <token id="31" string="only" />
            <token id="32" string="that" />
            <token id="33" string="she" />
            <token id="34" string="had" />
            <token id="35" string="minimal" />
            <token id="36" string="previous" />
            <token id="37" string="experience" />
            <token id="38" string="in" />
            <token id="39" string="the" />
            <token id="40" string="field" />
            <token id="41" string="but" />
            <token id="42" string="also" />
            <token id="43" string="that" />
            <token id="44" string="she" />
            <token id="45" string="was" />
            <token id="46" string="involved" />
            <token id="47" string="romantically" />
            <token id="48" string="with" />
            <token id="49" string="the" />
            <token id="50" string="television" />
            <token id="51" string="newsman" />
            <token id="52" string="who" />
            <token id="53" string="broke" />
            <token id="54" string="the" />
            <token id="55" string="story" />
          </tokens>
        </chunking>
        <chunking id="25" string="involved romantically with the television newsman who broke the story" type="VP">
          <tokens>
            <token id="46" string="involved" />
            <token id="47" string="romantically" />
            <token id="48" string="with" />
            <token id="49" string="the" />
            <token id="50" string="television" />
            <token id="51" string="newsman" />
            <token id="52" string="who" />
            <token id="53" string="broke" />
            <token id="54" string="the" />
            <token id="55" string="story" />
          </tokens>
        </chunking>
        <chunking id="26" string="children" type="NP">
          <tokens>
            <token id="14" string="children" />
          </tokens>
        </chunking>
        <chunking id="27" string="been molested" type="VP">
          <tokens>
            <token id="20" string="been" />
            <token id="21" string="molested" />
          </tokens>
        </chunking>
        <chunking id="28" string="that she was involved romantically with the television newsman who broke the story" type="SBAR">
          <tokens>
            <token id="43" string="that" />
            <token id="44" string="she" />
            <token id="45" string="was" />
            <token id="46" string="involved" />
            <token id="47" string="romantically" />
            <token id="48" string="with" />
            <token id="49" string="the" />
            <token id="50" string="television" />
            <token id="51" string="newsman" />
            <token id="52" string="who" />
            <token id="53" string="broke" />
            <token id="54" string="the" />
            <token id="55" string="story" />
          </tokens>
        </chunking>
        <chunking id="29" string="the television newsman who broke the story" type="NP">
          <tokens>
            <token id="49" string="the" />
            <token id="50" string="television" />
            <token id="51" string="newsman" />
            <token id="52" string="who" />
            <token id="53" string="broke" />
            <token id="54" string="the" />
            <token id="55" string="story" />
          </tokens>
        </chunking>
        <chunking id="30" string="many of the interviews in which children first disclosed that they had been molested" type="NP">
          <tokens>
            <token id="8" string="many" />
            <token id="9" string="of" />
            <token id="10" string="the" />
            <token id="11" string="interviews" />
            <token id="12" string="in" />
            <token id="13" string="which" />
            <token id="14" string="children" />
            <token id="15" string="first" />
            <token id="16" string="disclosed" />
            <token id="17" string="that" />
            <token id="18" string="they" />
            <token id="19" string="had" />
            <token id="20" string="been" />
            <token id="21" string="molested" />
          </tokens>
        </chunking>
        <chunking id="31" string="revealed not only that she had minimal previous experience in the field but also that she was involved romantically with the television newsman who broke the story" type="VP">
          <tokens>
            <token id="29" string="revealed" />
            <token id="30" string="not" />
            <token id="31" string="only" />
            <token id="32" string="that" />
            <token id="33" string="she" />
            <token id="34" string="had" />
            <token id="35" string="minimal" />
            <token id="36" string="previous" />
            <token id="37" string="experience" />
            <token id="38" string="in" />
            <token id="39" string="the" />
            <token id="40" string="field" />
            <token id="41" string="but" />
            <token id="42" string="also" />
            <token id="43" string="that" />
            <token id="44" string="she" />
            <token id="45" string="was" />
            <token id="46" string="involved" />
            <token id="47" string="romantically" />
            <token id="48" string="with" />
            <token id="49" string="the" />
            <token id="50" string="television" />
            <token id="51" string="newsman" />
            <token id="52" string="who" />
            <token id="53" string="broke" />
            <token id="54" string="the" />
            <token id="55" string="story" />
          </tokens>
        </chunking>
        <chunking id="32" string="fire" type="NP">
          <tokens>
            <token id="25" string="fire" />
          </tokens>
        </chunking>
        <chunking id="33" string="Therapist Kee MacFarlane" type="NP">
          <tokens>
            <token id="2" string="Therapist" />
            <token id="3" string="Kee" />
            <token id="4" string="MacFarlane" />
          </tokens>
        </chunking>
        <chunking id="34" string="in which children first disclosed that they had been molested" type="SBAR">
          <tokens>
            <token id="12" string="in" />
            <token id="13" string="which" />
            <token id="14" string="children" />
            <token id="15" string="first" />
            <token id="16" string="disclosed" />
            <token id="17" string="that" />
            <token id="18" string="they" />
            <token id="19" string="had" />
            <token id="20" string="been" />
            <token id="21" string="molested" />
          </tokens>
        </chunking>
        <chunking id="35" string="who conducted many of the interviews in which children first disclosed that they had been molested" type="SBAR">
          <tokens>
            <token id="6" string="who" />
            <token id="7" string="conducted" />
            <token id="8" string="many" />
            <token id="9" string="of" />
            <token id="10" string="the" />
            <token id="11" string="interviews" />
            <token id="12" string="in" />
            <token id="13" string="which" />
            <token id="14" string="children" />
            <token id="15" string="first" />
            <token id="16" string="disclosed" />
            <token id="17" string="that" />
            <token id="18" string="they" />
            <token id="19" string="had" />
            <token id="20" string="been" />
            <token id="21" string="molested" />
          </tokens>
        </chunking>
        <chunking id="36" string="minimal previous experience" type="NP">
          <tokens>
            <token id="35" string="minimal" />
            <token id="36" string="previous" />
            <token id="37" string="experience" />
          </tokens>
        </chunking>
        <chunking id="37" string="molested" type="VP">
          <tokens>
            <token id="21" string="molested" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="dep">
          <governor id="23">came</governor>
          <dependent id="1">*</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">MacFarlane</governor>
          <dependent id="2">Therapist</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">MacFarlane</governor>
          <dependent id="3">Kee</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="23">came</governor>
          <dependent id="4">MacFarlane</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">conducted</governor>
          <dependent id="6">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="4">MacFarlane</governor>
          <dependent id="7">conducted</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">conducted</governor>
          <dependent id="8">many</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">interviews</governor>
          <dependent id="9">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">interviews</governor>
          <dependent id="10">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">many</governor>
          <dependent id="11">interviews</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">which</governor>
          <dependent id="12">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">disclosed</governor>
          <dependent id="13">which</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">disclosed</governor>
          <dependent id="14">children</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="16">disclosed</governor>
          <dependent id="15">first</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="11">interviews</governor>
          <dependent id="16">disclosed</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="21">molested</governor>
          <dependent id="17">that</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="21">molested</governor>
          <dependent id="18">they</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="21">molested</governor>
          <dependent id="19">had</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="21">molested</governor>
          <dependent id="20">been</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="16">disclosed</governor>
          <dependent id="21">molested</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="23">came</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">fire</governor>
          <dependent id="24">under</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">came</governor>
          <dependent id="25">fire</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="29">revealed</governor>
          <dependent id="26">when</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="29">revealed</governor>
          <dependent id="27">it</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="29">revealed</governor>
          <dependent id="28">was</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="23">came</governor>
          <dependent id="29">revealed</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="34">had</governor>
          <dependent id="30">not</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="34">had</governor>
          <dependent id="31">only</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="34">had</governor>
          <dependent id="32">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="34">had</governor>
          <dependent id="33">she</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="29">revealed</governor>
          <dependent id="34">had</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="37">experience</governor>
          <dependent id="35">minimal</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="37">experience</governor>
          <dependent id="36">previous</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="34">had</governor>
          <dependent id="37">experience</dependent>
        </dependency>
        <dependency type="case">
          <governor id="40">field</governor>
          <dependent id="38">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="40">field</governor>
          <dependent id="39">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="34">had</governor>
          <dependent id="40">field</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="42">also</governor>
          <dependent id="41">but</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="34">had</governor>
          <dependent id="42">also</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="46">involved</governor>
          <dependent id="43">that</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="46">involved</governor>
          <dependent id="44">she</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="46">involved</governor>
          <dependent id="45">was</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="34">had</governor>
          <dependent id="46">involved</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="46">involved</governor>
          <dependent id="47">romantically</dependent>
        </dependency>
        <dependency type="case">
          <governor id="51">newsman</governor>
          <dependent id="48">with</dependent>
        </dependency>
        <dependency type="det">
          <governor id="51">newsman</governor>
          <dependent id="49">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="51">newsman</governor>
          <dependent id="50">television</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="46">involved</governor>
          <dependent id="51">newsman</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="53">broke</governor>
          <dependent id="52">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="51">newsman</governor>
          <dependent id="53">broke</dependent>
        </dependency>
        <dependency type="det">
          <governor id="55">story</governor>
          <dependent id="54">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="53">broke</governor>
          <dependent id="55">story</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="first" type="ORDINAL" score="0.0">
          <tokens>
            <token id="15" string="first" />
          </tokens>
        </entity>
        <entity id="2" string="fire" type="CAUSE_OF_DEATH" score="0.0">
          <tokens>
            <token id="25" string="fire" />
          </tokens>
        </entity>
        <entity id="3" string="Kee MacFarlane" type="PERSON" score="0.0">
          <tokens>
            <token id="3" string="Kee" />
            <token id="4" string="MacFarlane" />
          </tokens>
        </entity>
        <entity id="4" string="Therapist" type="TITLE" score="0.0">
          <tokens>
            <token id="2" string="Therapist" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="118" has_coreference="true">
      <content>* Former prosecutor Glenn Stevens became a key defense witness after leaking information that he believed that the defendants were innocent, and signed a movie contract to tell his revised version of events.</content>
      <tokens>
        <token id="1" string="*" lemma="*" stem="*" pos="SYM" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="Former" lemma="former" stem="former" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="prosecutor" lemma="prosecutor" stem="prosecutor" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="Glenn" lemma="Glenn" stem="glenn" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="5" string="Stevens" lemma="Stevens" stem="steven" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="6" string="became" lemma="become" stem="becam" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="key" lemma="key" stem="kei" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="defense" lemma="defense" stem="defens" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="witness" lemma="witness" stem="wit" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="after" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="leaking" lemma="leak" stem="leak" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="information" lemma="information" stem="inform" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="16" string="believed" lemma="believe" stem="believ" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="defendants" lemma="defendant" stem="defend" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="innocent" lemma="innocent" stem="innoc" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="signed" lemma="sign" stem="sign" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="movie" lemma="movie" stem="movi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="contract" lemma="contract" stem="contract" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="tell" lemma="tell" stem="tell" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="31" string="revised" lemma="revise" stem="revis" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="version" lemma="version" stem="version" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="events" lemma="event" stem="event" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (FRAG (X (X (SYM *)) (NP (NP (JJ Former) (NN prosecutor)) (NP (NNP Glenn) (NNP Stevens)))) (VP (VBD became) (NP (DT a) (JJ key) (NN defense) (NN witness)) (PP (IN after) (S (VP (VBG leaking) (NP (NN information)) (SBAR (IN that) (S (NP (PRP he)) (VP (VP (VBD believed) (SBAR (IN that) (S (NP (DT the) (NNS defendants)) (VP (VBD were) (ADJP (JJ innocent)))))) (, ,) (CC and) (VP (VBD signed) (NP (DT a) (NN movie) (NN contract)) (S (VP (TO to) (VP (VB tell) (NP (NP (PRP$ his) (VBN revised) (NN version)) (PP (IN of) (NP (NNS events))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="believed that the defendants were innocent , and signed a movie contract to tell his revised version of events" type="VP">
          <tokens>
            <token id="16" string="believed" />
            <token id="17" string="that" />
            <token id="18" string="the" />
            <token id="19" string="defendants" />
            <token id="20" string="were" />
            <token id="21" string="innocent" />
            <token id="22" string="," />
            <token id="23" string="and" />
            <token id="24" string="signed" />
            <token id="25" string="a" />
            <token id="26" string="movie" />
            <token id="27" string="contract" />
            <token id="28" string="to" />
            <token id="29" string="tell" />
            <token id="30" string="his" />
            <token id="31" string="revised" />
            <token id="32" string="version" />
            <token id="33" string="of" />
            <token id="34" string="events" />
          </tokens>
        </chunking>
        <chunking id="2" string="Former prosecutor Glenn Stevens" type="NP">
          <tokens>
            <token id="2" string="Former" />
            <token id="3" string="prosecutor" />
            <token id="4" string="Glenn" />
            <token id="5" string="Stevens" />
          </tokens>
        </chunking>
        <chunking id="3" string="innocent" type="ADJP">
          <tokens>
            <token id="21" string="innocent" />
          </tokens>
        </chunking>
        <chunking id="4" string="his revised version of events" type="NP">
          <tokens>
            <token id="30" string="his" />
            <token id="31" string="revised" />
            <token id="32" string="version" />
            <token id="33" string="of" />
            <token id="34" string="events" />
          </tokens>
        </chunking>
        <chunking id="5" string="to tell his revised version of events" type="VP">
          <tokens>
            <token id="28" string="to" />
            <token id="29" string="tell" />
            <token id="30" string="his" />
            <token id="31" string="revised" />
            <token id="32" string="version" />
            <token id="33" string="of" />
            <token id="34" string="events" />
          </tokens>
        </chunking>
        <chunking id="6" string="were innocent" type="VP">
          <tokens>
            <token id="20" string="were" />
            <token id="21" string="innocent" />
          </tokens>
        </chunking>
        <chunking id="7" string="Glenn Stevens" type="NP">
          <tokens>
            <token id="4" string="Glenn" />
            <token id="5" string="Stevens" />
          </tokens>
        </chunking>
        <chunking id="8" string="leaking information that he believed that the defendants were innocent , and signed a movie contract to tell his revised version of events" type="VP">
          <tokens>
            <token id="12" string="leaking" />
            <token id="13" string="information" />
            <token id="14" string="that" />
            <token id="15" string="he" />
            <token id="16" string="believed" />
            <token id="17" string="that" />
            <token id="18" string="the" />
            <token id="19" string="defendants" />
            <token id="20" string="were" />
            <token id="21" string="innocent" />
            <token id="22" string="," />
            <token id="23" string="and" />
            <token id="24" string="signed" />
            <token id="25" string="a" />
            <token id="26" string="movie" />
            <token id="27" string="contract" />
            <token id="28" string="to" />
            <token id="29" string="tell" />
            <token id="30" string="his" />
            <token id="31" string="revised" />
            <token id="32" string="version" />
            <token id="33" string="of" />
            <token id="34" string="events" />
          </tokens>
        </chunking>
        <chunking id="9" string="believed that the defendants were innocent" type="VP">
          <tokens>
            <token id="16" string="believed" />
            <token id="17" string="that" />
            <token id="18" string="the" />
            <token id="19" string="defendants" />
            <token id="20" string="were" />
            <token id="21" string="innocent" />
          </tokens>
        </chunking>
        <chunking id="10" string="his revised version" type="NP">
          <tokens>
            <token id="30" string="his" />
            <token id="31" string="revised" />
            <token id="32" string="version" />
          </tokens>
        </chunking>
        <chunking id="11" string="Former prosecutor" type="NP">
          <tokens>
            <token id="2" string="Former" />
            <token id="3" string="prosecutor" />
          </tokens>
        </chunking>
        <chunking id="12" string="a key defense witness" type="NP">
          <tokens>
            <token id="7" string="a" />
            <token id="8" string="key" />
            <token id="9" string="defense" />
            <token id="10" string="witness" />
          </tokens>
        </chunking>
        <chunking id="13" string="the defendants" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="defendants" />
          </tokens>
        </chunking>
        <chunking id="14" string="signed a movie contract to tell his revised version of events" type="VP">
          <tokens>
            <token id="24" string="signed" />
            <token id="25" string="a" />
            <token id="26" string="movie" />
            <token id="27" string="contract" />
            <token id="28" string="to" />
            <token id="29" string="tell" />
            <token id="30" string="his" />
            <token id="31" string="revised" />
            <token id="32" string="version" />
            <token id="33" string="of" />
            <token id="34" string="events" />
          </tokens>
        </chunking>
        <chunking id="15" string="became a key defense witness after leaking information that he believed that the defendants were innocent , and signed a movie contract to tell his revised version of events" type="VP">
          <tokens>
            <token id="6" string="became" />
            <token id="7" string="a" />
            <token id="8" string="key" />
            <token id="9" string="defense" />
            <token id="10" string="witness" />
            <token id="11" string="after" />
            <token id="12" string="leaking" />
            <token id="13" string="information" />
            <token id="14" string="that" />
            <token id="15" string="he" />
            <token id="16" string="believed" />
            <token id="17" string="that" />
            <token id="18" string="the" />
            <token id="19" string="defendants" />
            <token id="20" string="were" />
            <token id="21" string="innocent" />
            <token id="22" string="," />
            <token id="23" string="and" />
            <token id="24" string="signed" />
            <token id="25" string="a" />
            <token id="26" string="movie" />
            <token id="27" string="contract" />
            <token id="28" string="to" />
            <token id="29" string="tell" />
            <token id="30" string="his" />
            <token id="31" string="revised" />
            <token id="32" string="version" />
            <token id="33" string="of" />
            <token id="34" string="events" />
          </tokens>
        </chunking>
        <chunking id="16" string="he" type="NP">
          <tokens>
            <token id="15" string="he" />
          </tokens>
        </chunking>
        <chunking id="17" string="that the defendants were innocent" type="SBAR">
          <tokens>
            <token id="17" string="that" />
            <token id="18" string="the" />
            <token id="19" string="defendants" />
            <token id="20" string="were" />
            <token id="21" string="innocent" />
          </tokens>
        </chunking>
        <chunking id="18" string="information" type="NP">
          <tokens>
            <token id="13" string="information" />
          </tokens>
        </chunking>
        <chunking id="19" string="that he believed that the defendants were innocent , and signed a movie contract to tell his revised version of events" type="SBAR">
          <tokens>
            <token id="14" string="that" />
            <token id="15" string="he" />
            <token id="16" string="believed" />
            <token id="17" string="that" />
            <token id="18" string="the" />
            <token id="19" string="defendants" />
            <token id="20" string="were" />
            <token id="21" string="innocent" />
            <token id="22" string="," />
            <token id="23" string="and" />
            <token id="24" string="signed" />
            <token id="25" string="a" />
            <token id="26" string="movie" />
            <token id="27" string="contract" />
            <token id="28" string="to" />
            <token id="29" string="tell" />
            <token id="30" string="his" />
            <token id="31" string="revised" />
            <token id="32" string="version" />
            <token id="33" string="of" />
            <token id="34" string="events" />
          </tokens>
        </chunking>
        <chunking id="20" string="a movie contract" type="NP">
          <tokens>
            <token id="25" string="a" />
            <token id="26" string="movie" />
            <token id="27" string="contract" />
          </tokens>
        </chunking>
        <chunking id="21" string="tell his revised version of events" type="VP">
          <tokens>
            <token id="29" string="tell" />
            <token id="30" string="his" />
            <token id="31" string="revised" />
            <token id="32" string="version" />
            <token id="33" string="of" />
            <token id="34" string="events" />
          </tokens>
        </chunking>
        <chunking id="22" string="events" type="NP">
          <tokens>
            <token id="34" string="events" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="dep">
          <governor id="3">prosecutor</governor>
          <dependent id="1">*</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">prosecutor</governor>
          <dependent id="2">Former</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="6">became</governor>
          <dependent id="3">prosecutor</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">Stevens</governor>
          <dependent id="4">Glenn</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="3">prosecutor</governor>
          <dependent id="5">Stevens</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">became</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">witness</governor>
          <dependent id="7">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="10">witness</governor>
          <dependent id="8">key</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">witness</governor>
          <dependent id="9">defense</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="6">became</governor>
          <dependent id="10">witness</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="12">leaking</governor>
          <dependent id="11">after</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="6">became</governor>
          <dependent id="12">leaking</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="12">leaking</governor>
          <dependent id="13">information</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="16">believed</governor>
          <dependent id="14">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">believed</governor>
          <dependent id="15">he</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="12">leaking</governor>
          <dependent id="16">believed</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="21">innocent</governor>
          <dependent id="17">that</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">defendants</governor>
          <dependent id="18">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">innocent</governor>
          <dependent id="19">defendants</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="21">innocent</governor>
          <dependent id="20">were</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="16">believed</governor>
          <dependent id="21">innocent</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="16">believed</governor>
          <dependent id="23">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="16">believed</governor>
          <dependent id="24">signed</dependent>
        </dependency>
        <dependency type="det">
          <governor id="27">contract</governor>
          <dependent id="25">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="27">contract</governor>
          <dependent id="26">movie</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="24">signed</governor>
          <dependent id="27">contract</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="29">tell</governor>
          <dependent id="28">to</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="24">signed</governor>
          <dependent id="29">tell</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="32">version</governor>
          <dependent id="30">his</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="32">version</governor>
          <dependent id="31">revised</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="29">tell</governor>
          <dependent id="32">version</dependent>
        </dependency>
        <dependency type="case">
          <governor id="34">events</governor>
          <dependent id="33">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="32">version</governor>
          <dependent id="34">events</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Glenn Stevens" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="Glenn" />
            <token id="5" string="Stevens" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="119" has_coreference="true">
      <content>* A 10-year-old boy, testifying at the preliminary hearing, identified everyone from the city attorney and a movie star to a priest and four nuns as among his molesters, and painted a picture of marching with his classmates to a cemetery where they dug up corpses with shovels and pick-axes.</content>
      <tokens>
        <token id="1" string="*" lemma="*" stem="*" pos="SYM" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="A" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="3" string="10-year-old" lemma="10-year-old" stem="10-year-old" pos="JJ" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="4" string="boy" lemma="boy" stem="boi" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="testifying" lemma="testify" stem="testifi" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="preliminary" lemma="preliminary" stem="preliminari" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="10" string="hearing" lemma="hearing" stem="hear" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="identified" lemma="identify" stem="identifi" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="everyone" lemma="everyone" stem="everyon" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="city" lemma="city" stem="citi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="attorney" lemma="attorney" stem="attornei" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="movie" lemma="movie" stem="movi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="star" lemma="star" stem="star" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="priest" lemma="priest" stem="priest" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="four" lemma="four" stem="four" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="27" string="nuns" lemma="nun" stem="nun" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="among" lemma="among" stem="among" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="31" string="molesters" lemma="molester" stem="molest" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="32" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="painted" lemma="paint" stem="paint" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="picture" lemma="picture" stem="pictur" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="37" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="marching" lemma="march" stem="march" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="40" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="41" string="classmates" lemma="classmate" stem="classmat" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="42" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="43" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="44" string="cemetery" lemma="cemetery" stem="cemeteri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="45" string="where" lemma="where" stem="where" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="46" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="47" string="dug" lemma="dig" stem="dug" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="48" string="up" lemma="up" stem="up" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="49" string="corpses" lemma="corpse" stem="corps" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="50" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="51" string="shovels" lemma="shovel" stem="shovel" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="52" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="53" string="pick-axes" lemma="pick-axis" stem="pick-ax" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="54" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (X (X (SYM *)) (NP (DT A) (JJ 10-year-old) (NN boy))) (, ,) (S (VP (VBG testifying) (PP (IN at) (NP (DT the) (JJ preliminary) (NN hearing))))) (, ,) (VP (VBD identified) (NP (NN everyone)) (PP (IN from) (NP (NP (DT the) (NN city) (NN attorney)) (CC and) (NP (DT a) (NN movie) (NN star)))) (PP (TO to) (NP (NP (DT a) (NN priest)) (CC and) (NP (CD four) (NNS nuns)))) (PP (IN as) (PP (IN among) (NP (PRP$ his) (NNS molesters)))))) (, ,) (CC and) (S (VP (VBD painted) (NP (NP (DT a) (NN picture)) (PP (IN of) (S (VP (VBG marching) (PP (IN with) (NP (PRP$ his) (NNS classmates))) (PP (TO to) (NP (NP (DT a) (NN cemetery)) (SBAR (WHADVP (WRB where)) (S (NP (PRP they)) (VP (VBD dug) (PRT (RP up)) (NP (NP (NNS corpses)) (PP (IN with) (NP (NNS shovels) (CC and) (NNS pick-axes))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a priest and four nuns" type="NP">
          <tokens>
            <token id="23" string="a" />
            <token id="24" string="priest" />
            <token id="25" string="and" />
            <token id="26" string="four" />
            <token id="27" string="nuns" />
          </tokens>
        </chunking>
        <chunking id="2" string="the preliminary hearing" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="preliminary" />
            <token id="10" string="hearing" />
          </tokens>
        </chunking>
        <chunking id="3" string="everyone" type="NP">
          <tokens>
            <token id="13" string="everyone" />
          </tokens>
        </chunking>
        <chunking id="4" string="where they dug up corpses with shovels and pick-axes" type="SBAR">
          <tokens>
            <token id="45" string="where" />
            <token id="46" string="they" />
            <token id="47" string="dug" />
            <token id="48" string="up" />
            <token id="49" string="corpses" />
            <token id="50" string="with" />
            <token id="51" string="shovels" />
            <token id="52" string="and" />
            <token id="53" string="pick-axes" />
          </tokens>
        </chunking>
        <chunking id="5" string="shovels and pick-axes" type="NP">
          <tokens>
            <token id="51" string="shovels" />
            <token id="52" string="and" />
            <token id="53" string="pick-axes" />
          </tokens>
        </chunking>
        <chunking id="6" string="corpses with shovels and pick-axes" type="NP">
          <tokens>
            <token id="49" string="corpses" />
            <token id="50" string="with" />
            <token id="51" string="shovels" />
            <token id="52" string="and" />
            <token id="53" string="pick-axes" />
          </tokens>
        </chunking>
        <chunking id="7" string="A 10-year-old boy" type="NP">
          <tokens>
            <token id="2" string="A" />
            <token id="3" string="10-year-old" />
            <token id="4" string="boy" />
          </tokens>
        </chunking>
        <chunking id="8" string="a priest" type="NP">
          <tokens>
            <token id="23" string="a" />
            <token id="24" string="priest" />
          </tokens>
        </chunking>
        <chunking id="9" string="marching with his classmates to a cemetery where they dug up corpses with shovels and pick-axes" type="VP">
          <tokens>
            <token id="38" string="marching" />
            <token id="39" string="with" />
            <token id="40" string="his" />
            <token id="41" string="classmates" />
            <token id="42" string="to" />
            <token id="43" string="a" />
            <token id="44" string="cemetery" />
            <token id="45" string="where" />
            <token id="46" string="they" />
            <token id="47" string="dug" />
            <token id="48" string="up" />
            <token id="49" string="corpses" />
            <token id="50" string="with" />
            <token id="51" string="shovels" />
            <token id="52" string="and" />
            <token id="53" string="pick-axes" />
          </tokens>
        </chunking>
        <chunking id="10" string="a picture of marching with his classmates to a cemetery where they dug up corpses with shovels and pick-axes" type="NP">
          <tokens>
            <token id="35" string="a" />
            <token id="36" string="picture" />
            <token id="37" string="of" />
            <token id="38" string="marching" />
            <token id="39" string="with" />
            <token id="40" string="his" />
            <token id="41" string="classmates" />
            <token id="42" string="to" />
            <token id="43" string="a" />
            <token id="44" string="cemetery" />
            <token id="45" string="where" />
            <token id="46" string="they" />
            <token id="47" string="dug" />
            <token id="48" string="up" />
            <token id="49" string="corpses" />
            <token id="50" string="with" />
            <token id="51" string="shovels" />
            <token id="52" string="and" />
            <token id="53" string="pick-axes" />
          </tokens>
        </chunking>
        <chunking id="11" string="a cemetery" type="NP">
          <tokens>
            <token id="43" string="a" />
            <token id="44" string="cemetery" />
          </tokens>
        </chunking>
        <chunking id="12" string="a movie star" type="NP">
          <tokens>
            <token id="19" string="a" />
            <token id="20" string="movie" />
            <token id="21" string="star" />
          </tokens>
        </chunking>
        <chunking id="13" string="the city attorney and a movie star" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="city" />
            <token id="17" string="attorney" />
            <token id="18" string="and" />
            <token id="19" string="a" />
            <token id="20" string="movie" />
            <token id="21" string="star" />
          </tokens>
        </chunking>
        <chunking id="14" string="his classmates" type="NP">
          <tokens>
            <token id="40" string="his" />
            <token id="41" string="classmates" />
          </tokens>
        </chunking>
        <chunking id="15" string="painted a picture of marching with his classmates to a cemetery where they dug up corpses with shovels and pick-axes" type="VP">
          <tokens>
            <token id="34" string="painted" />
            <token id="35" string="a" />
            <token id="36" string="picture" />
            <token id="37" string="of" />
            <token id="38" string="marching" />
            <token id="39" string="with" />
            <token id="40" string="his" />
            <token id="41" string="classmates" />
            <token id="42" string="to" />
            <token id="43" string="a" />
            <token id="44" string="cemetery" />
            <token id="45" string="where" />
            <token id="46" string="they" />
            <token id="47" string="dug" />
            <token id="48" string="up" />
            <token id="49" string="corpses" />
            <token id="50" string="with" />
            <token id="51" string="shovels" />
            <token id="52" string="and" />
            <token id="53" string="pick-axes" />
          </tokens>
        </chunking>
        <chunking id="16" string="testifying at the preliminary hearing" type="VP">
          <tokens>
            <token id="6" string="testifying" />
            <token id="7" string="at" />
            <token id="8" string="the" />
            <token id="9" string="preliminary" />
            <token id="10" string="hearing" />
          </tokens>
        </chunking>
        <chunking id="17" string="they" type="NP">
          <tokens>
            <token id="46" string="they" />
          </tokens>
        </chunking>
        <chunking id="18" string="his molesters" type="NP">
          <tokens>
            <token id="30" string="his" />
            <token id="31" string="molesters" />
          </tokens>
        </chunking>
        <chunking id="19" string="dug up corpses with shovels and pick-axes" type="VP">
          <tokens>
            <token id="47" string="dug" />
            <token id="48" string="up" />
            <token id="49" string="corpses" />
            <token id="50" string="with" />
            <token id="51" string="shovels" />
            <token id="52" string="and" />
            <token id="53" string="pick-axes" />
          </tokens>
        </chunking>
        <chunking id="20" string="the city attorney" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="city" />
            <token id="17" string="attorney" />
          </tokens>
        </chunking>
        <chunking id="21" string="a picture" type="NP">
          <tokens>
            <token id="35" string="a" />
            <token id="36" string="picture" />
          </tokens>
        </chunking>
        <chunking id="22" string="four nuns" type="NP">
          <tokens>
            <token id="26" string="four" />
            <token id="27" string="nuns" />
          </tokens>
        </chunking>
        <chunking id="23" string="where" type="WHADVP">
          <tokens>
            <token id="45" string="where" />
          </tokens>
        </chunking>
        <chunking id="24" string="corpses" type="NP">
          <tokens>
            <token id="49" string="corpses" />
          </tokens>
        </chunking>
        <chunking id="25" string="identified everyone from the city attorney and a movie star to a priest and four nuns as among his molesters" type="VP">
          <tokens>
            <token id="12" string="identified" />
            <token id="13" string="everyone" />
            <token id="14" string="from" />
            <token id="15" string="the" />
            <token id="16" string="city" />
            <token id="17" string="attorney" />
            <token id="18" string="and" />
            <token id="19" string="a" />
            <token id="20" string="movie" />
            <token id="21" string="star" />
            <token id="22" string="to" />
            <token id="23" string="a" />
            <token id="24" string="priest" />
            <token id="25" string="and" />
            <token id="26" string="four" />
            <token id="27" string="nuns" />
            <token id="28" string="as" />
            <token id="29" string="among" />
            <token id="30" string="his" />
            <token id="31" string="molesters" />
          </tokens>
        </chunking>
        <chunking id="26" string="a cemetery where they dug up corpses with shovels and pick-axes" type="NP">
          <tokens>
            <token id="43" string="a" />
            <token id="44" string="cemetery" />
            <token id="45" string="where" />
            <token id="46" string="they" />
            <token id="47" string="dug" />
            <token id="48" string="up" />
            <token id="49" string="corpses" />
            <token id="50" string="with" />
            <token id="51" string="shovels" />
            <token id="52" string="and" />
            <token id="53" string="pick-axes" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="dep">
          <governor id="4">boy</governor>
          <dependent id="1">*</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">boy</governor>
          <dependent id="2">A</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="4">boy</governor>
          <dependent id="3">10-year-old</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="12">identified</governor>
          <dependent id="4">boy</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="12">identified</governor>
          <dependent id="6">testifying</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">hearing</governor>
          <dependent id="7">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">hearing</governor>
          <dependent id="8">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="10">hearing</governor>
          <dependent id="9">preliminary</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">testifying</governor>
          <dependent id="10">hearing</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="12">identified</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="12">identified</governor>
          <dependent id="13">everyone</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">attorney</governor>
          <dependent id="14">from</dependent>
        </dependency>
        <dependency type="det">
          <governor id="17">attorney</governor>
          <dependent id="15">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="17">attorney</governor>
          <dependent id="16">city</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">identified</governor>
          <dependent id="17">attorney</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="17">attorney</governor>
          <dependent id="18">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="21">star</governor>
          <dependent id="19">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="21">star</governor>
          <dependent id="20">movie</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="17">attorney</governor>
          <dependent id="21">star</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">priest</governor>
          <dependent id="22">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="24">priest</governor>
          <dependent id="23">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">identified</governor>
          <dependent id="24">priest</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="24">priest</governor>
          <dependent id="25">and</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="27">nuns</governor>
          <dependent id="26">four</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="24">priest</governor>
          <dependent id="27">nuns</dependent>
        </dependency>
        <dependency type="case">
          <governor id="31">molesters</governor>
          <dependent id="28">as</dependent>
        </dependency>
        <dependency type="case">
          <governor id="31">molesters</governor>
          <dependent id="29">among</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="31">molesters</governor>
          <dependent id="30">his</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">identified</governor>
          <dependent id="31">molesters</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="12">identified</governor>
          <dependent id="33">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="12">identified</governor>
          <dependent id="34">painted</dependent>
        </dependency>
        <dependency type="det">
          <governor id="36">picture</governor>
          <dependent id="35">a</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="34">painted</governor>
          <dependent id="36">picture</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="38">marching</governor>
          <dependent id="37">of</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="36">picture</governor>
          <dependent id="38">marching</dependent>
        </dependency>
        <dependency type="case">
          <governor id="41">classmates</governor>
          <dependent id="39">with</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="41">classmates</governor>
          <dependent id="40">his</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="38">marching</governor>
          <dependent id="41">classmates</dependent>
        </dependency>
        <dependency type="case">
          <governor id="44">cemetery</governor>
          <dependent id="42">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="44">cemetery</governor>
          <dependent id="43">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="38">marching</governor>
          <dependent id="44">cemetery</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="47">dug</governor>
          <dependent id="45">where</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="47">dug</governor>
          <dependent id="46">they</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="44">cemetery</governor>
          <dependent id="47">dug</dependent>
        </dependency>
        <dependency type="compound:prt">
          <governor id="47">dug</governor>
          <dependent id="48">up</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="47">dug</governor>
          <dependent id="49">corpses</dependent>
        </dependency>
        <dependency type="case">
          <governor id="51">shovels</governor>
          <dependent id="50">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="49">corpses</governor>
          <dependent id="51">shovels</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="51">shovels</governor>
          <dependent id="52">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="51">shovels</governor>
          <dependent id="53">pick-axes</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="four" type="NUMBER" score="0.0">
          <tokens>
            <token id="26" string="four" />
          </tokens>
        </entity>
        <entity id="2" string="10-year-old" type="DURATION" score="0.0">
          <tokens>
            <token id="3" string="10-year-old" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="120" has_coreference="true">
      <content>KEY FIGURES IN THE McMARTIN TRIAL THE JUDGE: WILLIAM R. POUNDERS, 50 -- Superior Court judge since 1985.</content>
      <tokens>
        <token id="1" string="KEY" lemma="key" stem="key" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="FIGURES" lemma="figure" stem="figures" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="IN" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="THE" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="McMARTIN" lemma="mcmartin" stem="mcmartin" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="TRIAL" lemma="trial" stem="trial" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="THE" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="JUDGE" lemma="judge" stem="judge" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="9" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="WILLIAM" lemma="WILLIAM" stem="william" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="11" string="R." lemma="R." stem="r." pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="12" string="POUNDERS" lemma="POUNDERS" stem="pounders" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="50" lemma="50" stem="50" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="15" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="Superior" lemma="Superior" stem="superior" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="17" string="Court" lemma="Court" stem="court" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="18" string="judge" lemma="judge" stem="judg" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="since" lemma="since" stem="sinc" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="1985" lemma="1985" stem="1985" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="21" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NP (NP (JJ KEY) (NNS FIGURES)) (PP (IN IN) (NP (DT THE) (NN McMARTIN) (NN TRIAL)))) (NP (DT THE) (NN JUDGE))) (: :) (NP (NP (NNP WILLIAM) (NNP R.) (NNP POUNDERS)) (, ,) (NP (CD 50))) (: --) (NP (NP (NNP Superior) (NNP Court) (NN judge)) (PP (IN since) (NP (CD 1985)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Superior Court judge" type="NP">
          <tokens>
            <token id="16" string="Superior" />
            <token id="17" string="Court" />
            <token id="18" string="judge" />
          </tokens>
        </chunking>
        <chunking id="2" string="1985" type="NP">
          <tokens>
            <token id="20" string="1985" />
          </tokens>
        </chunking>
        <chunking id="3" string="KEY FIGURES IN THE McMARTIN TRIAL THE JUDGE : WILLIAM R. POUNDERS , 50 -- Superior Court judge since 1985 ." type="NP">
          <tokens>
            <token id="1" string="KEY" />
            <token id="2" string="FIGURES" />
            <token id="3" string="IN" />
            <token id="4" string="THE" />
            <token id="5" string="McMARTIN" />
            <token id="6" string="TRIAL" />
            <token id="7" string="THE" />
            <token id="8" string="JUDGE" />
            <token id="9" string=":" />
            <token id="10" string="WILLIAM" />
            <token id="11" string="R." />
            <token id="12" string="POUNDERS" />
            <token id="13" string="," />
            <token id="14" string="50" />
            <token id="15" string="--" />
            <token id="16" string="Superior" />
            <token id="17" string="Court" />
            <token id="18" string="judge" />
            <token id="19" string="since" />
            <token id="20" string="1985" />
            <token id="21" string="." />
          </tokens>
        </chunking>
        <chunking id="4" string="WILLIAM R. POUNDERS , 50" type="NP">
          <tokens>
            <token id="10" string="WILLIAM" />
            <token id="11" string="R." />
            <token id="12" string="POUNDERS" />
            <token id="13" string="," />
            <token id="14" string="50" />
          </tokens>
        </chunking>
        <chunking id="5" string="Superior Court judge since 1985" type="NP">
          <tokens>
            <token id="16" string="Superior" />
            <token id="17" string="Court" />
            <token id="18" string="judge" />
            <token id="19" string="since" />
            <token id="20" string="1985" />
          </tokens>
        </chunking>
        <chunking id="6" string="50" type="NP">
          <tokens>
            <token id="14" string="50" />
          </tokens>
        </chunking>
        <chunking id="7" string="KEY FIGURES IN THE McMARTIN TRIAL" type="NP">
          <tokens>
            <token id="1" string="KEY" />
            <token id="2" string="FIGURES" />
            <token id="3" string="IN" />
            <token id="4" string="THE" />
            <token id="5" string="McMARTIN" />
            <token id="6" string="TRIAL" />
          </tokens>
        </chunking>
        <chunking id="8" string="THE JUDGE" type="NP">
          <tokens>
            <token id="7" string="THE" />
            <token id="8" string="JUDGE" />
          </tokens>
        </chunking>
        <chunking id="9" string="THE McMARTIN TRIAL" type="NP">
          <tokens>
            <token id="4" string="THE" />
            <token id="5" string="McMARTIN" />
            <token id="6" string="TRIAL" />
          </tokens>
        </chunking>
        <chunking id="10" string="WILLIAM R. POUNDERS" type="NP">
          <tokens>
            <token id="10" string="WILLIAM" />
            <token id="11" string="R." />
            <token id="12" string="POUNDERS" />
          </tokens>
        </chunking>
        <chunking id="11" string="KEY FIGURES IN THE McMARTIN TRIAL THE JUDGE" type="NP">
          <tokens>
            <token id="1" string="KEY" />
            <token id="2" string="FIGURES" />
            <token id="3" string="IN" />
            <token id="4" string="THE" />
            <token id="5" string="McMARTIN" />
            <token id="6" string="TRIAL" />
            <token id="7" string="THE" />
            <token id="8" string="JUDGE" />
          </tokens>
        </chunking>
        <chunking id="12" string="KEY FIGURES" type="NP">
          <tokens>
            <token id="1" string="KEY" />
            <token id="2" string="FIGURES" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="amod">
          <governor id="2">FIGURES</governor>
          <dependent id="1">KEY</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">FIGURES</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">TRIAL</governor>
          <dependent id="3">IN</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">TRIAL</governor>
          <dependent id="4">THE</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">TRIAL</governor>
          <dependent id="5">McMARTIN</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">FIGURES</governor>
          <dependent id="6">TRIAL</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">JUDGE</governor>
          <dependent id="7">THE</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="2">FIGURES</governor>
          <dependent id="8">JUDGE</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">POUNDERS</governor>
          <dependent id="10">WILLIAM</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">POUNDERS</governor>
          <dependent id="11">R.</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="2">FIGURES</governor>
          <dependent id="12">POUNDERS</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">POUNDERS</governor>
          <dependent id="14">50</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="18">judge</governor>
          <dependent id="16">Superior</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="18">judge</governor>
          <dependent id="17">Court</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="2">FIGURES</governor>
          <dependent id="18">judge</dependent>
        </dependency>
        <dependency type="case">
          <governor id="20">1985</governor>
          <dependent id="19">since</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="18">judge</governor>
          <dependent id="20">1985</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Superior Court" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="16" string="Superior" />
            <token id="17" string="Court" />
          </tokens>
        </entity>
        <entity id="2" string="1985" type="DATE" score="0.0">
          <tokens>
            <token id="20" string="1985" />
          </tokens>
        </entity>
        <entity id="3" string="50" type="NUMBER" score="0.0">
          <tokens>
            <token id="14" string="50" />
          </tokens>
        </entity>
        <entity id="4" string="WILLIAM R. POUNDERS" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="WILLIAM" />
            <token id="11" string="R." />
            <token id="12" string="POUNDERS" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="121" has_coreference="false">
      <content>A former state prosecutor who also did appellate work, he was assigned to the case after the litigants rejected three other judges as prejudiced.</content>
      <tokens>
        <token id="1" string="A" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="former" lemma="former" stem="former" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="state" lemma="state" stem="state" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="prosecutor" lemma="prosecutor" stem="prosecutor" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="6" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="did" lemma="do" stem="did" pos="VBD" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="appellate" lemma="appellate" stem="appel" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="work" lemma="work" stem="work" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="assigned" lemma="assign" stem="assign" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="after" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="litigants" lemma="litigant" stem="litig" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="rejected" lemma="reject" stem="reject" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="three" lemma="three" stem="three" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="22" string="other" lemma="other" stem="other" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="judges" lemma="judge" stem="judg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="prejudiced" lemma="prejudiced" stem="prejud" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP-TMP (NP (DT A) (JJ former) (NN state) (NN prosecutor)) (SBAR (WHNP (WP who)) (S (ADVP (RB also)) (VP (VBD did) (NP (JJ appellate) (NN work)))))) (, ,) (NP (PRP he)) (VP (VBD was) (VP (VBN assigned) (PP (TO to) (NP (DT the) (NN case))) (SBAR (IN after) (S (NP (DT the) (NNS litigants)) (VP (VBD rejected) (NP (CD three) (JJ other) (NNS judges)) (PP (IN as) (NP (JJ prejudiced)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="appellate work" type="NP">
          <tokens>
            <token id="8" string="appellate" />
            <token id="9" string="work" />
          </tokens>
        </chunking>
        <chunking id="2" string="assigned to the case after the litigants rejected three other judges as prejudiced" type="VP">
          <tokens>
            <token id="13" string="assigned" />
            <token id="14" string="to" />
            <token id="15" string="the" />
            <token id="16" string="case" />
            <token id="17" string="after" />
            <token id="18" string="the" />
            <token id="19" string="litigants" />
            <token id="20" string="rejected" />
            <token id="21" string="three" />
            <token id="22" string="other" />
            <token id="23" string="judges" />
            <token id="24" string="as" />
            <token id="25" string="prejudiced" />
          </tokens>
        </chunking>
        <chunking id="3" string="the case" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="case" />
          </tokens>
        </chunking>
        <chunking id="4" string="who also did appellate work" type="SBAR">
          <tokens>
            <token id="5" string="who" />
            <token id="6" string="also" />
            <token id="7" string="did" />
            <token id="8" string="appellate" />
            <token id="9" string="work" />
          </tokens>
        </chunking>
        <chunking id="5" string="was assigned to the case after the litigants rejected three other judges as prejudiced" type="VP">
          <tokens>
            <token id="12" string="was" />
            <token id="13" string="assigned" />
            <token id="14" string="to" />
            <token id="15" string="the" />
            <token id="16" string="case" />
            <token id="17" string="after" />
            <token id="18" string="the" />
            <token id="19" string="litigants" />
            <token id="20" string="rejected" />
            <token id="21" string="three" />
            <token id="22" string="other" />
            <token id="23" string="judges" />
            <token id="24" string="as" />
            <token id="25" string="prejudiced" />
          </tokens>
        </chunking>
        <chunking id="6" string="after the litigants rejected three other judges as prejudiced" type="SBAR">
          <tokens>
            <token id="17" string="after" />
            <token id="18" string="the" />
            <token id="19" string="litigants" />
            <token id="20" string="rejected" />
            <token id="21" string="three" />
            <token id="22" string="other" />
            <token id="23" string="judges" />
            <token id="24" string="as" />
            <token id="25" string="prejudiced" />
          </tokens>
        </chunking>
        <chunking id="7" string="did appellate work" type="VP">
          <tokens>
            <token id="7" string="did" />
            <token id="8" string="appellate" />
            <token id="9" string="work" />
          </tokens>
        </chunking>
        <chunking id="8" string="prejudiced" type="NP">
          <tokens>
            <token id="25" string="prejudiced" />
          </tokens>
        </chunking>
        <chunking id="9" string="the litigants" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="litigants" />
          </tokens>
        </chunking>
        <chunking id="10" string="rejected three other judges as prejudiced" type="VP">
          <tokens>
            <token id="20" string="rejected" />
            <token id="21" string="three" />
            <token id="22" string="other" />
            <token id="23" string="judges" />
            <token id="24" string="as" />
            <token id="25" string="prejudiced" />
          </tokens>
        </chunking>
        <chunking id="11" string="he" type="NP">
          <tokens>
            <token id="11" string="he" />
          </tokens>
        </chunking>
        <chunking id="12" string="three other judges" type="NP">
          <tokens>
            <token id="21" string="three" />
            <token id="22" string="other" />
            <token id="23" string="judges" />
          </tokens>
        </chunking>
        <chunking id="13" string="A former state prosecutor" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="former" />
            <token id="3" string="state" />
            <token id="4" string="prosecutor" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="4">prosecutor</governor>
          <dependent id="1">A</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="4">prosecutor</governor>
          <dependent id="2">former</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">prosecutor</governor>
          <dependent id="3">state</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="13">assigned</governor>
          <dependent id="4">prosecutor</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">did</governor>
          <dependent id="5">who</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="7">did</governor>
          <dependent id="6">also</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="4">prosecutor</governor>
          <dependent id="7">did</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">work</governor>
          <dependent id="8">appellate</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">did</governor>
          <dependent id="9">work</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="13">assigned</governor>
          <dependent id="11">he</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="13">assigned</governor>
          <dependent id="12">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="13">assigned</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">case</governor>
          <dependent id="14">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="16">case</governor>
          <dependent id="15">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">assigned</governor>
          <dependent id="16">case</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="20">rejected</governor>
          <dependent id="17">after</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">litigants</governor>
          <dependent id="18">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="20">rejected</governor>
          <dependent id="19">litigants</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="13">assigned</governor>
          <dependent id="20">rejected</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="23">judges</governor>
          <dependent id="21">three</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="23">judges</governor>
          <dependent id="22">other</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="20">rejected</governor>
          <dependent id="23">judges</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">prejudiced</governor>
          <dependent id="24">as</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">rejected</governor>
          <dependent id="25">prejudiced</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="three" type="NUMBER" score="0.0">
          <tokens>
            <token id="21" string="three" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="122" has_coreference="true">
      <content>An intellectual who often uses expressions derived from his Air Force experience building nuclear missiles, Pounders&amp;apost; strengths are meticulous organization and an extensive knowledge of the law, which he augments with a 3-by-5 index card file system, as well as a new computer.</content>
      <tokens>
        <token id="1" string="An" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="intellectual" lemma="intellectual" stem="intellectu" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="4" string="often" lemma="often" stem="often" pos="RB" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="uses" lemma="use" stem="us" pos="VBZ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="expressions" lemma="expression" stem="express" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="derived" lemma="derive" stem="deriv" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="Air" lemma="Air" stem="air" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="11" string="Force" lemma="Force" stem="forc" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="12" string="experience" lemma="experience" stem="experi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="building" lemma="building" stem="build" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="nuclear" lemma="nuclear" stem="nuclear" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="missiles" lemma="missile" stem="missil" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="Pounders" lemma="Pounders" stem="pounder" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="18" string="'" lemma="'" stem="'" pos="POS" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="strengths" lemma="strength" stem="strength" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="are" lemma="be" stem="ar" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="meticulous" lemma="meticulous" stem="meticul" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="organization" lemma="organization" stem="organ" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="extensive" lemma="extensive" stem="extens" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="knowledge" lemma="knowledge" stem="knowledg" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="29" string="law" lemma="law" stem="law" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="30" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="which" lemma="which" stem="which" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="augments" lemma="augment" stem="augment" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="3-by-5" lemma="3-by-5" stem="3-by-5" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="37" string="index" lemma="index" stem="index" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="38" string="card" lemma="card" stem="card" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="file" lemma="file" stem="file" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="40" string="system" lemma="system" stem="system" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="41" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="42" string="as" lemma="as" stem="a" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="43" string="well" lemma="well" stem="well" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="44" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="45" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="46" string="new" lemma="new" stem="new" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="47" string="computer" lemma="computer" stem="comput" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="48" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (NP (DT An) (NN intellectual)) (SBAR (WHNP (WP who)) (S (ADVP (RB often)) (VP (VBZ uses) (NP (NNS expressions)))))) (VP (VBN derived) (PP (IN from) (NP (PRP$ his) (NNP Air) (NNP Force) (NN experience) (NN building) (JJ nuclear) (NNS missiles))))) (, ,) (NP (NP (NNP Pounders) (POS ')) (NNS strengths)) (VP (VBP are) (NP (NP (NP (JJ meticulous) (NN organization)) (CC and) (NP (NP (DT an) (JJ extensive) (NN knowledge)) (PP (IN of) (NP (NP (DT the) (NN law)) (, ,) (SBAR (WHNP (WDT which)) (S (NP (PRP he)) (VP (VBZ augments) (PP (IN with) (NP (DT a) (JJ 3-by-5) (NN index) (NN card) (NN file) (NN system)))))))))) (, ,) (CONJP (RB as) (RB well) (IN as)) (NP (DT a) (JJ new) (NN computer)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="An intellectual who often uses expressions" type="NP">
          <tokens>
            <token id="1" string="An" />
            <token id="2" string="intellectual" />
            <token id="3" string="who" />
            <token id="4" string="often" />
            <token id="5" string="uses" />
            <token id="6" string="expressions" />
          </tokens>
        </chunking>
        <chunking id="2" string="uses expressions" type="VP">
          <tokens>
            <token id="5" string="uses" />
            <token id="6" string="expressions" />
          </tokens>
        </chunking>
        <chunking id="3" string="derived from his Air Force experience building nuclear missiles" type="VP">
          <tokens>
            <token id="7" string="derived" />
            <token id="8" string="from" />
            <token id="9" string="his" />
            <token id="10" string="Air" />
            <token id="11" string="Force" />
            <token id="12" string="experience" />
            <token id="13" string="building" />
            <token id="14" string="nuclear" />
            <token id="15" string="missiles" />
          </tokens>
        </chunking>
        <chunking id="4" string="the law , which he augments with a 3-by-5 index card file system" type="NP">
          <tokens>
            <token id="28" string="the" />
            <token id="29" string="law" />
            <token id="30" string="," />
            <token id="31" string="which" />
            <token id="32" string="he" />
            <token id="33" string="augments" />
            <token id="34" string="with" />
            <token id="35" string="a" />
            <token id="36" string="3-by-5" />
            <token id="37" string="index" />
            <token id="38" string="card" />
            <token id="39" string="file" />
            <token id="40" string="system" />
          </tokens>
        </chunking>
        <chunking id="5" string="meticulous organization" type="NP">
          <tokens>
            <token id="21" string="meticulous" />
            <token id="22" string="organization" />
          </tokens>
        </chunking>
        <chunking id="6" string="an extensive knowledge" type="NP">
          <tokens>
            <token id="24" string="an" />
            <token id="25" string="extensive" />
            <token id="26" string="knowledge" />
          </tokens>
        </chunking>
        <chunking id="7" string="who often uses expressions" type="SBAR">
          <tokens>
            <token id="3" string="who" />
            <token id="4" string="often" />
            <token id="5" string="uses" />
            <token id="6" string="expressions" />
          </tokens>
        </chunking>
        <chunking id="8" string="expressions" type="NP">
          <tokens>
            <token id="6" string="expressions" />
          </tokens>
        </chunking>
        <chunking id="9" string="a new computer" type="NP">
          <tokens>
            <token id="45" string="a" />
            <token id="46" string="new" />
            <token id="47" string="computer" />
          </tokens>
        </chunking>
        <chunking id="10" string="augments with a 3-by-5 index card file system" type="VP">
          <tokens>
            <token id="33" string="augments" />
            <token id="34" string="with" />
            <token id="35" string="a" />
            <token id="36" string="3-by-5" />
            <token id="37" string="index" />
            <token id="38" string="card" />
            <token id="39" string="file" />
            <token id="40" string="system" />
          </tokens>
        </chunking>
        <chunking id="11" string="Pounders ' strengths" type="NP">
          <tokens>
            <token id="17" string="Pounders" />
            <token id="18" string="'" />
            <token id="19" string="strengths" />
          </tokens>
        </chunking>
        <chunking id="12" string="which he augments with a 3-by-5 index card file system" type="SBAR">
          <tokens>
            <token id="31" string="which" />
            <token id="32" string="he" />
            <token id="33" string="augments" />
            <token id="34" string="with" />
            <token id="35" string="a" />
            <token id="36" string="3-by-5" />
            <token id="37" string="index" />
            <token id="38" string="card" />
            <token id="39" string="file" />
            <token id="40" string="system" />
          </tokens>
        </chunking>
        <chunking id="13" string="meticulous organization and an extensive knowledge of the law , which he augments with a 3-by-5 index card file system , as well as a new computer" type="NP">
          <tokens>
            <token id="21" string="meticulous" />
            <token id="22" string="organization" />
            <token id="23" string="and" />
            <token id="24" string="an" />
            <token id="25" string="extensive" />
            <token id="26" string="knowledge" />
            <token id="27" string="of" />
            <token id="28" string="the" />
            <token id="29" string="law" />
            <token id="30" string="," />
            <token id="31" string="which" />
            <token id="32" string="he" />
            <token id="33" string="augments" />
            <token id="34" string="with" />
            <token id="35" string="a" />
            <token id="36" string="3-by-5" />
            <token id="37" string="index" />
            <token id="38" string="card" />
            <token id="39" string="file" />
            <token id="40" string="system" />
            <token id="41" string="," />
            <token id="42" string="as" />
            <token id="43" string="well" />
            <token id="44" string="as" />
            <token id="45" string="a" />
            <token id="46" string="new" />
            <token id="47" string="computer" />
          </tokens>
        </chunking>
        <chunking id="14" string="an extensive knowledge of the law , which he augments with a 3-by-5 index card file system" type="NP">
          <tokens>
            <token id="24" string="an" />
            <token id="25" string="extensive" />
            <token id="26" string="knowledge" />
            <token id="27" string="of" />
            <token id="28" string="the" />
            <token id="29" string="law" />
            <token id="30" string="," />
            <token id="31" string="which" />
            <token id="32" string="he" />
            <token id="33" string="augments" />
            <token id="34" string="with" />
            <token id="35" string="a" />
            <token id="36" string="3-by-5" />
            <token id="37" string="index" />
            <token id="38" string="card" />
            <token id="39" string="file" />
            <token id="40" string="system" />
          </tokens>
        </chunking>
        <chunking id="15" string="the law" type="NP">
          <tokens>
            <token id="28" string="the" />
            <token id="29" string="law" />
          </tokens>
        </chunking>
        <chunking id="16" string="a 3-by-5 index card file system" type="NP">
          <tokens>
            <token id="35" string="a" />
            <token id="36" string="3-by-5" />
            <token id="37" string="index" />
            <token id="38" string="card" />
            <token id="39" string="file" />
            <token id="40" string="system" />
          </tokens>
        </chunking>
        <chunking id="17" string="his Air Force experience building nuclear missiles" type="NP">
          <tokens>
            <token id="9" string="his" />
            <token id="10" string="Air" />
            <token id="11" string="Force" />
            <token id="12" string="experience" />
            <token id="13" string="building" />
            <token id="14" string="nuclear" />
            <token id="15" string="missiles" />
          </tokens>
        </chunking>
        <chunking id="18" string="meticulous organization and an extensive knowledge of the law , which he augments with a 3-by-5 index card file system" type="NP">
          <tokens>
            <token id="21" string="meticulous" />
            <token id="22" string="organization" />
            <token id="23" string="and" />
            <token id="24" string="an" />
            <token id="25" string="extensive" />
            <token id="26" string="knowledge" />
            <token id="27" string="of" />
            <token id="28" string="the" />
            <token id="29" string="law" />
            <token id="30" string="," />
            <token id="31" string="which" />
            <token id="32" string="he" />
            <token id="33" string="augments" />
            <token id="34" string="with" />
            <token id="35" string="a" />
            <token id="36" string="3-by-5" />
            <token id="37" string="index" />
            <token id="38" string="card" />
            <token id="39" string="file" />
            <token id="40" string="system" />
          </tokens>
        </chunking>
        <chunking id="19" string="he" type="NP">
          <tokens>
            <token id="32" string="he" />
          </tokens>
        </chunking>
        <chunking id="20" string="An intellectual" type="NP">
          <tokens>
            <token id="1" string="An" />
            <token id="2" string="intellectual" />
          </tokens>
        </chunking>
        <chunking id="21" string="are meticulous organization and an extensive knowledge of the law , which he augments with a 3-by-5 index card file system , as well as a new computer" type="VP">
          <tokens>
            <token id="20" string="are" />
            <token id="21" string="meticulous" />
            <token id="22" string="organization" />
            <token id="23" string="and" />
            <token id="24" string="an" />
            <token id="25" string="extensive" />
            <token id="26" string="knowledge" />
            <token id="27" string="of" />
            <token id="28" string="the" />
            <token id="29" string="law" />
            <token id="30" string="," />
            <token id="31" string="which" />
            <token id="32" string="he" />
            <token id="33" string="augments" />
            <token id="34" string="with" />
            <token id="35" string="a" />
            <token id="36" string="3-by-5" />
            <token id="37" string="index" />
            <token id="38" string="card" />
            <token id="39" string="file" />
            <token id="40" string="system" />
            <token id="41" string="," />
            <token id="42" string="as" />
            <token id="43" string="well" />
            <token id="44" string="as" />
            <token id="45" string="a" />
            <token id="46" string="new" />
            <token id="47" string="computer" />
          </tokens>
        </chunking>
        <chunking id="22" string="Pounders '" type="NP">
          <tokens>
            <token id="17" string="Pounders" />
            <token id="18" string="'" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">intellectual</governor>
          <dependent id="1">An</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">derived</governor>
          <dependent id="2">intellectual</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">uses</governor>
          <dependent id="3">who</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">uses</governor>
          <dependent id="4">often</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="2">intellectual</governor>
          <dependent id="5">uses</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">uses</governor>
          <dependent id="6">expressions</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="22">organization</governor>
          <dependent id="7">derived</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">missiles</governor>
          <dependent id="8">from</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="15">missiles</governor>
          <dependent id="9">his</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">missiles</governor>
          <dependent id="10">Air</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">missiles</governor>
          <dependent id="11">Force</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">missiles</governor>
          <dependent id="12">experience</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">missiles</governor>
          <dependent id="13">building</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">missiles</governor>
          <dependent id="14">nuclear</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">derived</governor>
          <dependent id="15">missiles</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="19">strengths</governor>
          <dependent id="17">Pounders</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">Pounders</governor>
          <dependent id="18">'</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">organization</governor>
          <dependent id="19">strengths</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="22">organization</governor>
          <dependent id="20">are</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="22">organization</governor>
          <dependent id="21">meticulous</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="22">organization</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="22">organization</governor>
          <dependent id="23">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="26">knowledge</governor>
          <dependent id="24">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="26">knowledge</governor>
          <dependent id="25">extensive</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="22">organization</governor>
          <dependent id="26">knowledge</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">law</governor>
          <dependent id="27">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="29">law</governor>
          <dependent id="28">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">knowledge</governor>
          <dependent id="29">law</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="33">augments</governor>
          <dependent id="31">which</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="33">augments</governor>
          <dependent id="32">he</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="29">law</governor>
          <dependent id="33">augments</dependent>
        </dependency>
        <dependency type="case">
          <governor id="40">system</governor>
          <dependent id="34">with</dependent>
        </dependency>
        <dependency type="det">
          <governor id="40">system</governor>
          <dependent id="35">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="40">system</governor>
          <dependent id="36">3-by-5</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="40">system</governor>
          <dependent id="37">index</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="40">system</governor>
          <dependent id="38">card</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="40">system</governor>
          <dependent id="39">file</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="33">augments</governor>
          <dependent id="40">system</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="22">organization</governor>
          <dependent id="42">as</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="42">as</governor>
          <dependent id="43">well</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="42">as</governor>
          <dependent id="44">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="47">computer</governor>
          <dependent id="45">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="47">computer</governor>
          <dependent id="46">new</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="22">organization</governor>
          <dependent id="47">computer</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Pounders" type="PERSON" score="0.0">
          <tokens>
            <token id="17" string="Pounders" />
          </tokens>
        </entity>
        <entity id="2" string="Air Force" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="10" string="Air" />
            <token id="11" string="Force" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="123" has_coreference="false">
      <content>A graduate of Loyola Law School, he is widely respected as a fair and thorough judge.</content>
      <tokens>
        <token id="1" string="A" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="graduate" lemma="graduate" stem="graduat" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="4" string="Loyola" lemma="Loyola" stem="loyola" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="5" string="Law" lemma="Law" stem="law" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="6" string="School" lemma="School" stem="school" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="widely" lemma="widely" stem="wide" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="respected" lemma="respect" stem="respect" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="fair" lemma="fair" stem="fair" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="thorough" lemma="thorough" stem="thorough" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="judge" lemma="judge" stem="judg" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (NP (DT A) (NN graduate)) (PP (IN of) (NP (NNP Loyola) (NNP Law) (NNP School))))) (, ,) (NP (PRP he)) (VP (VBZ is) (ADJP (RB widely) (VBN respected) (PP (IN as) (NP (DT a) (JJ fair) (CC and) (JJ thorough) (NN judge))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="widely respected as a fair and thorough judge" type="ADJP">
          <tokens>
            <token id="10" string="widely" />
            <token id="11" string="respected" />
            <token id="12" string="as" />
            <token id="13" string="a" />
            <token id="14" string="fair" />
            <token id="15" string="and" />
            <token id="16" string="thorough" />
            <token id="17" string="judge" />
          </tokens>
        </chunking>
        <chunking id="2" string="A graduate of Loyola Law School" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="graduate" />
            <token id="3" string="of" />
            <token id="4" string="Loyola" />
            <token id="5" string="Law" />
            <token id="6" string="School" />
          </tokens>
        </chunking>
        <chunking id="3" string="is widely respected as a fair and thorough judge" type="VP">
          <tokens>
            <token id="9" string="is" />
            <token id="10" string="widely" />
            <token id="11" string="respected" />
            <token id="12" string="as" />
            <token id="13" string="a" />
            <token id="14" string="fair" />
            <token id="15" string="and" />
            <token id="16" string="thorough" />
            <token id="17" string="judge" />
          </tokens>
        </chunking>
        <chunking id="4" string="A graduate" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="graduate" />
          </tokens>
        </chunking>
        <chunking id="5" string="a fair and thorough judge" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="fair" />
            <token id="15" string="and" />
            <token id="16" string="thorough" />
            <token id="17" string="judge" />
          </tokens>
        </chunking>
        <chunking id="6" string="he" type="NP">
          <tokens>
            <token id="8" string="he" />
          </tokens>
        </chunking>
        <chunking id="7" string="Loyola Law School" type="NP">
          <tokens>
            <token id="4" string="Loyola" />
            <token id="5" string="Law" />
            <token id="6" string="School" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">graduate</governor>
          <dependent id="1">A</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="11">respected</governor>
          <dependent id="2">graduate</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">School</governor>
          <dependent id="3">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">School</governor>
          <dependent id="4">Loyola</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">School</governor>
          <dependent id="5">Law</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">graduate</governor>
          <dependent id="6">School</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="11">respected</governor>
          <dependent id="8">he</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="11">respected</governor>
          <dependent id="9">is</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">respected</governor>
          <dependent id="10">widely</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="11">respected</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">judge</governor>
          <dependent id="12">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="17">judge</governor>
          <dependent id="13">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="17">judge</governor>
          <dependent id="14">fair</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="14">fair</governor>
          <dependent id="15">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">fair</governor>
          <dependent id="16">thorough</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">respected</governor>
          <dependent id="17">judge</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Loyola Law School" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="4" string="Loyola" />
            <token id="5" string="Law" />
            <token id="6" string="School" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="124" has_coreference="true">
      <content>THE DEFENDANTS: RAY BUCKEY, 31 -- Key defendant in the McMartin case, Buckey is the grandson of nursery school founder Virginia McMartin and son of co-defendant Peggy McMartin Buckey.</content>
      <tokens>
        <token id="1" string="THE" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="DEFENDANTS" lemma="defendant" stem="defendants" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="RAY" lemma="RAY" stem="ray" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="5" string="BUCKEY" lemma="BUCKEY" stem="buckey" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="31" lemma="31" stem="31" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="8" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="Key" lemma="key" stem="kei" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="defendant" lemma="defendant" stem="defend" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="14" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="15" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="17" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="grandson" lemma="grandson" stem="grandson" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="nursery" lemma="nursery" stem="nurseri" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="22" string="school" lemma="school" stem="school" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="23" string="founder" lemma="founder" stem="founder" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="24" string="Virginia" lemma="Virginia" stem="virginia" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="25" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="26" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="son" lemma="son" stem="son" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="co-defendant" lemma="co-defendant" stem="co-defend" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="30" string="Peggy" lemma="Peggy" stem="peggi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="31" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="32" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="33" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (DT THE) (NNS DEFENDANTS)) (: :) (NP (NNP RAY) (NNP BUCKEY)) (, ,) (NP (CD 31)) (: --) (NP (NP (NN Key) (NN defendant)) (PP (IN in) (NP (DT the) (NNP McMartin) (NN case)))) (, ,) (S (NP (NNP Buckey)) (VP (VBZ is) (NP (NP (NP (DT the) (NN grandson)) (PP (IN of) (NP (NN nursery) (NN school) (NN founder) (NNP Virginia) (NNP McMartin)))) (CC and) (NP (NP (NN son)) (PP (IN of) (NP (NN co-defendant) (NNP Peggy) (NNP McMartin) (NNP Buckey))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the grandson" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="grandson" />
          </tokens>
        </chunking>
        <chunking id="2" string="RAY BUCKEY" type="NP">
          <tokens>
            <token id="4" string="RAY" />
            <token id="5" string="BUCKEY" />
          </tokens>
        </chunking>
        <chunking id="3" string="son of co-defendant Peggy McMartin Buckey" type="NP">
          <tokens>
            <token id="27" string="son" />
            <token id="28" string="of" />
            <token id="29" string="co-defendant" />
            <token id="30" string="Peggy" />
            <token id="31" string="McMartin" />
            <token id="32" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="4" string="Key defendant" type="NP">
          <tokens>
            <token id="9" string="Key" />
            <token id="10" string="defendant" />
          </tokens>
        </chunking>
        <chunking id="5" string="son" type="NP">
          <tokens>
            <token id="27" string="son" />
          </tokens>
        </chunking>
        <chunking id="6" string="Key defendant in the McMartin case" type="NP">
          <tokens>
            <token id="9" string="Key" />
            <token id="10" string="defendant" />
            <token id="11" string="in" />
            <token id="12" string="the" />
            <token id="13" string="McMartin" />
            <token id="14" string="case" />
          </tokens>
        </chunking>
        <chunking id="7" string="Buckey" type="NP">
          <tokens>
            <token id="16" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="8" string="nursery school founder Virginia McMartin" type="NP">
          <tokens>
            <token id="21" string="nursery" />
            <token id="22" string="school" />
            <token id="23" string="founder" />
            <token id="24" string="Virginia" />
            <token id="25" string="McMartin" />
          </tokens>
        </chunking>
        <chunking id="9" string="THE DEFENDANTS : RAY BUCKEY , 31 -- Key defendant in the McMartin case , Buckey is the grandson of nursery school founder Virginia McMartin and son of co-defendant Peggy McMartin Buckey ." type="NP">
          <tokens>
            <token id="1" string="THE" />
            <token id="2" string="DEFENDANTS" />
            <token id="3" string=":" />
            <token id="4" string="RAY" />
            <token id="5" string="BUCKEY" />
            <token id="6" string="," />
            <token id="7" string="31" />
            <token id="8" string="--" />
            <token id="9" string="Key" />
            <token id="10" string="defendant" />
            <token id="11" string="in" />
            <token id="12" string="the" />
            <token id="13" string="McMartin" />
            <token id="14" string="case" />
            <token id="15" string="," />
            <token id="16" string="Buckey" />
            <token id="17" string="is" />
            <token id="18" string="the" />
            <token id="19" string="grandson" />
            <token id="20" string="of" />
            <token id="21" string="nursery" />
            <token id="22" string="school" />
            <token id="23" string="founder" />
            <token id="24" string="Virginia" />
            <token id="25" string="McMartin" />
            <token id="26" string="and" />
            <token id="27" string="son" />
            <token id="28" string="of" />
            <token id="29" string="co-defendant" />
            <token id="30" string="Peggy" />
            <token id="31" string="McMartin" />
            <token id="32" string="Buckey" />
            <token id="33" string="." />
          </tokens>
        </chunking>
        <chunking id="10" string="THE DEFENDANTS" type="NP">
          <tokens>
            <token id="1" string="THE" />
            <token id="2" string="DEFENDANTS" />
          </tokens>
        </chunking>
        <chunking id="11" string="the grandson of nursery school founder Virginia McMartin" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="grandson" />
            <token id="20" string="of" />
            <token id="21" string="nursery" />
            <token id="22" string="school" />
            <token id="23" string="founder" />
            <token id="24" string="Virginia" />
            <token id="25" string="McMartin" />
          </tokens>
        </chunking>
        <chunking id="12" string="is the grandson of nursery school founder Virginia McMartin and son of co-defendant Peggy McMartin Buckey" type="VP">
          <tokens>
            <token id="17" string="is" />
            <token id="18" string="the" />
            <token id="19" string="grandson" />
            <token id="20" string="of" />
            <token id="21" string="nursery" />
            <token id="22" string="school" />
            <token id="23" string="founder" />
            <token id="24" string="Virginia" />
            <token id="25" string="McMartin" />
            <token id="26" string="and" />
            <token id="27" string="son" />
            <token id="28" string="of" />
            <token id="29" string="co-defendant" />
            <token id="30" string="Peggy" />
            <token id="31" string="McMartin" />
            <token id="32" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="13" string="the McMartin case" type="NP">
          <tokens>
            <token id="12" string="the" />
            <token id="13" string="McMartin" />
            <token id="14" string="case" />
          </tokens>
        </chunking>
        <chunking id="14" string="the grandson of nursery school founder Virginia McMartin and son of co-defendant Peggy McMartin Buckey" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="grandson" />
            <token id="20" string="of" />
            <token id="21" string="nursery" />
            <token id="22" string="school" />
            <token id="23" string="founder" />
            <token id="24" string="Virginia" />
            <token id="25" string="McMartin" />
            <token id="26" string="and" />
            <token id="27" string="son" />
            <token id="28" string="of" />
            <token id="29" string="co-defendant" />
            <token id="30" string="Peggy" />
            <token id="31" string="McMartin" />
            <token id="32" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="15" string="31" type="NP">
          <tokens>
            <token id="7" string="31" />
          </tokens>
        </chunking>
        <chunking id="16" string="co-defendant Peggy McMartin Buckey" type="NP">
          <tokens>
            <token id="29" string="co-defendant" />
            <token id="30" string="Peggy" />
            <token id="31" string="McMartin" />
            <token id="32" string="Buckey" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">DEFENDANTS</governor>
          <dependent id="1">THE</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">DEFENDANTS</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">BUCKEY</governor>
          <dependent id="4">RAY</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="2">DEFENDANTS</governor>
          <dependent id="5">BUCKEY</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="2">DEFENDANTS</governor>
          <dependent id="7">31</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">defendant</governor>
          <dependent id="9">Key</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="2">DEFENDANTS</governor>
          <dependent id="10">defendant</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">case</governor>
          <dependent id="11">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">case</governor>
          <dependent id="12">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">case</governor>
          <dependent id="13">McMartin</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">defendant</governor>
          <dependent id="14">case</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">grandson</governor>
          <dependent id="16">Buckey</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="19">grandson</governor>
          <dependent id="17">is</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">grandson</governor>
          <dependent id="18">the</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="2">DEFENDANTS</governor>
          <dependent id="19">grandson</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">McMartin</governor>
          <dependent id="20">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">McMartin</governor>
          <dependent id="21">nursery</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">McMartin</governor>
          <dependent id="22">school</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">McMartin</governor>
          <dependent id="23">founder</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">McMartin</governor>
          <dependent id="24">Virginia</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">grandson</governor>
          <dependent id="25">McMartin</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="19">grandson</governor>
          <dependent id="26">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="19">grandson</governor>
          <dependent id="27">son</dependent>
        </dependency>
        <dependency type="case">
          <governor id="32">Buckey</governor>
          <dependent id="28">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="32">Buckey</governor>
          <dependent id="29">co-defendant</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="32">Buckey</governor>
          <dependent id="30">Peggy</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="32">Buckey</governor>
          <dependent id="31">McMartin</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">son</governor>
          <dependent id="32">Buckey</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="16" string="Buckey" />
          </tokens>
        </entity>
        <entity id="2" string="Peggy McMartin Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="30" string="Peggy" />
            <token id="31" string="McMartin" />
            <token id="32" string="Buckey" />
          </tokens>
        </entity>
        <entity id="3" string="RAY BUCKEY" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="RAY" />
            <token id="5" string="BUCKEY" />
          </tokens>
        </entity>
        <entity id="4" string="Virginia McMartin" type="PERSON" score="0.0">
          <tokens>
            <token id="24" string="Virginia" />
            <token id="25" string="McMartin" />
          </tokens>
        </entity>
        <entity id="5" string="31" type="NUMBER" score="0.0">
          <tokens>
            <token id="7" string="31" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="125" has_coreference="true">
      <content>A quiet man who is a college dropout and has admitted having problems with drugs and alcohol, Buckey worked briefly at a San Diego child care center before joining his family&amp;apost;s enterprise in Manhattan Beach.</content>
      <tokens>
        <token id="1" string="A" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="quiet" lemma="quiet" stem="quiet" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="man" lemma="man" stem="man" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="6" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="college" lemma="college" stem="colleg" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="dropout" lemma="dropout" stem="dropout" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="admitted" lemma="admit" stem="admit" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="having" lemma="have" stem="have" pos="VBG" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="problems" lemma="problem" stem="problem" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="drugs" lemma="drug" stem="drug" pos="NNS" type="Word" isStopWord="false" ner="CAUSE_OF_DEATH" is_referenced="true" is_refers="false" />
        <token id="16" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="17" string="alcohol" lemma="alcohol" stem="alcohol" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="18" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="20" string="worked" lemma="work" stem="work" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="briefly" lemma="briefly" stem="briefli" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="San" lemma="San" stem="san" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="25" string="Diego" lemma="Diego" stem="diego" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="26" string="child" lemma="child" stem="child" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="care" lemma="care" stem="care" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="center" lemma="center" stem="center" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="before" lemma="before" stem="befor" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="joining" lemma="join" stem="join" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="family" lemma="family" stem="famili" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="enterprise" lemma="enterprise" stem="enterpris" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="Manhattan" lemma="Manhattan" stem="manhattan" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="37" string="Beach" lemma="Beach" stem="beach" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="38" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP-TMP (NP (DT A) (JJ quiet) (NN man)) (SBAR (WHNP (WP who)) (S (VP (VP (VBZ is) (NP (DT a) (NN college) (NN dropout))) (CC and) (VP (VBZ has) (VP (VBN admitted) (S (VP (VBG having) (NP (NNS problems)) (PP (IN with) (NP (NNS drugs) (CC and) (NN alcohol))))))))))) (, ,) (NP (NNP Buckey)) (VP (VBD worked) (ADVP (NN briefly)) (PP (IN at) (NP (DT a) (NNP San) (NNP Diego) (NN child) (NN care) (NN center))) (PP (IN before) (S (VP (VBG joining) (NP (NP (NP (PRP$ his) (NN family) (POS 's)) (NN enterprise)) (PP (IN in) (NP (NNP Manhattan) (NNP Beach)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a San Diego child care center" type="NP">
          <tokens>
            <token id="23" string="a" />
            <token id="24" string="San" />
            <token id="25" string="Diego" />
            <token id="26" string="child" />
            <token id="27" string="care" />
            <token id="28" string="center" />
          </tokens>
        </chunking>
        <chunking id="2" string="his family 's enterprise in Manhattan Beach" type="NP">
          <tokens>
            <token id="31" string="his" />
            <token id="32" string="family" />
            <token id="33" string="'s" />
            <token id="34" string="enterprise" />
            <token id="35" string="in" />
            <token id="36" string="Manhattan" />
            <token id="37" string="Beach" />
          </tokens>
        </chunking>
        <chunking id="3" string="who is a college dropout and has admitted having problems with drugs and alcohol" type="SBAR">
          <tokens>
            <token id="4" string="who" />
            <token id="5" string="is" />
            <token id="6" string="a" />
            <token id="7" string="college" />
            <token id="8" string="dropout" />
            <token id="9" string="and" />
            <token id="10" string="has" />
            <token id="11" string="admitted" />
            <token id="12" string="having" />
            <token id="13" string="problems" />
            <token id="14" string="with" />
            <token id="15" string="drugs" />
            <token id="16" string="and" />
            <token id="17" string="alcohol" />
          </tokens>
        </chunking>
        <chunking id="4" string="has admitted having problems with drugs and alcohol" type="VP">
          <tokens>
            <token id="10" string="has" />
            <token id="11" string="admitted" />
            <token id="12" string="having" />
            <token id="13" string="problems" />
            <token id="14" string="with" />
            <token id="15" string="drugs" />
            <token id="16" string="and" />
            <token id="17" string="alcohol" />
          </tokens>
        </chunking>
        <chunking id="5" string="admitted having problems with drugs and alcohol" type="VP">
          <tokens>
            <token id="11" string="admitted" />
            <token id="12" string="having" />
            <token id="13" string="problems" />
            <token id="14" string="with" />
            <token id="15" string="drugs" />
            <token id="16" string="and" />
            <token id="17" string="alcohol" />
          </tokens>
        </chunking>
        <chunking id="6" string="his family 's" type="NP">
          <tokens>
            <token id="31" string="his" />
            <token id="32" string="family" />
            <token id="33" string="'s" />
          </tokens>
        </chunking>
        <chunking id="7" string="is a college dropout" type="VP">
          <tokens>
            <token id="5" string="is" />
            <token id="6" string="a" />
            <token id="7" string="college" />
            <token id="8" string="dropout" />
          </tokens>
        </chunking>
        <chunking id="8" string="Buckey" type="NP">
          <tokens>
            <token id="19" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="9" string="A quiet man" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="quiet" />
            <token id="3" string="man" />
          </tokens>
        </chunking>
        <chunking id="10" string="problems" type="NP">
          <tokens>
            <token id="13" string="problems" />
          </tokens>
        </chunking>
        <chunking id="11" string="worked briefly at a San Diego child care center before joining his family 's enterprise in Manhattan Beach" type="VP">
          <tokens>
            <token id="20" string="worked" />
            <token id="21" string="briefly" />
            <token id="22" string="at" />
            <token id="23" string="a" />
            <token id="24" string="San" />
            <token id="25" string="Diego" />
            <token id="26" string="child" />
            <token id="27" string="care" />
            <token id="28" string="center" />
            <token id="29" string="before" />
            <token id="30" string="joining" />
            <token id="31" string="his" />
            <token id="32" string="family" />
            <token id="33" string="'s" />
            <token id="34" string="enterprise" />
            <token id="35" string="in" />
            <token id="36" string="Manhattan" />
            <token id="37" string="Beach" />
          </tokens>
        </chunking>
        <chunking id="12" string="joining his family 's enterprise in Manhattan Beach" type="VP">
          <tokens>
            <token id="30" string="joining" />
            <token id="31" string="his" />
            <token id="32" string="family" />
            <token id="33" string="'s" />
            <token id="34" string="enterprise" />
            <token id="35" string="in" />
            <token id="36" string="Manhattan" />
            <token id="37" string="Beach" />
          </tokens>
        </chunking>
        <chunking id="13" string="is a college dropout and has admitted having problems with drugs and alcohol" type="VP">
          <tokens>
            <token id="5" string="is" />
            <token id="6" string="a" />
            <token id="7" string="college" />
            <token id="8" string="dropout" />
            <token id="9" string="and" />
            <token id="10" string="has" />
            <token id="11" string="admitted" />
            <token id="12" string="having" />
            <token id="13" string="problems" />
            <token id="14" string="with" />
            <token id="15" string="drugs" />
            <token id="16" string="and" />
            <token id="17" string="alcohol" />
          </tokens>
        </chunking>
        <chunking id="14" string="a college dropout" type="NP">
          <tokens>
            <token id="6" string="a" />
            <token id="7" string="college" />
            <token id="8" string="dropout" />
          </tokens>
        </chunking>
        <chunking id="15" string="having problems with drugs and alcohol" type="VP">
          <tokens>
            <token id="12" string="having" />
            <token id="13" string="problems" />
            <token id="14" string="with" />
            <token id="15" string="drugs" />
            <token id="16" string="and" />
            <token id="17" string="alcohol" />
          </tokens>
        </chunking>
        <chunking id="16" string="drugs and alcohol" type="NP">
          <tokens>
            <token id="15" string="drugs" />
            <token id="16" string="and" />
            <token id="17" string="alcohol" />
          </tokens>
        </chunking>
        <chunking id="17" string="his family 's enterprise" type="NP">
          <tokens>
            <token id="31" string="his" />
            <token id="32" string="family" />
            <token id="33" string="'s" />
            <token id="34" string="enterprise" />
          </tokens>
        </chunking>
        <chunking id="18" string="Manhattan Beach" type="NP">
          <tokens>
            <token id="36" string="Manhattan" />
            <token id="37" string="Beach" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">man</governor>
          <dependent id="1">A</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">man</governor>
          <dependent id="2">quiet</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="20">worked</governor>
          <dependent id="3">man</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">dropout</governor>
          <dependent id="4">who</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="8">dropout</governor>
          <dependent id="5">is</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">dropout</governor>
          <dependent id="6">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">dropout</governor>
          <dependent id="7">college</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="3">man</governor>
          <dependent id="8">dropout</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="8">dropout</governor>
          <dependent id="9">and</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="11">admitted</governor>
          <dependent id="10">has</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="8">dropout</governor>
          <dependent id="11">admitted</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="11">admitted</governor>
          <dependent id="12">having</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="12">having</governor>
          <dependent id="13">problems</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">drugs</governor>
          <dependent id="14">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">having</governor>
          <dependent id="15">drugs</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="15">drugs</governor>
          <dependent id="16">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="15">drugs</governor>
          <dependent id="17">alcohol</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="20">worked</governor>
          <dependent id="19">Buckey</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="20">worked</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="20">worked</governor>
          <dependent id="21">briefly</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">center</governor>
          <dependent id="22">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="28">center</governor>
          <dependent id="23">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="28">center</governor>
          <dependent id="24">San</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="28">center</governor>
          <dependent id="25">Diego</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="28">center</governor>
          <dependent id="26">child</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="28">center</governor>
          <dependent id="27">care</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">worked</governor>
          <dependent id="28">center</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="30">joining</governor>
          <dependent id="29">before</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="20">worked</governor>
          <dependent id="30">joining</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="32">family</governor>
          <dependent id="31">his</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="34">enterprise</governor>
          <dependent id="32">family</dependent>
        </dependency>
        <dependency type="case">
          <governor id="32">family</governor>
          <dependent id="33">'s</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="30">joining</governor>
          <dependent id="34">enterprise</dependent>
        </dependency>
        <dependency type="case">
          <governor id="37">Beach</governor>
          <dependent id="35">in</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="37">Beach</governor>
          <dependent id="36">Manhattan</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="34">enterprise</governor>
          <dependent id="37">Beach</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="19" string="Buckey" />
          </tokens>
        </entity>
        <entity id="2" string="drugs" type="CAUSE_OF_DEATH" score="0.0">
          <tokens>
            <token id="15" string="drugs" />
          </tokens>
        </entity>
        <entity id="3" string="San Diego" type="LOCATION" score="0.0">
          <tokens>
            <token id="24" string="San" />
            <token id="25" string="Diego" />
          </tokens>
        </entity>
        <entity id="4" string="Manhattan Beach" type="LOCATION" score="0.0">
          <tokens>
            <token id="36" string="Manhattan" />
            <token id="37" string="Beach" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="126" has_coreference="true">
      <content>He was charged on 52 counts of child molestation involving 11 children and one count of conspiracy shared with his mother.</content>
      <tokens>
        <token id="1" string="He" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="charged" lemma="charge" stem="charg" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="52" lemma="52" stem="52" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="6" string="counts" lemma="count" stem="count" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="child" lemma="child" stem="child" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="molestation" lemma="molestation" stem="molest" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="involving" lemma="involve" stem="involv" pos="VBG" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="11" lemma="11" stem="11" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="12" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="13" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="one" lemma="one" stem="on" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="15" string="count" lemma="count" stem="count" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="16" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="17" string="conspiracy" lemma="conspiracy" stem="conspiraci" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="18" string="shared" lemma="share" stem="share" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="19" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="20" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="21" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="22" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP He)) (VP (VBD was) (VP (VBN charged) (PP (IN on) (NP (NP (CD 52) (NNS counts)) (PP (IN of) (NP (NP (NN child) (NN molestation)) (PP (VBG involving) (NP (NP (CD 11) (NNS children)) (CC and) (NP (NP (CD one) (NN count)) (PP (IN of) (NP (NP (NN conspiracy)) (VP (VBN shared) (PP (IN with) (NP (PRP$ his) (NN mother))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="shared with his mother" type="VP">
          <tokens>
            <token id="18" string="shared" />
            <token id="19" string="with" />
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
        <chunking id="2" string="charged on 52 counts of child molestation involving 11 children and one count of conspiracy shared with his mother" type="VP">
          <tokens>
            <token id="3" string="charged" />
            <token id="4" string="on" />
            <token id="5" string="52" />
            <token id="6" string="counts" />
            <token id="7" string="of" />
            <token id="8" string="child" />
            <token id="9" string="molestation" />
            <token id="10" string="involving" />
            <token id="11" string="11" />
            <token id="12" string="children" />
            <token id="13" string="and" />
            <token id="14" string="one" />
            <token id="15" string="count" />
            <token id="16" string="of" />
            <token id="17" string="conspiracy" />
            <token id="18" string="shared" />
            <token id="19" string="with" />
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
        <chunking id="3" string="52 counts of child molestation involving 11 children and one count of conspiracy shared with his mother" type="NP">
          <tokens>
            <token id="5" string="52" />
            <token id="6" string="counts" />
            <token id="7" string="of" />
            <token id="8" string="child" />
            <token id="9" string="molestation" />
            <token id="10" string="involving" />
            <token id="11" string="11" />
            <token id="12" string="children" />
            <token id="13" string="and" />
            <token id="14" string="one" />
            <token id="15" string="count" />
            <token id="16" string="of" />
            <token id="17" string="conspiracy" />
            <token id="18" string="shared" />
            <token id="19" string="with" />
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
        <chunking id="4" string="52 counts" type="NP">
          <tokens>
            <token id="5" string="52" />
            <token id="6" string="counts" />
          </tokens>
        </chunking>
        <chunking id="5" string="11 children and one count of conspiracy shared with his mother" type="NP">
          <tokens>
            <token id="11" string="11" />
            <token id="12" string="children" />
            <token id="13" string="and" />
            <token id="14" string="one" />
            <token id="15" string="count" />
            <token id="16" string="of" />
            <token id="17" string="conspiracy" />
            <token id="18" string="shared" />
            <token id="19" string="with" />
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
        <chunking id="6" string="child molestation" type="NP">
          <tokens>
            <token id="8" string="child" />
            <token id="9" string="molestation" />
          </tokens>
        </chunking>
        <chunking id="7" string="one count" type="NP">
          <tokens>
            <token id="14" string="one" />
            <token id="15" string="count" />
          </tokens>
        </chunking>
        <chunking id="8" string="one count of conspiracy shared with his mother" type="NP">
          <tokens>
            <token id="14" string="one" />
            <token id="15" string="count" />
            <token id="16" string="of" />
            <token id="17" string="conspiracy" />
            <token id="18" string="shared" />
            <token id="19" string="with" />
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
        <chunking id="9" string="conspiracy" type="NP">
          <tokens>
            <token id="17" string="conspiracy" />
          </tokens>
        </chunking>
        <chunking id="10" string="11 children" type="NP">
          <tokens>
            <token id="11" string="11" />
            <token id="12" string="children" />
          </tokens>
        </chunking>
        <chunking id="11" string="was charged on 52 counts of child molestation involving 11 children and one count of conspiracy shared with his mother" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="charged" />
            <token id="4" string="on" />
            <token id="5" string="52" />
            <token id="6" string="counts" />
            <token id="7" string="of" />
            <token id="8" string="child" />
            <token id="9" string="molestation" />
            <token id="10" string="involving" />
            <token id="11" string="11" />
            <token id="12" string="children" />
            <token id="13" string="and" />
            <token id="14" string="one" />
            <token id="15" string="count" />
            <token id="16" string="of" />
            <token id="17" string="conspiracy" />
            <token id="18" string="shared" />
            <token id="19" string="with" />
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
        <chunking id="12" string="his mother" type="NP">
          <tokens>
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
        <chunking id="13" string="He" type="NP">
          <tokens>
            <token id="1" string="He" />
          </tokens>
        </chunking>
        <chunking id="14" string="conspiracy shared with his mother" type="NP">
          <tokens>
            <token id="17" string="conspiracy" />
            <token id="18" string="shared" />
            <token id="19" string="with" />
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
        <chunking id="15" string="child molestation involving 11 children and one count of conspiracy shared with his mother" type="NP">
          <tokens>
            <token id="8" string="child" />
            <token id="9" string="molestation" />
            <token id="10" string="involving" />
            <token id="11" string="11" />
            <token id="12" string="children" />
            <token id="13" string="and" />
            <token id="14" string="one" />
            <token id="15" string="count" />
            <token id="16" string="of" />
            <token id="17" string="conspiracy" />
            <token id="18" string="shared" />
            <token id="19" string="with" />
            <token id="20" string="his" />
            <token id="21" string="mother" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="3">charged</governor>
          <dependent id="1">He</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="3">charged</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">charged</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">counts</governor>
          <dependent id="4">on</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="6">counts</governor>
          <dependent id="5">52</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">charged</governor>
          <dependent id="6">counts</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">molestation</governor>
          <dependent id="7">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">molestation</governor>
          <dependent id="8">child</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">counts</governor>
          <dependent id="9">molestation</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">children</governor>
          <dependent id="10">involving</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="12">children</governor>
          <dependent id="11">11</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">molestation</governor>
          <dependent id="12">children</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="12">children</governor>
          <dependent id="13">and</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="15">count</governor>
          <dependent id="14">one</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="12">children</governor>
          <dependent id="15">count</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">conspiracy</governor>
          <dependent id="16">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">count</governor>
          <dependent id="17">conspiracy</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="17">conspiracy</governor>
          <dependent id="18">shared</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">mother</governor>
          <dependent id="19">with</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="21">mother</governor>
          <dependent id="20">his</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="18">shared</governor>
          <dependent id="21">mother</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="one" type="NUMBER" score="0.0">
          <tokens>
            <token id="14" string="one" />
          </tokens>
        </entity>
        <entity id="2" string="52" type="NUMBER" score="0.0">
          <tokens>
            <token id="5" string="52" />
          </tokens>
        </entity>
        <entity id="3" string="11" type="NUMBER" score="0.0">
          <tokens>
            <token id="11" string="11" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="127" has_coreference="true">
      <content>He denied all allegations against him.</content>
      <tokens>
        <token id="1" string="He" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="denied" lemma="deny" stem="deni" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="allegations" lemma="allegation" stem="alleg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="against" lemma="against" stem="against" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="him" lemma="he" stem="him" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP He)) (VP (VBD denied) (NP (DT all) (NNS allegations)) (PP (IN against) (NP (PRP him)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="denied all allegations against him" type="VP">
          <tokens>
            <token id="2" string="denied" />
            <token id="3" string="all" />
            <token id="4" string="allegations" />
            <token id="5" string="against" />
            <token id="6" string="him" />
          </tokens>
        </chunking>
        <chunking id="2" string="all allegations" type="NP">
          <tokens>
            <token id="3" string="all" />
            <token id="4" string="allegations" />
          </tokens>
        </chunking>
        <chunking id="3" string="him" type="NP">
          <tokens>
            <token id="6" string="him" />
          </tokens>
        </chunking>
        <chunking id="4" string="He" type="NP">
          <tokens>
            <token id="1" string="He" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">denied</governor>
          <dependent id="1">He</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">denied</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">allegations</governor>
          <dependent id="3">all</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">denied</governor>
          <dependent id="4">allegations</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">him</governor>
          <dependent id="5">against</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">denied</governor>
          <dependent id="6">him</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="128" has_coreference="true">
      <content>PEGGY MCMARTIN BUCKEY, 63 -- Co-defendant in the McMartin case, she both directed the Virginia McMartin Pre-School and taught there.</content>
      <tokens>
        <token id="1" string="PEGGY" lemma="PEGGY" stem="peggy" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="MCMARTIN" lemma="MCMARTIN" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="BUCKEY" lemma="BUCKEY" stem="buckey" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="63" lemma="63" stem="63" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="6" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="Co-defendant" lemma="co-defendant" stem="co-defend" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="11" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="both" lemma="both" stem="both" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="directed" lemma="direct" stem="direct" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="Virginia" lemma="Virginia" stem="virginia" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="18" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="19" string="Pre-School" lemma="Pre-School" stem="pre-school" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="20" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="taught" lemma="teach" stem="taught" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="there" lemma="there" stem="there" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP PEGGY) (NNP MCMARTIN) (NNP BUCKEY)) (, ,) (NP (CD 63)) (: --) (NP (NP (NN Co-defendant)) (PP (IN in) (NP (DT the) (NNP McMartin) (NN case)))) (, ,) (S (NP (PRP she)) (ADVP (DT both)) (VP (VP (VBD directed) (NP (DT the) (NNP Virginia) (NNP McMartin) (NNP Pre-School))) (CC and) (VP (VBD taught) (ADVP (RB there))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="directed the Virginia McMartin Pre-School" type="VP">
          <tokens>
            <token id="15" string="directed" />
            <token id="16" string="the" />
            <token id="17" string="Virginia" />
            <token id="18" string="McMartin" />
            <token id="19" string="Pre-School" />
          </tokens>
        </chunking>
        <chunking id="2" string="Co-defendant in the McMartin case" type="NP">
          <tokens>
            <token id="7" string="Co-defendant" />
            <token id="8" string="in" />
            <token id="9" string="the" />
            <token id="10" string="McMartin" />
            <token id="11" string="case" />
          </tokens>
        </chunking>
        <chunking id="3" string="taught there" type="VP">
          <tokens>
            <token id="21" string="taught" />
            <token id="22" string="there" />
          </tokens>
        </chunking>
        <chunking id="4" string="PEGGY MCMARTIN BUCKEY" type="NP">
          <tokens>
            <token id="1" string="PEGGY" />
            <token id="2" string="MCMARTIN" />
            <token id="3" string="BUCKEY" />
          </tokens>
        </chunking>
        <chunking id="5" string="directed the Virginia McMartin Pre-School and taught there" type="VP">
          <tokens>
            <token id="15" string="directed" />
            <token id="16" string="the" />
            <token id="17" string="Virginia" />
            <token id="18" string="McMartin" />
            <token id="19" string="Pre-School" />
            <token id="20" string="and" />
            <token id="21" string="taught" />
            <token id="22" string="there" />
          </tokens>
        </chunking>
        <chunking id="6" string="Co-defendant" type="NP">
          <tokens>
            <token id="7" string="Co-defendant" />
          </tokens>
        </chunking>
        <chunking id="7" string="the McMartin case" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="McMartin" />
            <token id="11" string="case" />
          </tokens>
        </chunking>
        <chunking id="8" string="PEGGY MCMARTIN BUCKEY , 63 -- Co-defendant in the McMartin case , she both directed the Virginia McMartin Pre-School and taught there ." type="NP">
          <tokens>
            <token id="1" string="PEGGY" />
            <token id="2" string="MCMARTIN" />
            <token id="3" string="BUCKEY" />
            <token id="4" string="," />
            <token id="5" string="63" />
            <token id="6" string="--" />
            <token id="7" string="Co-defendant" />
            <token id="8" string="in" />
            <token id="9" string="the" />
            <token id="10" string="McMartin" />
            <token id="11" string="case" />
            <token id="12" string="," />
            <token id="13" string="she" />
            <token id="14" string="both" />
            <token id="15" string="directed" />
            <token id="16" string="the" />
            <token id="17" string="Virginia" />
            <token id="18" string="McMartin" />
            <token id="19" string="Pre-School" />
            <token id="20" string="and" />
            <token id="21" string="taught" />
            <token id="22" string="there" />
            <token id="23" string="." />
          </tokens>
        </chunking>
        <chunking id="9" string="63" type="NP">
          <tokens>
            <token id="5" string="63" />
          </tokens>
        </chunking>
        <chunking id="10" string="the Virginia McMartin Pre-School" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="Virginia" />
            <token id="18" string="McMartin" />
            <token id="19" string="Pre-School" />
          </tokens>
        </chunking>
        <chunking id="11" string="she" type="NP">
          <tokens>
            <token id="13" string="she" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="3">BUCKEY</governor>
          <dependent id="1">PEGGY</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">BUCKEY</governor>
          <dependent id="2">MCMARTIN</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">BUCKEY</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">BUCKEY</governor>
          <dependent id="5">63</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="3">BUCKEY</governor>
          <dependent id="7">Co-defendant</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">case</governor>
          <dependent id="8">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">case</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">case</governor>
          <dependent id="10">McMartin</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">Co-defendant</governor>
          <dependent id="11">case</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">directed</governor>
          <dependent id="13">she</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="15">directed</governor>
          <dependent id="14">both</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="3">BUCKEY</governor>
          <dependent id="15">directed</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">Pre-School</governor>
          <dependent id="16">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">Pre-School</governor>
          <dependent id="17">Virginia</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">Pre-School</governor>
          <dependent id="18">McMartin</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">directed</governor>
          <dependent id="19">Pre-School</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="15">directed</governor>
          <dependent id="20">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="15">directed</governor>
          <dependent id="21">taught</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="21">taught</governor>
          <dependent id="22">there</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="McMartin" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="McMartin" />
          </tokens>
        </entity>
        <entity id="2" string="Virginia McMartin" type="PERSON" score="0.0">
          <tokens>
            <token id="17" string="Virginia" />
            <token id="18" string="McMartin" />
          </tokens>
        </entity>
        <entity id="3" string="PEGGY MCMARTIN BUCKEY" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="PEGGY" />
            <token id="2" string="MCMARTIN" />
            <token id="3" string="BUCKEY" />
          </tokens>
        </entity>
        <entity id="4" string="63" type="NUMBER" score="0.0">
          <tokens>
            <token id="5" string="63" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="129" has_coreference="true">
      <content>She also hired her son as a teacher.</content>
      <tokens>
        <token id="1" string="She" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="hired" lemma="hire" stem="hire" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="son" lemma="son" stem="son" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="6" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="teacher" lemma="teacher" stem="teacher" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP She)) (ADVP (RB also)) (VP (VBD hired) (NP (PRP$ her) (NN son)) (PP (IN as) (NP (DT a) (NN teacher)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="hired her son as a teacher" type="VP">
          <tokens>
            <token id="3" string="hired" />
            <token id="4" string="her" />
            <token id="5" string="son" />
            <token id="6" string="as" />
            <token id="7" string="a" />
            <token id="8" string="teacher" />
          </tokens>
        </chunking>
        <chunking id="2" string="a teacher" type="NP">
          <tokens>
            <token id="7" string="a" />
            <token id="8" string="teacher" />
          </tokens>
        </chunking>
        <chunking id="3" string="She" type="NP">
          <tokens>
            <token id="1" string="She" />
          </tokens>
        </chunking>
        <chunking id="4" string="her son" type="NP">
          <tokens>
            <token id="4" string="her" />
            <token id="5" string="son" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">hired</governor>
          <dependent id="1">She</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="3">hired</governor>
          <dependent id="2">also</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">hired</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="5">son</governor>
          <dependent id="4">her</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">hired</governor>
          <dependent id="5">son</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">teacher</governor>
          <dependent id="6">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">teacher</governor>
          <dependent id="7">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">hired</governor>
          <dependent id="8">teacher</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="130" has_coreference="true">
      <content>A talkative, religious woman who drew pictures throughout the tedious proceedings and crocheted during breaks, she was charged on 12 counts of molestation involving four children and one count of conspiracy.</content>
      <tokens>
        <token id="1" string="A" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="talkative" lemma="talkative" stem="talk" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="4" string="religious" lemma="religious" stem="religi" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="woman" lemma="woman" stem="woman" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="drew" lemma="draw" stem="drew" pos="VBD" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="pictures" lemma="picture" stem="pictur" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="throughout" lemma="throughout" stem="throughout" pos="IN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="tedious" lemma="tedious" stem="tediou" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="proceedings" lemma="proceedings" stem="proceed" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="13" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="crocheted" lemma="crochet" stem="crochet" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="during" lemma="during" stem="dure" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="16" string="breaks" lemma="break" stem="break" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="17" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="charged" lemma="charge" stem="charg" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="12" lemma="12" stem="12" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="23" string="counts" lemma="count" stem="count" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="molestation" lemma="molestation" stem="molest" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="involving" lemma="involve" stem="involv" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="four" lemma="four" stem="four" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="28" string="children" lemma="child" stem="children" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="one" lemma="one" stem="on" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="31" string="count" lemma="count" stem="count" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="conspiracy" lemma="conspiracy" stem="conspiraci" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP-TMP (NP (DT A) (JJ talkative) (, ,) (JJ religious) (NN woman)) (SBAR (WHNP (WP who)) (S (VP (VP (VBD drew) (NP (NNS pictures)) (PP (IN throughout) (NP (DT the) (JJ tedious) (NNS proceedings)))) (CC and) (VP (VBN crocheted) (PP (IN during) (NP (NNS breaks)))))))) (, ,) (NP (PRP she)) (VP (VBD was) (VP (VBN charged) (PP (IN on) (NP (NP (CD 12) (NNS counts)) (PP (IN of) (NP (NP (NN molestation)) (PP (VBG involving) (NP (NP (CD four) (NNS children)) (CC and) (NP (NP (CD one) (NN count)) (PP (IN of) (NP (NN conspiracy)))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="four children and one count of conspiracy" type="NP">
          <tokens>
            <token id="27" string="four" />
            <token id="28" string="children" />
            <token id="29" string="and" />
            <token id="30" string="one" />
            <token id="31" string="count" />
            <token id="32" string="of" />
            <token id="33" string="conspiracy" />
          </tokens>
        </chunking>
        <chunking id="2" string="the tedious proceedings" type="NP">
          <tokens>
            <token id="10" string="the" />
            <token id="11" string="tedious" />
            <token id="12" string="proceedings" />
          </tokens>
        </chunking>
        <chunking id="3" string="molestation" type="NP">
          <tokens>
            <token id="25" string="molestation" />
          </tokens>
        </chunking>
        <chunking id="4" string="breaks" type="NP">
          <tokens>
            <token id="16" string="breaks" />
          </tokens>
        </chunking>
        <chunking id="5" string="one count of conspiracy" type="NP">
          <tokens>
            <token id="30" string="one" />
            <token id="31" string="count" />
            <token id="32" string="of" />
            <token id="33" string="conspiracy" />
          </tokens>
        </chunking>
        <chunking id="6" string="who drew pictures throughout the tedious proceedings and crocheted during breaks" type="SBAR">
          <tokens>
            <token id="6" string="who" />
            <token id="7" string="drew" />
            <token id="8" string="pictures" />
            <token id="9" string="throughout" />
            <token id="10" string="the" />
            <token id="11" string="tedious" />
            <token id="12" string="proceedings" />
            <token id="13" string="and" />
            <token id="14" string="crocheted" />
            <token id="15" string="during" />
            <token id="16" string="breaks" />
          </tokens>
        </chunking>
        <chunking id="7" string="crocheted during breaks" type="VP">
          <tokens>
            <token id="14" string="crocheted" />
            <token id="15" string="during" />
            <token id="16" string="breaks" />
          </tokens>
        </chunking>
        <chunking id="8" string="molestation involving four children and one count of conspiracy" type="NP">
          <tokens>
            <token id="25" string="molestation" />
            <token id="26" string="involving" />
            <token id="27" string="four" />
            <token id="28" string="children" />
            <token id="29" string="and" />
            <token id="30" string="one" />
            <token id="31" string="count" />
            <token id="32" string="of" />
            <token id="33" string="conspiracy" />
          </tokens>
        </chunking>
        <chunking id="9" string="drew pictures throughout the tedious proceedings" type="VP">
          <tokens>
            <token id="7" string="drew" />
            <token id="8" string="pictures" />
            <token id="9" string="throughout" />
            <token id="10" string="the" />
            <token id="11" string="tedious" />
            <token id="12" string="proceedings" />
          </tokens>
        </chunking>
        <chunking id="10" string="12 counts of molestation involving four children and one count of conspiracy" type="NP">
          <tokens>
            <token id="22" string="12" />
            <token id="23" string="counts" />
            <token id="24" string="of" />
            <token id="25" string="molestation" />
            <token id="26" string="involving" />
            <token id="27" string="four" />
            <token id="28" string="children" />
            <token id="29" string="and" />
            <token id="30" string="one" />
            <token id="31" string="count" />
            <token id="32" string="of" />
            <token id="33" string="conspiracy" />
          </tokens>
        </chunking>
        <chunking id="11" string="she" type="NP">
          <tokens>
            <token id="18" string="she" />
          </tokens>
        </chunking>
        <chunking id="12" string="pictures" type="NP">
          <tokens>
            <token id="8" string="pictures" />
          </tokens>
        </chunking>
        <chunking id="13" string="charged on 12 counts of molestation involving four children and one count of conspiracy" type="VP">
          <tokens>
            <token id="20" string="charged" />
            <token id="21" string="on" />
            <token id="22" string="12" />
            <token id="23" string="counts" />
            <token id="24" string="of" />
            <token id="25" string="molestation" />
            <token id="26" string="involving" />
            <token id="27" string="four" />
            <token id="28" string="children" />
            <token id="29" string="and" />
            <token id="30" string="one" />
            <token id="31" string="count" />
            <token id="32" string="of" />
            <token id="33" string="conspiracy" />
          </tokens>
        </chunking>
        <chunking id="14" string="one count" type="NP">
          <tokens>
            <token id="30" string="one" />
            <token id="31" string="count" />
          </tokens>
        </chunking>
        <chunking id="15" string="conspiracy" type="NP">
          <tokens>
            <token id="33" string="conspiracy" />
          </tokens>
        </chunking>
        <chunking id="16" string="four children" type="NP">
          <tokens>
            <token id="27" string="four" />
            <token id="28" string="children" />
          </tokens>
        </chunking>
        <chunking id="17" string="was charged on 12 counts of molestation involving four children and one count of conspiracy" type="VP">
          <tokens>
            <token id="19" string="was" />
            <token id="20" string="charged" />
            <token id="21" string="on" />
            <token id="22" string="12" />
            <token id="23" string="counts" />
            <token id="24" string="of" />
            <token id="25" string="molestation" />
            <token id="26" string="involving" />
            <token id="27" string="four" />
            <token id="28" string="children" />
            <token id="29" string="and" />
            <token id="30" string="one" />
            <token id="31" string="count" />
            <token id="32" string="of" />
            <token id="33" string="conspiracy" />
          </tokens>
        </chunking>
        <chunking id="18" string="drew pictures throughout the tedious proceedings and crocheted during breaks" type="VP">
          <tokens>
            <token id="7" string="drew" />
            <token id="8" string="pictures" />
            <token id="9" string="throughout" />
            <token id="10" string="the" />
            <token id="11" string="tedious" />
            <token id="12" string="proceedings" />
            <token id="13" string="and" />
            <token id="14" string="crocheted" />
            <token id="15" string="during" />
            <token id="16" string="breaks" />
          </tokens>
        </chunking>
        <chunking id="19" string="12 counts" type="NP">
          <tokens>
            <token id="22" string="12" />
            <token id="23" string="counts" />
          </tokens>
        </chunking>
        <chunking id="20" string="A talkative , religious woman" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="talkative" />
            <token id="3" string="," />
            <token id="4" string="religious" />
            <token id="5" string="woman" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="5">woman</governor>
          <dependent id="1">A</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">woman</governor>
          <dependent id="2">talkative</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">woman</governor>
          <dependent id="4">religious</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="20">charged</governor>
          <dependent id="5">woman</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">drew</governor>
          <dependent id="6">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="5">woman</governor>
          <dependent id="7">drew</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">drew</governor>
          <dependent id="8">pictures</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">proceedings</governor>
          <dependent id="9">throughout</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">proceedings</governor>
          <dependent id="10">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">proceedings</governor>
          <dependent id="11">tedious</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">drew</governor>
          <dependent id="12">proceedings</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">drew</governor>
          <dependent id="13">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">drew</governor>
          <dependent id="14">crocheted</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">breaks</governor>
          <dependent id="15">during</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">crocheted</governor>
          <dependent id="16">breaks</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="20">charged</governor>
          <dependent id="18">she</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="20">charged</governor>
          <dependent id="19">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="20">charged</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">counts</governor>
          <dependent id="21">on</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="23">counts</governor>
          <dependent id="22">12</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">charged</governor>
          <dependent id="23">counts</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">molestation</governor>
          <dependent id="24">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">counts</governor>
          <dependent id="25">molestation</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">children</governor>
          <dependent id="26">involving</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="28">children</governor>
          <dependent id="27">four</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="25">molestation</governor>
          <dependent id="28">children</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="28">children</governor>
          <dependent id="29">and</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="31">count</governor>
          <dependent id="30">one</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="28">children</governor>
          <dependent id="31">count</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">conspiracy</governor>
          <dependent id="32">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="31">count</governor>
          <dependent id="33">conspiracy</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="12" type="NUMBER" score="0.0">
          <tokens>
            <token id="22" string="12" />
          </tokens>
        </entity>
        <entity id="2" string="one" type="NUMBER" score="0.0">
          <tokens>
            <token id="30" string="one" />
          </tokens>
        </entity>
        <entity id="3" string="four" type="NUMBER" score="0.0">
          <tokens>
            <token id="27" string="four" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="131" has_coreference="true">
      <content>She has maintained that neither she nor any member of her staff engaged in improper conduct.</content>
      <tokens>
        <token id="1" string="She" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="maintained" lemma="maintain" stem="maintain" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="neither" lemma="neither" stem="neither" pos="CC" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="nor" lemma="nor" stem="nor" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="any" lemma="any" stem="ani" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="member" lemma="member" stem="member" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="staff" lemma="staff" stem="staff" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="engaged" lemma="engage" stem="engag" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="improper" lemma="improper" stem="improp" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="conduct" lemma="conduct" stem="conduct" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP She)) (VP (VBZ has) (VP (VBN maintained) (SBAR (IN that) (S (NP (CC neither) (NP (PRP she)) (CC nor) (NP (NP (DT any) (NN member)) (PP (IN of) (NP (PRP$ her) (NN staff))))) (VP (VBD engaged) (PP (IN in) (NP (JJ improper) (NN conduct)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="that neither she nor any member of her staff engaged in improper conduct" type="SBAR">
          <tokens>
            <token id="4" string="that" />
            <token id="5" string="neither" />
            <token id="6" string="she" />
            <token id="7" string="nor" />
            <token id="8" string="any" />
            <token id="9" string="member" />
            <token id="10" string="of" />
            <token id="11" string="her" />
            <token id="12" string="staff" />
            <token id="13" string="engaged" />
            <token id="14" string="in" />
            <token id="15" string="improper" />
            <token id="16" string="conduct" />
          </tokens>
        </chunking>
        <chunking id="2" string="neither she nor any member of her staff" type="NP">
          <tokens>
            <token id="5" string="neither" />
            <token id="6" string="she" />
            <token id="7" string="nor" />
            <token id="8" string="any" />
            <token id="9" string="member" />
            <token id="10" string="of" />
            <token id="11" string="her" />
            <token id="12" string="staff" />
          </tokens>
        </chunking>
        <chunking id="3" string="her staff" type="NP">
          <tokens>
            <token id="11" string="her" />
            <token id="12" string="staff" />
          </tokens>
        </chunking>
        <chunking id="4" string="engaged in improper conduct" type="VP">
          <tokens>
            <token id="13" string="engaged" />
            <token id="14" string="in" />
            <token id="15" string="improper" />
            <token id="16" string="conduct" />
          </tokens>
        </chunking>
        <chunking id="5" string="has maintained that neither she nor any member of her staff engaged in improper conduct" type="VP">
          <tokens>
            <token id="2" string="has" />
            <token id="3" string="maintained" />
            <token id="4" string="that" />
            <token id="5" string="neither" />
            <token id="6" string="she" />
            <token id="7" string="nor" />
            <token id="8" string="any" />
            <token id="9" string="member" />
            <token id="10" string="of" />
            <token id="11" string="her" />
            <token id="12" string="staff" />
            <token id="13" string="engaged" />
            <token id="14" string="in" />
            <token id="15" string="improper" />
            <token id="16" string="conduct" />
          </tokens>
        </chunking>
        <chunking id="6" string="any member of her staff" type="NP">
          <tokens>
            <token id="8" string="any" />
            <token id="9" string="member" />
            <token id="10" string="of" />
            <token id="11" string="her" />
            <token id="12" string="staff" />
          </tokens>
        </chunking>
        <chunking id="7" string="improper conduct" type="NP">
          <tokens>
            <token id="15" string="improper" />
            <token id="16" string="conduct" />
          </tokens>
        </chunking>
        <chunking id="8" string="maintained that neither she nor any member of her staff engaged in improper conduct" type="VP">
          <tokens>
            <token id="3" string="maintained" />
            <token id="4" string="that" />
            <token id="5" string="neither" />
            <token id="6" string="she" />
            <token id="7" string="nor" />
            <token id="8" string="any" />
            <token id="9" string="member" />
            <token id="10" string="of" />
            <token id="11" string="her" />
            <token id="12" string="staff" />
            <token id="13" string="engaged" />
            <token id="14" string="in" />
            <token id="15" string="improper" />
            <token id="16" string="conduct" />
          </tokens>
        </chunking>
        <chunking id="9" string="She" type="NP">
          <tokens>
            <token id="1" string="She" />
          </tokens>
        </chunking>
        <chunking id="10" string="she" type="NP">
          <tokens>
            <token id="6" string="she" />
          </tokens>
        </chunking>
        <chunking id="11" string="any member" type="NP">
          <tokens>
            <token id="8" string="any" />
            <token id="9" string="member" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">maintained</governor>
          <dependent id="1">She</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="3">maintained</governor>
          <dependent id="2">has</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">maintained</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="13">engaged</governor>
          <dependent id="4">that</dependent>
        </dependency>
        <dependency type="cc:preconj">
          <governor id="6">she</governor>
          <dependent id="5">neither</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="13">engaged</governor>
          <dependent id="6">she</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="6">she</governor>
          <dependent id="7">nor</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">member</governor>
          <dependent id="8">any</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="6">she</governor>
          <dependent id="9">member</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">staff</governor>
          <dependent id="10">of</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="12">staff</governor>
          <dependent id="11">her</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">member</governor>
          <dependent id="12">staff</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">maintained</governor>
          <dependent id="13">engaged</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">conduct</governor>
          <dependent id="14">in</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="16">conduct</governor>
          <dependent id="15">improper</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">engaged</governor>
          <dependent id="16">conduct</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="132" has_coreference="true">
      <content>THE PROSECUTORS: LAEL R. RUBIN, 47 -- Co-prosecutor.</content>
      <tokens>
        <token id="1" string="THE" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="PROSECUTORS" lemma="prosecutor" stem="prosecutors" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="LAEL" lemma="LAEL" stem="lael" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="5" string="R." lemma="R." stem="r." pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="6" string="RUBIN" lemma="RUBIN" stem="rubin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="47" lemma="47" stem="47" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="9" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="Co-prosecutor" lemma="co-prosecutor" stem="co-prosecutor" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (DT THE) (NNS PROSECUTORS)) (: :) (NP (NP (NP (NNP LAEL) (NNP R.) (NNP RUBIN)) (, ,) (NP (CD 47))) (: --) (NP (NN Co-prosecutor))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="THE PROSECUTORS : LAEL R. RUBIN , 47 -- Co-prosecutor ." type="NP">
          <tokens>
            <token id="1" string="THE" />
            <token id="2" string="PROSECUTORS" />
            <token id="3" string=":" />
            <token id="4" string="LAEL" />
            <token id="5" string="R." />
            <token id="6" string="RUBIN" />
            <token id="7" string="," />
            <token id="8" string="47" />
            <token id="9" string="--" />
            <token id="10" string="Co-prosecutor" />
            <token id="11" string="." />
          </tokens>
        </chunking>
        <chunking id="2" string="47" type="NP">
          <tokens>
            <token id="8" string="47" />
          </tokens>
        </chunking>
        <chunking id="3" string="Co-prosecutor" type="NP">
          <tokens>
            <token id="10" string="Co-prosecutor" />
          </tokens>
        </chunking>
        <chunking id="4" string="LAEL R. RUBIN , 47 -- Co-prosecutor" type="NP">
          <tokens>
            <token id="4" string="LAEL" />
            <token id="5" string="R." />
            <token id="6" string="RUBIN" />
            <token id="7" string="," />
            <token id="8" string="47" />
            <token id="9" string="--" />
            <token id="10" string="Co-prosecutor" />
          </tokens>
        </chunking>
        <chunking id="5" string="LAEL R. RUBIN" type="NP">
          <tokens>
            <token id="4" string="LAEL" />
            <token id="5" string="R." />
            <token id="6" string="RUBIN" />
          </tokens>
        </chunking>
        <chunking id="6" string="THE PROSECUTORS" type="NP">
          <tokens>
            <token id="1" string="THE" />
            <token id="2" string="PROSECUTORS" />
          </tokens>
        </chunking>
        <chunking id="7" string="LAEL R. RUBIN , 47" type="NP">
          <tokens>
            <token id="4" string="LAEL" />
            <token id="5" string="R." />
            <token id="6" string="RUBIN" />
            <token id="7" string="," />
            <token id="8" string="47" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">PROSECUTORS</governor>
          <dependent id="1">THE</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">PROSECUTORS</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">RUBIN</governor>
          <dependent id="4">LAEL</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">RUBIN</governor>
          <dependent id="5">R.</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="2">PROSECUTORS</governor>
          <dependent id="6">RUBIN</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">RUBIN</governor>
          <dependent id="8">47</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="6">RUBIN</governor>
          <dependent id="10">Co-prosecutor</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="47" type="NUMBER" score="0.0">
          <tokens>
            <token id="8" string="47" />
          </tokens>
        </entity>
        <entity id="2" string="LAEL R. RUBIN" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="LAEL" />
            <token id="5" string="R." />
            <token id="6" string="RUBIN" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="133" has_coreference="true">
      <content>A former high school English teacher, Rubin was a latecomer to the legal profession, graduating from the University of West Los Angeles School of Law in 1978.</content>
      <tokens>
        <token id="1" string="A" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="former" lemma="former" stem="former" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="high" lemma="high" stem="high" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="school" lemma="school" stem="school" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="English" lemma="English" stem="english" pos="NNP" type="Word" isStopWord="false" ner="MISC" is_referenced="true" is_refers="false" />
        <token id="6" string="teacher" lemma="teacher" stem="teacher" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Rubin" lemma="Rubin" stem="rubin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="9" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="latecomer" lemma="latecomer" stem="latecom" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="12" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="legal" lemma="legal" stem="legal" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="15" string="profession" lemma="profession" stem="profess" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="graduating" lemma="graduate" stem="graduat" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="University" lemma="University" stem="univers" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="21" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="22" string="West" lemma="West" stem="west" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="23" string="Los" lemma="Los" stem="lo" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="24" string="Angeles" lemma="Angeles" stem="angele" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="25" string="School" lemma="School" stem="school" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="26" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="27" string="Law" lemma="Law" stem="law" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="28" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="1978" lemma="1978" stem="1978" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="30" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (DT A) (JJ former) (JJ high) (NN school) (NNP English) (NN teacher)) (, ,) (S (NP (NNP Rubin)) (VP (VBD was) (NP (NP (DT a) (NN latecomer)) (PP (TO to) (NP (DT the) (JJ legal) (NN profession)))))) (, ,) (VP (VBG graduating) (PP (IN from) (NP (NP (DT the) (NNP University)) (PP (IN of) (NP (NP (NNP West) (NNP Los) (NNP Angeles) (NNP School)) (PP (IN of) (NP (NNP Law))))))) (PP (IN in) (NP (CD 1978)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="A former high school English teacher , Rubin was a latecomer to the legal profession , graduating from the University of West Los Angeles School of Law in 1978 ." type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="former" />
            <token id="3" string="high" />
            <token id="4" string="school" />
            <token id="5" string="English" />
            <token id="6" string="teacher" />
            <token id="7" string="," />
            <token id="8" string="Rubin" />
            <token id="9" string="was" />
            <token id="10" string="a" />
            <token id="11" string="latecomer" />
            <token id="12" string="to" />
            <token id="13" string="the" />
            <token id="14" string="legal" />
            <token id="15" string="profession" />
            <token id="16" string="," />
            <token id="17" string="graduating" />
            <token id="18" string="from" />
            <token id="19" string="the" />
            <token id="20" string="University" />
            <token id="21" string="of" />
            <token id="22" string="West" />
            <token id="23" string="Los" />
            <token id="24" string="Angeles" />
            <token id="25" string="School" />
            <token id="26" string="of" />
            <token id="27" string="Law" />
            <token id="28" string="in" />
            <token id="29" string="1978" />
            <token id="30" string="." />
          </tokens>
        </chunking>
        <chunking id="2" string="Law" type="NP">
          <tokens>
            <token id="27" string="Law" />
          </tokens>
        </chunking>
        <chunking id="3" string="the University of West Los Angeles School of Law" type="NP">
          <tokens>
            <token id="19" string="the" />
            <token id="20" string="University" />
            <token id="21" string="of" />
            <token id="22" string="West" />
            <token id="23" string="Los" />
            <token id="24" string="Angeles" />
            <token id="25" string="School" />
            <token id="26" string="of" />
            <token id="27" string="Law" />
          </tokens>
        </chunking>
        <chunking id="4" string="West Los Angeles School" type="NP">
          <tokens>
            <token id="22" string="West" />
            <token id="23" string="Los" />
            <token id="24" string="Angeles" />
            <token id="25" string="School" />
          </tokens>
        </chunking>
        <chunking id="5" string="West Los Angeles School of Law" type="NP">
          <tokens>
            <token id="22" string="West" />
            <token id="23" string="Los" />
            <token id="24" string="Angeles" />
            <token id="25" string="School" />
            <token id="26" string="of" />
            <token id="27" string="Law" />
          </tokens>
        </chunking>
        <chunking id="6" string="the legal profession" type="NP">
          <tokens>
            <token id="13" string="the" />
            <token id="14" string="legal" />
            <token id="15" string="profession" />
          </tokens>
        </chunking>
        <chunking id="7" string="a latecomer" type="NP">
          <tokens>
            <token id="10" string="a" />
            <token id="11" string="latecomer" />
          </tokens>
        </chunking>
        <chunking id="8" string="A former high school English teacher" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="former" />
            <token id="3" string="high" />
            <token id="4" string="school" />
            <token id="5" string="English" />
            <token id="6" string="teacher" />
          </tokens>
        </chunking>
        <chunking id="9" string="a latecomer to the legal profession" type="NP">
          <tokens>
            <token id="10" string="a" />
            <token id="11" string="latecomer" />
            <token id="12" string="to" />
            <token id="13" string="the" />
            <token id="14" string="legal" />
            <token id="15" string="profession" />
          </tokens>
        </chunking>
        <chunking id="10" string="Rubin" type="NP">
          <tokens>
            <token id="8" string="Rubin" />
          </tokens>
        </chunking>
        <chunking id="11" string="was a latecomer to the legal profession" type="VP">
          <tokens>
            <token id="9" string="was" />
            <token id="10" string="a" />
            <token id="11" string="latecomer" />
            <token id="12" string="to" />
            <token id="13" string="the" />
            <token id="14" string="legal" />
            <token id="15" string="profession" />
          </tokens>
        </chunking>
        <chunking id="12" string="the University" type="NP">
          <tokens>
            <token id="19" string="the" />
            <token id="20" string="University" />
          </tokens>
        </chunking>
        <chunking id="13" string="graduating from the University of West Los Angeles School of Law in 1978" type="VP">
          <tokens>
            <token id="17" string="graduating" />
            <token id="18" string="from" />
            <token id="19" string="the" />
            <token id="20" string="University" />
            <token id="21" string="of" />
            <token id="22" string="West" />
            <token id="23" string="Los" />
            <token id="24" string="Angeles" />
            <token id="25" string="School" />
            <token id="26" string="of" />
            <token id="27" string="Law" />
            <token id="28" string="in" />
            <token id="29" string="1978" />
          </tokens>
        </chunking>
        <chunking id="14" string="1978" type="NP">
          <tokens>
            <token id="29" string="1978" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="6">teacher</governor>
          <dependent id="1">A</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">teacher</governor>
          <dependent id="2">former</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">teacher</governor>
          <dependent id="3">high</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">teacher</governor>
          <dependent id="4">school</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">teacher</governor>
          <dependent id="5">English</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">teacher</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">latecomer</governor>
          <dependent id="8">Rubin</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="11">latecomer</governor>
          <dependent id="9">was</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">latecomer</governor>
          <dependent id="10">a</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="6">teacher</governor>
          <dependent id="11">latecomer</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">profession</governor>
          <dependent id="12">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">profession</governor>
          <dependent id="13">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">profession</governor>
          <dependent id="14">legal</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">latecomer</governor>
          <dependent id="15">profession</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="6">teacher</governor>
          <dependent id="17">graduating</dependent>
        </dependency>
        <dependency type="case">
          <governor id="20">University</governor>
          <dependent id="18">from</dependent>
        </dependency>
        <dependency type="det">
          <governor id="20">University</governor>
          <dependent id="19">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">graduating</governor>
          <dependent id="20">University</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">School</governor>
          <dependent id="21">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">School</governor>
          <dependent id="22">West</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">School</governor>
          <dependent id="23">Los</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">School</governor>
          <dependent id="24">Angeles</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">University</governor>
          <dependent id="25">School</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">Law</governor>
          <dependent id="26">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="25">School</governor>
          <dependent id="27">Law</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">1978</governor>
          <dependent id="28">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">graduating</governor>
          <dependent id="29">1978</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="University of West Los Angeles School of Law" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="20" string="University" />
            <token id="21" string="of" />
            <token id="22" string="West" />
            <token id="23" string="Los" />
            <token id="24" string="Angeles" />
            <token id="25" string="School" />
            <token id="26" string="of" />
            <token id="27" string="Law" />
          </tokens>
        </entity>
        <entity id="2" string="Rubin" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Rubin" />
          </tokens>
        </entity>
        <entity id="3" string="English" type="MISC" score="0.0">
          <tokens>
            <token id="5" string="English" />
          </tokens>
        </entity>
        <entity id="4" string="1978" type="DATE" score="0.0">
          <tokens>
            <token id="29" string="1978" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="134" has_coreference="true">
      <content>She quickly distinguished herself after joining the district attorney&amp;apost;s office, successfully prosecuting &amp;quot;Black Cathy&amp;quot; Wilson for child pornography, Harry Sassounian for the assassination of the Turkish consul general in Los Angeles, and his brother, Harout Sassounian, for firebombing the Turkish consul general&amp;apost;s house.</content>
      <tokens>
        <token id="1" string="She" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="quickly" lemma="quickly" stem="quickli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="distinguished" lemma="distinguish" stem="distinguish" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="herself" lemma="herself" stem="herself" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="after" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="joining" lemma="join" stem="join" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="8" string="district" lemma="district" stem="district" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="9" string="attorney" lemma="attorney" stem="attornei" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="10" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="11" string="office" lemma="office" stem="offic" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="successfully" lemma="successfully" stem="successfulli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="prosecuting" lemma="prosecute" stem="prosecut" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="Black" lemma="black" stem="black" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="17" string="Cathy" lemma="cathy" stem="cathi" pos="NN" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="18" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="19" string="Wilson" lemma="Wilson" stem="wilson" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="20" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="21" string="child" lemma="child" stem="child" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="22" string="pornography" lemma="pornography" stem="pornographi" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="23" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="Harry" lemma="Harry" stem="harri" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="25" string="Sassounian" lemma="Sassounian" stem="sassounian" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="26" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="assassination" lemma="assassination" stem="assassin" pos="NN" type="Word" isStopWord="false" ner="CRIMINAL_CHARGE" is_referenced="false" is_refers="false" />
        <token id="29" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="31" string="Turkish" lemma="turkish" stem="turkish" pos="JJ" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="true" is_refers="false" />
        <token id="32" string="consul" lemma="consul" stem="consul" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="33" string="general" lemma="general" stem="gener" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="34" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="35" string="Los" lemma="Los" stem="lo" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="36" string="Angeles" lemma="Angeles" stem="angele" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="37" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="39" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="40" string="brother" lemma="brother" stem="brother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="41" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="42" string="Harout" lemma="Harout" stem="harout" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="43" string="Sassounian" lemma="Sassounian" stem="sassounian" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="44" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="45" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="46" string="firebombing" lemma="firebomb" stem="firebomb" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="47" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="48" string="Turkish" lemma="turkish" stem="turkish" pos="JJ" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="false" is_refers="false" />
        <token id="49" string="consul" lemma="consul" stem="consul" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="50" string="general" lemma="general" stem="gener" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="51" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="52" string="house" lemma="house" stem="hous" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="53" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP She)) (ADVP (RB quickly)) (VP (VBD distinguished) (NP (PRP herself)) (PP (IN after) (S (VP (VBG joining) (NP (NP (DT the) (NN district) (NN attorney) (POS 's)) (NN office))))) (, ,) (S (ADVP (RB successfully)) (VP (VBG prosecuting) (`` ``) (NP (NP (NP (JJ Black) (NN Cathy) ('' '') (NNP Wilson)) (PP (IN for) (NP (NN child) (NN pornography)))) (, ,) (NP (NP (NNP Harry) (NNP Sassounian)) (PP (IN for) (NP (NP (DT the) (NN assassination)) (PP (IN of) (NP (NP (DT the) (JJ Turkish) (NN consul) (NN general)) (PP (IN in) (NP (NNP Los) (NNP Angeles)))))))) (, ,) (CC and) (NP (NP (PRP$ his) (NN brother)) (, ,) (NP (NNP Harout) (NNP Sassounian)))) (, ,) (PP (IN for) (S (VP (VBG firebombing) (NP (NP (DT the) (JJ Turkish) (NN consul) (NN general) (POS 's)) (NN house)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="joining the district attorney 's office" type="VP">
          <tokens>
            <token id="6" string="joining" />
            <token id="7" string="the" />
            <token id="8" string="district" />
            <token id="9" string="attorney" />
            <token id="10" string="'s" />
            <token id="11" string="office" />
          </tokens>
        </chunking>
        <chunking id="2" string="his brother , Harout Sassounian" type="NP">
          <tokens>
            <token id="39" string="his" />
            <token id="40" string="brother" />
            <token id="41" string="," />
            <token id="42" string="Harout" />
            <token id="43" string="Sassounian" />
          </tokens>
        </chunking>
        <chunking id="3" string="firebombing the Turkish consul general 's house" type="VP">
          <tokens>
            <token id="46" string="firebombing" />
            <token id="47" string="the" />
            <token id="48" string="Turkish" />
            <token id="49" string="consul" />
            <token id="50" string="general" />
            <token id="51" string="'s" />
            <token id="52" string="house" />
          </tokens>
        </chunking>
        <chunking id="4" string="Black Cathy '' Wilson for child pornography , Harry Sassounian for the assassination of the Turkish consul general in Los Angeles , and his brother , Harout Sassounian" type="NP">
          <tokens>
            <token id="16" string="Black" />
            <token id="17" string="Cathy" />
            <token id="18" string="&quot;" />
            <token id="19" string="Wilson" />
            <token id="20" string="for" />
            <token id="21" string="child" />
            <token id="22" string="pornography" />
            <token id="23" string="," />
            <token id="24" string="Harry" />
            <token id="25" string="Sassounian" />
            <token id="26" string="for" />
            <token id="27" string="the" />
            <token id="28" string="assassination" />
            <token id="29" string="of" />
            <token id="30" string="the" />
            <token id="31" string="Turkish" />
            <token id="32" string="consul" />
            <token id="33" string="general" />
            <token id="34" string="in" />
            <token id="35" string="Los" />
            <token id="36" string="Angeles" />
            <token id="37" string="," />
            <token id="38" string="and" />
            <token id="39" string="his" />
            <token id="40" string="brother" />
            <token id="41" string="," />
            <token id="42" string="Harout" />
            <token id="43" string="Sassounian" />
          </tokens>
        </chunking>
        <chunking id="5" string="the assassination of the Turkish consul general in Los Angeles" type="NP">
          <tokens>
            <token id="27" string="the" />
            <token id="28" string="assassination" />
            <token id="29" string="of" />
            <token id="30" string="the" />
            <token id="31" string="Turkish" />
            <token id="32" string="consul" />
            <token id="33" string="general" />
            <token id="34" string="in" />
            <token id="35" string="Los" />
            <token id="36" string="Angeles" />
          </tokens>
        </chunking>
        <chunking id="6" string="Harout Sassounian" type="NP">
          <tokens>
            <token id="42" string="Harout" />
            <token id="43" string="Sassounian" />
          </tokens>
        </chunking>
        <chunking id="7" string="Harry Sassounian for the assassination of the Turkish consul general in Los Angeles" type="NP">
          <tokens>
            <token id="24" string="Harry" />
            <token id="25" string="Sassounian" />
            <token id="26" string="for" />
            <token id="27" string="the" />
            <token id="28" string="assassination" />
            <token id="29" string="of" />
            <token id="30" string="the" />
            <token id="31" string="Turkish" />
            <token id="32" string="consul" />
            <token id="33" string="general" />
            <token id="34" string="in" />
            <token id="35" string="Los" />
            <token id="36" string="Angeles" />
          </tokens>
        </chunking>
        <chunking id="8" string="Harry Sassounian" type="NP">
          <tokens>
            <token id="24" string="Harry" />
            <token id="25" string="Sassounian" />
          </tokens>
        </chunking>
        <chunking id="9" string="child pornography" type="NP">
          <tokens>
            <token id="21" string="child" />
            <token id="22" string="pornography" />
          </tokens>
        </chunking>
        <chunking id="10" string="the Turkish consul general in Los Angeles" type="NP">
          <tokens>
            <token id="30" string="the" />
            <token id="31" string="Turkish" />
            <token id="32" string="consul" />
            <token id="33" string="general" />
            <token id="34" string="in" />
            <token id="35" string="Los" />
            <token id="36" string="Angeles" />
          </tokens>
        </chunking>
        <chunking id="11" string="distinguished herself after joining the district attorney 's office , successfully prosecuting `` Black Cathy '' Wilson for child pornography , Harry Sassounian for the assassination of the Turkish consul general in Los Angeles , and his brother , Harout Sassounian , for firebombing the Turkish consul general 's house" type="VP">
          <tokens>
            <token id="3" string="distinguished" />
            <token id="4" string="herself" />
            <token id="5" string="after" />
            <token id="6" string="joining" />
            <token id="7" string="the" />
            <token id="8" string="district" />
            <token id="9" string="attorney" />
            <token id="10" string="'s" />
            <token id="11" string="office" />
            <token id="12" string="," />
            <token id="13" string="successfully" />
            <token id="14" string="prosecuting" />
            <token id="15" string="&quot;" />
            <token id="16" string="Black" />
            <token id="17" string="Cathy" />
            <token id="18" string="&quot;" />
            <token id="19" string="Wilson" />
            <token id="20" string="for" />
            <token id="21" string="child" />
            <token id="22" string="pornography" />
            <token id="23" string="," />
            <token id="24" string="Harry" />
            <token id="25" string="Sassounian" />
            <token id="26" string="for" />
            <token id="27" string="the" />
            <token id="28" string="assassination" />
            <token id="29" string="of" />
            <token id="30" string="the" />
            <token id="31" string="Turkish" />
            <token id="32" string="consul" />
            <token id="33" string="general" />
            <token id="34" string="in" />
            <token id="35" string="Los" />
            <token id="36" string="Angeles" />
            <token id="37" string="," />
            <token id="38" string="and" />
            <token id="39" string="his" />
            <token id="40" string="brother" />
            <token id="41" string="," />
            <token id="42" string="Harout" />
            <token id="43" string="Sassounian" />
            <token id="44" string="," />
            <token id="45" string="for" />
            <token id="46" string="firebombing" />
            <token id="47" string="the" />
            <token id="48" string="Turkish" />
            <token id="49" string="consul" />
            <token id="50" string="general" />
            <token id="51" string="'s" />
            <token id="52" string="house" />
          </tokens>
        </chunking>
        <chunking id="12" string="She" type="NP">
          <tokens>
            <token id="1" string="She" />
          </tokens>
        </chunking>
        <chunking id="13" string="the Turkish consul general 's house" type="NP">
          <tokens>
            <token id="47" string="the" />
            <token id="48" string="Turkish" />
            <token id="49" string="consul" />
            <token id="50" string="general" />
            <token id="51" string="'s" />
            <token id="52" string="house" />
          </tokens>
        </chunking>
        <chunking id="14" string="the district attorney 's" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="district" />
            <token id="9" string="attorney" />
            <token id="10" string="'s" />
          </tokens>
        </chunking>
        <chunking id="15" string="his brother" type="NP">
          <tokens>
            <token id="39" string="his" />
            <token id="40" string="brother" />
          </tokens>
        </chunking>
        <chunking id="16" string="the Turkish consul general" type="NP">
          <tokens>
            <token id="30" string="the" />
            <token id="31" string="Turkish" />
            <token id="32" string="consul" />
            <token id="33" string="general" />
          </tokens>
        </chunking>
        <chunking id="17" string="prosecuting `` Black Cathy '' Wilson for child pornography , Harry Sassounian for the assassination of the Turkish consul general in Los Angeles , and his brother , Harout Sassounian , for firebombing the Turkish consul general 's house" type="VP">
          <tokens>
            <token id="14" string="prosecuting" />
            <token id="15" string="&quot;" />
            <token id="16" string="Black" />
            <token id="17" string="Cathy" />
            <token id="18" string="&quot;" />
            <token id="19" string="Wilson" />
            <token id="20" string="for" />
            <token id="21" string="child" />
            <token id="22" string="pornography" />
            <token id="23" string="," />
            <token id="24" string="Harry" />
            <token id="25" string="Sassounian" />
            <token id="26" string="for" />
            <token id="27" string="the" />
            <token id="28" string="assassination" />
            <token id="29" string="of" />
            <token id="30" string="the" />
            <token id="31" string="Turkish" />
            <token id="32" string="consul" />
            <token id="33" string="general" />
            <token id="34" string="in" />
            <token id="35" string="Los" />
            <token id="36" string="Angeles" />
            <token id="37" string="," />
            <token id="38" string="and" />
            <token id="39" string="his" />
            <token id="40" string="brother" />
            <token id="41" string="," />
            <token id="42" string="Harout" />
            <token id="43" string="Sassounian" />
            <token id="44" string="," />
            <token id="45" string="for" />
            <token id="46" string="firebombing" />
            <token id="47" string="the" />
            <token id="48" string="Turkish" />
            <token id="49" string="consul" />
            <token id="50" string="general" />
            <token id="51" string="'s" />
            <token id="52" string="house" />
          </tokens>
        </chunking>
        <chunking id="18" string="Black Cathy '' Wilson for child pornography" type="NP">
          <tokens>
            <token id="16" string="Black" />
            <token id="17" string="Cathy" />
            <token id="18" string="&quot;" />
            <token id="19" string="Wilson" />
            <token id="20" string="for" />
            <token id="21" string="child" />
            <token id="22" string="pornography" />
          </tokens>
        </chunking>
        <chunking id="19" string="the Turkish consul general 's" type="NP">
          <tokens>
            <token id="47" string="the" />
            <token id="48" string="Turkish" />
            <token id="49" string="consul" />
            <token id="50" string="general" />
            <token id="51" string="'s" />
          </tokens>
        </chunking>
        <chunking id="20" string="Los Angeles" type="NP">
          <tokens>
            <token id="35" string="Los" />
            <token id="36" string="Angeles" />
          </tokens>
        </chunking>
        <chunking id="21" string="herself" type="NP">
          <tokens>
            <token id="4" string="herself" />
          </tokens>
        </chunking>
        <chunking id="22" string="the assassination" type="NP">
          <tokens>
            <token id="27" string="the" />
            <token id="28" string="assassination" />
          </tokens>
        </chunking>
        <chunking id="23" string="the district attorney 's office" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="district" />
            <token id="9" string="attorney" />
            <token id="10" string="'s" />
            <token id="11" string="office" />
          </tokens>
        </chunking>
        <chunking id="24" string="Black Cathy '' Wilson" type="NP">
          <tokens>
            <token id="16" string="Black" />
            <token id="17" string="Cathy" />
            <token id="18" string="&quot;" />
            <token id="19" string="Wilson" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">distinguished</governor>
          <dependent id="1">She</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="3">distinguished</governor>
          <dependent id="2">quickly</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">distinguished</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">distinguished</governor>
          <dependent id="4">herself</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="6">joining</governor>
          <dependent id="5">after</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="3">distinguished</governor>
          <dependent id="6">joining</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">attorney</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">attorney</governor>
          <dependent id="8">district</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="11">office</governor>
          <dependent id="9">attorney</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">attorney</governor>
          <dependent id="10">'s</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">joining</governor>
          <dependent id="11">office</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="14">prosecuting</governor>
          <dependent id="13">successfully</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="3">distinguished</governor>
          <dependent id="14">prosecuting</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">Wilson</governor>
          <dependent id="16">Black</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">Wilson</governor>
          <dependent id="17">Cathy</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="14">prosecuting</governor>
          <dependent id="19">Wilson</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">pornography</governor>
          <dependent id="20">for</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="22">pornography</governor>
          <dependent id="21">child</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">Wilson</governor>
          <dependent id="22">pornography</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">Sassounian</governor>
          <dependent id="24">Harry</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="19">Wilson</governor>
          <dependent id="25">Sassounian</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">assassination</governor>
          <dependent id="26">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="28">assassination</governor>
          <dependent id="27">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="25">Sassounian</governor>
          <dependent id="28">assassination</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">general</governor>
          <dependent id="29">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="33">general</governor>
          <dependent id="30">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="33">general</governor>
          <dependent id="31">Turkish</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="33">general</governor>
          <dependent id="32">consul</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="28">assassination</governor>
          <dependent id="33">general</dependent>
        </dependency>
        <dependency type="case">
          <governor id="36">Angeles</governor>
          <dependent id="34">in</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="36">Angeles</governor>
          <dependent id="35">Los</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="33">general</governor>
          <dependent id="36">Angeles</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="19">Wilson</governor>
          <dependent id="38">and</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="40">brother</governor>
          <dependent id="39">his</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="19">Wilson</governor>
          <dependent id="40">brother</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="43">Sassounian</governor>
          <dependent id="42">Harout</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="40">brother</governor>
          <dependent id="43">Sassounian</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="46">firebombing</governor>
          <dependent id="45">for</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="14">prosecuting</governor>
          <dependent id="46">firebombing</dependent>
        </dependency>
        <dependency type="det">
          <governor id="50">general</governor>
          <dependent id="47">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="50">general</governor>
          <dependent id="48">Turkish</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="50">general</governor>
          <dependent id="49">consul</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="52">house</governor>
          <dependent id="50">general</dependent>
        </dependency>
        <dependency type="case">
          <governor id="50">general</governor>
          <dependent id="51">'s</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="46">firebombing</governor>
          <dependent id="52">house</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Cathy &quot; Wilson" type="PERSON" score="0.0">
          <tokens>
            <token id="17" string="Cathy" />
            <token id="18" string="&quot;" />
            <token id="19" string="Wilson" />
          </tokens>
        </entity>
        <entity id="2" string="Harout Sassounian" type="PERSON" score="0.0">
          <tokens>
            <token id="42" string="Harout" />
            <token id="43" string="Sassounian" />
          </tokens>
        </entity>
        <entity id="3" string="assassination" type="CRIMINAL_CHARGE" score="0.0">
          <tokens>
            <token id="28" string="assassination" />
          </tokens>
        </entity>
        <entity id="4" string="Harry Sassounian" type="PERSON" score="0.0">
          <tokens>
            <token id="24" string="Harry" />
            <token id="25" string="Sassounian" />
          </tokens>
        </entity>
        <entity id="5" string="Turkish" type="NATIONALITY" score="0.0">
          <tokens>
            <token id="31" string="Turkish" />
          </tokens>
        </entity>
        <entity id="6" string="Los Angeles" type="LOCATION" score="0.0">
          <tokens>
            <token id="35" string="Los" />
            <token id="36" string="Angeles" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="135" has_coreference="true">
      <content>She is the only original prosecutor remaining on the McMartin case.</content>
      <tokens>
        <token id="1" string="She" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="only" lemma="only" stem="onli" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="original" lemma="original" stem="origin" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="prosecutor" lemma="prosecutor" stem="prosecutor" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="remaining" lemma="remain" stem="remain" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="11" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP She)) (VP (VBZ is) (NP (NP (DT the) (JJ only) (JJ original) (NN prosecutor)) (VP (VBG remaining) (PP (IN on) (NP (DT the) (NNP McMartin) (NN case)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="remaining on the McMartin case" type="VP">
          <tokens>
            <token id="7" string="remaining" />
            <token id="8" string="on" />
            <token id="9" string="the" />
            <token id="10" string="McMartin" />
            <token id="11" string="case" />
          </tokens>
        </chunking>
        <chunking id="2" string="the only original prosecutor remaining on the McMartin case" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="only" />
            <token id="5" string="original" />
            <token id="6" string="prosecutor" />
            <token id="7" string="remaining" />
            <token id="8" string="on" />
            <token id="9" string="the" />
            <token id="10" string="McMartin" />
            <token id="11" string="case" />
          </tokens>
        </chunking>
        <chunking id="3" string="is the only original prosecutor remaining on the McMartin case" type="VP">
          <tokens>
            <token id="2" string="is" />
            <token id="3" string="the" />
            <token id="4" string="only" />
            <token id="5" string="original" />
            <token id="6" string="prosecutor" />
            <token id="7" string="remaining" />
            <token id="8" string="on" />
            <token id="9" string="the" />
            <token id="10" string="McMartin" />
            <token id="11" string="case" />
          </tokens>
        </chunking>
        <chunking id="4" string="the only original prosecutor" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="only" />
            <token id="5" string="original" />
            <token id="6" string="prosecutor" />
          </tokens>
        </chunking>
        <chunking id="5" string="the McMartin case" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="McMartin" />
            <token id="11" string="case" />
          </tokens>
        </chunking>
        <chunking id="6" string="She" type="NP">
          <tokens>
            <token id="1" string="She" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="6">prosecutor</governor>
          <dependent id="1">She</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="6">prosecutor</governor>
          <dependent id="2">is</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">prosecutor</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">prosecutor</governor>
          <dependent id="4">only</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">prosecutor</governor>
          <dependent id="5">original</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">prosecutor</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="6">prosecutor</governor>
          <dependent id="7">remaining</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">case</governor>
          <dependent id="8">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">case</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">case</governor>
          <dependent id="10">McMartin</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">remaining</governor>
          <dependent id="11">case</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="McMartin" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="McMartin" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="136" has_coreference="false">
      <content>ROGER J. GUNSON, 50 -- Co-prosecutor.</content>
      <tokens>
        <token id="1" string="ROGER" lemma="ROGER" stem="roger" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="2" string="J." lemma="J." stem="j." pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="3" string="GUNSON" lemma="GUNSON" stem="gunson" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="50" lemma="50" stem="50" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="6" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="Co-prosecutor" lemma="co-prosecutor" stem="co-prosecutor" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NP (NNP ROGER) (NNP J.) (NNP GUNSON)) (, ,) (NP (CD 50))) (: --) (NP (NN Co-prosecutor)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Co-prosecutor" type="NP">
          <tokens>
            <token id="7" string="Co-prosecutor" />
          </tokens>
        </chunking>
        <chunking id="2" string="ROGER J. GUNSON" type="NP">
          <tokens>
            <token id="1" string="ROGER" />
            <token id="2" string="J." />
            <token id="3" string="GUNSON" />
          </tokens>
        </chunking>
        <chunking id="3" string="ROGER J. GUNSON , 50 -- Co-prosecutor ." type="NP">
          <tokens>
            <token id="1" string="ROGER" />
            <token id="2" string="J." />
            <token id="3" string="GUNSON" />
            <token id="4" string="," />
            <token id="5" string="50" />
            <token id="6" string="--" />
            <token id="7" string="Co-prosecutor" />
            <token id="8" string="." />
          </tokens>
        </chunking>
        <chunking id="4" string="50" type="NP">
          <tokens>
            <token id="5" string="50" />
          </tokens>
        </chunking>
        <chunking id="5" string="ROGER J. GUNSON , 50" type="NP">
          <tokens>
            <token id="1" string="ROGER" />
            <token id="2" string="J." />
            <token id="3" string="GUNSON" />
            <token id="4" string="," />
            <token id="5" string="50" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="3">GUNSON</governor>
          <dependent id="1">ROGER</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">GUNSON</governor>
          <dependent id="2">J.</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">GUNSON</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">GUNSON</governor>
          <dependent id="5">50</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="3">GUNSON</governor>
          <dependent id="7">Co-prosecutor</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="ROGER J. GUNSON" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="ROGER" />
            <token id="2" string="J." />
            <token id="3" string="GUNSON" />
          </tokens>
        </entity>
        <entity id="2" string="50" type="NUMBER" score="0.0">
          <tokens>
            <token id="5" string="50" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="137" has_coreference="true">
      <content>A 20-year veteran of the district attorney&amp;apost;s office and a graduate of UCLA Law School, the soft-spoken, unassuming Gunson most recently served as head deputy in the Sex Crimes and Child Abuse Division.</content>
      <tokens>
        <token id="1" string="A" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="20-year" lemma="20-year" stem="20-year" pos="JJ" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="3" string="veteran" lemma="veteran" stem="veteran" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="6" string="district" lemma="district" stem="district" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="7" string="attorney" lemma="attorney" stem="attornei" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="8" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="9" string="office" lemma="office" stem="offic" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="12" string="graduate" lemma="graduate" stem="graduat" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="13" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="UCLA" lemma="UCLA" stem="ucla" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="15" string="Law" lemma="Law" stem="law" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="16" string="School" lemma="School" stem="school" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="17" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="18" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="19" string="soft-spoken" lemma="soft-spoken" stem="soft-spoken" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="21" string="unassuming" lemma="unassuming" stem="unassum" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="22" string="Gunson" lemma="Gunson" stem="gunson" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="23" string="most" lemma="most" stem="most" pos="RBS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="recently" lemma="recently" stem="recent" pos="RB" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="25" string="served" lemma="serve" stem="serv" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="head" lemma="head" stem="head" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="deputy" lemma="deputy" stem="deputi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="Sex" lemma="sex" stem="sex" pos="NN" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="32" string="Crimes" lemma="crime" stem="crime" pos="NNS" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="33" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="34" string="Child" lemma="child" stem="child" pos="NN" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="35" string="Abuse" lemma="abuse" stem="abuse" pos="NN" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="36" string="Division" lemma="division" stem="divis" pos="NN" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="37" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT A) (JJ 20-year) (NN veteran)) (PP (IN of) (NP (NP (NP (DT the) (NN district) (NN attorney) (POS 's)) (NN office)) (CC and) (NP (NP (DT a) (NN graduate)) (PP (IN of) (NP (NP (NNP UCLA) (NNP Law) (NNP School)) (, ,) (NP (DT the) (JJ soft-spoken) (, ,) (JJ unassuming) (NNP Gunson)))))))) (ADVP (RBS most) (RB recently)) (VP (VBD served) (PP (IN as) (NP (NN head) (NN deputy))) (PP (IN in) (NP (DT the) (NN Sex) (NNS Crimes) (CC and) (NN Child) (NN Abuse) (NN Division)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="A 20-year veteran of the district attorney 's office and a graduate of UCLA Law School , the soft-spoken , unassuming Gunson" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="20-year" />
            <token id="3" string="veteran" />
            <token id="4" string="of" />
            <token id="5" string="the" />
            <token id="6" string="district" />
            <token id="7" string="attorney" />
            <token id="8" string="'s" />
            <token id="9" string="office" />
            <token id="10" string="and" />
            <token id="11" string="a" />
            <token id="12" string="graduate" />
            <token id="13" string="of" />
            <token id="14" string="UCLA" />
            <token id="15" string="Law" />
            <token id="16" string="School" />
            <token id="17" string="," />
            <token id="18" string="the" />
            <token id="19" string="soft-spoken" />
            <token id="20" string="," />
            <token id="21" string="unassuming" />
            <token id="22" string="Gunson" />
          </tokens>
        </chunking>
        <chunking id="2" string="the district attorney 's office and a graduate of UCLA Law School , the soft-spoken , unassuming Gunson" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="district" />
            <token id="7" string="attorney" />
            <token id="8" string="'s" />
            <token id="9" string="office" />
            <token id="10" string="and" />
            <token id="11" string="a" />
            <token id="12" string="graduate" />
            <token id="13" string="of" />
            <token id="14" string="UCLA" />
            <token id="15" string="Law" />
            <token id="16" string="School" />
            <token id="17" string="," />
            <token id="18" string="the" />
            <token id="19" string="soft-spoken" />
            <token id="20" string="," />
            <token id="21" string="unassuming" />
            <token id="22" string="Gunson" />
          </tokens>
        </chunking>
        <chunking id="3" string="the soft-spoken , unassuming Gunson" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="soft-spoken" />
            <token id="20" string="," />
            <token id="21" string="unassuming" />
            <token id="22" string="Gunson" />
          </tokens>
        </chunking>
        <chunking id="4" string="A 20-year veteran" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="20-year" />
            <token id="3" string="veteran" />
          </tokens>
        </chunking>
        <chunking id="5" string="served as head deputy in the Sex Crimes and Child Abuse Division" type="VP">
          <tokens>
            <token id="25" string="served" />
            <token id="26" string="as" />
            <token id="27" string="head" />
            <token id="28" string="deputy" />
            <token id="29" string="in" />
            <token id="30" string="the" />
            <token id="31" string="Sex" />
            <token id="32" string="Crimes" />
            <token id="33" string="and" />
            <token id="34" string="Child" />
            <token id="35" string="Abuse" />
            <token id="36" string="Division" />
          </tokens>
        </chunking>
        <chunking id="6" string="a graduate of UCLA Law School , the soft-spoken , unassuming Gunson" type="NP">
          <tokens>
            <token id="11" string="a" />
            <token id="12" string="graduate" />
            <token id="13" string="of" />
            <token id="14" string="UCLA" />
            <token id="15" string="Law" />
            <token id="16" string="School" />
            <token id="17" string="," />
            <token id="18" string="the" />
            <token id="19" string="soft-spoken" />
            <token id="20" string="," />
            <token id="21" string="unassuming" />
            <token id="22" string="Gunson" />
          </tokens>
        </chunking>
        <chunking id="7" string="the district attorney 's" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="district" />
            <token id="7" string="attorney" />
            <token id="8" string="'s" />
          </tokens>
        </chunking>
        <chunking id="8" string="a graduate" type="NP">
          <tokens>
            <token id="11" string="a" />
            <token id="12" string="graduate" />
          </tokens>
        </chunking>
        <chunking id="9" string="the Sex Crimes and Child Abuse Division" type="NP">
          <tokens>
            <token id="30" string="the" />
            <token id="31" string="Sex" />
            <token id="32" string="Crimes" />
            <token id="33" string="and" />
            <token id="34" string="Child" />
            <token id="35" string="Abuse" />
            <token id="36" string="Division" />
          </tokens>
        </chunking>
        <chunking id="10" string="UCLA Law School" type="NP">
          <tokens>
            <token id="14" string="UCLA" />
            <token id="15" string="Law" />
            <token id="16" string="School" />
          </tokens>
        </chunking>
        <chunking id="11" string="the district attorney 's office" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="district" />
            <token id="7" string="attorney" />
            <token id="8" string="'s" />
            <token id="9" string="office" />
          </tokens>
        </chunking>
        <chunking id="12" string="UCLA Law School , the soft-spoken , unassuming Gunson" type="NP">
          <tokens>
            <token id="14" string="UCLA" />
            <token id="15" string="Law" />
            <token id="16" string="School" />
            <token id="17" string="," />
            <token id="18" string="the" />
            <token id="19" string="soft-spoken" />
            <token id="20" string="," />
            <token id="21" string="unassuming" />
            <token id="22" string="Gunson" />
          </tokens>
        </chunking>
        <chunking id="13" string="head deputy" type="NP">
          <tokens>
            <token id="27" string="head" />
            <token id="28" string="deputy" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">veteran</governor>
          <dependent id="1">A</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">veteran</governor>
          <dependent id="2">20-year</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="25">served</governor>
          <dependent id="3">veteran</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">office</governor>
          <dependent id="4">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">attorney</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">attorney</governor>
          <dependent id="6">district</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="9">office</governor>
          <dependent id="7">attorney</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">attorney</governor>
          <dependent id="8">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">veteran</governor>
          <dependent id="9">office</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="9">office</governor>
          <dependent id="10">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">graduate</governor>
          <dependent id="11">a</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="9">office</governor>
          <dependent id="12">graduate</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">School</governor>
          <dependent id="13">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="16">School</governor>
          <dependent id="14">UCLA</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="16">School</governor>
          <dependent id="15">Law</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">graduate</governor>
          <dependent id="16">School</dependent>
        </dependency>
        <dependency type="det">
          <governor id="22">Gunson</governor>
          <dependent id="18">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="22">Gunson</governor>
          <dependent id="19">soft-spoken</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="22">Gunson</governor>
          <dependent id="21">unassuming</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="16">School</governor>
          <dependent id="22">Gunson</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="24">recently</governor>
          <dependent id="23">most</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="25">served</governor>
          <dependent id="24">recently</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="25">served</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">deputy</governor>
          <dependent id="26">as</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="28">deputy</governor>
          <dependent id="27">head</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="25">served</governor>
          <dependent id="28">deputy</dependent>
        </dependency>
        <dependency type="case">
          <governor id="32">Crimes</governor>
          <dependent id="29">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="32">Crimes</governor>
          <dependent id="30">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="32">Crimes</governor>
          <dependent id="31">Sex</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="25">served</governor>
          <dependent id="32">Crimes</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="32">Crimes</governor>
          <dependent id="33">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="36">Division</governor>
          <dependent id="34">Child</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="36">Division</governor>
          <dependent id="35">Abuse</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="32">Crimes</governor>
          <dependent id="36">Division</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="20-year" type="DURATION" score="0.0">
          <tokens>
            <token id="2" string="20-year" />
          </tokens>
        </entity>
        <entity id="2" string="recently" type="DATE" score="0.0">
          <tokens>
            <token id="24" string="recently" />
          </tokens>
        </entity>
        <entity id="3" string="Sex Crimes and Child Abuse Division" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="31" string="Sex" />
            <token id="32" string="Crimes" />
            <token id="33" string="and" />
            <token id="34" string="Child" />
            <token id="35" string="Abuse" />
            <token id="36" string="Division" />
          </tokens>
        </entity>
        <entity id="4" string="UCLA Law School" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="14" string="UCLA" />
            <token id="15" string="Law" />
            <token id="16" string="School" />
          </tokens>
        </entity>
        <entity id="5" string="Gunson" type="PERSON" score="0.0">
          <tokens>
            <token id="22" string="Gunson" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="138" has_coreference="true">
      <content>He successfully prosecuted film director Roman Polanski for unlawful sex with a minor, and Susan Brophy, wife of former state assemblyman Bill Brophy, for vehicular manslaughter.</content>
      <tokens>
        <token id="1" string="He" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="successfully" lemma="successfully" stem="successfulli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="prosecuted" lemma="prosecute" stem="prosecut" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="film" lemma="film" stem="film" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="director" lemma="director" stem="director" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="Roman" lemma="Roman" stem="roman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="7" string="Polanski" lemma="Polanski" stem="polanski" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="8" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="unlawful" lemma="unlawful" stem="unlaw" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="sex" lemma="sex" stem="sex" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="minor" lemma="minor" stem="minor" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="Susan" lemma="Susan" stem="susan" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="17" string="Brophy" lemma="Brophy" stem="brophi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="18" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="wife" lemma="wife" stem="wife" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="former" lemma="former" stem="former" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="state" lemma="state" stem="state" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="assemblyman" lemma="assemblyman" stem="assemblyman" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="Bill" lemma="Bill" stem="bill" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="25" string="Brophy" lemma="Brophy" stem="brophi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="26" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="vehicular" lemma="vehicular" stem="vehicular" pos="JJ" type="Word" isStopWord="false" ner="CRIMINAL_CHARGE" is_referenced="false" is_refers="false" />
        <token id="29" string="manslaughter" lemma="manslaughter" stem="manslaught" pos="NN" type="Word" isStopWord="false" ner="CRIMINAL_CHARGE" is_referenced="false" is_refers="false" />
        <token id="30" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP He)) (ADVP (RB successfully)) (VP (VBD prosecuted) (NP (NP (NN film) (NN director) (NNP Roman) (NNP Polanski)) (PP (IN for) (NP (JJ unlawful) (NN sex)))) (PP (IN with) (NP (NP (DT a) (JJ minor) (, ,) (CC and) (NNP Susan) (NNP Brophy)) (, ,) (NP (NP (NN wife)) (PP (IN of) (NP (JJ former) (NN state) (NN assemblyman) (NNP Bill) (NNP Brophy)))) (, ,))) (PP (IN for) (NP (JJ vehicular) (NN manslaughter)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="film director Roman Polanski for unlawful sex" type="NP">
          <tokens>
            <token id="4" string="film" />
            <token id="5" string="director" />
            <token id="6" string="Roman" />
            <token id="7" string="Polanski" />
            <token id="8" string="for" />
            <token id="9" string="unlawful" />
            <token id="10" string="sex" />
          </tokens>
        </chunking>
        <chunking id="2" string="prosecuted film director Roman Polanski for unlawful sex with a minor , and Susan Brophy , wife of former state assemblyman Bill Brophy , for vehicular manslaughter" type="VP">
          <tokens>
            <token id="3" string="prosecuted" />
            <token id="4" string="film" />
            <token id="5" string="director" />
            <token id="6" string="Roman" />
            <token id="7" string="Polanski" />
            <token id="8" string="for" />
            <token id="9" string="unlawful" />
            <token id="10" string="sex" />
            <token id="11" string="with" />
            <token id="12" string="a" />
            <token id="13" string="minor" />
            <token id="14" string="," />
            <token id="15" string="and" />
            <token id="16" string="Susan" />
            <token id="17" string="Brophy" />
            <token id="18" string="," />
            <token id="19" string="wife" />
            <token id="20" string="of" />
            <token id="21" string="former" />
            <token id="22" string="state" />
            <token id="23" string="assemblyman" />
            <token id="24" string="Bill" />
            <token id="25" string="Brophy" />
            <token id="26" string="," />
            <token id="27" string="for" />
            <token id="28" string="vehicular" />
            <token id="29" string="manslaughter" />
          </tokens>
        </chunking>
        <chunking id="3" string="a minor , and Susan Brophy , wife of former state assemblyman Bill Brophy ," type="NP">
          <tokens>
            <token id="12" string="a" />
            <token id="13" string="minor" />
            <token id="14" string="," />
            <token id="15" string="and" />
            <token id="16" string="Susan" />
            <token id="17" string="Brophy" />
            <token id="18" string="," />
            <token id="19" string="wife" />
            <token id="20" string="of" />
            <token id="21" string="former" />
            <token id="22" string="state" />
            <token id="23" string="assemblyman" />
            <token id="24" string="Bill" />
            <token id="25" string="Brophy" />
            <token id="26" string="," />
          </tokens>
        </chunking>
        <chunking id="4" string="unlawful sex" type="NP">
          <tokens>
            <token id="9" string="unlawful" />
            <token id="10" string="sex" />
          </tokens>
        </chunking>
        <chunking id="5" string="wife" type="NP">
          <tokens>
            <token id="19" string="wife" />
          </tokens>
        </chunking>
        <chunking id="6" string="former state assemblyman Bill Brophy" type="NP">
          <tokens>
            <token id="21" string="former" />
            <token id="22" string="state" />
            <token id="23" string="assemblyman" />
            <token id="24" string="Bill" />
            <token id="25" string="Brophy" />
          </tokens>
        </chunking>
        <chunking id="7" string="He" type="NP">
          <tokens>
            <token id="1" string="He" />
          </tokens>
        </chunking>
        <chunking id="8" string="a minor , and Susan Brophy" type="NP">
          <tokens>
            <token id="12" string="a" />
            <token id="13" string="minor" />
            <token id="14" string="," />
            <token id="15" string="and" />
            <token id="16" string="Susan" />
            <token id="17" string="Brophy" />
          </tokens>
        </chunking>
        <chunking id="9" string="film director Roman Polanski" type="NP">
          <tokens>
            <token id="4" string="film" />
            <token id="5" string="director" />
            <token id="6" string="Roman" />
            <token id="7" string="Polanski" />
          </tokens>
        </chunking>
        <chunking id="10" string="wife of former state assemblyman Bill Brophy" type="NP">
          <tokens>
            <token id="19" string="wife" />
            <token id="20" string="of" />
            <token id="21" string="former" />
            <token id="22" string="state" />
            <token id="23" string="assemblyman" />
            <token id="24" string="Bill" />
            <token id="25" string="Brophy" />
          </tokens>
        </chunking>
        <chunking id="11" string="vehicular manslaughter" type="NP">
          <tokens>
            <token id="28" string="vehicular" />
            <token id="29" string="manslaughter" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">prosecuted</governor>
          <dependent id="1">He</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="3">prosecuted</governor>
          <dependent id="2">successfully</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">prosecuted</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">Polanski</governor>
          <dependent id="4">film</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">Polanski</governor>
          <dependent id="5">director</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">Polanski</governor>
          <dependent id="6">Roman</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">prosecuted</governor>
          <dependent id="7">Polanski</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">sex</governor>
          <dependent id="8">for</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="10">sex</governor>
          <dependent id="9">unlawful</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">Polanski</governor>
          <dependent id="10">sex</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">minor</governor>
          <dependent id="11">with</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">minor</governor>
          <dependent id="12">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">prosecuted</governor>
          <dependent id="13">minor</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="13">minor</governor>
          <dependent id="15">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="17">Brophy</governor>
          <dependent id="16">Susan</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="13">minor</governor>
          <dependent id="17">Brophy</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="13">minor</governor>
          <dependent id="19">wife</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">Brophy</governor>
          <dependent id="20">of</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="25">Brophy</governor>
          <dependent id="21">former</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">Brophy</governor>
          <dependent id="22">state</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">Brophy</governor>
          <dependent id="23">assemblyman</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">Brophy</governor>
          <dependent id="24">Bill</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">wife</governor>
          <dependent id="25">Brophy</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">manslaughter</governor>
          <dependent id="27">for</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="29">manslaughter</governor>
          <dependent id="28">vehicular</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">prosecuted</governor>
          <dependent id="29">manslaughter</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Roman Polanski" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Roman" />
            <token id="7" string="Polanski" />
          </tokens>
        </entity>
        <entity id="2" string="Susan Brophy" type="PERSON" score="0.0">
          <tokens>
            <token id="16" string="Susan" />
            <token id="17" string="Brophy" />
          </tokens>
        </entity>
        <entity id="3" string="Bill Brophy" type="PERSON" score="0.0">
          <tokens>
            <token id="24" string="Bill" />
            <token id="25" string="Brophy" />
          </tokens>
        </entity>
        <entity id="4" string="vehicular manslaughter" type="CRIMINAL_CHARGE" score="0.0">
          <tokens>
            <token id="28" string="vehicular" />
            <token id="29" string="manslaughter" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="139" has_coreference="true">
      <content>THE DEFENSE ATTORNEYS: DANIEL G. DAVIS, 43 -- Defense attorney representing Ray Buckey.</content>
      <tokens>
        <token id="1" string="THE" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="DEFENSE" lemma="defense" stem="defense" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="ATTORNEYS" lemma="attorney" stem="attorneys" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="DANIEL" lemma="DANIEL" stem="daniel" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="6" string="G." lemma="G." stem="g." pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="7" string="DAVIS" lemma="DAVIS" stem="davis" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="43" lemma="43" stem="43" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="10" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="Defense" lemma="Defense" stem="defens" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="attorney" lemma="attorney" stem="attornei" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="13" string="representing" lemma="represent" stem="repres" pos="VBG" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="15" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="16" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (DT THE) (NN DEFENSE) (NNS ATTORNEYS)) (: :) (NP (NP (NP (NNP DANIEL) (NNP G.) (NNP DAVIS)) (, ,) (NP (CD 43))) (: --) (NP (NP (NNP Defense) (NN attorney)) (VP (VBG representing) (NP (NNP Ray) (NNP Buckey))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="THE DEFENSE ATTORNEYS : DANIEL G. DAVIS , 43 -- Defense attorney representing Ray Buckey ." type="NP">
          <tokens>
            <token id="1" string="THE" />
            <token id="2" string="DEFENSE" />
            <token id="3" string="ATTORNEYS" />
            <token id="4" string=":" />
            <token id="5" string="DANIEL" />
            <token id="6" string="G." />
            <token id="7" string="DAVIS" />
            <token id="8" string="," />
            <token id="9" string="43" />
            <token id="10" string="--" />
            <token id="11" string="Defense" />
            <token id="12" string="attorney" />
            <token id="13" string="representing" />
            <token id="14" string="Ray" />
            <token id="15" string="Buckey" />
            <token id="16" string="." />
          </tokens>
        </chunking>
        <chunking id="2" string="DANIEL G. DAVIS , 43 -- Defense attorney representing Ray Buckey" type="NP">
          <tokens>
            <token id="5" string="DANIEL" />
            <token id="6" string="G." />
            <token id="7" string="DAVIS" />
            <token id="8" string="," />
            <token id="9" string="43" />
            <token id="10" string="--" />
            <token id="11" string="Defense" />
            <token id="12" string="attorney" />
            <token id="13" string="representing" />
            <token id="14" string="Ray" />
            <token id="15" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="3" string="DANIEL G. DAVIS , 43" type="NP">
          <tokens>
            <token id="5" string="DANIEL" />
            <token id="6" string="G." />
            <token id="7" string="DAVIS" />
            <token id="8" string="," />
            <token id="9" string="43" />
          </tokens>
        </chunking>
        <chunking id="4" string="representing Ray Buckey" type="VP">
          <tokens>
            <token id="13" string="representing" />
            <token id="14" string="Ray" />
            <token id="15" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="5" string="THE DEFENSE ATTORNEYS" type="NP">
          <tokens>
            <token id="1" string="THE" />
            <token id="2" string="DEFENSE" />
            <token id="3" string="ATTORNEYS" />
          </tokens>
        </chunking>
        <chunking id="6" string="DANIEL G. DAVIS" type="NP">
          <tokens>
            <token id="5" string="DANIEL" />
            <token id="6" string="G." />
            <token id="7" string="DAVIS" />
          </tokens>
        </chunking>
        <chunking id="7" string="Defense attorney representing Ray Buckey" type="NP">
          <tokens>
            <token id="11" string="Defense" />
            <token id="12" string="attorney" />
            <token id="13" string="representing" />
            <token id="14" string="Ray" />
            <token id="15" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="8" string="Defense attorney" type="NP">
          <tokens>
            <token id="11" string="Defense" />
            <token id="12" string="attorney" />
          </tokens>
        </chunking>
        <chunking id="9" string="Ray Buckey" type="NP">
          <tokens>
            <token id="14" string="Ray" />
            <token id="15" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="10" string="43" type="NP">
          <tokens>
            <token id="9" string="43" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">ATTORNEYS</governor>
          <dependent id="1">THE</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">ATTORNEYS</governor>
          <dependent id="2">DEFENSE</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">ATTORNEYS</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">DAVIS</governor>
          <dependent id="5">DANIEL</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">DAVIS</governor>
          <dependent id="6">G.</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="3">ATTORNEYS</governor>
          <dependent id="7">DAVIS</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">DAVIS</governor>
          <dependent id="9">43</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">attorney</governor>
          <dependent id="11">Defense</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="7">DAVIS</governor>
          <dependent id="12">attorney</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="12">attorney</governor>
          <dependent id="13">representing</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">Buckey</governor>
          <dependent id="14">Ray</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="13">representing</governor>
          <dependent id="15">Buckey</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="DANIEL G. DAVIS" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="DANIEL" />
            <token id="6" string="G." />
            <token id="7" string="DAVIS" />
          </tokens>
        </entity>
        <entity id="2" string="Ray Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="14" string="Ray" />
            <token id="15" string="Buckey" />
          </tokens>
        </entity>
        <entity id="3" string="43" type="NUMBER" score="0.0">
          <tokens>
            <token id="9" string="43" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="140" has_coreference="true">
      <content>A graduate of the University of Texas law school, Davis was a civil attorney with the firm of Lillick McHose &amp;amp;amp; Charles for two years before turning to criminal law.</content>
      <tokens>
        <token id="1" string="A" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="graduate" lemma="graduate" stem="graduat" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="University" lemma="University" stem="univers" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="6" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="7" string="Texas" lemma="Texas" stem="texa" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="8" string="law" lemma="law" stem="law" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="school" lemma="school" stem="school" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="Davis" lemma="Davis" stem="davi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="12" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="civil" lemma="civil" stem="civil" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="attorney" lemma="attorney" stem="attornei" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="16" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="17" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="18" string="firm" lemma="firm" stem="firm" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="19" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="20" string="Lillick" lemma="Lillick" stem="lillick" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="21" string="McHose" lemma="McHose" stem="mchose" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="22" string="&amp;amp;" lemma="&amp;" stem="&amp;amp;" pos="CC" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="23" string="Charles" lemma="Charles" stem="charl" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="24" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="25" string="two" lemma="two" stem="two" pos="CD" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="true" />
        <token id="26" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="true" />
        <token id="27" string="before" lemma="before" stem="befor" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="turning" lemma="turn" stem="turn" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="criminal" lemma="criminal" stem="crimin" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="law" lemma="law" stem="law" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT A) (NN graduate)) (PP (IN of) (NP (NP (DT the) (NAC (NNP University) (PP (IN of) (NP (NNP Texas)))) (NN law) (NN school)) (, ,) (NP (NNP Davis))))) (VP (VBD was) (NP (NP (DT a) (JJ civil) (NN attorney)) (PP (IN with) (NP (NP (DT the) (NN firm)) (PP (IN of) (NP (NP (NNP Lillick) (NNP McHose) (CC &amp;) (NNP Charles)) (PP (IN for) (NP (CD two) (NNS years)))))))) (PP (IN before) (S (VP (VBG turning) (PP (TO to) (NP (JJ criminal) (NN law))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Lillick McHose &amp; Charles for two years" type="NP">
          <tokens>
            <token id="20" string="Lillick" />
            <token id="21" string="McHose" />
            <token id="22" string="&amp;amp;" />
            <token id="23" string="Charles" />
            <token id="24" string="for" />
            <token id="25" string="two" />
            <token id="26" string="years" />
          </tokens>
        </chunking>
        <chunking id="2" string="A graduate of the University of Texas law school , Davis" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="graduate" />
            <token id="3" string="of" />
            <token id="4" string="the" />
            <token id="5" string="University" />
            <token id="6" string="of" />
            <token id="7" string="Texas" />
            <token id="8" string="law" />
            <token id="9" string="school" />
            <token id="10" string="," />
            <token id="11" string="Davis" />
          </tokens>
        </chunking>
        <chunking id="3" string="the firm of Lillick McHose &amp; Charles for two years" type="NP">
          <tokens>
            <token id="17" string="the" />
            <token id="18" string="firm" />
            <token id="19" string="of" />
            <token id="20" string="Lillick" />
            <token id="21" string="McHose" />
            <token id="22" string="&amp;amp;" />
            <token id="23" string="Charles" />
            <token id="24" string="for" />
            <token id="25" string="two" />
            <token id="26" string="years" />
          </tokens>
        </chunking>
        <chunking id="4" string="Lillick McHose &amp; Charles" type="NP">
          <tokens>
            <token id="20" string="Lillick" />
            <token id="21" string="McHose" />
            <token id="22" string="&amp;amp;" />
            <token id="23" string="Charles" />
          </tokens>
        </chunking>
        <chunking id="5" string="Davis" type="NP">
          <tokens>
            <token id="11" string="Davis" />
          </tokens>
        </chunking>
        <chunking id="6" string="the firm" type="NP">
          <tokens>
            <token id="17" string="the" />
            <token id="18" string="firm" />
          </tokens>
        </chunking>
        <chunking id="7" string="two years" type="NP">
          <tokens>
            <token id="25" string="two" />
            <token id="26" string="years" />
          </tokens>
        </chunking>
        <chunking id="8" string="turning to criminal law" type="VP">
          <tokens>
            <token id="28" string="turning" />
            <token id="29" string="to" />
            <token id="30" string="criminal" />
            <token id="31" string="law" />
          </tokens>
        </chunking>
        <chunking id="9" string="the University of Texas law school , Davis" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="University" />
            <token id="6" string="of" />
            <token id="7" string="Texas" />
            <token id="8" string="law" />
            <token id="9" string="school" />
            <token id="10" string="," />
            <token id="11" string="Davis" />
          </tokens>
        </chunking>
        <chunking id="10" string="Texas" type="NP">
          <tokens>
            <token id="7" string="Texas" />
          </tokens>
        </chunking>
        <chunking id="11" string="was a civil attorney with the firm of Lillick McHose &amp; Charles for two years before turning to criminal law" type="VP">
          <tokens>
            <token id="12" string="was" />
            <token id="13" string="a" />
            <token id="14" string="civil" />
            <token id="15" string="attorney" />
            <token id="16" string="with" />
            <token id="17" string="the" />
            <token id="18" string="firm" />
            <token id="19" string="of" />
            <token id="20" string="Lillick" />
            <token id="21" string="McHose" />
            <token id="22" string="&amp;amp;" />
            <token id="23" string="Charles" />
            <token id="24" string="for" />
            <token id="25" string="two" />
            <token id="26" string="years" />
            <token id="27" string="before" />
            <token id="28" string="turning" />
            <token id="29" string="to" />
            <token id="30" string="criminal" />
            <token id="31" string="law" />
          </tokens>
        </chunking>
        <chunking id="12" string="A graduate" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="graduate" />
          </tokens>
        </chunking>
        <chunking id="13" string="criminal law" type="NP">
          <tokens>
            <token id="30" string="criminal" />
            <token id="31" string="law" />
          </tokens>
        </chunking>
        <chunking id="14" string="the University of Texas law school" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="University" />
            <token id="6" string="of" />
            <token id="7" string="Texas" />
            <token id="8" string="law" />
            <token id="9" string="school" />
          </tokens>
        </chunking>
        <chunking id="15" string="a civil attorney with the firm of Lillick McHose &amp; Charles for two years" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="civil" />
            <token id="15" string="attorney" />
            <token id="16" string="with" />
            <token id="17" string="the" />
            <token id="18" string="firm" />
            <token id="19" string="of" />
            <token id="20" string="Lillick" />
            <token id="21" string="McHose" />
            <token id="22" string="&amp;amp;" />
            <token id="23" string="Charles" />
            <token id="24" string="for" />
            <token id="25" string="two" />
            <token id="26" string="years" />
          </tokens>
        </chunking>
        <chunking id="16" string="a civil attorney" type="NP">
          <tokens>
            <token id="13" string="a" />
            <token id="14" string="civil" />
            <token id="15" string="attorney" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">graduate</governor>
          <dependent id="1">A</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">attorney</governor>
          <dependent id="2">graduate</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">school</governor>
          <dependent id="3">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">school</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="9">school</governor>
          <dependent id="5">University</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">Texas</governor>
          <dependent id="6">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">University</governor>
          <dependent id="7">Texas</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">school</governor>
          <dependent id="8">law</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">graduate</governor>
          <dependent id="9">school</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="9">school</governor>
          <dependent id="11">Davis</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="15">attorney</governor>
          <dependent id="12">was</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">attorney</governor>
          <dependent id="13">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">attorney</governor>
          <dependent id="14">civil</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="15">attorney</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">firm</governor>
          <dependent id="16">with</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">firm</governor>
          <dependent id="17">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">attorney</governor>
          <dependent id="18">firm</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">McHose</governor>
          <dependent id="19">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="21">McHose</governor>
          <dependent id="20">Lillick</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="18">firm</governor>
          <dependent id="21">McHose</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="21">McHose</governor>
          <dependent id="22">&amp;</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="21">McHose</governor>
          <dependent id="23">Charles</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">years</governor>
          <dependent id="24">for</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="26">years</governor>
          <dependent id="25">two</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="21">McHose</governor>
          <dependent id="26">years</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="28">turning</governor>
          <dependent id="27">before</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="15">attorney</governor>
          <dependent id="28">turning</dependent>
        </dependency>
        <dependency type="case">
          <governor id="31">law</governor>
          <dependent id="29">to</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="31">law</governor>
          <dependent id="30">criminal</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="28">turning</governor>
          <dependent id="31">law</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Lillick McHose &amp;amp; Charles" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="20" string="Lillick" />
            <token id="21" string="McHose" />
            <token id="22" string="&amp;amp;" />
            <token id="23" string="Charles" />
          </tokens>
        </entity>
        <entity id="2" string="University of Texas" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="5" string="University" />
            <token id="6" string="of" />
            <token id="7" string="Texas" />
          </tokens>
        </entity>
        <entity id="3" string="two years" type="DURATION" score="0.0">
          <tokens>
            <token id="25" string="two" />
            <token id="26" string="years" />
          </tokens>
        </entity>
        <entity id="4" string="Davis" type="PERSON" score="0.0">
          <tokens>
            <token id="11" string="Davis" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="141" has_coreference="true">
      <content>Initially hired by the Buckey family, he was appointed to finish the case after their money ran out.</content>
      <tokens>
        <token id="1" string="Initially" lemma="initially" stem="initial" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="hired" lemma="hire" stem="hire" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="6" string="family" lemma="family" stem="famili" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="appointed" lemma="appoint" stem="appoint" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="finish" lemma="finish" stem="finish" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="after" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="money" lemma="money" stem="monei" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="18" string="ran" lemma="run" stem="ran" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="out" lemma="out" stem="out" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (ADVP (RB Initially)) (VP (VBN hired) (PP (IN by) (NP (DT the) (NNP Buckey) (NN family))))) (, ,) (NP (PRP he)) (VP (VBD was) (VP (VBN appointed) (S (VP (TO to) (VP (VB finish) (NP (DT the) (NN case)) (SBAR (IN after) (S (NP (PRP$ their) (NN money)) (VP (VBD ran) (PRT (RP out)))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="was appointed to finish the case after their money ran out" type="VP">
          <tokens>
            <token id="9" string="was" />
            <token id="10" string="appointed" />
            <token id="11" string="to" />
            <token id="12" string="finish" />
            <token id="13" string="the" />
            <token id="14" string="case" />
            <token id="15" string="after" />
            <token id="16" string="their" />
            <token id="17" string="money" />
            <token id="18" string="ran" />
            <token id="19" string="out" />
          </tokens>
        </chunking>
        <chunking id="2" string="hired by the Buckey family" type="VP">
          <tokens>
            <token id="2" string="hired" />
            <token id="3" string="by" />
            <token id="4" string="the" />
            <token id="5" string="Buckey" />
            <token id="6" string="family" />
          </tokens>
        </chunking>
        <chunking id="3" string="finish the case after their money ran out" type="VP">
          <tokens>
            <token id="12" string="finish" />
            <token id="13" string="the" />
            <token id="14" string="case" />
            <token id="15" string="after" />
            <token id="16" string="their" />
            <token id="17" string="money" />
            <token id="18" string="ran" />
            <token id="19" string="out" />
          </tokens>
        </chunking>
        <chunking id="4" string="the case" type="NP">
          <tokens>
            <token id="13" string="the" />
            <token id="14" string="case" />
          </tokens>
        </chunking>
        <chunking id="5" string="after their money ran out" type="SBAR">
          <tokens>
            <token id="15" string="after" />
            <token id="16" string="their" />
            <token id="17" string="money" />
            <token id="18" string="ran" />
            <token id="19" string="out" />
          </tokens>
        </chunking>
        <chunking id="6" string="ran out" type="VP">
          <tokens>
            <token id="18" string="ran" />
            <token id="19" string="out" />
          </tokens>
        </chunking>
        <chunking id="7" string="to finish the case after their money ran out" type="VP">
          <tokens>
            <token id="11" string="to" />
            <token id="12" string="finish" />
            <token id="13" string="the" />
            <token id="14" string="case" />
            <token id="15" string="after" />
            <token id="16" string="their" />
            <token id="17" string="money" />
            <token id="18" string="ran" />
            <token id="19" string="out" />
          </tokens>
        </chunking>
        <chunking id="8" string="the Buckey family" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="Buckey" />
            <token id="6" string="family" />
          </tokens>
        </chunking>
        <chunking id="9" string="appointed to finish the case after their money ran out" type="VP">
          <tokens>
            <token id="10" string="appointed" />
            <token id="11" string="to" />
            <token id="12" string="finish" />
            <token id="13" string="the" />
            <token id="14" string="case" />
            <token id="15" string="after" />
            <token id="16" string="their" />
            <token id="17" string="money" />
            <token id="18" string="ran" />
            <token id="19" string="out" />
          </tokens>
        </chunking>
        <chunking id="10" string="he" type="NP">
          <tokens>
            <token id="8" string="he" />
          </tokens>
        </chunking>
        <chunking id="11" string="their money" type="NP">
          <tokens>
            <token id="16" string="their" />
            <token id="17" string="money" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="2">hired</governor>
          <dependent id="1">Initially</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="10">appointed</governor>
          <dependent id="2">hired</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">family</governor>
          <dependent id="3">by</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">family</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">family</governor>
          <dependent id="5">Buckey</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">hired</governor>
          <dependent id="6">family</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="10">appointed</governor>
          <dependent id="8">he</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="10">appointed</governor>
          <dependent id="9">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="10">appointed</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="12">finish</governor>
          <dependent id="11">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="10">appointed</governor>
          <dependent id="12">finish</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">case</governor>
          <dependent id="13">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="12">finish</governor>
          <dependent id="14">case</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="18">ran</governor>
          <dependent id="15">after</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="17">money</governor>
          <dependent id="16">their</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="18">ran</governor>
          <dependent id="17">money</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="12">finish</governor>
          <dependent id="18">ran</dependent>
        </dependency>
        <dependency type="compound:prt">
          <governor id="18">ran</governor>
          <dependent id="19">out</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="Buckey" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="142" has_coreference="true">
      <content>He owns the now-defunct nursery school.</content>
      <tokens>
        <token id="1" string="He" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="owns" lemma="own" stem="own" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="now-defunct" lemma="now-defunct" stem="now-defunct" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="nursery" lemma="nursery" stem="nurseri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="school" lemma="school" stem="school" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP He)) (VP (VBZ owns) (NP (DT the) (JJ now-defunct) (NN nursery) (NN school))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the now-defunct nursery school" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="now-defunct" />
            <token id="5" string="nursery" />
            <token id="6" string="school" />
          </tokens>
        </chunking>
        <chunking id="2" string="owns the now-defunct nursery school" type="VP">
          <tokens>
            <token id="2" string="owns" />
            <token id="3" string="the" />
            <token id="4" string="now-defunct" />
            <token id="5" string="nursery" />
            <token id="6" string="school" />
          </tokens>
        </chunking>
        <chunking id="3" string="He" type="NP">
          <tokens>
            <token id="1" string="He" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">owns</governor>
          <dependent id="1">He</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">owns</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">school</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">school</governor>
          <dependent id="4">now-defunct</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">school</governor>
          <dependent id="5">nursery</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">owns</governor>
          <dependent id="6">school</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="143" has_coreference="true">
      <content>DEAN R. GITS, 45 -- Defense attorney representing Peggy McMartin Buckey.</content>
      <tokens>
        <token id="1" string="DEAN" lemma="dean" stem="dean" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="2" string="R." lemma="R." stem="r." pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="GITS" lemma="GITS" stem="gits" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="45" lemma="45" stem="45" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="6" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="Defense" lemma="Defense" stem="defens" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="attorney" lemma="attorney" stem="attornei" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="representing" lemma="represent" stem="repres" pos="VBG" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="Peggy" lemma="Peggy" stem="peggi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="11" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="12" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NP (NN DEAN) (NNP R.) (NNP GITS)) (, ,) (NP (CD 45))) (: --) (NP (NP (NNP Defense) (NN attorney)) (VP (VBG representing) (NP (NNP Peggy) (NNP McMartin) (NNP Buckey)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="45" type="NP">
          <tokens>
            <token id="5" string="45" />
          </tokens>
        </chunking>
        <chunking id="2" string="Peggy McMartin Buckey" type="NP">
          <tokens>
            <token id="10" string="Peggy" />
            <token id="11" string="McMartin" />
            <token id="12" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="3" string="Defense attorney representing Peggy McMartin Buckey" type="NP">
          <tokens>
            <token id="7" string="Defense" />
            <token id="8" string="attorney" />
            <token id="9" string="representing" />
            <token id="10" string="Peggy" />
            <token id="11" string="McMartin" />
            <token id="12" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="4" string="representing Peggy McMartin Buckey" type="VP">
          <tokens>
            <token id="9" string="representing" />
            <token id="10" string="Peggy" />
            <token id="11" string="McMartin" />
            <token id="12" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="5" string="DEAN R. GITS , 45 -- Defense attorney representing Peggy McMartin Buckey ." type="NP">
          <tokens>
            <token id="1" string="DEAN" />
            <token id="2" string="R." />
            <token id="3" string="GITS" />
            <token id="4" string="," />
            <token id="5" string="45" />
            <token id="6" string="--" />
            <token id="7" string="Defense" />
            <token id="8" string="attorney" />
            <token id="9" string="representing" />
            <token id="10" string="Peggy" />
            <token id="11" string="McMartin" />
            <token id="12" string="Buckey" />
            <token id="13" string="." />
          </tokens>
        </chunking>
        <chunking id="6" string="DEAN R. GITS" type="NP">
          <tokens>
            <token id="1" string="DEAN" />
            <token id="2" string="R." />
            <token id="3" string="GITS" />
          </tokens>
        </chunking>
        <chunking id="7" string="Defense attorney" type="NP">
          <tokens>
            <token id="7" string="Defense" />
            <token id="8" string="attorney" />
          </tokens>
        </chunking>
        <chunking id="8" string="DEAN R. GITS , 45" type="NP">
          <tokens>
            <token id="1" string="DEAN" />
            <token id="2" string="R." />
            <token id="3" string="GITS" />
            <token id="4" string="," />
            <token id="5" string="45" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="3">GITS</governor>
          <dependent id="1">DEAN</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">GITS</governor>
          <dependent id="2">R.</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">GITS</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">GITS</governor>
          <dependent id="5">45</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">attorney</governor>
          <dependent id="7">Defense</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="3">GITS</governor>
          <dependent id="8">attorney</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="8">attorney</governor>
          <dependent id="9">representing</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">Buckey</governor>
          <dependent id="10">Peggy</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">Buckey</governor>
          <dependent id="11">McMartin</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="9">representing</governor>
          <dependent id="12">Buckey</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="45" type="NUMBER" score="0.0">
          <tokens>
            <token id="5" string="45" />
          </tokens>
        </entity>
        <entity id="2" string="Peggy McMartin Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="Peggy" />
            <token id="11" string="McMartin" />
            <token id="12" string="Buckey" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="144" has_coreference="true">
      <content>A gentlemanly, cum laude graduate of the William Mitchell College of Law in St. Paul, Gits describes himself as &amp;quot;a simple country lawyer from Minnesota.&amp;quot;</content>
      <tokens>
        <token id="1" string="A" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="gentlemanly" lemma="gentlemanly" stem="gentlemanli" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="4" string="cum" lemma="cum" stem="cum" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="laude" lemma="laude" stem="laud" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="6" string="graduate" lemma="graduate" stem="graduat" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="William" lemma="William" stem="william" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="10" string="Mitchell" lemma="Mitchell" stem="mitchel" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="11" string="College" lemma="College" stem="colleg" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="12" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="13" string="Law" lemma="Law" stem="law" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="14" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="St." lemma="St." stem="st." pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="16" string="Paul" lemma="Paul" stem="paul" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="17" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="18" string="Gits" lemma="Gits" stem="git" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="19" string="describes" lemma="describe" stem="describ" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="himself" lemma="himself" stem="himself" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="simple" lemma="simple" stem="simpl" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="country" lemma="country" stem="countri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="lawyer" lemma="lawyer" stem="lawyer" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="Minnesota" lemma="Minnesota" stem="minnesota" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="29" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT A) (JJ gentlemanly) (, ,) (NN cum) (NN laude) (NN graduate)) (PP (IN of) (NP (NP (DT the) (NNP William) (NNP Mitchell) (NNP College)) (PP (IN of) (NP (NP (NNP Law)) (PP (IN in) (NP (NP (NNP St.) (NNP Paul)) (, ,) (NP (NNP Gits))))))))) (VP (VBZ describes) (NP (PRP himself)) (PP (IN as) (`` ``) (NP (NP (DT a) (JJ simple) (NN country) (NN lawyer)) (PP (IN from) (NP (NNP Minnesota)))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="Law" type="NP">
          <tokens>
            <token id="13" string="Law" />
          </tokens>
        </chunking>
        <chunking id="2" string="Gits" type="NP">
          <tokens>
            <token id="18" string="Gits" />
          </tokens>
        </chunking>
        <chunking id="3" string="St. Paul" type="NP">
          <tokens>
            <token id="15" string="St." />
            <token id="16" string="Paul" />
          </tokens>
        </chunking>
        <chunking id="4" string="describes himself as `` a simple country lawyer from Minnesota" type="VP">
          <tokens>
            <token id="19" string="describes" />
            <token id="20" string="himself" />
            <token id="21" string="as" />
            <token id="22" string="&quot;" />
            <token id="23" string="a" />
            <token id="24" string="simple" />
            <token id="25" string="country" />
            <token id="26" string="lawyer" />
            <token id="27" string="from" />
            <token id="28" string="Minnesota" />
          </tokens>
        </chunking>
        <chunking id="5" string="Minnesota" type="NP">
          <tokens>
            <token id="28" string="Minnesota" />
          </tokens>
        </chunking>
        <chunking id="6" string="a simple country lawyer" type="NP">
          <tokens>
            <token id="23" string="a" />
            <token id="24" string="simple" />
            <token id="25" string="country" />
            <token id="26" string="lawyer" />
          </tokens>
        </chunking>
        <chunking id="7" string="St. Paul , Gits" type="NP">
          <tokens>
            <token id="15" string="St." />
            <token id="16" string="Paul" />
            <token id="17" string="," />
            <token id="18" string="Gits" />
          </tokens>
        </chunking>
        <chunking id="8" string="the William Mitchell College" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="William" />
            <token id="10" string="Mitchell" />
            <token id="11" string="College" />
          </tokens>
        </chunking>
        <chunking id="9" string="Law in St. Paul , Gits" type="NP">
          <tokens>
            <token id="13" string="Law" />
            <token id="14" string="in" />
            <token id="15" string="St." />
            <token id="16" string="Paul" />
            <token id="17" string="," />
            <token id="18" string="Gits" />
          </tokens>
        </chunking>
        <chunking id="10" string="A gentlemanly , cum laude graduate of the William Mitchell College of Law in St. Paul , Gits" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="gentlemanly" />
            <token id="3" string="," />
            <token id="4" string="cum" />
            <token id="5" string="laude" />
            <token id="6" string="graduate" />
            <token id="7" string="of" />
            <token id="8" string="the" />
            <token id="9" string="William" />
            <token id="10" string="Mitchell" />
            <token id="11" string="College" />
            <token id="12" string="of" />
            <token id="13" string="Law" />
            <token id="14" string="in" />
            <token id="15" string="St." />
            <token id="16" string="Paul" />
            <token id="17" string="," />
            <token id="18" string="Gits" />
          </tokens>
        </chunking>
        <chunking id="11" string="the William Mitchell College of Law in St. Paul , Gits" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="William" />
            <token id="10" string="Mitchell" />
            <token id="11" string="College" />
            <token id="12" string="of" />
            <token id="13" string="Law" />
            <token id="14" string="in" />
            <token id="15" string="St." />
            <token id="16" string="Paul" />
            <token id="17" string="," />
            <token id="18" string="Gits" />
          </tokens>
        </chunking>
        <chunking id="12" string="A gentlemanly , cum laude graduate" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="gentlemanly" />
            <token id="3" string="," />
            <token id="4" string="cum" />
            <token id="5" string="laude" />
            <token id="6" string="graduate" />
          </tokens>
        </chunking>
        <chunking id="13" string="a simple country lawyer from Minnesota" type="NP">
          <tokens>
            <token id="23" string="a" />
            <token id="24" string="simple" />
            <token id="25" string="country" />
            <token id="26" string="lawyer" />
            <token id="27" string="from" />
            <token id="28" string="Minnesota" />
          </tokens>
        </chunking>
        <chunking id="14" string="himself" type="NP">
          <tokens>
            <token id="20" string="himself" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="6">graduate</governor>
          <dependent id="1">A</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">graduate</governor>
          <dependent id="2">gentlemanly</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="6">graduate</governor>
          <dependent id="4">cum</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">graduate</governor>
          <dependent id="5">laude</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">describes</governor>
          <dependent id="6">graduate</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">College</governor>
          <dependent id="7">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">College</governor>
          <dependent id="8">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">College</governor>
          <dependent id="9">William</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">College</governor>
          <dependent id="10">Mitchell</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">graduate</governor>
          <dependent id="11">College</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">Law</governor>
          <dependent id="12">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">College</governor>
          <dependent id="13">Law</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">Paul</governor>
          <dependent id="14">in</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="16">Paul</governor>
          <dependent id="15">St.</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">Law</governor>
          <dependent id="16">Paul</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="16">Paul</governor>
          <dependent id="18">Gits</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="19">describes</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="19">describes</governor>
          <dependent id="20">himself</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">lawyer</governor>
          <dependent id="21">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="26">lawyer</governor>
          <dependent id="23">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="26">lawyer</governor>
          <dependent id="24">simple</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="26">lawyer</governor>
          <dependent id="25">country</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">describes</governor>
          <dependent id="26">lawyer</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">Minnesota</governor>
          <dependent id="27">from</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">lawyer</governor>
          <dependent id="28">Minnesota</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="William Mitchell College of Law" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="9" string="William" />
            <token id="10" string="Mitchell" />
            <token id="11" string="College" />
            <token id="12" string="of" />
            <token id="13" string="Law" />
          </tokens>
        </entity>
        <entity id="2" string="St. Paul" type="LOCATION" score="0.0">
          <tokens>
            <token id="15" string="St." />
            <token id="16" string="Paul" />
          </tokens>
        </entity>
        <entity id="3" string="Minnesota" type="LOCATION" score="0.0">
          <tokens>
            <token id="28" string="Minnesota" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="145" has_coreference="true">
      <content>Known for his painstakingly thorough work, he was a deputy public defender for nearly nine years before going into private practice with the firm of Overland, Berke, Wesley, Gits, Randolph &amp;amp;amp; Levanas.</content>
      <tokens>
        <token id="1" string="Known" lemma="know" stem="known" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="painstakingly" lemma="painstakingly" stem="painstakingli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="thorough" lemma="thorough" stem="thorough" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="work" lemma="work" stem="work" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="deputy" lemma="deputy" stem="deputi" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="12" string="public" lemma="public" stem="public" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="13" string="defender" lemma="defender" stem="defend" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="14" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="nearly" lemma="nearly" stem="nearli" pos="RB" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="true" />
        <token id="16" string="nine" lemma="nine" stem="nine" pos="CD" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="true" />
        <token id="17" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="true" />
        <token id="18" string="before" lemma="before" stem="befor" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="going" lemma="go" stem="go" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="into" lemma="into" stem="into" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="private" lemma="private" stem="privat" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="practice" lemma="practice" stem="practic" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="firm" lemma="firm" stem="firm" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="Overland" lemma="Overland" stem="overland" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="28" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="Berke" lemma="Berke" stem="berk" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="30" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="Wesley" lemma="Wesley" stem="weslei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="32" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="Gits" lemma="Gits" stem="git" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="34" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="Randolph" lemma="Randolph" stem="randolph" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="36" string="&amp;amp;" lemma="&amp;" stem="&amp;amp;" pos="CC" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="37" string="Levanas" lemma="Levanas" stem="levana" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="38" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (VP (VBN Known) (PRT (IN for)) (NP (PRP$ his) (RB painstakingly) (JJ thorough) (NN work)))) (, ,) (NP (PRP he)) (VP (VBD was) (NP (NP (DT a) (JJ deputy) (JJ public) (NN defender)) (PP (IN for) (NP (RB nearly) (CD nine) (NNS years)))) (PP (IN before) (S (VP (VBG going) (PP (IN into) (NP (JJ private) (NN practice))) (PP (IN with) (NP (NP (DT the) (NN firm)) (PP (IN of) (NP (NNP Overland) (, ,) (NNP Berke) (, ,) (NNP Wesley) (, ,) (NNP Gits) (, ,) (NNP Randolph) (CC &amp;) (NNP Levanas))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the firm" type="NP">
          <tokens>
            <token id="24" string="the" />
            <token id="25" string="firm" />
          </tokens>
        </chunking>
        <chunking id="2" string="his painstakingly thorough work" type="NP">
          <tokens>
            <token id="3" string="his" />
            <token id="4" string="painstakingly" />
            <token id="5" string="thorough" />
            <token id="6" string="work" />
          </tokens>
        </chunking>
        <chunking id="3" string="Known for his painstakingly thorough work" type="VP">
          <tokens>
            <token id="1" string="Known" />
            <token id="2" string="for" />
            <token id="3" string="his" />
            <token id="4" string="painstakingly" />
            <token id="5" string="thorough" />
            <token id="6" string="work" />
          </tokens>
        </chunking>
        <chunking id="4" string="a deputy public defender" type="NP">
          <tokens>
            <token id="10" string="a" />
            <token id="11" string="deputy" />
            <token id="12" string="public" />
            <token id="13" string="defender" />
          </tokens>
        </chunking>
        <chunking id="5" string="private practice" type="NP">
          <tokens>
            <token id="21" string="private" />
            <token id="22" string="practice" />
          </tokens>
        </chunking>
        <chunking id="6" string="was a deputy public defender for nearly nine years before going into private practice with the firm of Overland , Berke , Wesley , Gits , Randolph &amp; Levanas" type="VP">
          <tokens>
            <token id="9" string="was" />
            <token id="10" string="a" />
            <token id="11" string="deputy" />
            <token id="12" string="public" />
            <token id="13" string="defender" />
            <token id="14" string="for" />
            <token id="15" string="nearly" />
            <token id="16" string="nine" />
            <token id="17" string="years" />
            <token id="18" string="before" />
            <token id="19" string="going" />
            <token id="20" string="into" />
            <token id="21" string="private" />
            <token id="22" string="practice" />
            <token id="23" string="with" />
            <token id="24" string="the" />
            <token id="25" string="firm" />
            <token id="26" string="of" />
            <token id="27" string="Overland" />
            <token id="28" string="," />
            <token id="29" string="Berke" />
            <token id="30" string="," />
            <token id="31" string="Wesley" />
            <token id="32" string="," />
            <token id="33" string="Gits" />
            <token id="34" string="," />
            <token id="35" string="Randolph" />
            <token id="36" string="&amp;amp;" />
            <token id="37" string="Levanas" />
          </tokens>
        </chunking>
        <chunking id="7" string="Overland , Berke , Wesley , Gits , Randolph &amp; Levanas" type="NP">
          <tokens>
            <token id="27" string="Overland" />
            <token id="28" string="," />
            <token id="29" string="Berke" />
            <token id="30" string="," />
            <token id="31" string="Wesley" />
            <token id="32" string="," />
            <token id="33" string="Gits" />
            <token id="34" string="," />
            <token id="35" string="Randolph" />
            <token id="36" string="&amp;amp;" />
            <token id="37" string="Levanas" />
          </tokens>
        </chunking>
        <chunking id="8" string="he" type="NP">
          <tokens>
            <token id="8" string="he" />
          </tokens>
        </chunking>
        <chunking id="9" string="a deputy public defender for nearly nine years" type="NP">
          <tokens>
            <token id="10" string="a" />
            <token id="11" string="deputy" />
            <token id="12" string="public" />
            <token id="13" string="defender" />
            <token id="14" string="for" />
            <token id="15" string="nearly" />
            <token id="16" string="nine" />
            <token id="17" string="years" />
          </tokens>
        </chunking>
        <chunking id="10" string="going into private practice with the firm of Overland , Berke , Wesley , Gits , Randolph &amp; Levanas" type="VP">
          <tokens>
            <token id="19" string="going" />
            <token id="20" string="into" />
            <token id="21" string="private" />
            <token id="22" string="practice" />
            <token id="23" string="with" />
            <token id="24" string="the" />
            <token id="25" string="firm" />
            <token id="26" string="of" />
            <token id="27" string="Overland" />
            <token id="28" string="," />
            <token id="29" string="Berke" />
            <token id="30" string="," />
            <token id="31" string="Wesley" />
            <token id="32" string="," />
            <token id="33" string="Gits" />
            <token id="34" string="," />
            <token id="35" string="Randolph" />
            <token id="36" string="&amp;amp;" />
            <token id="37" string="Levanas" />
          </tokens>
        </chunking>
        <chunking id="11" string="nearly nine years" type="NP">
          <tokens>
            <token id="15" string="nearly" />
            <token id="16" string="nine" />
            <token id="17" string="years" />
          </tokens>
        </chunking>
        <chunking id="12" string="the firm of Overland , Berke , Wesley , Gits , Randolph &amp; Levanas" type="NP">
          <tokens>
            <token id="24" string="the" />
            <token id="25" string="firm" />
            <token id="26" string="of" />
            <token id="27" string="Overland" />
            <token id="28" string="," />
            <token id="29" string="Berke" />
            <token id="30" string="," />
            <token id="31" string="Wesley" />
            <token id="32" string="," />
            <token id="33" string="Gits" />
            <token id="34" string="," />
            <token id="35" string="Randolph" />
            <token id="36" string="&amp;amp;" />
            <token id="37" string="Levanas" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advcl">
          <governor id="13">defender</governor>
          <dependent id="1">Known</dependent>
        </dependency>
        <dependency type="compound:prt">
          <governor id="1">Known</governor>
          <dependent id="2">for</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="6">work</governor>
          <dependent id="3">his</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="6">work</governor>
          <dependent id="4">painstakingly</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">work</governor>
          <dependent id="5">thorough</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="1">Known</governor>
          <dependent id="6">work</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="13">defender</governor>
          <dependent id="8">he</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="13">defender</governor>
          <dependent id="9">was</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">defender</governor>
          <dependent id="10">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">defender</governor>
          <dependent id="11">deputy</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">defender</governor>
          <dependent id="12">public</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="13">defender</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">years</governor>
          <dependent id="14">for</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="17">years</governor>
          <dependent id="15">nearly</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="17">years</governor>
          <dependent id="16">nine</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">defender</governor>
          <dependent id="17">years</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="19">going</governor>
          <dependent id="18">before</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="13">defender</governor>
          <dependent id="19">going</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">practice</governor>
          <dependent id="20">into</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="22">practice</governor>
          <dependent id="21">private</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">going</governor>
          <dependent id="22">practice</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">firm</governor>
          <dependent id="23">with</dependent>
        </dependency>
        <dependency type="det">
          <governor id="25">firm</governor>
          <dependent id="24">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">going</governor>
          <dependent id="25">firm</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">Overland</governor>
          <dependent id="26">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="25">firm</governor>
          <dependent id="27">Overland</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="27">Overland</governor>
          <dependent id="29">Berke</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="27">Overland</governor>
          <dependent id="31">Wesley</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="27">Overland</governor>
          <dependent id="33">Gits</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="27">Overland</governor>
          <dependent id="35">Randolph</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="27">Overland</governor>
          <dependent id="36">&amp;</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="27">Overland</governor>
          <dependent id="37">Levanas</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Overland" type="LOCATION" score="0.0">
          <tokens>
            <token id="27" string="Overland" />
          </tokens>
        </entity>
        <entity id="2" string="Wesley" type="PERSON" score="0.0">
          <tokens>
            <token id="31" string="Wesley" />
          </tokens>
        </entity>
        <entity id="3" string="Randolph &amp;amp; Levanas" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="35" string="Randolph" />
            <token id="36" string="&amp;amp;" />
            <token id="37" string="Levanas" />
          </tokens>
        </entity>
        <entity id="4" string="Berke" type="PERSON" score="0.0">
          <tokens>
            <token id="29" string="Berke" />
          </tokens>
        </entity>
        <entity id="5" string="nearly nine years" type="DURATION" score="0.0">
          <tokens>
            <token id="15" string="nearly" />
            <token id="16" string="nine" />
            <token id="17" string="years" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="146" has_coreference="true">
      <content>He has more than 15 years of criminal trial experience.</content>
      <tokens>
        <token id="1" string="He" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="more" lemma="more" stem="more" pos="JJR" type="Word" isStopWord="true" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="4" string="than" lemma="than" stem="than" pos="IN" type="Word" isStopWord="true" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="5" string="15" lemma="15" stem="15" pos="CD" type="Number" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="6" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="7" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="criminal" lemma="criminal" stem="crimin" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="trial" lemma="trial" stem="trial" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="experience" lemma="experience" stem="experi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP He)) (VP (VBZ has) (NP (NP (QP (JJR more) (IN than) (CD 15)) (NNS years)) (PP (IN of) (NP (JJ criminal) (NN trial) (NN experience))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="more than 15 years of criminal trial experience" type="NP">
          <tokens>
            <token id="3" string="more" />
            <token id="4" string="than" />
            <token id="5" string="15" />
            <token id="6" string="years" />
            <token id="7" string="of" />
            <token id="8" string="criminal" />
            <token id="9" string="trial" />
            <token id="10" string="experience" />
          </tokens>
        </chunking>
        <chunking id="2" string="more than 15 years" type="NP">
          <tokens>
            <token id="3" string="more" />
            <token id="4" string="than" />
            <token id="5" string="15" />
            <token id="6" string="years" />
          </tokens>
        </chunking>
        <chunking id="3" string="He" type="NP">
          <tokens>
            <token id="1" string="He" />
          </tokens>
        </chunking>
        <chunking id="4" string="criminal trial experience" type="NP">
          <tokens>
            <token id="8" string="criminal" />
            <token id="9" string="trial" />
            <token id="10" string="experience" />
          </tokens>
        </chunking>
        <chunking id="5" string="has more than 15 years of criminal trial experience" type="VP">
          <tokens>
            <token id="2" string="has" />
            <token id="3" string="more" />
            <token id="4" string="than" />
            <token id="5" string="15" />
            <token id="6" string="years" />
            <token id="7" string="of" />
            <token id="8" string="criminal" />
            <token id="9" string="trial" />
            <token id="10" string="experience" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">has</governor>
          <dependent id="1">He</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">has</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">15</governor>
          <dependent id="3">more</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="3">more</governor>
          <dependent id="4">than</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="6">years</governor>
          <dependent id="5">15</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">has</governor>
          <dependent id="6">years</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">experience</governor>
          <dependent id="7">of</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="10">experience</governor>
          <dependent id="8">criminal</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">experience</governor>
          <dependent id="9">trial</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">years</governor>
          <dependent id="10">experience</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="more than 15 years" type="DURATION" score="0.0">
          <tokens>
            <token id="3" string="more" />
            <token id="4" string="than" />
            <token id="5" string="15" />
            <token id="6" string="years" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="147" has_coreference="true">
      <content>He was court-appointed to the McMartin case at its beginning.</content>
      <tokens>
        <token id="1" string="He" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="court-appointed" lemma="court-appointed" stem="court-appoint" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="its" lemma="its" stem="it" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="beginning" lemma="beginning" stem="begin" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="11" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP He)) (VP (VBD was) (ADJP (JJ court-appointed) (PP (TO to) (NP (DT the) (NNP McMartin) (NN case)))) (PP (IN at) (NP (PRP$ its) (NN beginning)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="was court-appointed to the McMartin case at its beginning" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="court-appointed" />
            <token id="4" string="to" />
            <token id="5" string="the" />
            <token id="6" string="McMartin" />
            <token id="7" string="case" />
            <token id="8" string="at" />
            <token id="9" string="its" />
            <token id="10" string="beginning" />
          </tokens>
        </chunking>
        <chunking id="2" string="its beginning" type="NP">
          <tokens>
            <token id="9" string="its" />
            <token id="10" string="beginning" />
          </tokens>
        </chunking>
        <chunking id="3" string="the McMartin case" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="McMartin" />
            <token id="7" string="case" />
          </tokens>
        </chunking>
        <chunking id="4" string="He" type="NP">
          <tokens>
            <token id="1" string="He" />
          </tokens>
        </chunking>
        <chunking id="5" string="court-appointed to the McMartin case" type="ADJP">
          <tokens>
            <token id="3" string="court-appointed" />
            <token id="4" string="to" />
            <token id="5" string="the" />
            <token id="6" string="McMartin" />
            <token id="7" string="case" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">court-appointed</governor>
          <dependent id="1">He</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="3">court-appointed</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">court-appointed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">case</governor>
          <dependent id="4">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">case</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">case</governor>
          <dependent id="6">McMartin</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">court-appointed</governor>
          <dependent id="7">case</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">beginning</governor>
          <dependent id="8">at</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="10">beginning</governor>
          <dependent id="9">its</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">court-appointed</governor>
          <dependent id="10">beginning</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="148" has_coreference="true">
      <content>McMARTIN TRIAL CHRONOLOGY Aug. 12, 1983 -- Initial complaint filed against Ray Buckey.</content>
      <tokens>
        <token id="1" string="McMARTIN" lemma="McMARTIN" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="TRIAL" lemma="trial" stem="trial" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="CHRONOLOGY" lemma="chronology" stem="chronology" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="Aug." lemma="Aug." stem="aug." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="12" lemma="12" stem="12" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="7" string="1983" lemma="1983" stem="1983" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="8" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="Initial" lemma="initial" stem="initial" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="complaint" lemma="complaint" stem="complaint" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="filed" lemma="file" stem="file" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="against" lemma="against" stem="against" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="14" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="15" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NP (NNP McMARTIN) (NN TRIAL) (NN CHRONOLOGY)) (NP-TMP (NNP Aug.) (CD 12) (, ,) (CD 1983))) (: --) (NP (NP (JJ Initial) (NN complaint)) (VP (VBN filed) (PP (IN against) (NP (NNP Ray) (NNP Buckey))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="McMARTIN TRIAL CHRONOLOGY Aug. 12 , 1983" type="NP">
          <tokens>
            <token id="1" string="McMARTIN" />
            <token id="2" string="TRIAL" />
            <token id="3" string="CHRONOLOGY" />
            <token id="4" string="Aug." />
            <token id="5" string="12" />
            <token id="6" string="," />
            <token id="7" string="1983" />
          </tokens>
        </chunking>
        <chunking id="2" string="McMARTIN TRIAL CHRONOLOGY" type="NP">
          <tokens>
            <token id="1" string="McMARTIN" />
            <token id="2" string="TRIAL" />
            <token id="3" string="CHRONOLOGY" />
          </tokens>
        </chunking>
        <chunking id="3" string="Initial complaint filed against Ray Buckey" type="NP">
          <tokens>
            <token id="9" string="Initial" />
            <token id="10" string="complaint" />
            <token id="11" string="filed" />
            <token id="12" string="against" />
            <token id="13" string="Ray" />
            <token id="14" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="4" string="McMARTIN TRIAL CHRONOLOGY Aug. 12 , 1983 -- Initial complaint filed against Ray Buckey ." type="NP">
          <tokens>
            <token id="1" string="McMARTIN" />
            <token id="2" string="TRIAL" />
            <token id="3" string="CHRONOLOGY" />
            <token id="4" string="Aug." />
            <token id="5" string="12" />
            <token id="6" string="," />
            <token id="7" string="1983" />
            <token id="8" string="--" />
            <token id="9" string="Initial" />
            <token id="10" string="complaint" />
            <token id="11" string="filed" />
            <token id="12" string="against" />
            <token id="13" string="Ray" />
            <token id="14" string="Buckey" />
            <token id="15" string="." />
          </tokens>
        </chunking>
        <chunking id="5" string="filed against Ray Buckey" type="VP">
          <tokens>
            <token id="11" string="filed" />
            <token id="12" string="against" />
            <token id="13" string="Ray" />
            <token id="14" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="6" string="Ray Buckey" type="NP">
          <tokens>
            <token id="13" string="Ray" />
            <token id="14" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="7" string="Initial complaint" type="NP">
          <tokens>
            <token id="9" string="Initial" />
            <token id="10" string="complaint" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="3">CHRONOLOGY</governor>
          <dependent id="1">McMARTIN</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">CHRONOLOGY</governor>
          <dependent id="2">TRIAL</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">CHRONOLOGY</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="3">CHRONOLOGY</governor>
          <dependent id="4">Aug.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="4">Aug.</governor>
          <dependent id="5">12</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="4">Aug.</governor>
          <dependent id="7">1983</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="10">complaint</governor>
          <dependent id="9">Initial</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="3">CHRONOLOGY</governor>
          <dependent id="10">complaint</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="10">complaint</governor>
          <dependent id="11">filed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">Buckey</governor>
          <dependent id="12">against</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">Buckey</governor>
          <dependent id="13">Ray</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">filed</governor>
          <dependent id="14">Buckey</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Aug. 12 , 1983" type="DATE" score="0.0">
          <tokens>
            <token id="4" string="Aug." />
            <token id="5" string="12" />
            <token id="6" string="," />
            <token id="7" string="1983" />
          </tokens>
        </entity>
        <entity id="2" string="Ray Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="13" string="Ray" />
            <token id="14" string="Buckey" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="149" has_coreference="false">
      <content>Sept. 7, 1983 -- Ray Buckey arrested and released.</content>
      <tokens>
        <token id="1" string="Sept." lemma="Sept." stem="sept." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="7" lemma="7" stem="7" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1983" lemma="1983" stem="1983" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="7" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="8" string="arrested" lemma="arrest" stem="arrest" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="released" lemma="release" stem="releas" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP Sept.) (CD 7) (, ,) (CD 1983)) (: --) (NP (NP (NNP Ray) (NNP Buckey)) (VP (VBN arrested) (CC and) (VBN released))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Sept. 7 , 1983 -- Ray Buckey arrested and released ." type="NP">
          <tokens>
            <token id="1" string="Sept." />
            <token id="2" string="7" />
            <token id="3" string="," />
            <token id="4" string="1983" />
            <token id="5" string="--" />
            <token id="6" string="Ray" />
            <token id="7" string="Buckey" />
            <token id="8" string="arrested" />
            <token id="9" string="and" />
            <token id="10" string="released" />
            <token id="11" string="." />
          </tokens>
        </chunking>
        <chunking id="2" string="arrested and released" type="VP">
          <tokens>
            <token id="8" string="arrested" />
            <token id="9" string="and" />
            <token id="10" string="released" />
          </tokens>
        </chunking>
        <chunking id="3" string="Ray Buckey arrested and released" type="NP">
          <tokens>
            <token id="6" string="Ray" />
            <token id="7" string="Buckey" />
            <token id="8" string="arrested" />
            <token id="9" string="and" />
            <token id="10" string="released" />
          </tokens>
        </chunking>
        <chunking id="4" string="Sept. 7 , 1983" type="NP">
          <tokens>
            <token id="1" string="Sept." />
            <token id="2" string="7" />
            <token id="3" string="," />
            <token id="4" string="1983" />
          </tokens>
        </chunking>
        <chunking id="5" string="Ray Buckey" type="NP">
          <tokens>
            <token id="6" string="Ray" />
            <token id="7" string="Buckey" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Sept.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Sept.</governor>
          <dependent id="2">7</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Sept.</governor>
          <dependent id="4">1983</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">Buckey</governor>
          <dependent id="6">Ray</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="1">Sept.</governor>
          <dependent id="7">Buckey</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="7">Buckey</governor>
          <dependent id="8">arrested</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="8">arrested</governor>
          <dependent id="9">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="8">arrested</governor>
          <dependent id="10">released</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Sept. 7 , 1983" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Sept." />
            <token id="2" string="7" />
            <token id="3" string="," />
            <token id="4" string="1983" />
          </tokens>
        </entity>
        <entity id="2" string="Ray Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Ray" />
            <token id="7" string="Buckey" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="150" has_coreference="true">
      <content>March 22, 1984 -- Buckey and six others, including his sister, mother and grandmother, are indicted on 115 counts of child molestation and conspiracy.</content>
      <tokens>
        <token id="1" string="March" lemma="March" stem="march" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="22" lemma="22" stem="22" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1984" lemma="1984" stem="1984" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="7" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="six" lemma="six" stem="six" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="9" string="others" lemma="other" stem="other" pos="NNS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="including" lemma="include" stem="includ" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="sister" lemma="sister" stem="sister" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="mother" lemma="mother" stem="mother" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="16" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="grandmother" lemma="grandmother" stem="grandmoth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="18" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="are" lemma="be" stem="ar" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="indicted" lemma="indict" stem="indict" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="115" lemma="115" stem="115" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="23" string="counts" lemma="count" stem="count" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="child" lemma="child" stem="child" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="26" string="molestation" lemma="molestation" stem="molest" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="27" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="conspiracy" lemma="conspiracy" stem="conspiraci" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="29" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NP (NNP March) (CD 22) (, ,) (CD 1984)) (: --) (NP (NP (NNP Buckey)) (CC and) (NP (CD six) (NNS others)))) (, ,) (PP (VBG including) (NP (NP (PRP$ his) (NN sister)) (, ,) (NP (NN mother)) (CC and) (NP (NN grandmother)))) (, ,)) (VP (VBP are) (VP (VBN indicted) (PP (IN on) (NP (NP (CD 115) (NNS counts)) (PP (IN of) (NP (NN child) (NN molestation) (CC and) (NN conspiracy))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="March 22 , 1984" type="NP">
          <tokens>
            <token id="1" string="March" />
            <token id="2" string="22" />
            <token id="3" string="," />
            <token id="4" string="1984" />
          </tokens>
        </chunking>
        <chunking id="2" string="his sister" type="NP">
          <tokens>
            <token id="12" string="his" />
            <token id="13" string="sister" />
          </tokens>
        </chunking>
        <chunking id="3" string="six others" type="NP">
          <tokens>
            <token id="8" string="six" />
            <token id="9" string="others" />
          </tokens>
        </chunking>
        <chunking id="4" string="March 22 , 1984 -- Buckey and six others , including his sister , mother and grandmother ," type="NP">
          <tokens>
            <token id="1" string="March" />
            <token id="2" string="22" />
            <token id="3" string="," />
            <token id="4" string="1984" />
            <token id="5" string="--" />
            <token id="6" string="Buckey" />
            <token id="7" string="and" />
            <token id="8" string="six" />
            <token id="9" string="others" />
            <token id="10" string="," />
            <token id="11" string="including" />
            <token id="12" string="his" />
            <token id="13" string="sister" />
            <token id="14" string="," />
            <token id="15" string="mother" />
            <token id="16" string="and" />
            <token id="17" string="grandmother" />
            <token id="18" string="," />
          </tokens>
        </chunking>
        <chunking id="5" string="Buckey and six others" type="NP">
          <tokens>
            <token id="6" string="Buckey" />
            <token id="7" string="and" />
            <token id="8" string="six" />
            <token id="9" string="others" />
          </tokens>
        </chunking>
        <chunking id="6" string="March 22 , 1984 -- Buckey and six others" type="NP">
          <tokens>
            <token id="1" string="March" />
            <token id="2" string="22" />
            <token id="3" string="," />
            <token id="4" string="1984" />
            <token id="5" string="--" />
            <token id="6" string="Buckey" />
            <token id="7" string="and" />
            <token id="8" string="six" />
            <token id="9" string="others" />
          </tokens>
        </chunking>
        <chunking id="7" string="grandmother" type="NP">
          <tokens>
            <token id="17" string="grandmother" />
          </tokens>
        </chunking>
        <chunking id="8" string="child molestation and conspiracy" type="NP">
          <tokens>
            <token id="25" string="child" />
            <token id="26" string="molestation" />
            <token id="27" string="and" />
            <token id="28" string="conspiracy" />
          </tokens>
        </chunking>
        <chunking id="9" string="115 counts of child molestation and conspiracy" type="NP">
          <tokens>
            <token id="22" string="115" />
            <token id="23" string="counts" />
            <token id="24" string="of" />
            <token id="25" string="child" />
            <token id="26" string="molestation" />
            <token id="27" string="and" />
            <token id="28" string="conspiracy" />
          </tokens>
        </chunking>
        <chunking id="10" string="indicted on 115 counts of child molestation and conspiracy" type="VP">
          <tokens>
            <token id="20" string="indicted" />
            <token id="21" string="on" />
            <token id="22" string="115" />
            <token id="23" string="counts" />
            <token id="24" string="of" />
            <token id="25" string="child" />
            <token id="26" string="molestation" />
            <token id="27" string="and" />
            <token id="28" string="conspiracy" />
          </tokens>
        </chunking>
        <chunking id="11" string="his sister , mother and grandmother" type="NP">
          <tokens>
            <token id="12" string="his" />
            <token id="13" string="sister" />
            <token id="14" string="," />
            <token id="15" string="mother" />
            <token id="16" string="and" />
            <token id="17" string="grandmother" />
          </tokens>
        </chunking>
        <chunking id="12" string="mother" type="NP">
          <tokens>
            <token id="15" string="mother" />
          </tokens>
        </chunking>
        <chunking id="13" string="Buckey" type="NP">
          <tokens>
            <token id="6" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="14" string="115 counts" type="NP">
          <tokens>
            <token id="22" string="115" />
            <token id="23" string="counts" />
          </tokens>
        </chunking>
        <chunking id="15" string="are indicted on 115 counts of child molestation and conspiracy" type="VP">
          <tokens>
            <token id="19" string="are" />
            <token id="20" string="indicted" />
            <token id="21" string="on" />
            <token id="22" string="115" />
            <token id="23" string="counts" />
            <token id="24" string="of" />
            <token id="25" string="child" />
            <token id="26" string="molestation" />
            <token id="27" string="and" />
            <token id="28" string="conspiracy" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="20">indicted</governor>
          <dependent id="1">March</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">March</governor>
          <dependent id="2">22</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">March</governor>
          <dependent id="4">1984</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="1">March</governor>
          <dependent id="6">Buckey</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="6">Buckey</governor>
          <dependent id="7">and</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="9">others</governor>
          <dependent id="8">six</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="6">Buckey</governor>
          <dependent id="9">others</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">sister</governor>
          <dependent id="11">including</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="13">sister</governor>
          <dependent id="12">his</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="1">March</governor>
          <dependent id="13">sister</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="13">sister</governor>
          <dependent id="15">mother</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="13">sister</governor>
          <dependent id="16">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="13">sister</governor>
          <dependent id="17">grandmother</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="20">indicted</governor>
          <dependent id="19">are</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="20">indicted</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">counts</governor>
          <dependent id="21">on</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="23">counts</governor>
          <dependent id="22">115</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">indicted</governor>
          <dependent id="23">counts</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">molestation</governor>
          <dependent id="24">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="26">molestation</governor>
          <dependent id="25">child</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">counts</governor>
          <dependent id="26">molestation</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="26">molestation</governor>
          <dependent id="27">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="26">molestation</governor>
          <dependent id="28">conspiracy</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="March 22 , 1984" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="March" />
            <token id="2" string="22" />
            <token id="3" string="," />
            <token id="4" string="1984" />
          </tokens>
        </entity>
        <entity id="2" string="six" type="NUMBER" score="0.0">
          <tokens>
            <token id="8" string="six" />
          </tokens>
        </entity>
        <entity id="3" string="Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Buckey" />
          </tokens>
        </entity>
        <entity id="4" string="115" type="NUMBER" score="0.0">
          <tokens>
            <token id="22" string="115" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="151" has_coreference="true">
      <content>May 23, 1984 -- District attorney&amp;apost;s office files 208-count complaint that supersedes indictment.</content>
      <tokens>
        <token id="1" string="May" lemma="May" stem="mai" pos="NNP" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="23" lemma="23" stem="23" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1984" lemma="1984" stem="1984" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="District" lemma="District" stem="district" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="attorney" lemma="attorney" stem="attornei" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="office" lemma="office" stem="offic" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="files" lemma="file" stem="file" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="208-count" lemma="208-count" stem="208-count" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="complaint" lemma="complaint" stem="complaint" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="supersedes" lemma="supersede" stem="supersed" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="indictment" lemma="indictment" stem="indict" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP May) (CD 23) (, ,) (CD 1984)) (: --) (NP (NP (NNP District)) (PP (NP (NP (NN attorney) (POS 's)) (NN office) (NNS files)) (NP (NP (JJ 208-count) (NN complaint)) (SBAR (WHNP (WDT that)) (S (VP (VBZ supersedes) (NP (NN indictment)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="attorney 's office files" type="NP">
          <tokens>
            <token id="7" string="attorney" />
            <token id="8" string="'s" />
            <token id="9" string="office" />
            <token id="10" string="files" />
          </tokens>
        </chunking>
        <chunking id="2" string="208-count complaint" type="NP">
          <tokens>
            <token id="11" string="208-count" />
            <token id="12" string="complaint" />
          </tokens>
        </chunking>
        <chunking id="3" string="that supersedes indictment" type="SBAR">
          <tokens>
            <token id="13" string="that" />
            <token id="14" string="supersedes" />
            <token id="15" string="indictment" />
          </tokens>
        </chunking>
        <chunking id="4" string="District attorney 's office files 208-count complaint that supersedes indictment" type="NP">
          <tokens>
            <token id="6" string="District" />
            <token id="7" string="attorney" />
            <token id="8" string="'s" />
            <token id="9" string="office" />
            <token id="10" string="files" />
            <token id="11" string="208-count" />
            <token id="12" string="complaint" />
            <token id="13" string="that" />
            <token id="14" string="supersedes" />
            <token id="15" string="indictment" />
          </tokens>
        </chunking>
        <chunking id="5" string="208-count complaint that supersedes indictment" type="NP">
          <tokens>
            <token id="11" string="208-count" />
            <token id="12" string="complaint" />
            <token id="13" string="that" />
            <token id="14" string="supersedes" />
            <token id="15" string="indictment" />
          </tokens>
        </chunking>
        <chunking id="6" string="May 23 , 1984 -- District attorney 's office files 208-count complaint that supersedes indictment ." type="NP">
          <tokens>
            <token id="1" string="May" />
            <token id="2" string="23" />
            <token id="3" string="," />
            <token id="4" string="1984" />
            <token id="5" string="--" />
            <token id="6" string="District" />
            <token id="7" string="attorney" />
            <token id="8" string="'s" />
            <token id="9" string="office" />
            <token id="10" string="files" />
            <token id="11" string="208-count" />
            <token id="12" string="complaint" />
            <token id="13" string="that" />
            <token id="14" string="supersedes" />
            <token id="15" string="indictment" />
            <token id="16" string="." />
          </tokens>
        </chunking>
        <chunking id="7" string="May 23 , 1984" type="NP">
          <tokens>
            <token id="1" string="May" />
            <token id="2" string="23" />
            <token id="3" string="," />
            <token id="4" string="1984" />
          </tokens>
        </chunking>
        <chunking id="8" string="attorney 's" type="NP">
          <tokens>
            <token id="7" string="attorney" />
            <token id="8" string="'s" />
          </tokens>
        </chunking>
        <chunking id="9" string="supersedes indictment" type="VP">
          <tokens>
            <token id="14" string="supersedes" />
            <token id="15" string="indictment" />
          </tokens>
        </chunking>
        <chunking id="10" string="indictment" type="NP">
          <tokens>
            <token id="15" string="indictment" />
          </tokens>
        </chunking>
        <chunking id="11" string="District" type="NP">
          <tokens>
            <token id="6" string="District" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">May</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">May</governor>
          <dependent id="2">23</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">May</governor>
          <dependent id="4">1984</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="1">May</governor>
          <dependent id="6">District</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="10">files</governor>
          <dependent id="7">attorney</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">attorney</governor>
          <dependent id="8">'s</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">files</governor>
          <dependent id="9">office</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">District</governor>
          <dependent id="10">files</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">complaint</governor>
          <dependent id="11">208-count</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="10">files</governor>
          <dependent id="12">complaint</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">supersedes</governor>
          <dependent id="13">that</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="12">complaint</governor>
          <dependent id="14">supersedes</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="14">supersedes</governor>
          <dependent id="15">indictment</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="May 23 , 1984" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="May" />
            <token id="2" string="23" />
            <token id="3" string="," />
            <token id="4" string="1984" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="152" has_coreference="false">
      <content>June 8, 1984 -- Preliminary hearing for Ray Buckey begins.</content>
      <tokens>
        <token id="1" string="June" lemma="June" stem="june" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="8" lemma="8" stem="8" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1984" lemma="1984" stem="1984" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Preliminary" lemma="preliminary" stem="preliminari" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="hearing" lemma="hearing" stem="hear" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="10" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="11" string="begins" lemma="begin" stem="begin" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP June) (CD 8) (, ,) (CD 1984)) (: --) (NP (NP (JJ Preliminary) (NN hearing)) (PP (IN for) (NP (NP (NNP Ray) (NNP Buckey)) (VP (VBZ begins))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Preliminary hearing for Ray Buckey begins" type="NP">
          <tokens>
            <token id="6" string="Preliminary" />
            <token id="7" string="hearing" />
            <token id="8" string="for" />
            <token id="9" string="Ray" />
            <token id="10" string="Buckey" />
            <token id="11" string="begins" />
          </tokens>
        </chunking>
        <chunking id="2" string="Preliminary hearing" type="NP">
          <tokens>
            <token id="6" string="Preliminary" />
            <token id="7" string="hearing" />
          </tokens>
        </chunking>
        <chunking id="3" string="begins" type="VP">
          <tokens>
            <token id="11" string="begins" />
          </tokens>
        </chunking>
        <chunking id="4" string="June 8 , 1984 -- Preliminary hearing for Ray Buckey begins ." type="NP">
          <tokens>
            <token id="1" string="June" />
            <token id="2" string="8" />
            <token id="3" string="," />
            <token id="4" string="1984" />
            <token id="5" string="--" />
            <token id="6" string="Preliminary" />
            <token id="7" string="hearing" />
            <token id="8" string="for" />
            <token id="9" string="Ray" />
            <token id="10" string="Buckey" />
            <token id="11" string="begins" />
            <token id="12" string="." />
          </tokens>
        </chunking>
        <chunking id="5" string="June 8 , 1984" type="NP">
          <tokens>
            <token id="1" string="June" />
            <token id="2" string="8" />
            <token id="3" string="," />
            <token id="4" string="1984" />
          </tokens>
        </chunking>
        <chunking id="6" string="Ray Buckey begins" type="NP">
          <tokens>
            <token id="9" string="Ray" />
            <token id="10" string="Buckey" />
            <token id="11" string="begins" />
          </tokens>
        </chunking>
        <chunking id="7" string="Ray Buckey" type="NP">
          <tokens>
            <token id="9" string="Ray" />
            <token id="10" string="Buckey" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">June</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">June</governor>
          <dependent id="2">8</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">June</governor>
          <dependent id="4">1984</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">hearing</governor>
          <dependent id="6">Preliminary</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="1">June</governor>
          <dependent id="7">hearing</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">Buckey</governor>
          <dependent id="8">for</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">Buckey</governor>
          <dependent id="9">Ray</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">hearing</governor>
          <dependent id="10">Buckey</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="10">Buckey</governor>
          <dependent id="11">begins</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="June 8 , 1984" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="June" />
            <token id="2" string="8" />
            <token id="3" string="," />
            <token id="4" string="1984" />
          </tokens>
        </entity>
        <entity id="2" string="Ray Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="9" string="Ray" />
            <token id="10" string="Buckey" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="153" has_coreference="false">
      <content>Aug. 17, 1984 -- Consolidated preliminary hearing for seven defendants begins.</content>
      <tokens>
        <token id="1" string="Aug." lemma="Aug." stem="aug." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="17" lemma="17" stem="17" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1984" lemma="1984" stem="1984" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Consolidated" lemma="Consolidated" stem="consolid" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="preliminary" lemma="preliminary" stem="preliminari" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="hearing" lemma="hearing" stem="hear" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="seven" lemma="seven" stem="seven" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="11" string="defendants" lemma="defendant" stem="defend" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="begins" lemma="begin" stem="begin" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP Aug.) (CD 17) (, ,) (CD 1984)) (: --) (NP (NP (NNP Consolidated) (JJ preliminary) (NN hearing)) (PP (IN for) (NP (NP (CD seven) (NNS defendants)) (VP (VBZ begins))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="begins" type="VP">
          <tokens>
            <token id="12" string="begins" />
          </tokens>
        </chunking>
        <chunking id="2" string="seven defendants" type="NP">
          <tokens>
            <token id="10" string="seven" />
            <token id="11" string="defendants" />
          </tokens>
        </chunking>
        <chunking id="3" string="Consolidated preliminary hearing" type="NP">
          <tokens>
            <token id="6" string="Consolidated" />
            <token id="7" string="preliminary" />
            <token id="8" string="hearing" />
          </tokens>
        </chunking>
        <chunking id="4" string="Consolidated preliminary hearing for seven defendants begins" type="NP">
          <tokens>
            <token id="6" string="Consolidated" />
            <token id="7" string="preliminary" />
            <token id="8" string="hearing" />
            <token id="9" string="for" />
            <token id="10" string="seven" />
            <token id="11" string="defendants" />
            <token id="12" string="begins" />
          </tokens>
        </chunking>
        <chunking id="5" string="seven defendants begins" type="NP">
          <tokens>
            <token id="10" string="seven" />
            <token id="11" string="defendants" />
            <token id="12" string="begins" />
          </tokens>
        </chunking>
        <chunking id="6" string="Aug. 17 , 1984 -- Consolidated preliminary hearing for seven defendants begins ." type="NP">
          <tokens>
            <token id="1" string="Aug." />
            <token id="2" string="17" />
            <token id="3" string="," />
            <token id="4" string="1984" />
            <token id="5" string="--" />
            <token id="6" string="Consolidated" />
            <token id="7" string="preliminary" />
            <token id="8" string="hearing" />
            <token id="9" string="for" />
            <token id="10" string="seven" />
            <token id="11" string="defendants" />
            <token id="12" string="begins" />
            <token id="13" string="." />
          </tokens>
        </chunking>
        <chunking id="7" string="Aug. 17 , 1984" type="NP">
          <tokens>
            <token id="1" string="Aug." />
            <token id="2" string="17" />
            <token id="3" string="," />
            <token id="4" string="1984" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Aug.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Aug.</governor>
          <dependent id="2">17</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Aug.</governor>
          <dependent id="4">1984</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">hearing</governor>
          <dependent id="6">Consolidated</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">hearing</governor>
          <dependent id="7">preliminary</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="1">Aug.</governor>
          <dependent id="8">hearing</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">defendants</governor>
          <dependent id="9">for</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="11">defendants</governor>
          <dependent id="10">seven</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">hearing</governor>
          <dependent id="11">defendants</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="11">defendants</governor>
          <dependent id="12">begins</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="seven" type="NUMBER" score="0.0">
          <tokens>
            <token id="10" string="seven" />
          </tokens>
        </entity>
        <entity id="2" string="Aug. 17 , 1984" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Aug." />
            <token id="2" string="17" />
            <token id="3" string="," />
            <token id="4" string="1984" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="154" has_coreference="false">
      <content>October, 1984 -- Sheriff&amp;apost;s task force is created to investigate &amp;quot;uncharged McMartin suspects&amp;quot; and other South Bay nursery schools.</content>
      <tokens>
        <token id="1" string="October" lemma="October" stem="october" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="1984" lemma="1984" stem="1984" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="Sheriff" lemma="Sheriff" stem="sheriff" pos="NNP" type="Word" isStopWord="false" ner="TITLE" is_referenced="false" is_refers="false" />
        <token id="6" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="task" lemma="task" stem="task" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="force" lemma="force" stem="forc" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="created" lemma="create" stem="creat" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="investigate" lemma="investigate" stem="investig" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="uncharged" lemma="uncharged" stem="uncharg" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="McMartin" lemma="mcmartin" stem="mcmartin" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="suspects" lemma="suspect" stem="suspect" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="other" lemma="other" stem="other" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="South" lemma="South" stem="south" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="21" string="Bay" lemma="Bay" stem="bai" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="22" string="nursery" lemma="nursery" stem="nurseri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="schools" lemma="school" stem="school" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP October)) (, ,) (NP (NP (CD 1984)) (: --) (NP (NP (NNP Sheriff) (POS 's)) (NN task) (NN force)))) (VP (VBZ is) (VP (VBN created) (S (VP (TO to) (VP (VB investigate) (NP (NP (`` ``) (JJ uncharged) (NN McMartin) (NNS suspects) ('' '')) (CC and) (NP (JJ other) (NNP South) (NNP Bay) (NN nursery) (NNS schools)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="to investigate `` uncharged McMartin suspects '' and other South Bay nursery schools" type="VP">
          <tokens>
            <token id="11" string="to" />
            <token id="12" string="investigate" />
            <token id="13" string="&quot;" />
            <token id="14" string="uncharged" />
            <token id="15" string="McMartin" />
            <token id="16" string="suspects" />
            <token id="17" string="&quot;" />
            <token id="18" string="and" />
            <token id="19" string="other" />
            <token id="20" string="South" />
            <token id="21" string="Bay" />
            <token id="22" string="nursery" />
            <token id="23" string="schools" />
          </tokens>
        </chunking>
        <chunking id="2" string="other South Bay nursery schools" type="NP">
          <tokens>
            <token id="19" string="other" />
            <token id="20" string="South" />
            <token id="21" string="Bay" />
            <token id="22" string="nursery" />
            <token id="23" string="schools" />
          </tokens>
        </chunking>
        <chunking id="3" string="`` uncharged McMartin suspects ''" type="NP">
          <tokens>
            <token id="13" string="&quot;" />
            <token id="14" string="uncharged" />
            <token id="15" string="McMartin" />
            <token id="16" string="suspects" />
            <token id="17" string="&quot;" />
          </tokens>
        </chunking>
        <chunking id="4" string="October" type="NP">
          <tokens>
            <token id="1" string="October" />
          </tokens>
        </chunking>
        <chunking id="5" string="1984 -- Sheriff 's task force" type="NP">
          <tokens>
            <token id="3" string="1984" />
            <token id="4" string="--" />
            <token id="5" string="Sheriff" />
            <token id="6" string="'s" />
            <token id="7" string="task" />
            <token id="8" string="force" />
          </tokens>
        </chunking>
        <chunking id="6" string="1984" type="NP">
          <tokens>
            <token id="3" string="1984" />
          </tokens>
        </chunking>
        <chunking id="7" string="Sheriff 's" type="NP">
          <tokens>
            <token id="5" string="Sheriff" />
            <token id="6" string="'s" />
          </tokens>
        </chunking>
        <chunking id="8" string="is created to investigate `` uncharged McMartin suspects '' and other South Bay nursery schools" type="VP">
          <tokens>
            <token id="9" string="is" />
            <token id="10" string="created" />
            <token id="11" string="to" />
            <token id="12" string="investigate" />
            <token id="13" string="&quot;" />
            <token id="14" string="uncharged" />
            <token id="15" string="McMartin" />
            <token id="16" string="suspects" />
            <token id="17" string="&quot;" />
            <token id="18" string="and" />
            <token id="19" string="other" />
            <token id="20" string="South" />
            <token id="21" string="Bay" />
            <token id="22" string="nursery" />
            <token id="23" string="schools" />
          </tokens>
        </chunking>
        <chunking id="9" string="investigate `` uncharged McMartin suspects '' and other South Bay nursery schools" type="VP">
          <tokens>
            <token id="12" string="investigate" />
            <token id="13" string="&quot;" />
            <token id="14" string="uncharged" />
            <token id="15" string="McMartin" />
            <token id="16" string="suspects" />
            <token id="17" string="&quot;" />
            <token id="18" string="and" />
            <token id="19" string="other" />
            <token id="20" string="South" />
            <token id="21" string="Bay" />
            <token id="22" string="nursery" />
            <token id="23" string="schools" />
          </tokens>
        </chunking>
        <chunking id="10" string="Sheriff 's task force" type="NP">
          <tokens>
            <token id="5" string="Sheriff" />
            <token id="6" string="'s" />
            <token id="7" string="task" />
            <token id="8" string="force" />
          </tokens>
        </chunking>
        <chunking id="11" string="created to investigate `` uncharged McMartin suspects '' and other South Bay nursery schools" type="VP">
          <tokens>
            <token id="10" string="created" />
            <token id="11" string="to" />
            <token id="12" string="investigate" />
            <token id="13" string="&quot;" />
            <token id="14" string="uncharged" />
            <token id="15" string="McMartin" />
            <token id="16" string="suspects" />
            <token id="17" string="&quot;" />
            <token id="18" string="and" />
            <token id="19" string="other" />
            <token id="20" string="South" />
            <token id="21" string="Bay" />
            <token id="22" string="nursery" />
            <token id="23" string="schools" />
          </tokens>
        </chunking>
        <chunking id="12" string="October , 1984 -- Sheriff 's task force" type="NP">
          <tokens>
            <token id="1" string="October" />
            <token id="2" string="," />
            <token id="3" string="1984" />
            <token id="4" string="--" />
            <token id="5" string="Sheriff" />
            <token id="6" string="'s" />
            <token id="7" string="task" />
            <token id="8" string="force" />
          </tokens>
        </chunking>
        <chunking id="13" string="`` uncharged McMartin suspects '' and other South Bay nursery schools" type="NP">
          <tokens>
            <token id="13" string="&quot;" />
            <token id="14" string="uncharged" />
            <token id="15" string="McMartin" />
            <token id="16" string="suspects" />
            <token id="17" string="&quot;" />
            <token id="18" string="and" />
            <token id="19" string="other" />
            <token id="20" string="South" />
            <token id="21" string="Bay" />
            <token id="22" string="nursery" />
            <token id="23" string="schools" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="10">created</governor>
          <dependent id="1">October</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="1">October</governor>
          <dependent id="3">1984</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="8">force</governor>
          <dependent id="5">Sheriff</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">Sheriff</governor>
          <dependent id="6">'s</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">force</governor>
          <dependent id="7">task</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="3">1984</governor>
          <dependent id="8">force</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="10">created</governor>
          <dependent id="9">is</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="10">created</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="12">investigate</governor>
          <dependent id="11">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="10">created</governor>
          <dependent id="12">investigate</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="16">suspects</governor>
          <dependent id="14">uncharged</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="16">suspects</governor>
          <dependent id="15">McMartin</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="12">investigate</governor>
          <dependent id="16">suspects</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="16">suspects</governor>
          <dependent id="18">and</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="23">schools</governor>
          <dependent id="19">other</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="23">schools</governor>
          <dependent id="20">South</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="23">schools</governor>
          <dependent id="21">Bay</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="23">schools</governor>
          <dependent id="22">nursery</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="16">suspects</governor>
          <dependent id="23">schools</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="South Bay" type="LOCATION" score="0.0">
          <tokens>
            <token id="20" string="South" />
            <token id="21" string="Bay" />
          </tokens>
        </entity>
        <entity id="2" string="Sheriff" type="TITLE" score="0.0">
          <tokens>
            <token id="5" string="Sheriff" />
          </tokens>
        </entity>
        <entity id="3" string="October , 1984" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="October" />
            <token id="2" string="," />
            <token id="3" string="1984" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="155" has_coreference="false">
      <content>Jan. 9, 1986 -- Municipal Court Judge Aviva Bobb orders all seven defendants to stand trial.</content>
      <tokens>
        <token id="1" string="Jan." lemma="Jan." stem="jan." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="9" lemma="9" stem="9" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1986" lemma="1986" stem="1986" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Municipal" lemma="Municipal" stem="municip" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="Court" lemma="Court" stem="court" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="Judge" lemma="Judge" stem="judg" pos="NNP" type="Word" isStopWord="false" ner="TITLE" is_referenced="false" is_refers="false" />
        <token id="9" string="Aviva" lemma="Aviva" stem="aviva" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="10" string="Bobb" lemma="Bobb" stem="bobb" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="11" string="orders" lemma="order" stem="order" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="seven" lemma="seven" stem="seven" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="14" string="defendants" lemma="defendant" stem="defend" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="stand" lemma="stand" stem="stand" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="trial" lemma="trial" stem="trial" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP Jan.) (CD 9) (, ,) (CD 1986)) (: --) (NP (NP (NNP Municipal) (NNP Court) (NNP Judge) (NNP Aviva) (NNP Bobb) (NNS orders)) (SBAR (S (NP (DT all) (CD seven) (NNS defendants)) (VP (TO to) (VP (VB stand) (NP (NN trial))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Municipal Court Judge Aviva Bobb orders all seven defendants to stand trial" type="NP">
          <tokens>
            <token id="6" string="Municipal" />
            <token id="7" string="Court" />
            <token id="8" string="Judge" />
            <token id="9" string="Aviva" />
            <token id="10" string="Bobb" />
            <token id="11" string="orders" />
            <token id="12" string="all" />
            <token id="13" string="seven" />
            <token id="14" string="defendants" />
            <token id="15" string="to" />
            <token id="16" string="stand" />
            <token id="17" string="trial" />
          </tokens>
        </chunking>
        <chunking id="2" string="Municipal Court Judge Aviva Bobb orders" type="NP">
          <tokens>
            <token id="6" string="Municipal" />
            <token id="7" string="Court" />
            <token id="8" string="Judge" />
            <token id="9" string="Aviva" />
            <token id="10" string="Bobb" />
            <token id="11" string="orders" />
          </tokens>
        </chunking>
        <chunking id="3" string="Jan. 9 , 1986" type="NP">
          <tokens>
            <token id="1" string="Jan." />
            <token id="2" string="9" />
            <token id="3" string="," />
            <token id="4" string="1986" />
          </tokens>
        </chunking>
        <chunking id="4" string="all seven defendants to stand trial" type="SBAR">
          <tokens>
            <token id="12" string="all" />
            <token id="13" string="seven" />
            <token id="14" string="defendants" />
            <token id="15" string="to" />
            <token id="16" string="stand" />
            <token id="17" string="trial" />
          </tokens>
        </chunking>
        <chunking id="5" string="all seven defendants" type="NP">
          <tokens>
            <token id="12" string="all" />
            <token id="13" string="seven" />
            <token id="14" string="defendants" />
          </tokens>
        </chunking>
        <chunking id="6" string="Jan. 9 , 1986 -- Municipal Court Judge Aviva Bobb orders all seven defendants to stand trial ." type="NP">
          <tokens>
            <token id="1" string="Jan." />
            <token id="2" string="9" />
            <token id="3" string="," />
            <token id="4" string="1986" />
            <token id="5" string="--" />
            <token id="6" string="Municipal" />
            <token id="7" string="Court" />
            <token id="8" string="Judge" />
            <token id="9" string="Aviva" />
            <token id="10" string="Bobb" />
            <token id="11" string="orders" />
            <token id="12" string="all" />
            <token id="13" string="seven" />
            <token id="14" string="defendants" />
            <token id="15" string="to" />
            <token id="16" string="stand" />
            <token id="17" string="trial" />
            <token id="18" string="." />
          </tokens>
        </chunking>
        <chunking id="7" string="trial" type="NP">
          <tokens>
            <token id="17" string="trial" />
          </tokens>
        </chunking>
        <chunking id="8" string="stand trial" type="VP">
          <tokens>
            <token id="16" string="stand" />
            <token id="17" string="trial" />
          </tokens>
        </chunking>
        <chunking id="9" string="to stand trial" type="VP">
          <tokens>
            <token id="15" string="to" />
            <token id="16" string="stand" />
            <token id="17" string="trial" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Jan.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Jan.</governor>
          <dependent id="2">9</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Jan.</governor>
          <dependent id="4">1986</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">orders</governor>
          <dependent id="6">Municipal</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">orders</governor>
          <dependent id="7">Court</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">orders</governor>
          <dependent id="8">Judge</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">orders</governor>
          <dependent id="9">Aviva</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">orders</governor>
          <dependent id="10">Bobb</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="1">Jan.</governor>
          <dependent id="11">orders</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">defendants</governor>
          <dependent id="12">all</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="14">defendants</governor>
          <dependent id="13">seven</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">stand</governor>
          <dependent id="14">defendants</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="16">stand</governor>
          <dependent id="15">to</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="11">orders</governor>
          <dependent id="16">stand</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="16">stand</governor>
          <dependent id="17">trial</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Jan. 9 , 1986" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Jan." />
            <token id="2" string="9" />
            <token id="3" string="," />
            <token id="4" string="1986" />
          </tokens>
        </entity>
        <entity id="2" string="seven" type="NUMBER" score="0.0">
          <tokens>
            <token id="13" string="seven" />
          </tokens>
        </entity>
        <entity id="3" string="Aviva Bobb" type="PERSON" score="0.0">
          <tokens>
            <token id="9" string="Aviva" />
            <token id="10" string="Bobb" />
          </tokens>
        </entity>
        <entity id="4" string="Judge" type="TITLE" score="0.0">
          <tokens>
            <token id="8" string="Judge" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="156" has_coreference="true">
      <content>Jan. 17, 1986 -- District Atty. Ira Reiner drops charges against five of the seven defendants.</content>
      <tokens>
        <token id="1" string="Jan." lemma="Jan." stem="jan." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="2" string="17" lemma="17" stem="17" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="4" string="1986" lemma="1986" stem="1986" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="6" string="District" lemma="District" stem="district" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="Atty." lemma="Atty." stem="atty." pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="Ira" lemma="Ira" stem="ira" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="9" string="Reiner" lemma="Reiner" stem="reiner" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="10" string="drops" lemma="drop" stem="drop" pos="VBZ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="charges" lemma="charge" stem="charg" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="against" lemma="against" stem="against" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="five" lemma="five" stem="five" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="true" />
        <token id="14" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="16" string="seven" lemma="seven" stem="seven" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="17" string="defendants" lemma="defendant" stem="defend" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="18" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP Jan.) (CD 17) (, ,) (CD 1986)) (: --) (S (NP (NNP District) (NNP Atty.) (NNP Ira) (NNP Reiner)) (VP (VBZ drops) (NP (NP (NNS charges)) (PP (IN against) (NP (CD five))) (PP (IN of) (NP (DT the) (CD seven) (NNS defendants)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="charges against five of the seven defendants" type="NP">
          <tokens>
            <token id="11" string="charges" />
            <token id="12" string="against" />
            <token id="13" string="five" />
            <token id="14" string="of" />
            <token id="15" string="the" />
            <token id="16" string="seven" />
            <token id="17" string="defendants" />
          </tokens>
        </chunking>
        <chunking id="2" string="charges" type="NP">
          <tokens>
            <token id="11" string="charges" />
          </tokens>
        </chunking>
        <chunking id="3" string="the seven defendants" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="seven" />
            <token id="17" string="defendants" />
          </tokens>
        </chunking>
        <chunking id="4" string="Jan. 17 , 1986" type="NP">
          <tokens>
            <token id="1" string="Jan." />
            <token id="2" string="17" />
            <token id="3" string="," />
            <token id="4" string="1986" />
          </tokens>
        </chunking>
        <chunking id="5" string="drops charges against five of the seven defendants" type="VP">
          <tokens>
            <token id="10" string="drops" />
            <token id="11" string="charges" />
            <token id="12" string="against" />
            <token id="13" string="five" />
            <token id="14" string="of" />
            <token id="15" string="the" />
            <token id="16" string="seven" />
            <token id="17" string="defendants" />
          </tokens>
        </chunking>
        <chunking id="6" string="Jan. 17 , 1986 -- District Atty. Ira Reiner drops charges against five of the seven defendants ." type="NP">
          <tokens>
            <token id="1" string="Jan." />
            <token id="2" string="17" />
            <token id="3" string="," />
            <token id="4" string="1986" />
            <token id="5" string="--" />
            <token id="6" string="District" />
            <token id="7" string="Atty." />
            <token id="8" string="Ira" />
            <token id="9" string="Reiner" />
            <token id="10" string="drops" />
            <token id="11" string="charges" />
            <token id="12" string="against" />
            <token id="13" string="five" />
            <token id="14" string="of" />
            <token id="15" string="the" />
            <token id="16" string="seven" />
            <token id="17" string="defendants" />
            <token id="18" string="." />
          </tokens>
        </chunking>
        <chunking id="7" string="five" type="NP">
          <tokens>
            <token id="13" string="five" />
          </tokens>
        </chunking>
        <chunking id="8" string="District Atty. Ira Reiner" type="NP">
          <tokens>
            <token id="6" string="District" />
            <token id="7" string="Atty." />
            <token id="8" string="Ira" />
            <token id="9" string="Reiner" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Jan.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Jan.</governor>
          <dependent id="2">17</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Jan.</governor>
          <dependent id="4">1986</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">Reiner</governor>
          <dependent id="6">District</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">Reiner</governor>
          <dependent id="7">Atty.</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">Reiner</governor>
          <dependent id="8">Ira</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">drops</governor>
          <dependent id="9">Reiner</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="1">Jan.</governor>
          <dependent id="10">drops</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="10">drops</governor>
          <dependent id="11">charges</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">five</governor>
          <dependent id="12">against</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">charges</governor>
          <dependent id="13">five</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">defendants</governor>
          <dependent id="14">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="17">defendants</governor>
          <dependent id="15">the</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="17">defendants</governor>
          <dependent id="16">seven</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">charges</governor>
          <dependent id="17">defendants</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Jan. 17 , 1986" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Jan." />
            <token id="2" string="17" />
            <token id="3" string="," />
            <token id="4" string="1986" />
          </tokens>
        </entity>
        <entity id="2" string="seven" type="NUMBER" score="0.0">
          <tokens>
            <token id="16" string="seven" />
          </tokens>
        </entity>
        <entity id="3" string="Ira Reiner" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Ira" />
            <token id="9" string="Reiner" />
          </tokens>
        </entity>
        <entity id="4" string="five" type="NUMBER" score="0.0">
          <tokens>
            <token id="13" string="five" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="157" has_coreference="true">
      <content>Dec. 19, 1986 -- Mother whose allegations triggered investigation is found dead in her home.</content>
      <tokens>
        <token id="1" string="Dec." lemma="Dec." stem="dec." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="19" lemma="19" stem="19" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1986" lemma="1986" stem="1986" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Mother" lemma="Mother" stem="mother" pos="NNP" type="Word" isStopWord="false" ner="TITLE" is_referenced="false" is_refers="false" />
        <token id="7" string="whose" lemma="whose" stem="whose" pos="WP$" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="allegations" lemma="allegation" stem="alleg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="9" string="triggered" lemma="trigger" stem="trigger" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="investigation" lemma="investigation" stem="investig" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="found" lemma="find" stem="found" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="dead" lemma="dead" stem="dead" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="home" lemma="home" stem="home" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP Dec.) (CD 19) (, ,) (CD 1986)) (: --) (NP (NP (NNP Mother)) (SBAR (WP$ whose) (S (NP (NNS allegations)) (VP (VBD triggered) (SBAR (S (NP (NN investigation)) (VP (VBZ is) (VP (VBN found) (ADJP (JJ dead)) (PP (IN in) (NP (PRP$ her) (NN home))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="her home" type="NP">
          <tokens>
            <token id="15" string="her" />
            <token id="16" string="home" />
          </tokens>
        </chunking>
        <chunking id="2" string="triggered investigation is found dead in her home" type="VP">
          <tokens>
            <token id="9" string="triggered" />
            <token id="10" string="investigation" />
            <token id="11" string="is" />
            <token id="12" string="found" />
            <token id="13" string="dead" />
            <token id="14" string="in" />
            <token id="15" string="her" />
            <token id="16" string="home" />
          </tokens>
        </chunking>
        <chunking id="3" string="investigation is found dead in her home" type="SBAR">
          <tokens>
            <token id="10" string="investigation" />
            <token id="11" string="is" />
            <token id="12" string="found" />
            <token id="13" string="dead" />
            <token id="14" string="in" />
            <token id="15" string="her" />
            <token id="16" string="home" />
          </tokens>
        </chunking>
        <chunking id="4" string="found dead in her home" type="VP">
          <tokens>
            <token id="12" string="found" />
            <token id="13" string="dead" />
            <token id="14" string="in" />
            <token id="15" string="her" />
            <token id="16" string="home" />
          </tokens>
        </chunking>
        <chunking id="5" string="dead" type="ADJP">
          <tokens>
            <token id="13" string="dead" />
          </tokens>
        </chunking>
        <chunking id="6" string="Dec. 19 , 1986 -- Mother whose allegations triggered investigation is found dead in her home ." type="NP">
          <tokens>
            <token id="1" string="Dec." />
            <token id="2" string="19" />
            <token id="3" string="," />
            <token id="4" string="1986" />
            <token id="5" string="--" />
            <token id="6" string="Mother" />
            <token id="7" string="whose" />
            <token id="8" string="allegations" />
            <token id="9" string="triggered" />
            <token id="10" string="investigation" />
            <token id="11" string="is" />
            <token id="12" string="found" />
            <token id="13" string="dead" />
            <token id="14" string="in" />
            <token id="15" string="her" />
            <token id="16" string="home" />
            <token id="17" string="." />
          </tokens>
        </chunking>
        <chunking id="7" string="allegations" type="NP">
          <tokens>
            <token id="8" string="allegations" />
          </tokens>
        </chunking>
        <chunking id="8" string="Mother" type="NP">
          <tokens>
            <token id="6" string="Mother" />
          </tokens>
        </chunking>
        <chunking id="9" string="Dec. 19 , 1986" type="NP">
          <tokens>
            <token id="1" string="Dec." />
            <token id="2" string="19" />
            <token id="3" string="," />
            <token id="4" string="1986" />
          </tokens>
        </chunking>
        <chunking id="10" string="investigation" type="NP">
          <tokens>
            <token id="10" string="investigation" />
          </tokens>
        </chunking>
        <chunking id="11" string="whose allegations triggered investigation is found dead in her home" type="SBAR">
          <tokens>
            <token id="7" string="whose" />
            <token id="8" string="allegations" />
            <token id="9" string="triggered" />
            <token id="10" string="investigation" />
            <token id="11" string="is" />
            <token id="12" string="found" />
            <token id="13" string="dead" />
            <token id="14" string="in" />
            <token id="15" string="her" />
            <token id="16" string="home" />
          </tokens>
        </chunking>
        <chunking id="12" string="Mother whose allegations triggered investigation is found dead in her home" type="NP">
          <tokens>
            <token id="6" string="Mother" />
            <token id="7" string="whose" />
            <token id="8" string="allegations" />
            <token id="9" string="triggered" />
            <token id="10" string="investigation" />
            <token id="11" string="is" />
            <token id="12" string="found" />
            <token id="13" string="dead" />
            <token id="14" string="in" />
            <token id="15" string="her" />
            <token id="16" string="home" />
          </tokens>
        </chunking>
        <chunking id="13" string="is found dead in her home" type="VP">
          <tokens>
            <token id="11" string="is" />
            <token id="12" string="found" />
            <token id="13" string="dead" />
            <token id="14" string="in" />
            <token id="15" string="her" />
            <token id="16" string="home" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Dec.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Dec.</governor>
          <dependent id="2">19</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Dec.</governor>
          <dependent id="4">1986</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="1">Dec.</governor>
          <dependent id="6">Mother</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="9">triggered</governor>
          <dependent id="7">whose</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">triggered</governor>
          <dependent id="8">allegations</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="6">Mother</governor>
          <dependent id="9">triggered</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="12">found</governor>
          <dependent id="10">investigation</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="12">found</governor>
          <dependent id="11">is</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="9">triggered</governor>
          <dependent id="12">found</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="12">found</governor>
          <dependent id="13">dead</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">home</governor>
          <dependent id="14">in</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="16">home</governor>
          <dependent id="15">her</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">found</governor>
          <dependent id="16">home</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Mother" type="TITLE" score="0.0">
          <tokens>
            <token id="6" string="Mother" />
          </tokens>
        </entity>
        <entity id="2" string="Dec. 19 , 1986" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Dec." />
            <token id="2" string="19" />
            <token id="3" string="," />
            <token id="4" string="1986" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="158" has_coreference="false">
      <content>April 20, 1987 -- Jury selection begins.</content>
      <tokens>
        <token id="1" string="April" lemma="April" stem="april" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="20" lemma="20" stem="20" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1987" lemma="1987" stem="1987" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="selection" lemma="selection" stem="select" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="begins" lemma="begin" stem="begin" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP April) (CD 20) (, ,) (CD 1987)) (: --) (S (NP (NN Jury) (NN selection)) (VP (VBZ begins))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="begins" type="VP">
          <tokens>
            <token id="8" string="begins" />
          </tokens>
        </chunking>
        <chunking id="2" string="Jury selection" type="NP">
          <tokens>
            <token id="6" string="Jury" />
            <token id="7" string="selection" />
          </tokens>
        </chunking>
        <chunking id="3" string="April 20 , 1987 -- Jury selection begins ." type="NP">
          <tokens>
            <token id="1" string="April" />
            <token id="2" string="20" />
            <token id="3" string="," />
            <token id="4" string="1987" />
            <token id="5" string="--" />
            <token id="6" string="Jury" />
            <token id="7" string="selection" />
            <token id="8" string="begins" />
            <token id="9" string="." />
          </tokens>
        </chunking>
        <chunking id="4" string="April 20 , 1987" type="NP">
          <tokens>
            <token id="1" string="April" />
            <token id="2" string="20" />
            <token id="3" string="," />
            <token id="4" string="1987" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">April</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">April</governor>
          <dependent id="2">20</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">April</governor>
          <dependent id="4">1987</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">selection</governor>
          <dependent id="6">Jury</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">begins</governor>
          <dependent id="7">selection</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="1">April</governor>
          <dependent id="8">begins</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="April 20 , 1987" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="April" />
            <token id="2" string="20" />
            <token id="3" string="," />
            <token id="4" string="1987" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="159" has_coreference="true">
      <content>July 13, 1987 -- Trial of Ray Buckey and Peggy McMartin Buckey gets under way with opening statements, followed by presentation of prosecution&amp;apost;s case.</content>
      <tokens>
        <token id="1" string="July" lemma="July" stem="juli" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="13" lemma="13" stem="13" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1987" lemma="1987" stem="1987" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Trial" lemma="trial" stem="trial" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Ray" lemma="Ray" stem="rai" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="9" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="10" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="Peggy" lemma="Peggy" stem="peggi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="12" string="McMartin" lemma="McMartin" stem="mcmartin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="13" string="Buckey" lemma="Buckey" stem="buckei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="14" string="gets" lemma="get" stem="get" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="under" lemma="under" stem="under" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="way" lemma="way" stem="wai" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="opening" lemma="open" stem="open" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="statements" lemma="statement" stem="statement" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="followed" lemma="follow" stem="follow" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="presentation" lemma="presentation" stem="present" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="24" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="25" string="prosecution" lemma="prosecution" stem="prosecut" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="26" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="27" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="28" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP July) (CD 13) (, ,) (CD 1987)) (: --) (S (NP (NP (NN Trial)) (PP (IN of) (NP (NP (NNP Ray) (NNP Buckey)) (CC and) (NP (NNP Peggy) (NNP McMartin) (NNP Buckey))))) (VP (VBZ gets) (PP (IN under) (NP (NN way))) (PP (IN with) (NP (NP (VBG opening) (NNS statements)) (, ,) (VP (VBN followed) (PP (IN by) (NP (NP (NN presentation)) (PP (IN of) (NP (NP (NN prosecution) (POS 's)) (NN case)))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Trial" type="NP">
          <tokens>
            <token id="6" string="Trial" />
          </tokens>
        </chunking>
        <chunking id="2" string="Ray Buckey and Peggy McMartin Buckey" type="NP">
          <tokens>
            <token id="8" string="Ray" />
            <token id="9" string="Buckey" />
            <token id="10" string="and" />
            <token id="11" string="Peggy" />
            <token id="12" string="McMartin" />
            <token id="13" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="3" string="Peggy McMartin Buckey" type="NP">
          <tokens>
            <token id="11" string="Peggy" />
            <token id="12" string="McMartin" />
            <token id="13" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="4" string="presentation of prosecution 's case" type="NP">
          <tokens>
            <token id="23" string="presentation" />
            <token id="24" string="of" />
            <token id="25" string="prosecution" />
            <token id="26" string="'s" />
            <token id="27" string="case" />
          </tokens>
        </chunking>
        <chunking id="5" string="opening statements" type="NP">
          <tokens>
            <token id="18" string="opening" />
            <token id="19" string="statements" />
          </tokens>
        </chunking>
        <chunking id="6" string="Trial of Ray Buckey and Peggy McMartin Buckey" type="NP">
          <tokens>
            <token id="6" string="Trial" />
            <token id="7" string="of" />
            <token id="8" string="Ray" />
            <token id="9" string="Buckey" />
            <token id="10" string="and" />
            <token id="11" string="Peggy" />
            <token id="12" string="McMartin" />
            <token id="13" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="7" string="gets under way with opening statements , followed by presentation of prosecution 's case" type="VP">
          <tokens>
            <token id="14" string="gets" />
            <token id="15" string="under" />
            <token id="16" string="way" />
            <token id="17" string="with" />
            <token id="18" string="opening" />
            <token id="19" string="statements" />
            <token id="20" string="," />
            <token id="21" string="followed" />
            <token id="22" string="by" />
            <token id="23" string="presentation" />
            <token id="24" string="of" />
            <token id="25" string="prosecution" />
            <token id="26" string="'s" />
            <token id="27" string="case" />
          </tokens>
        </chunking>
        <chunking id="8" string="prosecution 's" type="NP">
          <tokens>
            <token id="25" string="prosecution" />
            <token id="26" string="'s" />
          </tokens>
        </chunking>
        <chunking id="9" string="Ray Buckey" type="NP">
          <tokens>
            <token id="8" string="Ray" />
            <token id="9" string="Buckey" />
          </tokens>
        </chunking>
        <chunking id="10" string="way" type="NP">
          <tokens>
            <token id="16" string="way" />
          </tokens>
        </chunking>
        <chunking id="11" string="July 13 , 1987" type="NP">
          <tokens>
            <token id="1" string="July" />
            <token id="2" string="13" />
            <token id="3" string="," />
            <token id="4" string="1987" />
          </tokens>
        </chunking>
        <chunking id="12" string="prosecution 's case" type="NP">
          <tokens>
            <token id="25" string="prosecution" />
            <token id="26" string="'s" />
            <token id="27" string="case" />
          </tokens>
        </chunking>
        <chunking id="13" string="presentation" type="NP">
          <tokens>
            <token id="23" string="presentation" />
          </tokens>
        </chunking>
        <chunking id="14" string="followed by presentation of prosecution 's case" type="VP">
          <tokens>
            <token id="21" string="followed" />
            <token id="22" string="by" />
            <token id="23" string="presentation" />
            <token id="24" string="of" />
            <token id="25" string="prosecution" />
            <token id="26" string="'s" />
            <token id="27" string="case" />
          </tokens>
        </chunking>
        <chunking id="15" string="July 13 , 1987 -- Trial of Ray Buckey and Peggy McMartin Buckey gets under way with opening statements , followed by presentation of prosecution 's case ." type="NP">
          <tokens>
            <token id="1" string="July" />
            <token id="2" string="13" />
            <token id="3" string="," />
            <token id="4" string="1987" />
            <token id="5" string="--" />
            <token id="6" string="Trial" />
            <token id="7" string="of" />
            <token id="8" string="Ray" />
            <token id="9" string="Buckey" />
            <token id="10" string="and" />
            <token id="11" string="Peggy" />
            <token id="12" string="McMartin" />
            <token id="13" string="Buckey" />
            <token id="14" string="gets" />
            <token id="15" string="under" />
            <token id="16" string="way" />
            <token id="17" string="with" />
            <token id="18" string="opening" />
            <token id="19" string="statements" />
            <token id="20" string="," />
            <token id="21" string="followed" />
            <token id="22" string="by" />
            <token id="23" string="presentation" />
            <token id="24" string="of" />
            <token id="25" string="prosecution" />
            <token id="26" string="'s" />
            <token id="27" string="case" />
            <token id="28" string="." />
          </tokens>
        </chunking>
        <chunking id="16" string="opening statements , followed by presentation of prosecution 's case" type="NP">
          <tokens>
            <token id="18" string="opening" />
            <token id="19" string="statements" />
            <token id="20" string="," />
            <token id="21" string="followed" />
            <token id="22" string="by" />
            <token id="23" string="presentation" />
            <token id="24" string="of" />
            <token id="25" string="prosecution" />
            <token id="26" string="'s" />
            <token id="27" string="case" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">July</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">July</governor>
          <dependent id="2">13</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">July</governor>
          <dependent id="4">1987</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">gets</governor>
          <dependent id="6">Trial</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">Buckey</governor>
          <dependent id="7">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">Buckey</governor>
          <dependent id="8">Ray</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">Trial</governor>
          <dependent id="9">Buckey</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="9">Buckey</governor>
          <dependent id="10">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="13">Buckey</governor>
          <dependent id="11">Peggy</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="13">Buckey</governor>
          <dependent id="12">McMartin</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="9">Buckey</governor>
          <dependent id="13">Buckey</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="1">July</governor>
          <dependent id="14">gets</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">way</governor>
          <dependent id="15">under</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">gets</governor>
          <dependent id="16">way</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">statements</governor>
          <dependent id="17">with</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">statements</governor>
          <dependent id="18">opening</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">gets</governor>
          <dependent id="19">statements</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="19">statements</governor>
          <dependent id="21">followed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">presentation</governor>
          <dependent id="22">by</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="21">followed</governor>
          <dependent id="23">presentation</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">case</governor>
          <dependent id="24">of</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="27">case</governor>
          <dependent id="25">prosecution</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">prosecution</governor>
          <dependent id="26">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">presentation</governor>
          <dependent id="27">case</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Peggy McMartin Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="11" string="Peggy" />
            <token id="12" string="McMartin" />
            <token id="13" string="Buckey" />
          </tokens>
        </entity>
        <entity id="2" string="July 13 , 1987" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="July" />
            <token id="2" string="13" />
            <token id="3" string="," />
            <token id="4" string="1987" />
          </tokens>
        </entity>
        <entity id="3" string="Ray Buckey" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Ray" />
            <token id="9" string="Buckey" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="160" has_coreference="true">
      <content>Oct. 19, 1988 -- Defense begins presentation of its case.</content>
      <tokens>
        <token id="1" string="Oct." lemma="Oct." stem="oct." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="19" lemma="19" stem="19" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1988" lemma="1988" stem="1988" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Defense" lemma="Defense" stem="defens" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="begins" lemma="begin" stem="begin" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="presentation" lemma="presentation" stem="present" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="9" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="its" lemma="its" stem="it" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="case" lemma="case" stem="case" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP Oct.) (CD 19) (, ,) (CD 1988)) (: --) (S (NP (NNP Defense)) (VP (VBZ begins) (NP (NP (NN presentation)) (PP (IN of) (NP (PRP$ its) (NN case)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Oct. 19 , 1988 -- Defense begins presentation of its case ." type="NP">
          <tokens>
            <token id="1" string="Oct." />
            <token id="2" string="19" />
            <token id="3" string="," />
            <token id="4" string="1988" />
            <token id="5" string="--" />
            <token id="6" string="Defense" />
            <token id="7" string="begins" />
            <token id="8" string="presentation" />
            <token id="9" string="of" />
            <token id="10" string="its" />
            <token id="11" string="case" />
            <token id="12" string="." />
          </tokens>
        </chunking>
        <chunking id="2" string="Defense" type="NP">
          <tokens>
            <token id="6" string="Defense" />
          </tokens>
        </chunking>
        <chunking id="3" string="presentation" type="NP">
          <tokens>
            <token id="8" string="presentation" />
          </tokens>
        </chunking>
        <chunking id="4" string="Oct. 19 , 1988" type="NP">
          <tokens>
            <token id="1" string="Oct." />
            <token id="2" string="19" />
            <token id="3" string="," />
            <token id="4" string="1988" />
          </tokens>
        </chunking>
        <chunking id="5" string="presentation of its case" type="NP">
          <tokens>
            <token id="8" string="presentation" />
            <token id="9" string="of" />
            <token id="10" string="its" />
            <token id="11" string="case" />
          </tokens>
        </chunking>
        <chunking id="6" string="begins presentation of its case" type="VP">
          <tokens>
            <token id="7" string="begins" />
            <token id="8" string="presentation" />
            <token id="9" string="of" />
            <token id="10" string="its" />
            <token id="11" string="case" />
          </tokens>
        </chunking>
        <chunking id="7" string="its case" type="NP">
          <tokens>
            <token id="10" string="its" />
            <token id="11" string="case" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Oct.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Oct.</governor>
          <dependent id="2">19</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Oct.</governor>
          <dependent id="4">1988</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">begins</governor>
          <dependent id="6">Defense</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="1">Oct.</governor>
          <dependent id="7">begins</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">begins</governor>
          <dependent id="8">presentation</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">case</governor>
          <dependent id="9">of</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="11">case</governor>
          <dependent id="10">its</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">presentation</governor>
          <dependent id="11">case</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Oct. 19 , 1988" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Oct." />
            <token id="2" string="19" />
            <token id="3" string="," />
            <token id="4" string="1988" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="161" has_coreference="false">
      <content>Oct. 12, 1989 -- Final arguments begin.</content>
      <tokens>
        <token id="1" string="Oct." lemma="Oct." stem="oct." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="12" lemma="12" stem="12" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1989" lemma="1989" stem="1989" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Final" lemma="final" stem="final" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="arguments" lemma="argument" stem="argument" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="begin" lemma="begin" stem="begin" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP Oct.) (CD 12) (, ,) (CD 1989)) (: --) (S (NP (JJ Final) (NNS arguments)) (VP (VBP begin))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Final arguments" type="NP">
          <tokens>
            <token id="6" string="Final" />
            <token id="7" string="arguments" />
          </tokens>
        </chunking>
        <chunking id="2" string="Oct. 12 , 1989" type="NP">
          <tokens>
            <token id="1" string="Oct." />
            <token id="2" string="12" />
            <token id="3" string="," />
            <token id="4" string="1989" />
          </tokens>
        </chunking>
        <chunking id="3" string="Oct. 12 , 1989 -- Final arguments begin ." type="NP">
          <tokens>
            <token id="1" string="Oct." />
            <token id="2" string="12" />
            <token id="3" string="," />
            <token id="4" string="1989" />
            <token id="5" string="--" />
            <token id="6" string="Final" />
            <token id="7" string="arguments" />
            <token id="8" string="begin" />
            <token id="9" string="." />
          </tokens>
        </chunking>
        <chunking id="4" string="begin" type="VP">
          <tokens>
            <token id="8" string="begin" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Oct.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Oct.</governor>
          <dependent id="2">12</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Oct.</governor>
          <dependent id="4">1989</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">arguments</governor>
          <dependent id="6">Final</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">begin</governor>
          <dependent id="7">arguments</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="1">Oct.</governor>
          <dependent id="8">begin</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Oct. 12 , 1989" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Oct." />
            <token id="2" string="12" />
            <token id="3" string="," />
            <token id="4" string="1989" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="162" has_coreference="true">
      <content>Nov. 2, 1989 -- Case goes to the jury.</content>
      <tokens>
        <token id="1" string="Nov." lemma="Nov." stem="nov." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="2" lemma="2" stem="2" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1989" lemma="1989" stem="1989" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Case" lemma="Case" stem="case" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="goes" lemma="go" stem="goe" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP Nov.) (CD 2) (, ,) (CD 1989)) (: --) (S (NP (NNP Case)) (VP (VBZ goes) (PP (TO to) (NP (DT the) (NN jury))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Case" type="NP">
          <tokens>
            <token id="6" string="Case" />
          </tokens>
        </chunking>
        <chunking id="2" string="goes to the jury" type="VP">
          <tokens>
            <token id="7" string="goes" />
            <token id="8" string="to" />
            <token id="9" string="the" />
            <token id="10" string="jury" />
          </tokens>
        </chunking>
        <chunking id="3" string="Nov. 2 , 1989 -- Case goes to the jury ." type="NP">
          <tokens>
            <token id="1" string="Nov." />
            <token id="2" string="2" />
            <token id="3" string="," />
            <token id="4" string="1989" />
            <token id="5" string="--" />
            <token id="6" string="Case" />
            <token id="7" string="goes" />
            <token id="8" string="to" />
            <token id="9" string="the" />
            <token id="10" string="jury" />
            <token id="11" string="." />
          </tokens>
        </chunking>
        <chunking id="4" string="the jury" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="jury" />
          </tokens>
        </chunking>
        <chunking id="5" string="Nov. 2 , 1989" type="NP">
          <tokens>
            <token id="1" string="Nov." />
            <token id="2" string="2" />
            <token id="3" string="," />
            <token id="4" string="1989" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Nov.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Nov.</governor>
          <dependent id="2">2</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Nov.</governor>
          <dependent id="4">1989</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">goes</governor>
          <dependent id="6">Case</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="1">Nov.</governor>
          <dependent id="7">goes</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">jury</governor>
          <dependent id="8">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">jury</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">goes</governor>
          <dependent id="10">jury</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Nov. 2 , 1989" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Nov." />
            <token id="2" string="2" />
            <token id="3" string="," />
            <token id="4" string="1989" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="163" has_coreference="false">
      <content>Nov. 29, 1989 -- First verdict returned, kept under seal.</content>
      <tokens>
        <token id="1" string="Nov." lemma="Nov." stem="nov." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="29" lemma="29" stem="29" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1989" lemma="1989" stem="1989" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="First" lemma="first" stem="first" pos="JJ" type="Word" isStopWord="false" ner="ORDINAL" is_referenced="false" is_refers="false" />
        <token id="7" string="verdict" lemma="verdict" stem="verdict" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="returned" lemma="return" stem="return" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="kept" lemma="keep" stem="kept" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="under" lemma="under" stem="under" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="seal" lemma="seal" stem="seal" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Nov.) (CD 29) (, ,) (CD 1989)) (: --) (NP (NP (JJ First) (NN verdict)) (VP (VBD returned))) (, ,)) (VP (VBD kept) (PP (IN under) (NP (NN seal)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="First verdict returned" type="NP">
          <tokens>
            <token id="6" string="First" />
            <token id="7" string="verdict" />
            <token id="8" string="returned" />
          </tokens>
        </chunking>
        <chunking id="2" string="First verdict" type="NP">
          <tokens>
            <token id="6" string="First" />
            <token id="7" string="verdict" />
          </tokens>
        </chunking>
        <chunking id="3" string="kept under seal" type="VP">
          <tokens>
            <token id="10" string="kept" />
            <token id="11" string="under" />
            <token id="12" string="seal" />
          </tokens>
        </chunking>
        <chunking id="4" string="seal" type="NP">
          <tokens>
            <token id="12" string="seal" />
          </tokens>
        </chunking>
        <chunking id="5" string="Nov. 29 , 1989" type="NP">
          <tokens>
            <token id="1" string="Nov." />
            <token id="2" string="29" />
            <token id="3" string="," />
            <token id="4" string="1989" />
          </tokens>
        </chunking>
        <chunking id="6" string="returned" type="VP">
          <tokens>
            <token id="8" string="returned" />
          </tokens>
        </chunking>
        <chunking id="7" string="Nov. 29 , 1989 -- First verdict returned ," type="NP">
          <tokens>
            <token id="1" string="Nov." />
            <token id="2" string="29" />
            <token id="3" string="," />
            <token id="4" string="1989" />
            <token id="5" string="--" />
            <token id="6" string="First" />
            <token id="7" string="verdict" />
            <token id="8" string="returned" />
            <token id="9" string="," />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="10">kept</governor>
          <dependent id="1">Nov.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Nov.</governor>
          <dependent id="2">29</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Nov.</governor>
          <dependent id="4">1989</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">verdict</governor>
          <dependent id="6">First</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="1">Nov.</governor>
          <dependent id="7">verdict</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="7">verdict</governor>
          <dependent id="8">returned</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="10">kept</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">seal</governor>
          <dependent id="11">under</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">kept</governor>
          <dependent id="12">seal</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="First" type="ORDINAL" score="0.0">
          <tokens>
            <token id="6" string="First" />
          </tokens>
        </entity>
        <entity id="2" string="Nov. 29 , 1989" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Nov." />
            <token id="2" string="29" />
            <token id="3" string="," />
            <token id="4" string="1989" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="164" has_coreference="false">
      <content>Jan. 11, 1990 -- Fifty-second verdict returned.</content>
      <tokens>
        <token id="1" string="Jan." lemma="Jan." stem="jan." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="11" lemma="11" stem="11" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1990" lemma="1990" stem="1990" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Fifty-second" lemma="fifty-second" stem="fifty-second" pos="JJ" type="Word" isStopWord="false" ner="ORDINAL" is_referenced="false" is_refers="false" />
        <token id="7" string="verdict" lemma="verdict" stem="verdict" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="returned" lemma="return" stem="return" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP Jan.) (CD 11) (, ,) (CD 1990)) (: --) (S (NP (JJ Fifty-second) (NN verdict)) (VP (VBD returned))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Jan. 11 , 1990 -- Fifty-second verdict returned ." type="NP">
          <tokens>
            <token id="1" string="Jan." />
            <token id="2" string="11" />
            <token id="3" string="," />
            <token id="4" string="1990" />
            <token id="5" string="--" />
            <token id="6" string="Fifty-second" />
            <token id="7" string="verdict" />
            <token id="8" string="returned" />
            <token id="9" string="." />
          </tokens>
        </chunking>
        <chunking id="2" string="Fifty-second verdict" type="NP">
          <tokens>
            <token id="6" string="Fifty-second" />
            <token id="7" string="verdict" />
          </tokens>
        </chunking>
        <chunking id="3" string="returned" type="VP">
          <tokens>
            <token id="8" string="returned" />
          </tokens>
        </chunking>
        <chunking id="4" string="Jan. 11 , 1990" type="NP">
          <tokens>
            <token id="1" string="Jan." />
            <token id="2" string="11" />
            <token id="3" string="," />
            <token id="4" string="1990" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Jan.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Jan.</governor>
          <dependent id="2">11</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Jan.</governor>
          <dependent id="4">1990</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">verdict</governor>
          <dependent id="6">Fifty-second</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">returned</governor>
          <dependent id="7">verdict</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="1">Jan.</governor>
          <dependent id="8">returned</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Jan. 11 , 1990" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Jan." />
            <token id="2" string="11" />
            <token id="3" string="," />
            <token id="4" string="1990" />
          </tokens>
        </entity>
        <entity id="2" string="Fifty-second" type="ORDINAL" score="0.0">
          <tokens>
            <token id="6" string="Fifty-second" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="165" has_coreference="true">
      <content>Jan. 17, 1990 -- Jury deadlocks on remaining 13 counts.</content>
      <tokens>
        <token id="1" string="Jan." lemma="Jan." stem="jan." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="2" string="17" lemma="17" stem="17" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="4" string="1990" lemma="1990" stem="1990" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="Jury" lemma="jury" stem="juri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="deadlocks" lemma="deadlock" stem="deadlock" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="remaining" lemma="remain" stem="remain" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="10" string="13" lemma="13" stem="13" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="true" />
        <token id="11" string="counts" lemma="count" stem="count" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP Jan.) (CD 17) (, ,) (CD 1990)) (: --) (S (NP (NN Jury)) (VP (VBZ deadlocks) (PP (IN on) (S (VP (VBG remaining) (NP (CD 13) (NNS counts))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Jury" type="NP">
          <tokens>
            <token id="6" string="Jury" />
          </tokens>
        </chunking>
        <chunking id="2" string="remaining 13 counts" type="VP">
          <tokens>
            <token id="9" string="remaining" />
            <token id="10" string="13" />
            <token id="11" string="counts" />
          </tokens>
        </chunking>
        <chunking id="3" string="Jan. 17 , 1990 -- Jury deadlocks on remaining 13 counts ." type="NP">
          <tokens>
            <token id="1" string="Jan." />
            <token id="2" string="17" />
            <token id="3" string="," />
            <token id="4" string="1990" />
            <token id="5" string="--" />
            <token id="6" string="Jury" />
            <token id="7" string="deadlocks" />
            <token id="8" string="on" />
            <token id="9" string="remaining" />
            <token id="10" string="13" />
            <token id="11" string="counts" />
            <token id="12" string="." />
          </tokens>
        </chunking>
        <chunking id="4" string="deadlocks on remaining 13 counts" type="VP">
          <tokens>
            <token id="7" string="deadlocks" />
            <token id="8" string="on" />
            <token id="9" string="remaining" />
            <token id="10" string="13" />
            <token id="11" string="counts" />
          </tokens>
        </chunking>
        <chunking id="5" string="Jan. 17 , 1990" type="NP">
          <tokens>
            <token id="1" string="Jan." />
            <token id="2" string="17" />
            <token id="3" string="," />
            <token id="4" string="1990" />
          </tokens>
        </chunking>
        <chunking id="6" string="13 counts" type="NP">
          <tokens>
            <token id="10" string="13" />
            <token id="11" string="counts" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Jan.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Jan.</governor>
          <dependent id="2">17</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Jan.</governor>
          <dependent id="4">1990</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">deadlocks</governor>
          <dependent id="6">Jury</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="1">Jan.</governor>
          <dependent id="7">deadlocks</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="9">remaining</governor>
          <dependent id="8">on</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="7">deadlocks</governor>
          <dependent id="9">remaining</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="11">counts</governor>
          <dependent id="10">13</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="9">remaining</governor>
          <dependent id="11">counts</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="13" type="NUMBER" score="0.0">
          <tokens>
            <token id="10" string="13" />
          </tokens>
        </entity>
        <entity id="2" string="Jan. 17 , 1990" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Jan." />
            <token id="2" string="17" />
            <token id="3" string="," />
            <token id="4" string="1990" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="166" has_coreference="true">
      <content>Jan. 18, 1990 -- Verdicts announced.</content>
      <tokens>
        <token id="1" string="Jan." lemma="Jan." stem="jan." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="18" lemma="18" stem="18" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="1990" lemma="1990" stem="1990" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="--" lemma="--" stem="--" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Verdicts" lemma="verdict" stem="verdict" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="announced" lemma="announce" stem="announc" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNP Jan.) (CD 18) (, ,) (CD 1990)) (: --) (S (NP (NNS Verdicts)) (VP (VBD announced))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="announced" type="VP">
          <tokens>
            <token id="7" string="announced" />
          </tokens>
        </chunking>
        <chunking id="2" string="Jan. 18 , 1990 -- Verdicts announced ." type="NP">
          <tokens>
            <token id="1" string="Jan." />
            <token id="2" string="18" />
            <token id="3" string="," />
            <token id="4" string="1990" />
            <token id="5" string="--" />
            <token id="6" string="Verdicts" />
            <token id="7" string="announced" />
            <token id="8" string="." />
          </tokens>
        </chunking>
        <chunking id="3" string="Jan. 18 , 1990" type="NP">
          <tokens>
            <token id="1" string="Jan." />
            <token id="2" string="18" />
            <token id="3" string="," />
            <token id="4" string="1990" />
          </tokens>
        </chunking>
        <chunking id="4" string="Verdicts" type="NP">
          <tokens>
            <token id="6" string="Verdicts" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Jan.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Jan.</governor>
          <dependent id="2">18</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="1">Jan.</governor>
          <dependent id="4">1990</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">announced</governor>
          <dependent id="6">Verdicts</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="1">Jan.</governor>
          <dependent id="7">announced</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Jan. 18 , 1990" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Jan." />
            <token id="2" string="18" />
            <token id="3" string="," />
            <token id="4" string="1990" />
          </tokens>
        </entity>
      </entities>
    </sentence>
  </sentences>
  <coreferences>
    <coreference id="1" type="PROPER">
      <referenced ids_tokens="1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18" string="Jan. 17 , 1986 -- District Atty. Ira Reiner drops charges against five of the seven defendants ." id_sentence="156" />
      <mentions>
        <mention ids_tokens="1-12" string="Jan. 17 , 1990 -- Jury deadlocks on remaining 13 counts ." id_sentence="165" />
      </mentions>
    </coreference>
    <coreference id="2" type="PROPER">
      <referenced ids_tokens="29-30-31-32" string="co-defendant Peggy McMartin Buckey" id_sentence="124" />
      <mentions>
        <mention ids_tokens="1-2" string="Ray Buckey" id_sentence="1" />
        <mention ids_tokens="7-9" string="Peggy McMartin Buckey" id_sentence="1" />
        <mention ids_tokens="1-4" string="Ray Buckey , 31" id_sentence="4" />
        <mention ids_tokens="1-2" string="Ray Buckey" id_sentence="4" />
        <mention ids_tokens="9-10" string="Ray Buckey" id_sentence="9" />
        <mention ids_tokens="1-3" string="Ray Buckey's" id_sentence="18" />
        <mention ids_tokens="3-5" string="Peggy McMartin Buckey" id_sentence="21" />
        <mention ids_tokens="8-9" string="Buckey's" id_sentence="24" />
        <mention ids_tokens="27-29" string="you , Ray" id_sentence="24" />
        <mention ids_tokens="27" string="you" id_sentence="24" />
        <mention ids_tokens="29" string="Ray" id_sentence="24" />
        <mention ids_tokens="8-9" string="Ray Buckey" id_sentence="63" />
        <mention ids_tokens="18" string="him" id_sentence="63" />
        <mention ids_tokens="20" string="his" id_sentence="63" />
        <mention ids_tokens="5-7" string="Peggy McMartin Buckey" id_sentence="64" />
        <mention ids_tokens="2" string="We" id_sentence="65" />
        <mention ids_tokens="18" string="I" id_sentence="65" />
        <mention ids_tokens="7" string="them" id_sentence="66" />
        <mention ids_tokens="2" string="They" id_sentence="67" />
        <mention ids_tokens="2" string="McMartin" id_sentence="74" />
        <mention ids_tokens="20" string="Buckey" id_sentence="78" />
        <mention ids_tokens="28" string="he" id_sentence="78" />
        <mention ids_tokens="1-2" string="Buckey's" id_sentence="79" />
        <mention ids_tokens="2" string="McMartin" id_sentence="80" />
        <mention ids_tokens="22" string="Ray" id_sentence="81" />
        <mention ids_tokens="1" string="Buckey" id_sentence="82" />
        <mention ids_tokens="33" string="his" id_sentence="82" />
        <mention ids_tokens="6-7" string="Ray Buckey" id_sentence="85" />
        <mention ids_tokens="13" string="his" id_sentence="85" />
        <mention ids_tokens="20" string="his" id_sentence="85" />
        <mention ids_tokens="23-25" string="Peggy McMartin Buckey" id_sentence="85" />
        <mention ids_tokens="27" string="his" id_sentence="85" />
        <mention ids_tokens="27" string="Ray" id_sentence="92" />
        <mention ids_tokens="19" string="McMartin" id_sentence="95" />
        <mention ids_tokens="20" string="McMartin" id_sentence="99" />
        <mention ids_tokens="48-49" string="Ray Buckey" id_sentence="100" />
        <mention ids_tokens="51" string="his" id_sentence="100" />
        <mention ids_tokens="3" string="Buckey" id_sentence="108" />
        <mention ids_tokens="5" string="his" id_sentence="108" />
        <mention ids_tokens="19" string="Buckey" id_sentence="125" />
        <mention ids_tokens="1-3" string="PEGGY MCMARTIN BUCKEY" id_sentence="128" />
        <mention ids_tokens="7-11" string="Co-defendant in the McMartin case" id_sentence="128" />
        <mention ids_tokens="10" string="McMartin" id_sentence="128" />
        <mention ids_tokens="10" string="McMartin" id_sentence="135" />
        <mention ids_tokens="14-15" string="Ray Buckey" id_sentence="139" />
        <mention ids_tokens="5" string="Buckey" id_sentence="141" />
        <mention ids_tokens="10-12" string="Peggy McMartin Buckey" id_sentence="143" />
        <mention ids_tokens="13-14" string="Ray Buckey" id_sentence="148" />
        <mention ids_tokens="6" string="Buckey" id_sentence="150" />
        <mention ids_tokens="12" string="his" id_sentence="150" />
        <mention ids_tokens="8-9" string="Ray Buckey" id_sentence="159" />
        <mention ids_tokens="11-13" string="Peggy McMartin Buckey" id_sentence="159" />
      </mentions>
    </coreference>
    <coreference id="4" type="LIST">
      <referenced ids_tokens="1-2-3-4-5-6-7-8-9" string="Ray Buckey and his mother , Peggy McMartin Buckey" id_sentence="1" />
      <mentions>
        <mention ids_tokens="48-52" string="Ray Buckey and his mother" id_sentence="100" />
        <mention ids_tokens="8-13" string="Ray Buckey and Peggy McMartin Buckey" id_sentence="159" />
      </mentions>
    </coreference>
    <coreference id="5" type="NOMINAL">
      <referenced ids_tokens="4-5" string="his mother" id_sentence="1" />
      <mentions>
        <mention ids_tokens="5" string="her" id_sentence="19" />
        <mention ids_tokens="7" string="she" id_sentence="21" />
        <mention ids_tokens="2" string="I" id_sentence="22" />
        <mention ids_tokens="7" string="my" id_sentence="22" />
        <mention ids_tokens="10" string="me" id_sentence="22" />
        <mention ids_tokens="19" string="she" id_sentence="22" />
        <mention ids_tokens="22" string="she" id_sentence="22" />
        <mention ids_tokens="1" string="Her" id_sentence="23" />
        <mention ids_tokens="7" string="her" id_sentence="23" />
        <mention ids_tokens="13" string="she" id_sentence="23" />
        <mention ids_tokens="10" string="she" id_sentence="66" />
        <mention ids_tokens="13" string="she" id_sentence="128" />
        <mention ids_tokens="1" string="She" id_sentence="129" />
        <mention ids_tokens="4" string="her" id_sentence="129" />
      </mentions>
    </coreference>
    <coreference id="6" type="NOMINAL">
      <referenced ids_tokens="7-8" string="their children" id_sentence="33" />
      <mentions>
        <mention ids_tokens="18" string="children" id_sentence="1" />
        <mention ids_tokens="11" string="children" id_sentence="29" />
        <mention ids_tokens="18" string="that" id_sentence="29" />
        <mention ids_tokens="6-7" string="these children" id_sentence="38" />
        <mention ids_tokens="1-2" string="The children" id_sentence="41" />
        <mention ids_tokens="3" string="themselves" id_sentence="41" />
        <mention ids_tokens="9" string="their" id_sentence="41" />
        <mention ids_tokens="2" string="We" id_sentence="42" />
        <mention ids_tokens="6" string="we" id_sentence="42" />
        <mention ids_tokens="1" string="We" id_sentence="44" />
        <mention ids_tokens="3-4" string="the children" id_sentence="45" />
        <mention ids_tokens="22" string="my" id_sentence="45" />
        <mention ids_tokens="1" string="I" id_sentence="46" />
        <mention ids_tokens="6" string="me" id_sentence="47" />
        <mention ids_tokens="11" string="my" id_sentence="47" />
        <mention ids_tokens="1" string="I" id_sentence="48" />
        <mention ids_tokens="2-3" string="The children" id_sentence="52" />
        <mention ids_tokens="24" string="their" id_sentence="52" />
        <mention ids_tokens="30" string="them" id_sentence="52" />
        <mention ids_tokens="1" string="That" id_sentence="53" />
        <mention ids_tokens="17" string="me" id_sentence="54" />
        <mention ids_tokens="18" string="children" id_sentence="68" />
        <mention ids_tokens="16-17" string="the children" id_sentence="90" />
        <mention ids_tokens="1-3" string="The children's" id_sentence="107" />
        <mention ids_tokens="7-8" string="the children" id_sentence="110" />
        <mention ids_tokens="25" string="their" id_sentence="110" />
        <mention ids_tokens="30" string="their" id_sentence="110" />
        <mention ids_tokens="14-15" string="the children" id_sentence="111" />
        <mention ids_tokens="28" string="them" id_sentence="111" />
        <mention ids_tokens="4" string="their" id_sentence="112" />
        <mention ids_tokens="14" string="children" id_sentence="117" />
        <mention ids_tokens="18" string="they" id_sentence="117" />
      </mentions>
    </coreference>
    <coreference id="7" type="PROPER">
      <referenced ids_tokens="20-21-22-23" string="the family-run McMartin Pre-School" id_sentence="1" />
      <mentions>
        <mention ids_tokens="16-19" string="the Virginia McMartin Pre-School" id_sentence="128" />
      </mentions>
    </coreference>
    <coreference id="9" type="NOMINAL">
      <referenced ids_tokens="1-2" string="An eight-man" id_sentence="2" />
      <mentions>
        <mention ids_tokens="14" string="his" id_sentence="4" />
        <mention ids_tokens="2" string="his" id_sentence="5" />
      </mentions>
    </coreference>
    <coreference id="10" type="NOMINAL">
      <referenced ids_tokens="23-24-25-26-27" string="presentation of prosecution 's case" id_sentence="159" />
      <mentions>
        <mention ids_tokens="8-11" string="presentation of its case" id_sentence="160" />
      </mentions>
    </coreference>
    <coreference id="11" type="PROPER">
      <referenced ids_tokens="6" string="Case" id_sentence="162" />
      <mentions>
        <mention ids_tokens="25-27" string="prosecution's case" id_sentence="159" />
        <mention ids_tokens="10-11" string="its case" id_sentence="160" />
      </mentions>
    </coreference>
    <coreference id="13" type="PROPER">
      <referenced ids_tokens="29-30" string="the Buckeys" id_sentence="8" />
      <mentions>
        <mention ids_tokens="15-21" string="the Buckeys of 52 counts of molestation" id_sentence="2" />
        <mention ids_tokens="10" string="he" id_sentence="10" />
        <mention ids_tokens="4" string="he" id_sentence="11" />
        <mention ids_tokens="1-16" string="The Buckeys , who spent years in jail as the case progressed through the judicial system" id_sentence="13" />
        <mention ids_tokens="20" string="them" id_sentence="14" />
        <mention ids_tokens="32-44" string="We the jury in the above entitled actions find the defendants not guilty" id_sentence="15" />
        <mention ids_tokens="1" string="I" id_sentence="68" />
      </mentions>
    </coreference>
    <coreference id="14" type="NOMINAL">
      <referenced ids_tokens="18-19-20-21" string="52 counts of molestation" id_sentence="2" />
      <mentions>
        <mention ids_tokens="10" string="counts" id_sentence="3" />
      </mentions>
    </coreference>
    <coreference id="15" type="PROPER">
      <referenced ids_tokens="37-38-39-40" string="more than two years" id_sentence="2" />
      <mentions>
        <mention ids_tokens="6" string="years" id_sentence="13" />
        <mention ids_tokens="7" string="years" id_sentence="68" />
        <mention ids_tokens="18" string="years" id_sentence="110" />
      </mentions>
    </coreference>
    <coreference id="17" type="NOMINAL">
      <referenced ids_tokens="1-2" string="The jury" id_sentence="3" />
      <mentions>
        <mention ids_tokens="6-8" string="the jury's" id_sentence="65" />
        <mention ids_tokens="31" string="it" id_sentence="77" />
      </mentions>
    </coreference>
    <coreference id="23" type="NOMINAL">
      <referenced ids_tokens="24-25-26" string="the district attorney" id_sentence="4" />
      <mentions>
        <mention ids_tokens="7-10" string="the district attorney's" id_sentence="134" />
        <mention ids_tokens="5-8" string="the district attorney's" id_sentence="137" />
      </mentions>
    </coreference>
    <coreference id="24" type="NOMINAL">
      <referenced ids_tokens="31-32-33-34" string="the 13 undecided counts" id_sentence="4" />
      <mentions>
        <mention ids_tokens="5-6" string="the counts" id_sentence="15" />
      </mentions>
    </coreference>
    <coreference id="25" type="LIST">
      <referenced ids_tokens="2-3-4-5-6" string="his 63-year-old mother and co-defendant" id_sentence="5" />
      <mentions>
        <mention ids_tokens="10" string="they" id_sentence="7" />
      </mentions>
    </coreference>
    <coreference id="26" type="PROPER">
      <referenced ids_tokens="8" string="47" id_sentence="132" />
      <mentions>
        <mention ids_tokens="2" string="I" id_sentence="6" />
        <mention ids_tokens="9" string="we" id_sentence="6" />
        <mention ids_tokens="15" string="she" id_sentence="6" />
        <mention ids_tokens="2" string="My" id_sentence="7" />
        <mention ids_tokens="6" string="my" id_sentence="7" />
        <mention ids_tokens="17" string="my" id_sentence="7" />
      </mentions>
    </coreference>
    <coreference id="28" type="NOMINAL">
      <referenced ids_tokens="6-7" string="my son" id_sentence="7" />
      <mentions>
        <mention ids_tokens="7-8" string="her son" id_sentence="23" />
        <mention ids_tokens="4" string="his" id_sentence="24" />
        <mention ids_tokens="22" string="him" id_sentence="24" />
        <mention ids_tokens="33" string="he" id_sentence="24" />
        <mention ids_tokens="36" string="he" id_sentence="24" />
        <mention ids_tokens="41" string="he" id_sentence="24" />
        <mention ids_tokens="14-15" string="her son" id_sentence="81" />
        <mention ids_tokens="4-5" string="her son" id_sentence="129" />
      </mentions>
    </coreference>
    <coreference id="29" type="PROPER">
      <referenced ids_tokens="1" string="Jurors" id_sentence="49" />
      <mentions>
        <mention ids_tokens="1-2" string="Most jurors" id_sentence="8" />
        <mention ids_tokens="6" string="they" id_sentence="8" />
        <mention ids_tokens="2" string="I" id_sentence="9" />
        <mention ids_tokens="7-11" string="he ( Ray Buckey )" id_sentence="9" />
        <mention ids_tokens="8" string="me" id_sentence="10" />
        <mention ids_tokens="2" string="I" id_sentence="11" />
        <mention ids_tokens="4" string="they" id_sentence="50" />
        <mention ids_tokens="1" string="They" id_sentence="51" />
      </mentions>
    </coreference>
    <coreference id="30" type="NOMINAL">
      <referenced ids_tokens="18-19" string="the prosecution" id_sentence="8" />
      <mentions>
        <mention ids_tokens="6-8" string="the prosecution's" id_sentence="50" />
        <mention ids_tokens="25-26" string="prosecution's" id_sentence="159" />
      </mentions>
    </coreference>
    <coreference id="31" type="PROPER">
      <referenced ids_tokens="21" string="27" id_sentence="9" />
      <mentions>
        <mention ids_tokens="12" string="it" id_sentence="10" />
        <mention ids_tokens="6" string="it" id_sentence="11" />
        <mention ids_tokens="9" string="it" id_sentence="11" />
      </mentions>
    </coreference>
    <coreference id="32" type="NOMINAL">
      <referenced ids_tokens="1-2-3-4-5" string="The end of the case" id_sentence="12" />
      <mentions>
        <mention ids_tokens="2-3" string="the end" id_sentence="97" />
      </mentions>
    </coreference>
    <coreference id="33" type="NOMINAL">
      <referenced ids_tokens="4-5" string="the case" id_sentence="12" />
      <mentions>
        <mention ids_tokens="15" string="it" id_sentence="27" />
        <mention ids_tokens="9-10" string="this case" id_sentence="30" />
        <mention ids_tokens="7" string="its" id_sentence="31" />
        <mention ids_tokens="12-13" string="this case" id_sentence="71" />
        <mention ids_tokens="8" string="it" id_sentence="87" />
      </mentions>
    </coreference>
    <coreference id="34" type="NOMINAL">
      <referenced ids_tokens="14-15-16" string="the judicial system" id_sentence="13" />
      <mentions>
        <mention ids_tokens="2-3" string="The system" id_sentence="37" />
        <mention ids_tokens="2-3" string="The system" id_sentence="66" />
      </mentions>
    </coreference>
    <coreference id="35" type="PROPER">
      <referenced ids_tokens="11-12-13-14" string="Los Angeles Superior Court" id_sentence="14" />
      <mentions>
        <mention ids_tokens="16-17" string="Superior Court" id_sentence="120" />
      </mentions>
    </coreference>
    <coreference id="36" type="PROPER">
      <referenced ids_tokens="15" string="Judge" id_sentence="14" />
      <mentions>
        <mention ids_tokens="17-18" string="the judge" id_sentence="57" />
        <mention ids_tokens="33-35" string="the judge's" id_sentence="114" />
        <mention ids_tokens="7-8" string="THE JUDGE" id_sentence="120" />
      </mentions>
    </coreference>
    <coreference id="38" type="NOMINAL">
      <referenced ids_tokens="22-23-24" string="his court clerk" id_sentence="14" />
      <mentions>
        <mention ids_tokens="9-10" string="the clerk" id_sentence="45" />
      </mentions>
    </coreference>
    <coreference id="40" type="NOMINAL">
      <referenced ids_tokens="41-42" string="the defendants" id_sentence="15" />
      <mentions>
        <mention ids_tokens="10" string="their" id_sentence="17" />
        <mention ids_tokens="1" string="They" id_sentence="20" />
        <mention ids_tokens="13" string="we" id_sentence="22" />
      </mentions>
    </coreference>
    <coreference id="42" type="NOMINAL">
      <referenced ids_tokens="7-8-9-10-11" string="stoic , then their eyes" id_sentence="17" />
      <mentions>
        <mention ids_tokens="5-6" string="her eyes" id_sentence="19" />
      </mentions>
    </coreference>
    <coreference id="44" type="NOMINAL">
      <referenced ids_tokens="1-2-3-4-5" string="Ray Buckey 's lower lip" id_sentence="18" />
      <mentions>
        <mention ids_tokens="1" string="His" id_sentence="19" />
      </mentions>
    </coreference>
    <coreference id="47" type="NOMINAL">
      <referenced ids_tokens="13-14" string="the verdicts" id_sentence="24" />
      <mentions>
        <mention ids_tokens="6" string="Verdicts" id_sentence="166" />
      </mentions>
    </coreference>
    <coreference id="48" type="NOMINAL">
      <referenced ids_tokens="4-5-6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-21-22-23-24-25-26-27-28-29-30-31-32-33-34-35-36-37-38-39-40" string="the longest criminal trial in history , a case that stemmed from a 2 1/2 - year-old 's report to his mother six years ago that he had been sodomized at his school by a &quot; Mr. Ray" id_sentence="25" />
      <mentions>
        <mention ids_tokens="28-29" string="this trial" id_sentence="58" />
        <mention ids_tokens="13-14" string="the trial" id_sentence="70" />
        <mention ids_tokens="1-2" string="The trial" id_sentence="101" />
        <mention ids_tokens="3" string="itself" id_sentence="101" />
        <mention ids_tokens="4-5" string="the trial" id_sentence="103" />
      </mentions>
    </coreference>
    <coreference id="49" type="PROPER">
      <referenced ids_tokens="16-17-18-19-20-21" string="a 2 1/2 - year-old 's report" id_sentence="25" />
      <mentions>
        <mention ids_tokens="11" string="2 1/2" id_sentence="73" />
      </mentions>
    </coreference>
    <coreference id="50" type="PROPER">
      <referenced ids_tokens="19-20" string="year-old 's" id_sentence="25" />
      <mentions>
        <mention ids_tokens="7" string="year-old" id_sentence="81" />
        <mention ids_tokens="30" string="he" id_sentence="81" />
      </mentions>
    </coreference>
    <coreference id="51" type="PROPER">
      <referenced ids_tokens="37-38-39-40" string="a &quot; Mr. Ray" id_sentence="25" />
      <mentions>
        <mention ids_tokens="21-22" string="Mr. Ray" id_sentence="81" />
      </mentions>
    </coreference>
    <coreference id="52" type="NOMINAL">
      <referenced ids_tokens="5" string="taxpayers" id_sentence="26" />
      <mentions>
        <mention ids_tokens="20" string="they" id_sentence="27" />
      </mentions>
    </coreference>
    <coreference id="53" type="PROPER">
      <referenced ids_tokens="10-11-12-13-14" string="WILLIAM R. POUNDERS , 50" id_sentence="120" />
      <mentions>
        <mention ids_tokens="1-2" string="Judge Pounders" id_sentence="27" />
        <mention ids_tokens="4" string="he" id_sentence="27" />
        <mention ids_tokens="13" string="I" id_sentence="27" />
        <mention ids_tokens="1" string="I" id_sentence="28" />
        <mention ids_tokens="16" string="Pounders" id_sentence="31" />
        <mention ids_tokens="18" string="he" id_sentence="31" />
        <mention ids_tokens="25" string="Pounders" id_sentence="32" />
        <mention ids_tokens="5" string="Pounders" id_sentence="78" />
        <mention ids_tokens="17-18" string="Pounders'" id_sentence="122" />
      </mentions>
    </coreference>
    <coreference id="55" type="NOMINAL">
      <referenced ids_tokens="2" string="People" id_sentence="32" />
      <mentions>
        <mention ids_tokens="7" string="their" id_sentence="33" />
        <mention ids_tokens="1" string="They" id_sentence="34" />
        <mention ids_tokens="1" string="They" id_sentence="35" />
      </mentions>
    </coreference>
    <coreference id="56" type="NOMINAL">
      <referenced ids_tokens="7-8-9-10" string="parents of alleged victims" id_sentence="36" />
      <mentions>
        <mention ids_tokens="7" string="us" id_sentence="37" />
      </mentions>
    </coreference>
    <coreference id="57" type="NOMINAL">
      <referenced ids_tokens="14-15" string="the prosecutors" id_sentence="36" />
      <mentions>
        <mention ids_tokens="5-7" string="the prosecutors'" id_sentence="49" />
      </mentions>
    </coreference>
    <coreference id="60" type="NOMINAL">
      <referenced ids_tokens="14-15-16-17-18-19" string="the father of two alleged victims" id_sentence="37" />
      <mentions>
        <mention ids_tokens="2" string="I" id_sentence="38" />
      </mentions>
    </coreference>
    <coreference id="61" type="NOMINAL">
      <referenced ids_tokens="17-18-19" string="two alleged victims" id_sentence="37" />
      <mentions>
        <mention ids_tokens="46-48" string="the alleged victims" id_sentence="90" />
        <mention ids_tokens="55" string="they" id_sentence="90" />
      </mentions>
    </coreference>
    <coreference id="62" type="NOMINAL">
      <referenced ids_tokens="4-5-6-7-8-9" string="no doubt these children were abused" id_sentence="38" />
      <mentions>
        <mention ids_tokens="10" string="it" id_sentence="39" />
      </mentions>
    </coreference>
    <coreference id="63" type="NOMINAL">
      <referenced ids_tokens="2-3-4" string="a tearful mother" id_sentence="39" />
      <mentions>
        <mention ids_tokens="1" string="I" id_sentence="40" />
      </mentions>
    </coreference>
    <coreference id="64" type="NOMINAL">
      <referenced ids_tokens="15-16" string="the truth" id_sentence="43" />
      <mentions>
        <mention ids_tokens="5" string="this" id_sentence="40" />
      </mentions>
    </coreference>
    <coreference id="66" type="PROPER">
      <referenced ids_tokens="4-5-6-7-8-9" string="the Manhattan Beach Police Department 's" id_sentence="51" />
      <mentions>
        <mention ids_tokens="1-5" string="The Manhattan Beach Police Department" id_sentence="83" />
      </mentions>
    </coreference>
    <coreference id="69" type="NOMINAL">
      <referenced ids_tokens="11-12-13-14-15-16-17-18-19-20-21" string="&quot; The key evidence that swayed me was the interview tapes" id_sentence="54" />
      <mentions>
        <mention ids_tokens="1" string="That" id_sentence="56" />
        <mention ids_tokens="3-7" string="the main crux of it" id_sentence="56" />
        <mention ids_tokens="7" string="it" id_sentence="56" />
      </mentions>
    </coreference>
    <coreference id="70" type="PRONOMINAL">
      <referenced ids_tokens="1" string="They" id_sentence="55" />
      <mentions>
        <mention ids_tokens="31" string="them" id_sentence="57" />
        <mention ids_tokens="32" string="their" id_sentence="59" />
      </mentions>
    </coreference>
    <coreference id="71" type="NOMINAL">
      <referenced ids_tokens="6-7-8-9-10-11-12" string="the jury deliberation room with open minds" id_sentence="57" />
      <mentions>
        <mention ids_tokens="5-7" string="the jury room" id_sentence="58" />
      </mentions>
    </coreference>
    <coreference id="72" type="PROPER">
      <referenced ids_tokens="33-34" string="Brenda Williams" id_sentence="58" />
      <mentions>
        <mention ids_tokens="2" string="I" id_sentence="59" />
        <mention ids_tokens="4" string="I" id_sentence="59" />
        <mention ids_tokens="12" string="I" id_sentence="59" />
        <mention ids_tokens="21" string="I" id_sentence="59" />
        <mention ids_tokens="26" string="my" id_sentence="59" />
        <mention ids_tokens="35" string="I" id_sentence="59" />
      </mentions>
    </coreference>
    <coreference id="73" type="PROPER">
      <referenced ids_tokens="15-16-17-18-19-20-21-22-23-24-25-26-27-28-29" string="the day I first sat in the jury box the first day of this trial" id_sentence="58" />
      <mentions>
        <mention ids_tokens="1" string="Day" id_sentence="114" />
        <mention ids_tokens="3" string="day" id_sentence="114" />
        <mention ids_tokens="5" string="it" id_sentence="114" />
      </mentions>
    </coreference>
    <coreference id="74" type="NOMINAL">
      <referenced ids_tokens="17-18-19" string="some important testimony" id_sentence="59" />
      <mentions>
        <mention ids_tokens="1-2" string="Their testimony" id_sentence="106" />
      </mentions>
    </coreference>
    <coreference id="75" type="NOMINAL">
      <referenced ids_tokens="24-25-26-27-28-29-30-31-32-33-34-35-36-37-38-39-40-41-42-43-44-45-46-47" string="some of my fellow jurors had something in their notes that I did n't have in mine that would make it all come together" id_sentence="59" />
      <mentions>
        <mention ids_tokens="12-13" string="&quot; We" id_sentence="60" />
      </mentions>
    </coreference>
    <coreference id="77" type="PROPER">
      <referenced ids_tokens="4-5-6" string="LAEL R. RUBIN" id_sentence="132" />
      <mentions>
        <mention ids_tokens="2-3" string="Lael Rubin" id_sentence="62" />
        <mention ids_tokens="12" string="Rubin" id_sentence="65" />
        <mention ids_tokens="8" string="Rubin" id_sentence="133" />
        <mention ids_tokens="10-15" string="a latecomer to the legal profession" id_sentence="133" />
      </mentions>
    </coreference>
    <coreference id="78" type="NOMINAL">
      <referenced ids_tokens="21-22-23-24-25-26-27" string="the 13 counts involved in a mistrial" id_sentence="62" />
      <mentions>
        <mention ids_tokens="1-2" string="These counts" id_sentence="63" />
        <mention ids_tokens="10-11" string="13 counts" id_sentence="165" />
      </mentions>
    </coreference>
    <coreference id="80" type="NOMINAL">
      <referenced ids_tokens="6-7-8-9" string="the jury 's decision" id_sentence="65" />
      <mentions>
        <mention ids_tokens="16-17" string="the decision" id_sentence="77" />
      </mentions>
    </coreference>
    <coreference id="81" type="PROPER">
      <referenced ids_tokens="6-7-8-9" string="District Atty. Ira Reiner" id_sentence="156" />
      <mentions>
        <mention ids_tokens="1-3" string="Atty. Ira Reiner" id_sentence="70" />
        <mention ids_tokens="18" string="he" id_sentence="70" />
        <mention ids_tokens="16-17" string="Ira Reiner" id_sentence="100" />
      </mentions>
    </coreference>
    <coreference id="82" type="NOMINAL">
      <referenced ids_tokens="6-7-8" string="too much money" id_sentence="71" />
      <mentions>
        <mention ids_tokens="16-17" string="their money" id_sentence="141" />
      </mentions>
    </coreference>
    <coreference id="83" type="NOMINAL">
      <referenced ids_tokens="5-6-7-8" string="the very worst example" id_sentence="74" />
      <mentions>
        <mention ids_tokens="2" string="it" id_sentence="75" />
        <mention ids_tokens="19-21" string="the McMartin case" id_sentence="99" />
        <mention ids_tokens="12-14" string="the McMartin case" id_sentence="124" />
        <mention ids_tokens="9-11" string="the McMartin case" id_sentence="128" />
        <mention ids_tokens="9-11" string="the McMartin case" id_sentence="135" />
        <mention ids_tokens="5-7" string="the McMartin case" id_sentence="147" />
        <mention ids_tokens="9" string="its" id_sentence="147" />
      </mentions>
    </coreference>
    <coreference id="84" type="PROPER">
      <referenced ids_tokens="3-4" string="Dean Gits" id_sentence="76" />
      <mentions>
        <mention ids_tokens="2" string="I" id_sentence="77" />
        <mention ids_tokens="7" string="Gits" id_sentence="77" />
        <mention ids_tokens="11" string="he" id_sentence="77" />
        <mention ids_tokens="1-3" string="DEAN R. GITS" id_sentence="143" />
        <mention ids_tokens="18" string="Gits" id_sentence="144" />
        <mention ids_tokens="33" string="Gits" id_sentence="145" />
      </mentions>
    </coreference>
    <coreference id="85" type="PROPER">
      <referenced ids_tokens="24-25" string="last year" id_sentence="78" />
      <mentions>
        <mention ids_tokens="7-8" string="that year" id_sentence="82" />
      </mentions>
    </coreference>
    <coreference id="86" type="PROPER">
      <referenced ids_tokens="33-34-35-36" string="five years in jail" id_sentence="78" />
      <mentions>
        <mention ids_tokens="34-35" string="five years" id_sentence="96" />
      </mentions>
    </coreference>
    <coreference id="87" type="PROPER">
      <referenced ids_tokens="15-16-17-18-19" string="nearly two years in jail" id_sentence="79" />
      <mentions>
        <mention ids_tokens="25-26" string="two years" id_sentence="140" />
      </mentions>
    </coreference>
    <coreference id="92" type="NOMINAL">
      <referenced ids_tokens="21-22-23" string="six other teachers" id_sentence="82" />
      <mentions>
        <mention ids_tokens="34" string="teachers" id_sentence="85" />
        <mention ids_tokens="32-33" string="her teachers" id_sentence="96" />
      </mentions>
    </coreference>
    <coreference id="93" type="NOMINAL">
      <referenced ids_tokens="25-26-27-28-29" string="the Manhattan Beach nursery school" id_sentence="82" />
      <mentions>
        <mention ids_tokens="35-37" string="the nursery school" id_sentence="92" />
      </mentions>
    </coreference>
    <coreference id="94" type="NOMINAL">
      <referenced ids_tokens="33-34-35-36-37-38-39-40-41-42-43-44-45-46-47-48-49" string="his grandmother , dozens of &quot; uncharged suspects , &quot; and eight other South Bay nursery schools" id_sentence="82" />
      <mentions>
        <mention ids_tokens="27-28" string="his grandmother" id_sentence="85" />
      </mentions>
    </coreference>
    <coreference id="95" type="PROPER">
      <referenced ids_tokens="7-8-9-10" string="Children 's Institute International" id_sentence="84" />
      <mentions>
        <mention ids_tokens="1-9" string="Children's Institute International , deluged with concerned parents" id_sentence="90" />
        <mention ids_tokens="17" string="it" id_sentence="92" />
      </mentions>
    </coreference>
    <coreference id="97" type="PROPER">
      <referenced ids_tokens="21-22-23-24-25" string="nursery school founder Virginia McMartin" id_sentence="124" />
      <mentions>
        <mention ids_tokens="30-31" string="Virginia McMartin" id_sentence="85" />
        <mention ids_tokens="16-19" string="family matriarch Virginia McMartin" id_sentence="114" />
        <mention ids_tokens="18-19" string="Virginia McMartin" id_sentence="114" />
        <mention ids_tokens="17-18" string="Virginia McMartin" id_sentence="128" />
      </mentions>
    </coreference>
    <coreference id="99" type="LIST">
      <referenced ids_tokens="13-14-15-16-17-18-19-20-21-22-23-24-25-26-27-28-29-30-31-32-33-34" string="his sister , Peggy Ann Buckey , his mother , Peggy McMartin Buckey , his grandmother , Virginia McMartin , and teachers" id_sentence="85" />
      <mentions>
        <mention ids_tokens="7" string="them" id_sentence="86" />
        <mention ids_tokens="23" string="them" id_sentence="86" />
        <mention ids_tokens="12-17" string="his sister , mother and grandmother" id_sentence="150" />
      </mentions>
    </coreference>
    <coreference id="102" type="NOMINAL">
      <referenced ids_tokens="12-13" string="the beginning" id_sentence="87" />
      <mentions>
        <mention ids_tokens="9-10" string="its beginning" id_sentence="147" />
      </mentions>
    </coreference>
    <coreference id="103" type="PROPER">
      <referenced ids_tokens="2-3" string="Robert Philibosian" id_sentence="89" />
      <mentions>
        <mention ids_tokens="25" string="Philibosian" id_sentence="100" />
      </mentions>
    </coreference>
    <coreference id="104" type="NOMINAL">
      <referenced ids_tokens="12-13-14-15-16-17" string="untrained therapists to assess the children" id_sentence="90" />
      <mentions>
        <mention ids_tokens="19" string="therapists" id_sentence="111" />
      </mentions>
    </coreference>
    <coreference id="105" type="PROPER">
      <referenced ids_tokens="6" string="Defense" id_sentence="160" />
      <mentions>
        <mention ids_tokens="39-40" string="the defense" id_sentence="90" />
        <mention ids_tokens="1-2" string="The defense" id_sentence="111" />
        <mention ids_tokens="1" string="It" id_sentence="112" />
      </mentions>
    </coreference>
    <coreference id="107" type="PROPER">
      <referenced ids_tokens="30-31" string="Aviva Bobb" id_sentence="93" />
      <mentions>
        <mention ids_tokens="5" string="Bobb" id_sentence="97" />
      </mentions>
    </coreference>
    <coreference id="108" type="NOMINAL">
      <referenced ids_tokens="1-2-3-4" string="The 18-month preliminary hearing" id_sentence="93" />
      <mentions>
        <mention ids_tokens="8-10" string="the preliminary hearing" id_sentence="119" />
      </mentions>
    </coreference>
    <coreference id="109" type="NOMINAL">
      <referenced ids_tokens="21-22" string="three prosecutors" id_sentence="93" />
      <mentions>
        <mention ids_tokens="17" string="their" id_sentence="94" />
        <mention ids_tokens="22" string="their" id_sentence="94" />
      </mentions>
    </coreference>
    <coreference id="110" type="NOMINAL">
      <referenced ids_tokens="10-11-12-13-14-15-16-17-18-19-20-21-22-23-24" string="youngsters clambering atop a booster chair in their Sunday best to face their alleged molesters" id_sentence="94" />
      <mentions>
        <mention ids_tokens="1" string="They" id_sentence="95" />
        <mention ids_tokens="13" string="they" id_sentence="95" />
        <mention ids_tokens="16" string="their" id_sentence="95" />
      </mentions>
    </coreference>
    <coreference id="111" type="NOMINAL">
      <referenced ids_tokens="22-23-24" string="their alleged molesters" id_sentence="94" />
      <mentions>
        <mention ids_tokens="30-31" string="his molesters" id_sentence="119" />
      </mentions>
    </coreference>
    <coreference id="114" type="NOMINAL">
      <referenced ids_tokens="1-2-3" string="The remaining charges" id_sentence="98" />
      <mentions>
        <mention ids_tokens="28" string="charges" id_sentence="100" />
      </mentions>
    </coreference>
    <coreference id="115" type="NOMINAL">
      <referenced ids_tokens="12-13-14-15-16-17-18-19-20-21" string="a new state law passed especially for the McMartin case" id_sentence="99" />
      <mentions>
        <mention ids_tokens="28-29" string="the law" id_sentence="122" />
      </mentions>
    </coreference>
    <coreference id="116" type="PROPER">
      <referenced ids_tokens="30-31-32-33-34" string="five of the seven defendants" id_sentence="100" />
      <mentions>
        <mention ids_tokens="13" string="five" id_sentence="156" />
      </mentions>
    </coreference>
    <coreference id="118" type="NOMINAL">
      <referenced ids_tokens="10-11" string="several parents" id_sentence="103" />
      <mentions>
        <mention ids_tokens="15" string="their" id_sentence="104" />
        <mention ids_tokens="6" string="their" id_sentence="105" />
        <mention ids_tokens="30-31" string="their parents" id_sentence="110" />
      </mentions>
    </coreference>
    <coreference id="119" type="NOMINAL">
      <referenced ids_tokens="8-9-10" string="the witness stand" id_sentence="104" />
      <mentions>
        <mention ids_tokens="9-10" string="the stand" id_sentence="108" />
      </mentions>
    </coreference>
    <coreference id="120" type="NOMINAL">
      <referenced ids_tokens="1-2-3-4-5-6-7-8" string="Toddlers at the time of their alleged abuse" id_sentence="105" />
      <mentions>
        <mention ids_tokens="1" string="Their" id_sentence="106" />
      </mentions>
    </coreference>
    <coreference id="121" type="NOMINAL">
      <referenced ids_tokens="1-2-3-4" string="The children 's allegations" id_sentence="107" />
      <mentions>
        <mention ids_tokens="8" string="allegations" id_sentence="157" />
      </mentions>
    </coreference>
    <coreference id="124" type="NOMINAL">
      <referenced ids_tokens="14-15-16" string="the small school" id_sentence="109" />
      <mentions>
        <mention ids_tokens="9-10" string="the school" id_sentence="111" />
      </mentions>
    </coreference>
    <coreference id="127" type="PROPER">
      <referenced ids_tokens="3-4" string="Kee MacFarlane" id_sentence="117" />
      <mentions>
        <mention ids_tokens="15" string="he" id_sentence="118" />
        <mention ids_tokens="30" string="his" id_sentence="118" />
      </mentions>
    </coreference>
    <coreference id="135" type="NOMINAL">
      <referenced ids_tokens="1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17" string="A quiet man who is a college dropout and has admitted having problems with drugs and alcohol" id_sentence="125" />
      <mentions>
        <mention ids_tokens="1" string="He" id_sentence="126" />
        <mention ids_tokens="20" string="his" id_sentence="126" />
        <mention ids_tokens="1" string="He" id_sentence="127" />
        <mention ids_tokens="6" string="him" id_sentence="127" />
      </mentions>
    </coreference>
    <coreference id="136" type="NOMINAL">
      <referenced ids_tokens="8-9-10-11-12-13-14-15-16-17-18-19-20-21" string="child molestation involving 11 children and one count of conspiracy shared with his mother" id_sentence="126" />
      <mentions>
        <mention ids_tokens="25-28" string="child molestation and conspiracy" id_sentence="150" />
      </mentions>
    </coreference>
    <coreference id="137" type="NOMINAL">
      <referenced ids_tokens="1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16" string="A talkative , religious woman who drew pictures throughout the tedious proceedings and crocheted during breaks" id_sentence="130" />
      <mentions>
        <mention ids_tokens="1" string="She" id_sentence="131" />
        <mention ids_tokens="6" string="she" id_sentence="131" />
        <mention ids_tokens="11" string="her" id_sentence="131" />
      </mentions>
    </coreference>
    <coreference id="138" type="NOMINAL">
      <referenced ids_tokens="1-2-3-4-5-6" string="A former high school English teacher" id_sentence="133" />
      <mentions>
        <mention ids_tokens="1" string="She" id_sentence="134" />
        <mention ids_tokens="4" string="herself" id_sentence="134" />
        <mention ids_tokens="1" string="She" id_sentence="135" />
      </mentions>
    </coreference>
    <coreference id="142" type="NOMINAL">
      <referenced ids_tokens="1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-21-22" string="A 20-year veteran of the district attorney 's office and a graduate of UCLA Law School , the soft-spoken , unassuming Gunson" id_sentence="137" />
      <mentions>
        <mention ids_tokens="1" string="He" id_sentence="138" />
      </mentions>
    </coreference>
    <coreference id="143" type="PROPER">
      <referenced ids_tokens="5-6-7" string="DANIEL G. DAVIS" id_sentence="139" />
      <mentions>
        <mention ids_tokens="11" string="Davis" id_sentence="140" />
      </mentions>
    </coreference>
    <coreference id="144" type="NOMINAL">
      <referenced ids_tokens="1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16" string="THE DEFENSE ATTORNEYS : DANIEL G. DAVIS , 43 -- Defense attorney representing Ray Buckey ." id_sentence="139" />
      <mentions>
        <mention ids_tokens="16" string="their" id_sentence="141" />
      </mentions>
    </coreference>
    <coreference id="146" type="NOMINAL">
      <referenced ids_tokens="4-5-6" string="the Buckey family" id_sentence="141" />
      <mentions>
        <mention ids_tokens="1" string="He" id_sentence="142" />
      </mentions>
    </coreference>
    <coreference id="147" type="NOMINAL">
      <referenced ids_tokens="7-8-9-10-11-12" string="Defense attorney representing Peggy McMartin Buckey" id_sentence="143" />
      <mentions>
        <mention ids_tokens="7-8" string="attorney's" id_sentence="151" />
      </mentions>
    </coreference>
    <coreference id="148" type="NOMINAL">
      <referenced ids_tokens="1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18" string="A gentlemanly , cum laude graduate of the William Mitchell College of Law in St. Paul , Gits" id_sentence="144" />
      <mentions>
        <mention ids_tokens="3" string="his" id_sentence="145" />
        <mention ids_tokens="8" string="he" id_sentence="145" />
        <mention ids_tokens="10-17" string="a deputy public defender for nearly nine years" id_sentence="145" />
        <mention ids_tokens="1" string="He" id_sentence="146" />
        <mention ids_tokens="1" string="He" id_sentence="147" />
      </mentions>
    </coreference>
  </coreferences>
</document>
