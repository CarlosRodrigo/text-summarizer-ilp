<?xml version="1.0" encoding="UTF-8"?>
<document id="0" name="LA060590-0086">
  <sentences>
    <sentence id="1" has_coreference="true">
      <content>British media entrepreneur Robert Maxwell said Monday that he is involved in negotiations to buy a U.S. newspaper valued &amp;quot;in the hundreds of millions of dollars.&amp;quot;</content>
      <tokens>
        <token id="1" string="British" lemma="british" stem="british" pos="JJ" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="false" is_refers="false" />
        <token id="2" string="media" lemma="media" stem="media" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="entrepreneur" lemma="entrepreneur" stem="entrepreneur" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="Robert" lemma="Robert" stem="robert" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="5" string="Maxwell" lemma="Maxwell" stem="maxwel" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="6" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="Monday" lemma="Monday" stem="mondai" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="8" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="involved" lemma="involve" stem="involv" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="negotiations" lemma="negotiation" stem="negoti" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="buy" lemma="buy" stem="bui" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="U.S." lemma="U.S." stem="u.s." pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="true" />
        <token id="18" string="newspaper" lemma="newspaper" stem="newspap" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="19" string="valued" lemma="value" stem="valu" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="20" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="21" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="22" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="23" string="hundreds" lemma="hundred" stem="hundr" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="24" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="25" string="millions" lemma="million" stem="million" pos="NNS" type="Word" isStopWord="false" ner="MONEY" is_referenced="false" is_refers="true" />
        <token id="26" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" ner="MONEY" is_referenced="false" is_refers="true" />
        <token id="27" string="dollars" lemma="dollar" stem="dollar" pos="NNS" type="Word" isStopWord="false" ner="MONEY" is_referenced="false" is_refers="true" />
        <token id="28" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (JJ British) (NNS media)) (NP (NN entrepreneur) (NNP Robert) (NNP Maxwell))) (VP (VBD said) (NP-TMP (NNP Monday)) (SBAR (IN that) (S (NP (PRP he)) (VP (VBZ is) (VP (VBN involved) (PP (IN in) (NP (NNS negotiations))) (S (VP (TO to) (VP (VB buy) (NP (NP (DT a) (NNP U.S.) (NN newspaper)) (VP (VBN valued) (PP (`` ``) (IN in) (NP (NP (DT the) (NNS hundreds)) (PP (IN of) (NP (NP (NNS millions)) (PP (IN of) (NP (NNS dollars))))))))))))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="to buy a U.S. newspaper valued `` in the hundreds of millions of dollars" type="VP">
          <tokens>
            <token id="14" string="to" />
            <token id="15" string="buy" />
            <token id="16" string="a" />
            <token id="17" string="U.S." />
            <token id="18" string="newspaper" />
            <token id="19" string="valued" />
            <token id="20" string="&quot;" />
            <token id="21" string="in" />
            <token id="22" string="the" />
            <token id="23" string="hundreds" />
            <token id="24" string="of" />
            <token id="25" string="millions" />
            <token id="26" string="of" />
            <token id="27" string="dollars" />
          </tokens>
        </chunking>
        <chunking id="2" string="British media" type="NP">
          <tokens>
            <token id="1" string="British" />
            <token id="2" string="media" />
          </tokens>
        </chunking>
        <chunking id="3" string="the hundreds of millions of dollars" type="NP">
          <tokens>
            <token id="22" string="the" />
            <token id="23" string="hundreds" />
            <token id="24" string="of" />
            <token id="25" string="millions" />
            <token id="26" string="of" />
            <token id="27" string="dollars" />
          </tokens>
        </chunking>
        <chunking id="4" string="buy a U.S. newspaper valued `` in the hundreds of millions of dollars" type="VP">
          <tokens>
            <token id="15" string="buy" />
            <token id="16" string="a" />
            <token id="17" string="U.S." />
            <token id="18" string="newspaper" />
            <token id="19" string="valued" />
            <token id="20" string="&quot;" />
            <token id="21" string="in" />
            <token id="22" string="the" />
            <token id="23" string="hundreds" />
            <token id="24" string="of" />
            <token id="25" string="millions" />
            <token id="26" string="of" />
            <token id="27" string="dollars" />
          </tokens>
        </chunking>
        <chunking id="5" string="negotiations" type="NP">
          <tokens>
            <token id="13" string="negotiations" />
          </tokens>
        </chunking>
        <chunking id="6" string="is involved in negotiations to buy a U.S. newspaper valued `` in the hundreds of millions of dollars" type="VP">
          <tokens>
            <token id="10" string="is" />
            <token id="11" string="involved" />
            <token id="12" string="in" />
            <token id="13" string="negotiations" />
            <token id="14" string="to" />
            <token id="15" string="buy" />
            <token id="16" string="a" />
            <token id="17" string="U.S." />
            <token id="18" string="newspaper" />
            <token id="19" string="valued" />
            <token id="20" string="&quot;" />
            <token id="21" string="in" />
            <token id="22" string="the" />
            <token id="23" string="hundreds" />
            <token id="24" string="of" />
            <token id="25" string="millions" />
            <token id="26" string="of" />
            <token id="27" string="dollars" />
          </tokens>
        </chunking>
        <chunking id="7" string="millions of dollars" type="NP">
          <tokens>
            <token id="25" string="millions" />
            <token id="26" string="of" />
            <token id="27" string="dollars" />
          </tokens>
        </chunking>
        <chunking id="8" string="dollars" type="NP">
          <tokens>
            <token id="27" string="dollars" />
          </tokens>
        </chunking>
        <chunking id="9" string="that he is involved in negotiations to buy a U.S. newspaper valued `` in the hundreds of millions of dollars" type="SBAR">
          <tokens>
            <token id="8" string="that" />
            <token id="9" string="he" />
            <token id="10" string="is" />
            <token id="11" string="involved" />
            <token id="12" string="in" />
            <token id="13" string="negotiations" />
            <token id="14" string="to" />
            <token id="15" string="buy" />
            <token id="16" string="a" />
            <token id="17" string="U.S." />
            <token id="18" string="newspaper" />
            <token id="19" string="valued" />
            <token id="20" string="&quot;" />
            <token id="21" string="in" />
            <token id="22" string="the" />
            <token id="23" string="hundreds" />
            <token id="24" string="of" />
            <token id="25" string="millions" />
            <token id="26" string="of" />
            <token id="27" string="dollars" />
          </tokens>
        </chunking>
        <chunking id="10" string="valued `` in the hundreds of millions of dollars" type="VP">
          <tokens>
            <token id="19" string="valued" />
            <token id="20" string="&quot;" />
            <token id="21" string="in" />
            <token id="22" string="the" />
            <token id="23" string="hundreds" />
            <token id="24" string="of" />
            <token id="25" string="millions" />
            <token id="26" string="of" />
            <token id="27" string="dollars" />
          </tokens>
        </chunking>
        <chunking id="11" string="said Monday that he is involved in negotiations to buy a U.S. newspaper valued `` in the hundreds of millions of dollars" type="VP">
          <tokens>
            <token id="6" string="said" />
            <token id="7" string="Monday" />
            <token id="8" string="that" />
            <token id="9" string="he" />
            <token id="10" string="is" />
            <token id="11" string="involved" />
            <token id="12" string="in" />
            <token id="13" string="negotiations" />
            <token id="14" string="to" />
            <token id="15" string="buy" />
            <token id="16" string="a" />
            <token id="17" string="U.S." />
            <token id="18" string="newspaper" />
            <token id="19" string="valued" />
            <token id="20" string="&quot;" />
            <token id="21" string="in" />
            <token id="22" string="the" />
            <token id="23" string="hundreds" />
            <token id="24" string="of" />
            <token id="25" string="millions" />
            <token id="26" string="of" />
            <token id="27" string="dollars" />
          </tokens>
        </chunking>
        <chunking id="12" string="a U.S. newspaper valued `` in the hundreds of millions of dollars" type="NP">
          <tokens>
            <token id="16" string="a" />
            <token id="17" string="U.S." />
            <token id="18" string="newspaper" />
            <token id="19" string="valued" />
            <token id="20" string="&quot;" />
            <token id="21" string="in" />
            <token id="22" string="the" />
            <token id="23" string="hundreds" />
            <token id="24" string="of" />
            <token id="25" string="millions" />
            <token id="26" string="of" />
            <token id="27" string="dollars" />
          </tokens>
        </chunking>
        <chunking id="13" string="a U.S. newspaper" type="NP">
          <tokens>
            <token id="16" string="a" />
            <token id="17" string="U.S." />
            <token id="18" string="newspaper" />
          </tokens>
        </chunking>
        <chunking id="14" string="British media entrepreneur Robert Maxwell" type="NP">
          <tokens>
            <token id="1" string="British" />
            <token id="2" string="media" />
            <token id="3" string="entrepreneur" />
            <token id="4" string="Robert" />
            <token id="5" string="Maxwell" />
          </tokens>
        </chunking>
        <chunking id="15" string="entrepreneur Robert Maxwell" type="NP">
          <tokens>
            <token id="3" string="entrepreneur" />
            <token id="4" string="Robert" />
            <token id="5" string="Maxwell" />
          </tokens>
        </chunking>
        <chunking id="16" string="involved in negotiations to buy a U.S. newspaper valued `` in the hundreds of millions of dollars" type="VP">
          <tokens>
            <token id="11" string="involved" />
            <token id="12" string="in" />
            <token id="13" string="negotiations" />
            <token id="14" string="to" />
            <token id="15" string="buy" />
            <token id="16" string="a" />
            <token id="17" string="U.S." />
            <token id="18" string="newspaper" />
            <token id="19" string="valued" />
            <token id="20" string="&quot;" />
            <token id="21" string="in" />
            <token id="22" string="the" />
            <token id="23" string="hundreds" />
            <token id="24" string="of" />
            <token id="25" string="millions" />
            <token id="26" string="of" />
            <token id="27" string="dollars" />
          </tokens>
        </chunking>
        <chunking id="17" string="millions" type="NP">
          <tokens>
            <token id="25" string="millions" />
          </tokens>
        </chunking>
        <chunking id="18" string="he" type="NP">
          <tokens>
            <token id="9" string="he" />
          </tokens>
        </chunking>
        <chunking id="19" string="the hundreds" type="NP">
          <tokens>
            <token id="22" string="the" />
            <token id="23" string="hundreds" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="amod">
          <governor id="2">media</governor>
          <dependent id="1">British</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">said</governor>
          <dependent id="2">media</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">Maxwell</governor>
          <dependent id="3">entrepreneur</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">Maxwell</governor>
          <dependent id="4">Robert</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="2">media</governor>
          <dependent id="5">Maxwell</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">said</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="6">said</governor>
          <dependent id="7">Monday</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="11">involved</governor>
          <dependent id="8">that</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="11">involved</governor>
          <dependent id="9">he</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="11">involved</governor>
          <dependent id="10">is</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="6">said</governor>
          <dependent id="11">involved</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">negotiations</governor>
          <dependent id="12">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">involved</governor>
          <dependent id="13">negotiations</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="15">buy</governor>
          <dependent id="14">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="11">involved</governor>
          <dependent id="15">buy</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">newspaper</governor>
          <dependent id="16">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="18">newspaper</governor>
          <dependent id="17">U.S.</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">buy</governor>
          <dependent id="18">newspaper</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="18">newspaper</governor>
          <dependent id="19">valued</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">hundreds</governor>
          <dependent id="21">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="23">hundreds</governor>
          <dependent id="22">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">valued</governor>
          <dependent id="23">hundreds</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">millions</governor>
          <dependent id="24">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">hundreds</governor>
          <dependent id="25">millions</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">dollars</governor>
          <dependent id="26">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="25">millions</governor>
          <dependent id="27">dollars</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="British" type="NATIONALITY" score="0.0">
          <tokens>
            <token id="1" string="British" />
          </tokens>
        </entity>
        <entity id="2" string="U.S." type="LOCATION" score="0.0">
          <tokens>
            <token id="17" string="U.S." />
          </tokens>
        </entity>
        <entity id="3" string="Robert Maxwell" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="Robert" />
            <token id="5" string="Maxwell" />
          </tokens>
        </entity>
        <entity id="4" string="Monday" type="DATE" score="0.0">
          <tokens>
            <token id="7" string="Monday" />
          </tokens>
        </entity>
        <entity id="5" string="millions of dollars" type="MONEY" score="0.0">
          <tokens>
            <token id="25" string="millions" />
            <token id="26" string="of" />
            <token id="27" string="dollars" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="2" has_coreference="true">
      <content>Maxwell declined to identify the newspaper, but his comments prompted industry speculation that it might be Gannett&amp;apost;s USA Today, Tribune Co.&amp;apost;s New York Daily News or the Chicago Sun-Times.</content>
      <tokens>
        <token id="1" string="Maxwell" lemma="Maxwell" stem="maxwel" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="declined" lemma="decline" stem="declin" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="identify" lemma="identify" stem="identifi" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="newspaper" lemma="newspaper" stem="newspap" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="comments" lemma="comment" stem="comment" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="prompted" lemma="prompt" stem="prompt" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="industry" lemma="industry" stem="industri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="speculation" lemma="speculation" stem="specul" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="might" lemma="might" stem="might" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="be" lemma="be" stem="be" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="Gannett" lemma="Gannett" stem="gannett" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="19" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="20" string="USA" lemma="USA" stem="usa" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="21" string="Today" lemma="today" stem="todai" pos="NN" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="22" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="Tribune" lemma="Tribune" stem="tribun" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="24" string="Co." lemma="Co." stem="co." pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="25" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="New" lemma="New" stem="new" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="27" string="York" lemma="York" stem="york" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="28" string="Daily" lemma="Daily" stem="daili" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="29" string="News" lemma="News" stem="new" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="30" string="or" lemma="or" stem="or" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="32" string="Chicago" lemma="Chicago" stem="chicago" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="33" string="Sun-Times" lemma="Sun-Times" stem="sun-tim" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="34" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (NNP Maxwell)) (VP (VBD declined) (S (VP (TO to) (VP (VB identify) (NP (DT the) (NN newspaper))))))) (, ,) (CC but) (S (NP (PRP$ his) (NNS comments)) (VP (VBD prompted) (NP (NN industry) (NN speculation)) (SBAR (IN that) (S (NP (PRP it)) (VP (MD might) (VP (VB be) (NP (NP (NP (NNP Gannett) (POS 's)) (NNP USA) (NN Today)) (, ,) (NP (NP (NP (NNP Tribune) (NNP Co.) (POS 's)) (NNP New) (NNP York) (NNP Daily) (NNP News)) (CC or) (NP (DT the) (NNP Chicago) (NNP Sun-Times)))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Tribune Co. 's New York Daily News or the Chicago Sun-Times" type="NP">
          <tokens>
            <token id="23" string="Tribune" />
            <token id="24" string="Co." />
            <token id="25" string="'s" />
            <token id="26" string="New" />
            <token id="27" string="York" />
            <token id="28" string="Daily" />
            <token id="29" string="News" />
            <token id="30" string="or" />
            <token id="31" string="the" />
            <token id="32" string="Chicago" />
            <token id="33" string="Sun-Times" />
          </tokens>
        </chunking>
        <chunking id="2" string="the newspaper" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="newspaper" />
          </tokens>
        </chunking>
        <chunking id="3" string="Maxwell" type="NP">
          <tokens>
            <token id="1" string="Maxwell" />
          </tokens>
        </chunking>
        <chunking id="4" string="declined to identify the newspaper" type="VP">
          <tokens>
            <token id="2" string="declined" />
            <token id="3" string="to" />
            <token id="4" string="identify" />
            <token id="5" string="the" />
            <token id="6" string="newspaper" />
          </tokens>
        </chunking>
        <chunking id="5" string="the Chicago Sun-Times" type="NP">
          <tokens>
            <token id="31" string="the" />
            <token id="32" string="Chicago" />
            <token id="33" string="Sun-Times" />
          </tokens>
        </chunking>
        <chunking id="6" string="it" type="NP">
          <tokens>
            <token id="15" string="it" />
          </tokens>
        </chunking>
        <chunking id="7" string="Tribune Co. 's New York Daily News" type="NP">
          <tokens>
            <token id="23" string="Tribune" />
            <token id="24" string="Co." />
            <token id="25" string="'s" />
            <token id="26" string="New" />
            <token id="27" string="York" />
            <token id="28" string="Daily" />
            <token id="29" string="News" />
          </tokens>
        </chunking>
        <chunking id="8" string="Gannett 's USA Today" type="NP">
          <tokens>
            <token id="18" string="Gannett" />
            <token id="19" string="'s" />
            <token id="20" string="USA" />
            <token id="21" string="Today" />
          </tokens>
        </chunking>
        <chunking id="9" string="Gannett 's" type="NP">
          <tokens>
            <token id="18" string="Gannett" />
            <token id="19" string="'s" />
          </tokens>
        </chunking>
        <chunking id="10" string="might be Gannett 's USA Today , Tribune Co. 's New York Daily News or the Chicago Sun-Times" type="VP">
          <tokens>
            <token id="16" string="might" />
            <token id="17" string="be" />
            <token id="18" string="Gannett" />
            <token id="19" string="'s" />
            <token id="20" string="USA" />
            <token id="21" string="Today" />
            <token id="22" string="," />
            <token id="23" string="Tribune" />
            <token id="24" string="Co." />
            <token id="25" string="'s" />
            <token id="26" string="New" />
            <token id="27" string="York" />
            <token id="28" string="Daily" />
            <token id="29" string="News" />
            <token id="30" string="or" />
            <token id="31" string="the" />
            <token id="32" string="Chicago" />
            <token id="33" string="Sun-Times" />
          </tokens>
        </chunking>
        <chunking id="11" string="prompted industry speculation that it might be Gannett 's USA Today , Tribune Co. 's New York Daily News or the Chicago Sun-Times" type="VP">
          <tokens>
            <token id="11" string="prompted" />
            <token id="12" string="industry" />
            <token id="13" string="speculation" />
            <token id="14" string="that" />
            <token id="15" string="it" />
            <token id="16" string="might" />
            <token id="17" string="be" />
            <token id="18" string="Gannett" />
            <token id="19" string="'s" />
            <token id="20" string="USA" />
            <token id="21" string="Today" />
            <token id="22" string="," />
            <token id="23" string="Tribune" />
            <token id="24" string="Co." />
            <token id="25" string="'s" />
            <token id="26" string="New" />
            <token id="27" string="York" />
            <token id="28" string="Daily" />
            <token id="29" string="News" />
            <token id="30" string="or" />
            <token id="31" string="the" />
            <token id="32" string="Chicago" />
            <token id="33" string="Sun-Times" />
          </tokens>
        </chunking>
        <chunking id="12" string="to identify the newspaper" type="VP">
          <tokens>
            <token id="3" string="to" />
            <token id="4" string="identify" />
            <token id="5" string="the" />
            <token id="6" string="newspaper" />
          </tokens>
        </chunking>
        <chunking id="13" string="Tribune Co. 's" type="NP">
          <tokens>
            <token id="23" string="Tribune" />
            <token id="24" string="Co." />
            <token id="25" string="'s" />
          </tokens>
        </chunking>
        <chunking id="14" string="industry speculation" type="NP">
          <tokens>
            <token id="12" string="industry" />
            <token id="13" string="speculation" />
          </tokens>
        </chunking>
        <chunking id="15" string="be Gannett 's USA Today , Tribune Co. 's New York Daily News or the Chicago Sun-Times" type="VP">
          <tokens>
            <token id="17" string="be" />
            <token id="18" string="Gannett" />
            <token id="19" string="'s" />
            <token id="20" string="USA" />
            <token id="21" string="Today" />
            <token id="22" string="," />
            <token id="23" string="Tribune" />
            <token id="24" string="Co." />
            <token id="25" string="'s" />
            <token id="26" string="New" />
            <token id="27" string="York" />
            <token id="28" string="Daily" />
            <token id="29" string="News" />
            <token id="30" string="or" />
            <token id="31" string="the" />
            <token id="32" string="Chicago" />
            <token id="33" string="Sun-Times" />
          </tokens>
        </chunking>
        <chunking id="16" string="Gannett 's USA Today , Tribune Co. 's New York Daily News or the Chicago Sun-Times" type="NP">
          <tokens>
            <token id="18" string="Gannett" />
            <token id="19" string="'s" />
            <token id="20" string="USA" />
            <token id="21" string="Today" />
            <token id="22" string="," />
            <token id="23" string="Tribune" />
            <token id="24" string="Co." />
            <token id="25" string="'s" />
            <token id="26" string="New" />
            <token id="27" string="York" />
            <token id="28" string="Daily" />
            <token id="29" string="News" />
            <token id="30" string="or" />
            <token id="31" string="the" />
            <token id="32" string="Chicago" />
            <token id="33" string="Sun-Times" />
          </tokens>
        </chunking>
        <chunking id="17" string="that it might be Gannett 's USA Today , Tribune Co. 's New York Daily News or the Chicago Sun-Times" type="SBAR">
          <tokens>
            <token id="14" string="that" />
            <token id="15" string="it" />
            <token id="16" string="might" />
            <token id="17" string="be" />
            <token id="18" string="Gannett" />
            <token id="19" string="'s" />
            <token id="20" string="USA" />
            <token id="21" string="Today" />
            <token id="22" string="," />
            <token id="23" string="Tribune" />
            <token id="24" string="Co." />
            <token id="25" string="'s" />
            <token id="26" string="New" />
            <token id="27" string="York" />
            <token id="28" string="Daily" />
            <token id="29" string="News" />
            <token id="30" string="or" />
            <token id="31" string="the" />
            <token id="32" string="Chicago" />
            <token id="33" string="Sun-Times" />
          </tokens>
        </chunking>
        <chunking id="18" string="identify the newspaper" type="VP">
          <tokens>
            <token id="4" string="identify" />
            <token id="5" string="the" />
            <token id="6" string="newspaper" />
          </tokens>
        </chunking>
        <chunking id="19" string="his comments" type="NP">
          <tokens>
            <token id="9" string="his" />
            <token id="10" string="comments" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">declined</governor>
          <dependent id="1">Maxwell</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">declined</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="4">identify</governor>
          <dependent id="3">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="2">declined</governor>
          <dependent id="4">identify</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">newspaper</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">identify</governor>
          <dependent id="6">newspaper</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="2">declined</governor>
          <dependent id="8">but</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="10">comments</governor>
          <dependent id="9">his</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">prompted</governor>
          <dependent id="10">comments</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="2">declined</governor>
          <dependent id="11">prompted</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="13">speculation</governor>
          <dependent id="12">industry</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">prompted</governor>
          <dependent id="13">speculation</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="21">Today</governor>
          <dependent id="14">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">Today</governor>
          <dependent id="15">it</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="21">Today</governor>
          <dependent id="16">might</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="21">Today</governor>
          <dependent id="17">be</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="21">Today</governor>
          <dependent id="18">Gannett</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">Gannett</governor>
          <dependent id="19">'s</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="21">Today</governor>
          <dependent id="20">USA</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="11">prompted</governor>
          <dependent id="21">Today</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="24">Co.</governor>
          <dependent id="23">Tribune</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="29">News</governor>
          <dependent id="24">Co.</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">Co.</governor>
          <dependent id="25">'s</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="29">News</governor>
          <dependent id="26">New</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="29">News</governor>
          <dependent id="27">York</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="29">News</governor>
          <dependent id="28">Daily</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="21">Today</governor>
          <dependent id="29">News</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="29">News</governor>
          <dependent id="30">or</dependent>
        </dependency>
        <dependency type="det">
          <governor id="33">Sun-Times</governor>
          <dependent id="31">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="33">Sun-Times</governor>
          <dependent id="32">Chicago</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="29">News</governor>
          <dependent id="33">Sun-Times</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="New York Daily News" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="26" string="New" />
            <token id="27" string="York" />
            <token id="28" string="Daily" />
            <token id="29" string="News" />
          </tokens>
        </entity>
        <entity id="2" string="Tribune Co." type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="23" string="Tribune" />
            <token id="24" string="Co." />
          </tokens>
        </entity>
        <entity id="3" string="Gannett" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="18" string="Gannett" />
          </tokens>
        </entity>
        <entity id="4" string="Maxwell" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Maxwell" />
          </tokens>
        </entity>
        <entity id="5" string="USA Today" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="20" string="USA" />
            <token id="21" string="Today" />
          </tokens>
        </entity>
        <entity id="6" string="Chicago Sun-Times" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="32" string="Chicago" />
            <token id="33" string="Sun-Times" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="3" has_coreference="true">
      <content>Officials of each of those newspapers denied that they were negotiating with Maxwell.</content>
      <tokens>
        <token id="1" string="Officials" lemma="official" stem="official" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="2" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="3" string="each" lemma="each" stem="each" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="4" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="those" lemma="those" stem="those" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="6" string="newspapers" lemma="newspaper" stem="newspap" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="7" string="denied" lemma="deny" stem="deni" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="negotiating" lemma="negotiate" stem="negoti" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="Maxwell" lemma="Maxwell" stem="maxwel" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNS Officials)) (PP (IN of) (NP (NP (DT each)) (PP (IN of) (NP (DT those) (NNS newspapers)))))) (VP (VBD denied) (SBAR (IN that) (S (NP (PRP they)) (VP (VBD were) (VP (VBG negotiating) (PP (IN with) (NP (NNP Maxwell)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="denied that they were negotiating with Maxwell" type="VP">
          <tokens>
            <token id="7" string="denied" />
            <token id="8" string="that" />
            <token id="9" string="they" />
            <token id="10" string="were" />
            <token id="11" string="negotiating" />
            <token id="12" string="with" />
            <token id="13" string="Maxwell" />
          </tokens>
        </chunking>
        <chunking id="2" string="they" type="NP">
          <tokens>
            <token id="9" string="they" />
          </tokens>
        </chunking>
        <chunking id="3" string="those newspapers" type="NP">
          <tokens>
            <token id="5" string="those" />
            <token id="6" string="newspapers" />
          </tokens>
        </chunking>
        <chunking id="4" string="that they were negotiating with Maxwell" type="SBAR">
          <tokens>
            <token id="8" string="that" />
            <token id="9" string="they" />
            <token id="10" string="were" />
            <token id="11" string="negotiating" />
            <token id="12" string="with" />
            <token id="13" string="Maxwell" />
          </tokens>
        </chunking>
        <chunking id="5" string="Officials" type="NP">
          <tokens>
            <token id="1" string="Officials" />
          </tokens>
        </chunking>
        <chunking id="6" string="were negotiating with Maxwell" type="VP">
          <tokens>
            <token id="10" string="were" />
            <token id="11" string="negotiating" />
            <token id="12" string="with" />
            <token id="13" string="Maxwell" />
          </tokens>
        </chunking>
        <chunking id="7" string="Maxwell" type="NP">
          <tokens>
            <token id="13" string="Maxwell" />
          </tokens>
        </chunking>
        <chunking id="8" string="each of those newspapers" type="NP">
          <tokens>
            <token id="3" string="each" />
            <token id="4" string="of" />
            <token id="5" string="those" />
            <token id="6" string="newspapers" />
          </tokens>
        </chunking>
        <chunking id="9" string="negotiating with Maxwell" type="VP">
          <tokens>
            <token id="11" string="negotiating" />
            <token id="12" string="with" />
            <token id="13" string="Maxwell" />
          </tokens>
        </chunking>
        <chunking id="10" string="Officials of each of those newspapers" type="NP">
          <tokens>
            <token id="1" string="Officials" />
            <token id="2" string="of" />
            <token id="3" string="each" />
            <token id="4" string="of" />
            <token id="5" string="those" />
            <token id="6" string="newspapers" />
          </tokens>
        </chunking>
        <chunking id="11" string="each" type="NP">
          <tokens>
            <token id="3" string="each" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="7">denied</governor>
          <dependent id="1">Officials</dependent>
        </dependency>
        <dependency type="case">
          <governor id="3">each</governor>
          <dependent id="2">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="1">Officials</governor>
          <dependent id="3">each</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">newspapers</governor>
          <dependent id="4">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">newspapers</governor>
          <dependent id="5">those</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">each</governor>
          <dependent id="6">newspapers</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">denied</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="11">negotiating</governor>
          <dependent id="8">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">negotiating</governor>
          <dependent id="9">they</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="11">negotiating</governor>
          <dependent id="10">were</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="7">denied</governor>
          <dependent id="11">negotiating</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">Maxwell</governor>
          <dependent id="12">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">negotiating</governor>
          <dependent id="13">Maxwell</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Maxwell" type="PERSON" score="0.0">
          <tokens>
            <token id="13" string="Maxwell" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="4" has_coreference="true">
      <content>Maxwell&amp;apost;s comments were made to reporters at the annual meeting in New York of Maxwell&amp;apost;s Berlitz International and were confirmed later by a spokesman for Maxwell Communications, the investor&amp;apost;s London-based holding company.</content>
      <tokens>
        <token id="1" string="Maxwell" lemma="Maxwell" stem="maxwel" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="comments" lemma="comment" stem="comment" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="made" lemma="make" stem="made" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="reporters" lemma="reporter" stem="report" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="annual" lemma="annual" stem="annual" pos="JJ" type="Word" isStopWord="false" ner="SET" is_referenced="false" is_refers="false" />
        <token id="11" string="meeting" lemma="meeting" stem="meet" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="New" lemma="New" stem="new" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="14" string="York" lemma="York" stem="york" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="15" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="Maxwell" lemma="Maxwell" stem="maxwel" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="17" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="Berlitz" lemma="Berlitz" stem="berlitz" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="19" string="International" lemma="International" stem="internat" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="20" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="confirmed" lemma="confirm" stem="confirm" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="later" lemma="later" stem="later" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="26" string="spokesman" lemma="spokesman" stem="spokesman" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="27" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="28" string="Maxwell" lemma="Maxwell" stem="maxwel" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="29" string="Communications" lemma="Communications" stem="commun" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="30" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="31" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="32" string="investor" lemma="investor" stem="investor" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="33" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="34" string="London-based" lemma="london-based" stem="london-bas" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="35" string="holding" lemma="hold" stem="hold" pos="VBG" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="36" string="company" lemma="company" stem="compani" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="37" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Maxwell) (POS 's)) (NNS comments)) (VP (VP (VBD were) (VP (VBN made) (PP (TO to) (NP (NP (NNS reporters)) (PP (IN at) (NP (NP (DT the) (JJ annual) (NN meeting)) (PP (IN in) (NP (NP (NNP New) (NNP York)) (PP (IN of) (NP (NP (NNP Maxwell) (POS 's)) (NNP Berlitz) (NNP International))))))))))) (CC and) (VP (VBD were) (VP (VBN confirmed) (ADVP (RB later)) (PP (IN by) (NP (NP (DT a) (NN spokesman)) (PP (IN for) (NP (NP (NNP Maxwell) (NNP Communications)) (, ,) (NP (NP (DT the) (NN investor) (POS 's)) (JJ London-based) (VBG holding) (NN company))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="New York" type="NP">
          <tokens>
            <token id="13" string="New" />
            <token id="14" string="York" />
          </tokens>
        </chunking>
        <chunking id="2" string="were made to reporters at the annual meeting in New York of Maxwell 's Berlitz International and were confirmed later by a spokesman for Maxwell Communications , the investor 's London-based holding company" type="VP">
          <tokens>
            <token id="4" string="were" />
            <token id="5" string="made" />
            <token id="6" string="to" />
            <token id="7" string="reporters" />
            <token id="8" string="at" />
            <token id="9" string="the" />
            <token id="10" string="annual" />
            <token id="11" string="meeting" />
            <token id="12" string="in" />
            <token id="13" string="New" />
            <token id="14" string="York" />
            <token id="15" string="of" />
            <token id="16" string="Maxwell" />
            <token id="17" string="'s" />
            <token id="18" string="Berlitz" />
            <token id="19" string="International" />
            <token id="20" string="and" />
            <token id="21" string="were" />
            <token id="22" string="confirmed" />
            <token id="23" string="later" />
            <token id="24" string="by" />
            <token id="25" string="a" />
            <token id="26" string="spokesman" />
            <token id="27" string="for" />
            <token id="28" string="Maxwell" />
            <token id="29" string="Communications" />
            <token id="30" string="," />
            <token id="31" string="the" />
            <token id="32" string="investor" />
            <token id="33" string="'s" />
            <token id="34" string="London-based" />
            <token id="35" string="holding" />
            <token id="36" string="company" />
          </tokens>
        </chunking>
        <chunking id="3" string="reporters" type="NP">
          <tokens>
            <token id="7" string="reporters" />
          </tokens>
        </chunking>
        <chunking id="4" string="Maxwell 's Berlitz International" type="NP">
          <tokens>
            <token id="16" string="Maxwell" />
            <token id="17" string="'s" />
            <token id="18" string="Berlitz" />
            <token id="19" string="International" />
          </tokens>
        </chunking>
        <chunking id="5" string="Maxwell Communications" type="NP">
          <tokens>
            <token id="28" string="Maxwell" />
            <token id="29" string="Communications" />
          </tokens>
        </chunking>
        <chunking id="6" string="confirmed later by a spokesman for Maxwell Communications , the investor 's London-based holding company" type="VP">
          <tokens>
            <token id="22" string="confirmed" />
            <token id="23" string="later" />
            <token id="24" string="by" />
            <token id="25" string="a" />
            <token id="26" string="spokesman" />
            <token id="27" string="for" />
            <token id="28" string="Maxwell" />
            <token id="29" string="Communications" />
            <token id="30" string="," />
            <token id="31" string="the" />
            <token id="32" string="investor" />
            <token id="33" string="'s" />
            <token id="34" string="London-based" />
            <token id="35" string="holding" />
            <token id="36" string="company" />
          </tokens>
        </chunking>
        <chunking id="7" string="a spokesman for Maxwell Communications , the investor 's London-based holding company" type="NP">
          <tokens>
            <token id="25" string="a" />
            <token id="26" string="spokesman" />
            <token id="27" string="for" />
            <token id="28" string="Maxwell" />
            <token id="29" string="Communications" />
            <token id="30" string="," />
            <token id="31" string="the" />
            <token id="32" string="investor" />
            <token id="33" string="'s" />
            <token id="34" string="London-based" />
            <token id="35" string="holding" />
            <token id="36" string="company" />
          </tokens>
        </chunking>
        <chunking id="8" string="the investor 's London-based holding company" type="NP">
          <tokens>
            <token id="31" string="the" />
            <token id="32" string="investor" />
            <token id="33" string="'s" />
            <token id="34" string="London-based" />
            <token id="35" string="holding" />
            <token id="36" string="company" />
          </tokens>
        </chunking>
        <chunking id="9" string="Maxwell 's" type="NP">
          <tokens>
            <token id="1" string="Maxwell" />
            <token id="2" string="'s" />
          </tokens>
        </chunking>
        <chunking id="10" string="the annual meeting in New York of Maxwell 's Berlitz International" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="annual" />
            <token id="11" string="meeting" />
            <token id="12" string="in" />
            <token id="13" string="New" />
            <token id="14" string="York" />
            <token id="15" string="of" />
            <token id="16" string="Maxwell" />
            <token id="17" string="'s" />
            <token id="18" string="Berlitz" />
            <token id="19" string="International" />
          </tokens>
        </chunking>
        <chunking id="11" string="the annual meeting" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="annual" />
            <token id="11" string="meeting" />
          </tokens>
        </chunking>
        <chunking id="12" string="were made to reporters at the annual meeting in New York of Maxwell 's Berlitz International" type="VP">
          <tokens>
            <token id="4" string="were" />
            <token id="5" string="made" />
            <token id="6" string="to" />
            <token id="7" string="reporters" />
            <token id="8" string="at" />
            <token id="9" string="the" />
            <token id="10" string="annual" />
            <token id="11" string="meeting" />
            <token id="12" string="in" />
            <token id="13" string="New" />
            <token id="14" string="York" />
            <token id="15" string="of" />
            <token id="16" string="Maxwell" />
            <token id="17" string="'s" />
            <token id="18" string="Berlitz" />
            <token id="19" string="International" />
          </tokens>
        </chunking>
        <chunking id="13" string="made to reporters at the annual meeting in New York of Maxwell 's Berlitz International" type="VP">
          <tokens>
            <token id="5" string="made" />
            <token id="6" string="to" />
            <token id="7" string="reporters" />
            <token id="8" string="at" />
            <token id="9" string="the" />
            <token id="10" string="annual" />
            <token id="11" string="meeting" />
            <token id="12" string="in" />
            <token id="13" string="New" />
            <token id="14" string="York" />
            <token id="15" string="of" />
            <token id="16" string="Maxwell" />
            <token id="17" string="'s" />
            <token id="18" string="Berlitz" />
            <token id="19" string="International" />
          </tokens>
        </chunking>
        <chunking id="14" string="Maxwell 's comments" type="NP">
          <tokens>
            <token id="1" string="Maxwell" />
            <token id="2" string="'s" />
            <token id="3" string="comments" />
          </tokens>
        </chunking>
        <chunking id="15" string="were confirmed later by a spokesman for Maxwell Communications , the investor 's London-based holding company" type="VP">
          <tokens>
            <token id="21" string="were" />
            <token id="22" string="confirmed" />
            <token id="23" string="later" />
            <token id="24" string="by" />
            <token id="25" string="a" />
            <token id="26" string="spokesman" />
            <token id="27" string="for" />
            <token id="28" string="Maxwell" />
            <token id="29" string="Communications" />
            <token id="30" string="," />
            <token id="31" string="the" />
            <token id="32" string="investor" />
            <token id="33" string="'s" />
            <token id="34" string="London-based" />
            <token id="35" string="holding" />
            <token id="36" string="company" />
          </tokens>
        </chunking>
        <chunking id="16" string="a spokesman" type="NP">
          <tokens>
            <token id="25" string="a" />
            <token id="26" string="spokesman" />
          </tokens>
        </chunking>
        <chunking id="17" string="Maxwell Communications , the investor 's London-based holding company" type="NP">
          <tokens>
            <token id="28" string="Maxwell" />
            <token id="29" string="Communications" />
            <token id="30" string="," />
            <token id="31" string="the" />
            <token id="32" string="investor" />
            <token id="33" string="'s" />
            <token id="34" string="London-based" />
            <token id="35" string="holding" />
            <token id="36" string="company" />
          </tokens>
        </chunking>
        <chunking id="18" string="New York of Maxwell 's Berlitz International" type="NP">
          <tokens>
            <token id="13" string="New" />
            <token id="14" string="York" />
            <token id="15" string="of" />
            <token id="16" string="Maxwell" />
            <token id="17" string="'s" />
            <token id="18" string="Berlitz" />
            <token id="19" string="International" />
          </tokens>
        </chunking>
        <chunking id="19" string="the investor 's" type="NP">
          <tokens>
            <token id="31" string="the" />
            <token id="32" string="investor" />
            <token id="33" string="'s" />
          </tokens>
        </chunking>
        <chunking id="20" string="reporters at the annual meeting in New York of Maxwell 's Berlitz International" type="NP">
          <tokens>
            <token id="7" string="reporters" />
            <token id="8" string="at" />
            <token id="9" string="the" />
            <token id="10" string="annual" />
            <token id="11" string="meeting" />
            <token id="12" string="in" />
            <token id="13" string="New" />
            <token id="14" string="York" />
            <token id="15" string="of" />
            <token id="16" string="Maxwell" />
            <token id="17" string="'s" />
            <token id="18" string="Berlitz" />
            <token id="19" string="International" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="3">comments</governor>
          <dependent id="1">Maxwell</dependent>
        </dependency>
        <dependency type="case">
          <governor id="1">Maxwell</governor>
          <dependent id="2">'s</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="5">made</governor>
          <dependent id="3">comments</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="5">made</governor>
          <dependent id="4">were</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">made</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">reporters</governor>
          <dependent id="6">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">made</governor>
          <dependent id="7">reporters</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">meeting</governor>
          <dependent id="8">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">meeting</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">meeting</governor>
          <dependent id="10">annual</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">reporters</governor>
          <dependent id="11">meeting</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">York</governor>
          <dependent id="12">in</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">York</governor>
          <dependent id="13">New</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">meeting</governor>
          <dependent id="14">York</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">International</governor>
          <dependent id="15">of</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="19">International</governor>
          <dependent id="16">Maxwell</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">Maxwell</governor>
          <dependent id="17">'s</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">International</governor>
          <dependent id="18">Berlitz</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">York</governor>
          <dependent id="19">International</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="5">made</governor>
          <dependent id="20">and</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="22">confirmed</governor>
          <dependent id="21">were</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="5">made</governor>
          <dependent id="22">confirmed</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="22">confirmed</governor>
          <dependent id="23">later</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">spokesman</governor>
          <dependent id="24">by</dependent>
        </dependency>
        <dependency type="det">
          <governor id="26">spokesman</governor>
          <dependent id="25">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="22">confirmed</governor>
          <dependent id="26">spokesman</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">Communications</governor>
          <dependent id="27">for</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="29">Communications</governor>
          <dependent id="28">Maxwell</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">spokesman</governor>
          <dependent id="29">Communications</dependent>
        </dependency>
        <dependency type="det">
          <governor id="32">investor</governor>
          <dependent id="31">the</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="36">company</governor>
          <dependent id="32">investor</dependent>
        </dependency>
        <dependency type="case">
          <governor id="32">investor</governor>
          <dependent id="33">'s</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="36">company</governor>
          <dependent id="34">London-based</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="36">company</governor>
          <dependent id="35">holding</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="29">Communications</governor>
          <dependent id="36">company</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="New York" type="LOCATION" score="0.0">
          <tokens>
            <token id="13" string="New" />
            <token id="14" string="York" />
          </tokens>
        </entity>
        <entity id="2" string="Maxwell Communications" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="28" string="Maxwell" />
            <token id="29" string="Communications" />
          </tokens>
        </entity>
        <entity id="3" string="Maxwell" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Maxwell" />
          </tokens>
        </entity>
        <entity id="4" string="annual" type="SET" score="0.0">
          <tokens>
            <token id="10" string="annual" />
          </tokens>
        </entity>
        <entity id="5" string="Berlitz International" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="18" string="Berlitz" />
            <token id="19" string="International" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="5" has_coreference="true">
      <content>The spokesman declined to elaborate on the remarks, and said Maxwell had left the United States.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="spokesman" lemma="spokesman" stem="spokesman" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="declined" lemma="decline" stem="declin" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="elaborate" lemma="elaborate" stem="elabor" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="remarks" lemma="remark" stem="remark" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="Maxwell" lemma="Maxwell" stem="maxwel" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="13" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="left" lemma="leave" stem="left" pos="VBN" type="Word" isStopWord="false" ner="IDEOLOGY" is_referenced="false" is_refers="false" />
        <token id="15" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="16" string="United" lemma="United" stem="unite" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="17" string="States" lemma="States" stem="state" pos="NNPS" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="18" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN spokesman)) (VP (VP (VBD declined) (S (VP (TO to) (VP (VB elaborate) (PP (IN on) (NP (DT the) (NNS remarks))))))) (, ,) (CC and) (VP (VBD said) (SBAR (S (NP (NNP Maxwell)) (VP (VBD had) (VP (VBN left) (NP (DT the) (NNP United) (NNPS States)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Maxwell had left the United States" type="SBAR">
          <tokens>
            <token id="12" string="Maxwell" />
            <token id="13" string="had" />
            <token id="14" string="left" />
            <token id="15" string="the" />
            <token id="16" string="United" />
            <token id="17" string="States" />
          </tokens>
        </chunking>
        <chunking id="2" string="the United States" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="United" />
            <token id="17" string="States" />
          </tokens>
        </chunking>
        <chunking id="3" string="The spokesman" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="spokesman" />
          </tokens>
        </chunking>
        <chunking id="4" string="elaborate on the remarks" type="VP">
          <tokens>
            <token id="5" string="elaborate" />
            <token id="6" string="on" />
            <token id="7" string="the" />
            <token id="8" string="remarks" />
          </tokens>
        </chunking>
        <chunking id="5" string="said Maxwell had left the United States" type="VP">
          <tokens>
            <token id="11" string="said" />
            <token id="12" string="Maxwell" />
            <token id="13" string="had" />
            <token id="14" string="left" />
            <token id="15" string="the" />
            <token id="16" string="United" />
            <token id="17" string="States" />
          </tokens>
        </chunking>
        <chunking id="6" string="declined to elaborate on the remarks , and said Maxwell had left the United States" type="VP">
          <tokens>
            <token id="3" string="declined" />
            <token id="4" string="to" />
            <token id="5" string="elaborate" />
            <token id="6" string="on" />
            <token id="7" string="the" />
            <token id="8" string="remarks" />
            <token id="9" string="," />
            <token id="10" string="and" />
            <token id="11" string="said" />
            <token id="12" string="Maxwell" />
            <token id="13" string="had" />
            <token id="14" string="left" />
            <token id="15" string="the" />
            <token id="16" string="United" />
            <token id="17" string="States" />
          </tokens>
        </chunking>
        <chunking id="7" string="declined to elaborate on the remarks" type="VP">
          <tokens>
            <token id="3" string="declined" />
            <token id="4" string="to" />
            <token id="5" string="elaborate" />
            <token id="6" string="on" />
            <token id="7" string="the" />
            <token id="8" string="remarks" />
          </tokens>
        </chunking>
        <chunking id="8" string="left the United States" type="VP">
          <tokens>
            <token id="14" string="left" />
            <token id="15" string="the" />
            <token id="16" string="United" />
            <token id="17" string="States" />
          </tokens>
        </chunking>
        <chunking id="9" string="the remarks" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="remarks" />
          </tokens>
        </chunking>
        <chunking id="10" string="Maxwell" type="NP">
          <tokens>
            <token id="12" string="Maxwell" />
          </tokens>
        </chunking>
        <chunking id="11" string="to elaborate on the remarks" type="VP">
          <tokens>
            <token id="4" string="to" />
            <token id="5" string="elaborate" />
            <token id="6" string="on" />
            <token id="7" string="the" />
            <token id="8" string="remarks" />
          </tokens>
        </chunking>
        <chunking id="12" string="had left the United States" type="VP">
          <tokens>
            <token id="13" string="had" />
            <token id="14" string="left" />
            <token id="15" string="the" />
            <token id="16" string="United" />
            <token id="17" string="States" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">spokesman</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">declined</governor>
          <dependent id="2">spokesman</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">declined</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="5">elaborate</governor>
          <dependent id="4">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="3">declined</governor>
          <dependent id="5">elaborate</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">remarks</governor>
          <dependent id="6">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">remarks</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">elaborate</governor>
          <dependent id="8">remarks</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="3">declined</governor>
          <dependent id="10">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="3">declined</governor>
          <dependent id="11">said</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">left</governor>
          <dependent id="12">Maxwell</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="14">left</governor>
          <dependent id="13">had</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="11">said</governor>
          <dependent id="14">left</dependent>
        </dependency>
        <dependency type="det">
          <governor id="17">States</governor>
          <dependent id="15">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="17">States</governor>
          <dependent id="16">United</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="14">left</governor>
          <dependent id="17">States</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="left" type="IDEOLOGY" score="0.0">
          <tokens>
            <token id="14" string="left" />
          </tokens>
        </entity>
        <entity id="2" string="United States" type="LOCATION" score="0.0">
          <tokens>
            <token id="16" string="United" />
            <token id="17" string="States" />
          </tokens>
        </entity>
        <entity id="3" string="Maxwell" type="PERSON" score="0.0">
          <tokens>
            <token id="12" string="Maxwell" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="6" has_coreference="true">
      <content>The Daily News has one of the United States&amp;apost; largest circulations, but in recent years it has struggled to restore profitability and maintain circulation in a market with three other dailies.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="Daily" lemma="Daily" stem="daili" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="3" string="News" lemma="News" stem="new" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="4" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="one" lemma="one" stem="on" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="6" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="United" lemma="United" stem="unite" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="true" />
        <token id="9" string="States" lemma="States" stem="state" pos="NNPS" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="true" />
        <token id="10" string="'" lemma="'" stem="'" pos="POS" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="largest" lemma="largest" stem="largest" pos="JJS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="circulations" lemma="circulation" stem="circul" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="recent" lemma="recent" stem="recent" pos="JJ" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="17" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="18" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="struggled" lemma="struggle" stem="struggl" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="restore" lemma="restore" stem="restor" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="profitability" lemma="profitability" stem="profit" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="maintain" lemma="maintain" stem="maintain" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="circulation" lemma="circulation" stem="circul" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="market" lemma="market" stem="market" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="three" lemma="three" stem="three" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="32" string="other" lemma="other" stem="other" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="dailies" lemma="daily" stem="daili" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (DT The) (NNP Daily) (NNP News)) (VP (VBZ has) (NP (NP (CD one)) (PP (IN of) (NP (NP (DT the) (NNP United) (NNPS States) (POS ')) (JJS largest) (NNS circulations)))))) (, ,) (CC but) (S (PP (IN in) (NP (JJ recent) (NNS years))) (NP (PRP it)) (VP (VBZ has) (VP (VBN struggled) (S (VP (TO to) (VP (VP (VB restore) (NP (NN profitability))) (CC and) (VP (VB maintain) (NP (NN circulation)) (PP (IN in) (NP (NP (DT a) (NN market)) (PP (IN with) (NP (CD three) (JJ other) (NNS dailies)))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="to restore profitability and maintain circulation in a market with three other dailies" type="VP">
          <tokens>
            <token id="21" string="to" />
            <token id="22" string="restore" />
            <token id="23" string="profitability" />
            <token id="24" string="and" />
            <token id="25" string="maintain" />
            <token id="26" string="circulation" />
            <token id="27" string="in" />
            <token id="28" string="a" />
            <token id="29" string="market" />
            <token id="30" string="with" />
            <token id="31" string="three" />
            <token id="32" string="other" />
            <token id="33" string="dailies" />
          </tokens>
        </chunking>
        <chunking id="2" string="profitability" type="NP">
          <tokens>
            <token id="23" string="profitability" />
          </tokens>
        </chunking>
        <chunking id="3" string="one" type="NP">
          <tokens>
            <token id="5" string="one" />
          </tokens>
        </chunking>
        <chunking id="4" string="restore profitability and maintain circulation in a market with three other dailies" type="VP">
          <tokens>
            <token id="22" string="restore" />
            <token id="23" string="profitability" />
            <token id="24" string="and" />
            <token id="25" string="maintain" />
            <token id="26" string="circulation" />
            <token id="27" string="in" />
            <token id="28" string="a" />
            <token id="29" string="market" />
            <token id="30" string="with" />
            <token id="31" string="three" />
            <token id="32" string="other" />
            <token id="33" string="dailies" />
          </tokens>
        </chunking>
        <chunking id="5" string="has one of the United States ' largest circulations" type="VP">
          <tokens>
            <token id="4" string="has" />
            <token id="5" string="one" />
            <token id="6" string="of" />
            <token id="7" string="the" />
            <token id="8" string="United" />
            <token id="9" string="States" />
            <token id="10" string="'" />
            <token id="11" string="largest" />
            <token id="12" string="circulations" />
          </tokens>
        </chunking>
        <chunking id="6" string="the United States ' largest circulations" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="United" />
            <token id="9" string="States" />
            <token id="10" string="'" />
            <token id="11" string="largest" />
            <token id="12" string="circulations" />
          </tokens>
        </chunking>
        <chunking id="7" string="it" type="NP">
          <tokens>
            <token id="18" string="it" />
          </tokens>
        </chunking>
        <chunking id="8" string="a market with three other dailies" type="NP">
          <tokens>
            <token id="28" string="a" />
            <token id="29" string="market" />
            <token id="30" string="with" />
            <token id="31" string="three" />
            <token id="32" string="other" />
            <token id="33" string="dailies" />
          </tokens>
        </chunking>
        <chunking id="9" string="The Daily News" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Daily" />
            <token id="3" string="News" />
          </tokens>
        </chunking>
        <chunking id="10" string="recent years" type="NP">
          <tokens>
            <token id="16" string="recent" />
            <token id="17" string="years" />
          </tokens>
        </chunking>
        <chunking id="11" string="circulation" type="NP">
          <tokens>
            <token id="26" string="circulation" />
          </tokens>
        </chunking>
        <chunking id="12" string="the United States '" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="United" />
            <token id="9" string="States" />
            <token id="10" string="'" />
          </tokens>
        </chunking>
        <chunking id="13" string="three other dailies" type="NP">
          <tokens>
            <token id="31" string="three" />
            <token id="32" string="other" />
            <token id="33" string="dailies" />
          </tokens>
        </chunking>
        <chunking id="14" string="has struggled to restore profitability and maintain circulation in a market with three other dailies" type="VP">
          <tokens>
            <token id="19" string="has" />
            <token id="20" string="struggled" />
            <token id="21" string="to" />
            <token id="22" string="restore" />
            <token id="23" string="profitability" />
            <token id="24" string="and" />
            <token id="25" string="maintain" />
            <token id="26" string="circulation" />
            <token id="27" string="in" />
            <token id="28" string="a" />
            <token id="29" string="market" />
            <token id="30" string="with" />
            <token id="31" string="three" />
            <token id="32" string="other" />
            <token id="33" string="dailies" />
          </tokens>
        </chunking>
        <chunking id="15" string="struggled to restore profitability and maintain circulation in a market with three other dailies" type="VP">
          <tokens>
            <token id="20" string="struggled" />
            <token id="21" string="to" />
            <token id="22" string="restore" />
            <token id="23" string="profitability" />
            <token id="24" string="and" />
            <token id="25" string="maintain" />
            <token id="26" string="circulation" />
            <token id="27" string="in" />
            <token id="28" string="a" />
            <token id="29" string="market" />
            <token id="30" string="with" />
            <token id="31" string="three" />
            <token id="32" string="other" />
            <token id="33" string="dailies" />
          </tokens>
        </chunking>
        <chunking id="16" string="restore profitability" type="VP">
          <tokens>
            <token id="22" string="restore" />
            <token id="23" string="profitability" />
          </tokens>
        </chunking>
        <chunking id="17" string="maintain circulation in a market with three other dailies" type="VP">
          <tokens>
            <token id="25" string="maintain" />
            <token id="26" string="circulation" />
            <token id="27" string="in" />
            <token id="28" string="a" />
            <token id="29" string="market" />
            <token id="30" string="with" />
            <token id="31" string="three" />
            <token id="32" string="other" />
            <token id="33" string="dailies" />
          </tokens>
        </chunking>
        <chunking id="18" string="one of the United States ' largest circulations" type="NP">
          <tokens>
            <token id="5" string="one" />
            <token id="6" string="of" />
            <token id="7" string="the" />
            <token id="8" string="United" />
            <token id="9" string="States" />
            <token id="10" string="'" />
            <token id="11" string="largest" />
            <token id="12" string="circulations" />
          </tokens>
        </chunking>
        <chunking id="19" string="a market" type="NP">
          <tokens>
            <token id="28" string="a" />
            <token id="29" string="market" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">News</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">News</governor>
          <dependent id="2">Daily</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">has</governor>
          <dependent id="3">News</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">has</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">has</governor>
          <dependent id="5">one</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">circulations</governor>
          <dependent id="6">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">States</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">States</governor>
          <dependent id="8">United</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="12">circulations</governor>
          <dependent id="9">States</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">States</governor>
          <dependent id="10">'</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">circulations</governor>
          <dependent id="11">largest</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">one</governor>
          <dependent id="12">circulations</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">has</governor>
          <dependent id="14">but</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">years</governor>
          <dependent id="15">in</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="17">years</governor>
          <dependent id="16">recent</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">struggled</governor>
          <dependent id="17">years</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="20">struggled</governor>
          <dependent id="18">it</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="20">struggled</governor>
          <dependent id="19">has</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">has</governor>
          <dependent id="20">struggled</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="22">restore</governor>
          <dependent id="21">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="20">struggled</governor>
          <dependent id="22">restore</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="22">restore</governor>
          <dependent id="23">profitability</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="22">restore</governor>
          <dependent id="24">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="22">restore</governor>
          <dependent id="25">maintain</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="25">maintain</governor>
          <dependent id="26">circulation</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">market</governor>
          <dependent id="27">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="29">market</governor>
          <dependent id="28">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="25">maintain</governor>
          <dependent id="29">market</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">dailies</governor>
          <dependent id="30">with</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="33">dailies</governor>
          <dependent id="31">three</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="33">dailies</governor>
          <dependent id="32">other</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="29">market</governor>
          <dependent id="33">dailies</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="one" type="NUMBER" score="0.0">
          <tokens>
            <token id="5" string="one" />
          </tokens>
        </entity>
        <entity id="2" string="United States" type="LOCATION" score="0.0">
          <tokens>
            <token id="8" string="United" />
            <token id="9" string="States" />
          </tokens>
        </entity>
        <entity id="3" string="recent years" type="DURATION" score="0.0">
          <tokens>
            <token id="16" string="recent" />
            <token id="17" string="years" />
          </tokens>
        </entity>
        <entity id="4" string="Daily News" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="2" string="Daily" />
            <token id="3" string="News" />
          </tokens>
        </entity>
        <entity id="5" string="three" type="NUMBER" score="0.0">
          <tokens>
            <token id="31" string="three" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="7" has_coreference="true">
      <content>The paper is conducting difficult labor contract negotiations with its unions.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="paper" lemma="paper" stem="paper" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="conducting" lemma="conduct" stem="conduct" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="difficult" lemma="difficult" stem="difficult" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="labor" lemma="labor" stem="labor" pos="NN" type="Word" isStopWord="false" ner="IDEOLOGY" is_referenced="false" is_refers="false" />
        <token id="7" string="contract" lemma="contract" stem="contract" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="negotiations" lemma="negotiation" stem="negoti" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="its" lemma="its" stem="it" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="unions" lemma="union" stem="union" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN paper)) (VP (VBZ is) (VP (VBG conducting) (NP (JJ difficult) (NN labor) (NN contract) (NNS negotiations)) (PP (IN with) (NP (PRP$ its) (NNS unions))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="is conducting difficult labor contract negotiations with its unions" type="VP">
          <tokens>
            <token id="3" string="is" />
            <token id="4" string="conducting" />
            <token id="5" string="difficult" />
            <token id="6" string="labor" />
            <token id="7" string="contract" />
            <token id="8" string="negotiations" />
            <token id="9" string="with" />
            <token id="10" string="its" />
            <token id="11" string="unions" />
          </tokens>
        </chunking>
        <chunking id="2" string="difficult labor contract negotiations" type="NP">
          <tokens>
            <token id="5" string="difficult" />
            <token id="6" string="labor" />
            <token id="7" string="contract" />
            <token id="8" string="negotiations" />
          </tokens>
        </chunking>
        <chunking id="3" string="The paper" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="paper" />
          </tokens>
        </chunking>
        <chunking id="4" string="conducting difficult labor contract negotiations with its unions" type="VP">
          <tokens>
            <token id="4" string="conducting" />
            <token id="5" string="difficult" />
            <token id="6" string="labor" />
            <token id="7" string="contract" />
            <token id="8" string="negotiations" />
            <token id="9" string="with" />
            <token id="10" string="its" />
            <token id="11" string="unions" />
          </tokens>
        </chunking>
        <chunking id="5" string="its unions" type="NP">
          <tokens>
            <token id="10" string="its" />
            <token id="11" string="unions" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">paper</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">conducting</governor>
          <dependent id="2">paper</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="4">conducting</governor>
          <dependent id="3">is</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">conducting</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">negotiations</governor>
          <dependent id="5">difficult</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">negotiations</governor>
          <dependent id="6">labor</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">negotiations</governor>
          <dependent id="7">contract</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">conducting</governor>
          <dependent id="8">negotiations</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">unions</governor>
          <dependent id="9">with</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="11">unions</governor>
          <dependent id="10">its</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">conducting</governor>
          <dependent id="11">unions</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="labor" type="IDEOLOGY" score="0.0">
          <tokens>
            <token id="6" string="labor" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="8" has_coreference="true">
      <content>Some analysts said it might make sense for Maxwell to try to purchase USA Today, since it would mesh well with his efforts to print and distribute his month-old &amp;quot;pan-European&amp;quot; English-language paper, The European, in this country.</content>
      <tokens>
        <token id="1" string="Some" lemma="some" stem="some" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="analysts" lemma="analyst" stem="analyst" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="might" lemma="might" stem="might" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="make" lemma="make" stem="make" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="sense" lemma="sense" stem="sens" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="Maxwell" lemma="Maxwell" stem="maxwel" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="10" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="try" lemma="try" stem="try" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="purchase" lemma="purchase" stem="purchas" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="USA" lemma="USA" stem="usa" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="15" string="Today" lemma="Today" stem="todai" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="since" lemma="since" stem="sinc" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="18" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="would" lemma="would" stem="would" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="20" string="mesh" lemma="mesh" stem="mesh" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="21" string="well" lemma="well" stem="well" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="22" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="23" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="24" string="efforts" lemma="effort" stem="effort" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="25" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="26" string="print" lemma="print" stem="print" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="27" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="distribute" lemma="distribute" stem="distribut" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="29" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="30" string="month-old" lemma="month-old" stem="month-old" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="31" string="&quot;" lemma="``" stem="&quot;" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="32" string="pan-European" lemma="pan-european" stem="pan-european" pos="JJ" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="true" />
        <token id="33" string="&quot;" lemma="''" stem="&quot;" pos="''" type="Symbol" isStopWord="true" ner="MISC" is_referenced="false" is_refers="true" />
        <token id="34" string="English-language" lemma="english-language" stem="english-languag" pos="JJ" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="true" />
        <token id="35" string="paper" lemma="paper" stem="paper" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="36" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="37" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="38" string="European" lemma="european" stem="european" pos="JJ" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="true" />
        <token id="39" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="40" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="41" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="42" string="country" lemma="country" stem="countri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="43" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT Some) (NNS analysts)) (VP (VBD said) (SBAR (S (NP (PRP it)) (VP (MD might) (VP (VB make) (NP (NP (NN sense)) (PP (IN for) (NP (NNP Maxwell)))) (S (VP (TO to) (VP (VB try) (S (VP (TO to) (VP (VB purchase) (NP (NP (NNP USA) (NNP Today)) (, ,) (SBAR (IN since) (S (NP (PRP it)) (VP (MD would) (VP (VB mesh) (PP (RB well) (IN with) (NP (PRP$ his) (NNS efforts) (S (VP (TO to) (VP (VB print) (CC and) (VB distribute) (NP (PRP$ his) (JJ month-old) (`` ``) (JJ pan-European) ('' '') (JJ English-language) (NN paper)))))) (, ,) (NP (NP (DT The) (JJ European)) (, ,) (PP (IN in) (NP (DT this) (NN country))))))))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="The European" type="NP">
          <tokens>
            <token id="37" string="The" />
            <token id="38" string="European" />
          </tokens>
        </chunking>
        <chunking id="2" string="to try to purchase USA Today , since it would mesh well with his efforts to print and distribute his month-old `` pan-European '' English-language paper , The European , in this country" type="VP">
          <tokens>
            <token id="10" string="to" />
            <token id="11" string="try" />
            <token id="12" string="to" />
            <token id="13" string="purchase" />
            <token id="14" string="USA" />
            <token id="15" string="Today" />
            <token id="16" string="," />
            <token id="17" string="since" />
            <token id="18" string="it" />
            <token id="19" string="would" />
            <token id="20" string="mesh" />
            <token id="21" string="well" />
            <token id="22" string="with" />
            <token id="23" string="his" />
            <token id="24" string="efforts" />
            <token id="25" string="to" />
            <token id="26" string="print" />
            <token id="27" string="and" />
            <token id="28" string="distribute" />
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
            <token id="36" string="," />
            <token id="37" string="The" />
            <token id="38" string="European" />
            <token id="39" string="," />
            <token id="40" string="in" />
            <token id="41" string="this" />
            <token id="42" string="country" />
          </tokens>
        </chunking>
        <chunking id="3" string="it might make sense for Maxwell to try to purchase USA Today , since it would mesh well with his efforts to print and distribute his month-old `` pan-European '' English-language paper , The European , in this country" type="SBAR">
          <tokens>
            <token id="4" string="it" />
            <token id="5" string="might" />
            <token id="6" string="make" />
            <token id="7" string="sense" />
            <token id="8" string="for" />
            <token id="9" string="Maxwell" />
            <token id="10" string="to" />
            <token id="11" string="try" />
            <token id="12" string="to" />
            <token id="13" string="purchase" />
            <token id="14" string="USA" />
            <token id="15" string="Today" />
            <token id="16" string="," />
            <token id="17" string="since" />
            <token id="18" string="it" />
            <token id="19" string="would" />
            <token id="20" string="mesh" />
            <token id="21" string="well" />
            <token id="22" string="with" />
            <token id="23" string="his" />
            <token id="24" string="efforts" />
            <token id="25" string="to" />
            <token id="26" string="print" />
            <token id="27" string="and" />
            <token id="28" string="distribute" />
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
            <token id="36" string="," />
            <token id="37" string="The" />
            <token id="38" string="European" />
            <token id="39" string="," />
            <token id="40" string="in" />
            <token id="41" string="this" />
            <token id="42" string="country" />
          </tokens>
        </chunking>
        <chunking id="4" string="Some analysts" type="NP">
          <tokens>
            <token id="1" string="Some" />
            <token id="2" string="analysts" />
          </tokens>
        </chunking>
        <chunking id="5" string="sense" type="NP">
          <tokens>
            <token id="7" string="sense" />
          </tokens>
        </chunking>
        <chunking id="6" string="purchase USA Today , since it would mesh well with his efforts to print and distribute his month-old `` pan-European '' English-language paper , The European , in this country" type="VP">
          <tokens>
            <token id="13" string="purchase" />
            <token id="14" string="USA" />
            <token id="15" string="Today" />
            <token id="16" string="," />
            <token id="17" string="since" />
            <token id="18" string="it" />
            <token id="19" string="would" />
            <token id="20" string="mesh" />
            <token id="21" string="well" />
            <token id="22" string="with" />
            <token id="23" string="his" />
            <token id="24" string="efforts" />
            <token id="25" string="to" />
            <token id="26" string="print" />
            <token id="27" string="and" />
            <token id="28" string="distribute" />
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
            <token id="36" string="," />
            <token id="37" string="The" />
            <token id="38" string="European" />
            <token id="39" string="," />
            <token id="40" string="in" />
            <token id="41" string="this" />
            <token id="42" string="country" />
          </tokens>
        </chunking>
        <chunking id="7" string="it" type="NP">
          <tokens>
            <token id="4" string="it" />
          </tokens>
        </chunking>
        <chunking id="8" string="might make sense for Maxwell to try to purchase USA Today , since it would mesh well with his efforts to print and distribute his month-old `` pan-European '' English-language paper , The European , in this country" type="VP">
          <tokens>
            <token id="5" string="might" />
            <token id="6" string="make" />
            <token id="7" string="sense" />
            <token id="8" string="for" />
            <token id="9" string="Maxwell" />
            <token id="10" string="to" />
            <token id="11" string="try" />
            <token id="12" string="to" />
            <token id="13" string="purchase" />
            <token id="14" string="USA" />
            <token id="15" string="Today" />
            <token id="16" string="," />
            <token id="17" string="since" />
            <token id="18" string="it" />
            <token id="19" string="would" />
            <token id="20" string="mesh" />
            <token id="21" string="well" />
            <token id="22" string="with" />
            <token id="23" string="his" />
            <token id="24" string="efforts" />
            <token id="25" string="to" />
            <token id="26" string="print" />
            <token id="27" string="and" />
            <token id="28" string="distribute" />
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
            <token id="36" string="," />
            <token id="37" string="The" />
            <token id="38" string="European" />
            <token id="39" string="," />
            <token id="40" string="in" />
            <token id="41" string="this" />
            <token id="42" string="country" />
          </tokens>
        </chunking>
        <chunking id="9" string="mesh well with his efforts to print and distribute his month-old `` pan-European '' English-language paper , The European , in this country" type="VP">
          <tokens>
            <token id="20" string="mesh" />
            <token id="21" string="well" />
            <token id="22" string="with" />
            <token id="23" string="his" />
            <token id="24" string="efforts" />
            <token id="25" string="to" />
            <token id="26" string="print" />
            <token id="27" string="and" />
            <token id="28" string="distribute" />
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
            <token id="36" string="," />
            <token id="37" string="The" />
            <token id="38" string="European" />
            <token id="39" string="," />
            <token id="40" string="in" />
            <token id="41" string="this" />
            <token id="42" string="country" />
          </tokens>
        </chunking>
        <chunking id="10" string="The European , in this country" type="NP">
          <tokens>
            <token id="37" string="The" />
            <token id="38" string="European" />
            <token id="39" string="," />
            <token id="40" string="in" />
            <token id="41" string="this" />
            <token id="42" string="country" />
          </tokens>
        </chunking>
        <chunking id="11" string="print and distribute his month-old `` pan-European '' English-language paper" type="VP">
          <tokens>
            <token id="26" string="print" />
            <token id="27" string="and" />
            <token id="28" string="distribute" />
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
          </tokens>
        </chunking>
        <chunking id="12" string="USA Today" type="NP">
          <tokens>
            <token id="14" string="USA" />
            <token id="15" string="Today" />
          </tokens>
        </chunking>
        <chunking id="13" string="USA Today , since it would mesh well with his efforts to print and distribute his month-old `` pan-European '' English-language paper , The European , in this country" type="NP">
          <tokens>
            <token id="14" string="USA" />
            <token id="15" string="Today" />
            <token id="16" string="," />
            <token id="17" string="since" />
            <token id="18" string="it" />
            <token id="19" string="would" />
            <token id="20" string="mesh" />
            <token id="21" string="well" />
            <token id="22" string="with" />
            <token id="23" string="his" />
            <token id="24" string="efforts" />
            <token id="25" string="to" />
            <token id="26" string="print" />
            <token id="27" string="and" />
            <token id="28" string="distribute" />
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
            <token id="36" string="," />
            <token id="37" string="The" />
            <token id="38" string="European" />
            <token id="39" string="," />
            <token id="40" string="in" />
            <token id="41" string="this" />
            <token id="42" string="country" />
          </tokens>
        </chunking>
        <chunking id="14" string="would mesh well with his efforts to print and distribute his month-old `` pan-European '' English-language paper , The European , in this country" type="VP">
          <tokens>
            <token id="19" string="would" />
            <token id="20" string="mesh" />
            <token id="21" string="well" />
            <token id="22" string="with" />
            <token id="23" string="his" />
            <token id="24" string="efforts" />
            <token id="25" string="to" />
            <token id="26" string="print" />
            <token id="27" string="and" />
            <token id="28" string="distribute" />
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
            <token id="36" string="," />
            <token id="37" string="The" />
            <token id="38" string="European" />
            <token id="39" string="," />
            <token id="40" string="in" />
            <token id="41" string="this" />
            <token id="42" string="country" />
          </tokens>
        </chunking>
        <chunking id="15" string="to purchase USA Today , since it would mesh well with his efforts to print and distribute his month-old `` pan-European '' English-language paper , The European , in this country" type="VP">
          <tokens>
            <token id="12" string="to" />
            <token id="13" string="purchase" />
            <token id="14" string="USA" />
            <token id="15" string="Today" />
            <token id="16" string="," />
            <token id="17" string="since" />
            <token id="18" string="it" />
            <token id="19" string="would" />
            <token id="20" string="mesh" />
            <token id="21" string="well" />
            <token id="22" string="with" />
            <token id="23" string="his" />
            <token id="24" string="efforts" />
            <token id="25" string="to" />
            <token id="26" string="print" />
            <token id="27" string="and" />
            <token id="28" string="distribute" />
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
            <token id="36" string="," />
            <token id="37" string="The" />
            <token id="38" string="European" />
            <token id="39" string="," />
            <token id="40" string="in" />
            <token id="41" string="this" />
            <token id="42" string="country" />
          </tokens>
        </chunking>
        <chunking id="16" string="Maxwell" type="NP">
          <tokens>
            <token id="9" string="Maxwell" />
          </tokens>
        </chunking>
        <chunking id="17" string="since it would mesh well with his efforts to print and distribute his month-old `` pan-European '' English-language paper , The European , in this country" type="SBAR">
          <tokens>
            <token id="17" string="since" />
            <token id="18" string="it" />
            <token id="19" string="would" />
            <token id="20" string="mesh" />
            <token id="21" string="well" />
            <token id="22" string="with" />
            <token id="23" string="his" />
            <token id="24" string="efforts" />
            <token id="25" string="to" />
            <token id="26" string="print" />
            <token id="27" string="and" />
            <token id="28" string="distribute" />
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
            <token id="36" string="," />
            <token id="37" string="The" />
            <token id="38" string="European" />
            <token id="39" string="," />
            <token id="40" string="in" />
            <token id="41" string="this" />
            <token id="42" string="country" />
          </tokens>
        </chunking>
        <chunking id="18" string="said it might make sense for Maxwell to try to purchase USA Today , since it would mesh well with his efforts to print and distribute his month-old `` pan-European '' English-language paper , The European , in this country" type="VP">
          <tokens>
            <token id="3" string="said" />
            <token id="4" string="it" />
            <token id="5" string="might" />
            <token id="6" string="make" />
            <token id="7" string="sense" />
            <token id="8" string="for" />
            <token id="9" string="Maxwell" />
            <token id="10" string="to" />
            <token id="11" string="try" />
            <token id="12" string="to" />
            <token id="13" string="purchase" />
            <token id="14" string="USA" />
            <token id="15" string="Today" />
            <token id="16" string="," />
            <token id="17" string="since" />
            <token id="18" string="it" />
            <token id="19" string="would" />
            <token id="20" string="mesh" />
            <token id="21" string="well" />
            <token id="22" string="with" />
            <token id="23" string="his" />
            <token id="24" string="efforts" />
            <token id="25" string="to" />
            <token id="26" string="print" />
            <token id="27" string="and" />
            <token id="28" string="distribute" />
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
            <token id="36" string="," />
            <token id="37" string="The" />
            <token id="38" string="European" />
            <token id="39" string="," />
            <token id="40" string="in" />
            <token id="41" string="this" />
            <token id="42" string="country" />
          </tokens>
        </chunking>
        <chunking id="19" string="make sense for Maxwell to try to purchase USA Today , since it would mesh well with his efforts to print and distribute his month-old `` pan-European '' English-language paper , The European , in this country" type="VP">
          <tokens>
            <token id="6" string="make" />
            <token id="7" string="sense" />
            <token id="8" string="for" />
            <token id="9" string="Maxwell" />
            <token id="10" string="to" />
            <token id="11" string="try" />
            <token id="12" string="to" />
            <token id="13" string="purchase" />
            <token id="14" string="USA" />
            <token id="15" string="Today" />
            <token id="16" string="," />
            <token id="17" string="since" />
            <token id="18" string="it" />
            <token id="19" string="would" />
            <token id="20" string="mesh" />
            <token id="21" string="well" />
            <token id="22" string="with" />
            <token id="23" string="his" />
            <token id="24" string="efforts" />
            <token id="25" string="to" />
            <token id="26" string="print" />
            <token id="27" string="and" />
            <token id="28" string="distribute" />
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
            <token id="36" string="," />
            <token id="37" string="The" />
            <token id="38" string="European" />
            <token id="39" string="," />
            <token id="40" string="in" />
            <token id="41" string="this" />
            <token id="42" string="country" />
          </tokens>
        </chunking>
        <chunking id="20" string="sense for Maxwell" type="NP">
          <tokens>
            <token id="7" string="sense" />
            <token id="8" string="for" />
            <token id="9" string="Maxwell" />
          </tokens>
        </chunking>
        <chunking id="21" string="this country" type="NP">
          <tokens>
            <token id="41" string="this" />
            <token id="42" string="country" />
          </tokens>
        </chunking>
        <chunking id="22" string="his month-old `` pan-European '' English-language paper" type="NP">
          <tokens>
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
          </tokens>
        </chunking>
        <chunking id="23" string="to print and distribute his month-old `` pan-European '' English-language paper" type="VP">
          <tokens>
            <token id="25" string="to" />
            <token id="26" string="print" />
            <token id="27" string="and" />
            <token id="28" string="distribute" />
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
          </tokens>
        </chunking>
        <chunking id="24" string="his efforts to print and distribute his month-old `` pan-European '' English-language paper" type="NP">
          <tokens>
            <token id="23" string="his" />
            <token id="24" string="efforts" />
            <token id="25" string="to" />
            <token id="26" string="print" />
            <token id="27" string="and" />
            <token id="28" string="distribute" />
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
          </tokens>
        </chunking>
        <chunking id="25" string="try to purchase USA Today , since it would mesh well with his efforts to print and distribute his month-old `` pan-European '' English-language paper , The European , in this country" type="VP">
          <tokens>
            <token id="11" string="try" />
            <token id="12" string="to" />
            <token id="13" string="purchase" />
            <token id="14" string="USA" />
            <token id="15" string="Today" />
            <token id="16" string="," />
            <token id="17" string="since" />
            <token id="18" string="it" />
            <token id="19" string="would" />
            <token id="20" string="mesh" />
            <token id="21" string="well" />
            <token id="22" string="with" />
            <token id="23" string="his" />
            <token id="24" string="efforts" />
            <token id="25" string="to" />
            <token id="26" string="print" />
            <token id="27" string="and" />
            <token id="28" string="distribute" />
            <token id="29" string="his" />
            <token id="30" string="month-old" />
            <token id="31" string="&quot;" />
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
            <token id="35" string="paper" />
            <token id="36" string="," />
            <token id="37" string="The" />
            <token id="38" string="European" />
            <token id="39" string="," />
            <token id="40" string="in" />
            <token id="41" string="this" />
            <token id="42" string="country" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">analysts</governor>
          <dependent id="1">Some</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">said</governor>
          <dependent id="2">analysts</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">said</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">make</governor>
          <dependent id="4">it</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="6">make</governor>
          <dependent id="5">might</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">said</governor>
          <dependent id="6">make</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">make</governor>
          <dependent id="7">sense</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">Maxwell</governor>
          <dependent id="8">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">sense</governor>
          <dependent id="9">Maxwell</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="11">try</governor>
          <dependent id="10">to</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="6">make</governor>
          <dependent id="11">try</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="13">purchase</governor>
          <dependent id="12">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="11">try</governor>
          <dependent id="13">purchase</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">Today</governor>
          <dependent id="14">USA</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="13">purchase</governor>
          <dependent id="15">Today</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="20">mesh</governor>
          <dependent id="17">since</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="20">mesh</governor>
          <dependent id="18">it</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="20">mesh</governor>
          <dependent id="19">would</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="15">Today</governor>
          <dependent id="20">mesh</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="24">efforts</governor>
          <dependent id="21">well</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">efforts</governor>
          <dependent id="22">with</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="24">efforts</governor>
          <dependent id="23">his</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">mesh</governor>
          <dependent id="24">efforts</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="26">print</governor>
          <dependent id="25">to</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="24">efforts</governor>
          <dependent id="26">print</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="26">print</governor>
          <dependent id="27">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="26">print</governor>
          <dependent id="28">distribute</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="35">paper</governor>
          <dependent id="29">his</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="35">paper</governor>
          <dependent id="30">month-old</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="35">paper</governor>
          <dependent id="32">pan-European</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="35">paper</governor>
          <dependent id="34">English-language</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="26">print</governor>
          <dependent id="35">paper</dependent>
        </dependency>
        <dependency type="det">
          <governor id="38">European</governor>
          <dependent id="37">The</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="24">efforts</governor>
          <dependent id="38">European</dependent>
        </dependency>
        <dependency type="case">
          <governor id="42">country</governor>
          <dependent id="40">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="42">country</governor>
          <dependent id="41">this</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="38">European</governor>
          <dependent id="42">country</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="pan-European &quot; English-language" type="MISC" score="0.0">
          <tokens>
            <token id="32" string="pan-European" />
            <token id="33" string="&quot;" />
            <token id="34" string="English-language" />
          </tokens>
        </entity>
        <entity id="2" string="Maxwell" type="PERSON" score="0.0">
          <tokens>
            <token id="9" string="Maxwell" />
          </tokens>
        </entity>
        <entity id="3" string="USA Today" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="14" string="USA" />
            <token id="15" string="Today" />
          </tokens>
        </entity>
        <entity id="4" string="European" type="MISC" score="0.0">
          <tokens>
            <token id="38" string="European" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="9" has_coreference="true">
      <content>Maxwell has already been in negotiations with USA Today officials about printing the paper for U.S. distribution.</content>
      <tokens>
        <token id="1" string="Maxwell" lemma="Maxwell" stem="maxwel" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="already" lemma="already" stem="alreadi" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="negotiations" lemma="negotiation" stem="negoti" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="USA" lemma="USA" stem="usa" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="9" string="Today" lemma="today" stem="todai" pos="NN" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="10" string="officials" lemma="official" stem="offici" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="printing" lemma="print" stem="print" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="paper" lemma="paper" stem="paper" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="U.S." lemma="U.S." stem="u.s." pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="17" string="distribution" lemma="distribution" stem="distribut" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Maxwell)) (VP (VBZ has) (ADVP (RB already)) (VP (VBN been) (PP (IN in) (NP (NP (NNS negotiations)) (PP (IN with) (NP (NNP USA) (NN Today) (NNS officials))))) (PP (IN about) (S (VP (VBG printing) (NP (NP (DT the) (NN paper)) (PP (IN for) (NP (NNP U.S.) (NN distribution))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="has already been in negotiations with USA Today officials about printing the paper for U.S. distribution" type="VP">
          <tokens>
            <token id="2" string="has" />
            <token id="3" string="already" />
            <token id="4" string="been" />
            <token id="5" string="in" />
            <token id="6" string="negotiations" />
            <token id="7" string="with" />
            <token id="8" string="USA" />
            <token id="9" string="Today" />
            <token id="10" string="officials" />
            <token id="11" string="about" />
            <token id="12" string="printing" />
            <token id="13" string="the" />
            <token id="14" string="paper" />
            <token id="15" string="for" />
            <token id="16" string="U.S." />
            <token id="17" string="distribution" />
          </tokens>
        </chunking>
        <chunking id="2" string="negotiations with USA Today officials" type="NP">
          <tokens>
            <token id="6" string="negotiations" />
            <token id="7" string="with" />
            <token id="8" string="USA" />
            <token id="9" string="Today" />
            <token id="10" string="officials" />
          </tokens>
        </chunking>
        <chunking id="3" string="USA Today officials" type="NP">
          <tokens>
            <token id="8" string="USA" />
            <token id="9" string="Today" />
            <token id="10" string="officials" />
          </tokens>
        </chunking>
        <chunking id="4" string="been in negotiations with USA Today officials about printing the paper for U.S. distribution" type="VP">
          <tokens>
            <token id="4" string="been" />
            <token id="5" string="in" />
            <token id="6" string="negotiations" />
            <token id="7" string="with" />
            <token id="8" string="USA" />
            <token id="9" string="Today" />
            <token id="10" string="officials" />
            <token id="11" string="about" />
            <token id="12" string="printing" />
            <token id="13" string="the" />
            <token id="14" string="paper" />
            <token id="15" string="for" />
            <token id="16" string="U.S." />
            <token id="17" string="distribution" />
          </tokens>
        </chunking>
        <chunking id="5" string="printing the paper for U.S. distribution" type="VP">
          <tokens>
            <token id="12" string="printing" />
            <token id="13" string="the" />
            <token id="14" string="paper" />
            <token id="15" string="for" />
            <token id="16" string="U.S." />
            <token id="17" string="distribution" />
          </tokens>
        </chunking>
        <chunking id="6" string="negotiations" type="NP">
          <tokens>
            <token id="6" string="negotiations" />
          </tokens>
        </chunking>
        <chunking id="7" string="Maxwell" type="NP">
          <tokens>
            <token id="1" string="Maxwell" />
          </tokens>
        </chunking>
        <chunking id="8" string="the paper" type="NP">
          <tokens>
            <token id="13" string="the" />
            <token id="14" string="paper" />
          </tokens>
        </chunking>
        <chunking id="9" string="the paper for U.S. distribution" type="NP">
          <tokens>
            <token id="13" string="the" />
            <token id="14" string="paper" />
            <token id="15" string="for" />
            <token id="16" string="U.S." />
            <token id="17" string="distribution" />
          </tokens>
        </chunking>
        <chunking id="10" string="U.S. distribution" type="NP">
          <tokens>
            <token id="16" string="U.S." />
            <token id="17" string="distribution" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="6">negotiations</governor>
          <dependent id="1">Maxwell</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="6">negotiations</governor>
          <dependent id="2">has</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="6">negotiations</governor>
          <dependent id="3">already</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="6">negotiations</governor>
          <dependent id="4">been</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">negotiations</governor>
          <dependent id="5">in</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">negotiations</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">officials</governor>
          <dependent id="7">with</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">officials</governor>
          <dependent id="8">USA</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">officials</governor>
          <dependent id="9">Today</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">negotiations</governor>
          <dependent id="10">officials</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="12">printing</governor>
          <dependent id="11">about</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="6">negotiations</governor>
          <dependent id="12">printing</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">paper</governor>
          <dependent id="13">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="12">printing</governor>
          <dependent id="14">paper</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">distribution</governor>
          <dependent id="15">for</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="17">distribution</governor>
          <dependent id="16">U.S.</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">paper</governor>
          <dependent id="17">distribution</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="U.S." type="LOCATION" score="0.0">
          <tokens>
            <token id="16" string="U.S." />
          </tokens>
        </entity>
        <entity id="2" string="Maxwell" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Maxwell" />
          </tokens>
        </entity>
        <entity id="3" string="USA Today" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="8" string="USA" />
            <token id="9" string="Today" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="10" has_coreference="true">
      <content>Other industry officials conjectured that Maxwell might be talking to the Chicago Sun-Times, which has seen daily circulation slide more than 100,000 papers, to about 533,000, since 1985.</content>
      <tokens>
        <token id="1" string="Other" lemma="other" stem="other" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="industry" lemma="industry" stem="industri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="officials" lemma="official" stem="offici" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="conjectured" lemma="conjecture" stem="conjectur" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Maxwell" lemma="Maxwell" stem="maxwel" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="7" string="might" lemma="might" stem="might" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="be" lemma="be" stem="be" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="talking" lemma="talk" stem="talk" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="Chicago" lemma="Chicago" stem="chicago" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="13" string="Sun-Times" lemma="Sun-Times" stem="sun-tim" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="which" lemma="which" stem="which" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="16" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="seen" lemma="see" stem="seen" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="18" string="daily" lemma="daily" stem="daili" pos="JJ" type="Word" isStopWord="false" ner="SET" is_referenced="false" is_refers="true" />
        <token id="19" string="circulation" lemma="circulation" stem="circul" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="20" string="slide" lemma="slide" stem="slide" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="21" string="more" lemma="more" stem="more" pos="JJR" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="22" string="than" lemma="than" stem="than" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="23" string="100,000" lemma="100,000" stem="100,000" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="true" />
        <token id="24" string="papers" lemma="papers" stem="paper" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="25" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="26" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="27" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="533,000" lemma="533,000" stem="533,000" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="true" />
        <token id="29" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="30" string="since" lemma="since" stem="sinc" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="31" string="1985" lemma="1985" stem="1985" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="32" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (JJ Other) (NN industry) (NNS officials)) (VP (VBD conjectured) (SBAR (IN that) (S (NP (NNP Maxwell)) (VP (MD might) (VP (VB be) (VP (VBG talking) (PP (TO to) (NP (NP (DT the) (NNP Chicago) (NNP Sun-Times)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBZ has) (VP (VBN seen) (NP (JJ daily) (NN circulation) (NN slide)) (PP (NP (NP (QP (JJR more) (IN than) (CD 100,000)) (NNS papers)) (, ,) (X (X (TO to)) (PP (IN about) (NP (CD 533,000)))) (, ,)) (IN since) (NP (CD 1985))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="conjectured that Maxwell might be talking to the Chicago Sun-Times , which has seen daily circulation slide more than 100,000 papers , to about 533,000 , since 1985" type="VP">
          <tokens>
            <token id="4" string="conjectured" />
            <token id="5" string="that" />
            <token id="6" string="Maxwell" />
            <token id="7" string="might" />
            <token id="8" string="be" />
            <token id="9" string="talking" />
            <token id="10" string="to" />
            <token id="11" string="the" />
            <token id="12" string="Chicago" />
            <token id="13" string="Sun-Times" />
            <token id="14" string="," />
            <token id="15" string="which" />
            <token id="16" string="has" />
            <token id="17" string="seen" />
            <token id="18" string="daily" />
            <token id="19" string="circulation" />
            <token id="20" string="slide" />
            <token id="21" string="more" />
            <token id="22" string="than" />
            <token id="23" string="100,000" />
            <token id="24" string="papers" />
            <token id="25" string="," />
            <token id="26" string="to" />
            <token id="27" string="about" />
            <token id="28" string="533,000" />
            <token id="29" string="," />
            <token id="30" string="since" />
            <token id="31" string="1985" />
          </tokens>
        </chunking>
        <chunking id="2" string="has seen daily circulation slide more than 100,000 papers , to about 533,000 , since 1985" type="VP">
          <tokens>
            <token id="16" string="has" />
            <token id="17" string="seen" />
            <token id="18" string="daily" />
            <token id="19" string="circulation" />
            <token id="20" string="slide" />
            <token id="21" string="more" />
            <token id="22" string="than" />
            <token id="23" string="100,000" />
            <token id="24" string="papers" />
            <token id="25" string="," />
            <token id="26" string="to" />
            <token id="27" string="about" />
            <token id="28" string="533,000" />
            <token id="29" string="," />
            <token id="30" string="since" />
            <token id="31" string="1985" />
          </tokens>
        </chunking>
        <chunking id="3" string="that Maxwell might be talking to the Chicago Sun-Times , which has seen daily circulation slide more than 100,000 papers , to about 533,000 , since 1985" type="SBAR">
          <tokens>
            <token id="5" string="that" />
            <token id="6" string="Maxwell" />
            <token id="7" string="might" />
            <token id="8" string="be" />
            <token id="9" string="talking" />
            <token id="10" string="to" />
            <token id="11" string="the" />
            <token id="12" string="Chicago" />
            <token id="13" string="Sun-Times" />
            <token id="14" string="," />
            <token id="15" string="which" />
            <token id="16" string="has" />
            <token id="17" string="seen" />
            <token id="18" string="daily" />
            <token id="19" string="circulation" />
            <token id="20" string="slide" />
            <token id="21" string="more" />
            <token id="22" string="than" />
            <token id="23" string="100,000" />
            <token id="24" string="papers" />
            <token id="25" string="," />
            <token id="26" string="to" />
            <token id="27" string="about" />
            <token id="28" string="533,000" />
            <token id="29" string="," />
            <token id="30" string="since" />
            <token id="31" string="1985" />
          </tokens>
        </chunking>
        <chunking id="4" string="more than 100,000 papers" type="NP">
          <tokens>
            <token id="21" string="more" />
            <token id="22" string="than" />
            <token id="23" string="100,000" />
            <token id="24" string="papers" />
          </tokens>
        </chunking>
        <chunking id="5" string="daily circulation slide" type="NP">
          <tokens>
            <token id="18" string="daily" />
            <token id="19" string="circulation" />
            <token id="20" string="slide" />
          </tokens>
        </chunking>
        <chunking id="6" string="Maxwell" type="NP">
          <tokens>
            <token id="6" string="Maxwell" />
          </tokens>
        </chunking>
        <chunking id="7" string="the Chicago Sun-Times" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="Chicago" />
            <token id="13" string="Sun-Times" />
          </tokens>
        </chunking>
        <chunking id="8" string="which has seen daily circulation slide more than 100,000 papers , to about 533,000 , since 1985" type="SBAR">
          <tokens>
            <token id="15" string="which" />
            <token id="16" string="has" />
            <token id="17" string="seen" />
            <token id="18" string="daily" />
            <token id="19" string="circulation" />
            <token id="20" string="slide" />
            <token id="21" string="more" />
            <token id="22" string="than" />
            <token id="23" string="100,000" />
            <token id="24" string="papers" />
            <token id="25" string="," />
            <token id="26" string="to" />
            <token id="27" string="about" />
            <token id="28" string="533,000" />
            <token id="29" string="," />
            <token id="30" string="since" />
            <token id="31" string="1985" />
          </tokens>
        </chunking>
        <chunking id="9" string="533,000" type="NP">
          <tokens>
            <token id="28" string="533,000" />
          </tokens>
        </chunking>
        <chunking id="10" string="the Chicago Sun-Times , which has seen daily circulation slide more than 100,000 papers , to about 533,000 , since 1985" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="Chicago" />
            <token id="13" string="Sun-Times" />
            <token id="14" string="," />
            <token id="15" string="which" />
            <token id="16" string="has" />
            <token id="17" string="seen" />
            <token id="18" string="daily" />
            <token id="19" string="circulation" />
            <token id="20" string="slide" />
            <token id="21" string="more" />
            <token id="22" string="than" />
            <token id="23" string="100,000" />
            <token id="24" string="papers" />
            <token id="25" string="," />
            <token id="26" string="to" />
            <token id="27" string="about" />
            <token id="28" string="533,000" />
            <token id="29" string="," />
            <token id="30" string="since" />
            <token id="31" string="1985" />
          </tokens>
        </chunking>
        <chunking id="11" string="Other industry officials" type="NP">
          <tokens>
            <token id="1" string="Other" />
            <token id="2" string="industry" />
            <token id="3" string="officials" />
          </tokens>
        </chunking>
        <chunking id="12" string="1985" type="NP">
          <tokens>
            <token id="31" string="1985" />
          </tokens>
        </chunking>
        <chunking id="13" string="be talking to the Chicago Sun-Times , which has seen daily circulation slide more than 100,000 papers , to about 533,000 , since 1985" type="VP">
          <tokens>
            <token id="8" string="be" />
            <token id="9" string="talking" />
            <token id="10" string="to" />
            <token id="11" string="the" />
            <token id="12" string="Chicago" />
            <token id="13" string="Sun-Times" />
            <token id="14" string="," />
            <token id="15" string="which" />
            <token id="16" string="has" />
            <token id="17" string="seen" />
            <token id="18" string="daily" />
            <token id="19" string="circulation" />
            <token id="20" string="slide" />
            <token id="21" string="more" />
            <token id="22" string="than" />
            <token id="23" string="100,000" />
            <token id="24" string="papers" />
            <token id="25" string="," />
            <token id="26" string="to" />
            <token id="27" string="about" />
            <token id="28" string="533,000" />
            <token id="29" string="," />
            <token id="30" string="since" />
            <token id="31" string="1985" />
          </tokens>
        </chunking>
        <chunking id="14" string="more than 100,000 papers , to about 533,000 ," type="NP">
          <tokens>
            <token id="21" string="more" />
            <token id="22" string="than" />
            <token id="23" string="100,000" />
            <token id="24" string="papers" />
            <token id="25" string="," />
            <token id="26" string="to" />
            <token id="27" string="about" />
            <token id="28" string="533,000" />
            <token id="29" string="," />
          </tokens>
        </chunking>
        <chunking id="15" string="might be talking to the Chicago Sun-Times , which has seen daily circulation slide more than 100,000 papers , to about 533,000 , since 1985" type="VP">
          <tokens>
            <token id="7" string="might" />
            <token id="8" string="be" />
            <token id="9" string="talking" />
            <token id="10" string="to" />
            <token id="11" string="the" />
            <token id="12" string="Chicago" />
            <token id="13" string="Sun-Times" />
            <token id="14" string="," />
            <token id="15" string="which" />
            <token id="16" string="has" />
            <token id="17" string="seen" />
            <token id="18" string="daily" />
            <token id="19" string="circulation" />
            <token id="20" string="slide" />
            <token id="21" string="more" />
            <token id="22" string="than" />
            <token id="23" string="100,000" />
            <token id="24" string="papers" />
            <token id="25" string="," />
            <token id="26" string="to" />
            <token id="27" string="about" />
            <token id="28" string="533,000" />
            <token id="29" string="," />
            <token id="30" string="since" />
            <token id="31" string="1985" />
          </tokens>
        </chunking>
        <chunking id="16" string="talking to the Chicago Sun-Times , which has seen daily circulation slide more than 100,000 papers , to about 533,000 , since 1985" type="VP">
          <tokens>
            <token id="9" string="talking" />
            <token id="10" string="to" />
            <token id="11" string="the" />
            <token id="12" string="Chicago" />
            <token id="13" string="Sun-Times" />
            <token id="14" string="," />
            <token id="15" string="which" />
            <token id="16" string="has" />
            <token id="17" string="seen" />
            <token id="18" string="daily" />
            <token id="19" string="circulation" />
            <token id="20" string="slide" />
            <token id="21" string="more" />
            <token id="22" string="than" />
            <token id="23" string="100,000" />
            <token id="24" string="papers" />
            <token id="25" string="," />
            <token id="26" string="to" />
            <token id="27" string="about" />
            <token id="28" string="533,000" />
            <token id="29" string="," />
            <token id="30" string="since" />
            <token id="31" string="1985" />
          </tokens>
        </chunking>
        <chunking id="17" string="seen daily circulation slide more than 100,000 papers , to about 533,000 , since 1985" type="VP">
          <tokens>
            <token id="17" string="seen" />
            <token id="18" string="daily" />
            <token id="19" string="circulation" />
            <token id="20" string="slide" />
            <token id="21" string="more" />
            <token id="22" string="than" />
            <token id="23" string="100,000" />
            <token id="24" string="papers" />
            <token id="25" string="," />
            <token id="26" string="to" />
            <token id="27" string="about" />
            <token id="28" string="533,000" />
            <token id="29" string="," />
            <token id="30" string="since" />
            <token id="31" string="1985" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="amod">
          <governor id="3">officials</governor>
          <dependent id="1">Other</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">officials</governor>
          <dependent id="2">industry</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">conjectured</governor>
          <dependent id="3">officials</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">conjectured</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="9">talking</governor>
          <dependent id="5">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">talking</governor>
          <dependent id="6">Maxwell</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="9">talking</governor>
          <dependent id="7">might</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="9">talking</governor>
          <dependent id="8">be</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="4">conjectured</governor>
          <dependent id="9">talking</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">Sun-Times</governor>
          <dependent id="10">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">Sun-Times</governor>
          <dependent id="11">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="13">Sun-Times</governor>
          <dependent id="12">Chicago</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">talking</governor>
          <dependent id="13">Sun-Times</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="17">seen</governor>
          <dependent id="15">which</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="17">seen</governor>
          <dependent id="16">has</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="13">Sun-Times</governor>
          <dependent id="17">seen</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="20">slide</governor>
          <dependent id="18">daily</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="20">slide</governor>
          <dependent id="19">circulation</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="17">seen</governor>
          <dependent id="20">slide</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="23">100,000</governor>
          <dependent id="21">more</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="21">more</governor>
          <dependent id="22">than</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="24">papers</governor>
          <dependent id="23">100,000</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">seen</governor>
          <dependent id="24">papers</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="28">533,000</governor>
          <dependent id="26">to</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">533,000</governor>
          <dependent id="27">about</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="24">papers</governor>
          <dependent id="28">533,000</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">papers</governor>
          <dependent id="30">since</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="24">papers</governor>
          <dependent id="31">1985</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="1985" type="DATE" score="0.0">
          <tokens>
            <token id="31" string="1985" />
          </tokens>
        </entity>
        <entity id="2" string="daily" type="SET" score="0.0">
          <tokens>
            <token id="18" string="daily" />
          </tokens>
        </entity>
        <entity id="3" string="100,000" type="NUMBER" score="0.0">
          <tokens>
            <token id="23" string="100,000" />
          </tokens>
        </entity>
        <entity id="4" string="Maxwell" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Maxwell" />
          </tokens>
        </entity>
        <entity id="5" string="533,000" type="NUMBER" score="0.0">
          <tokens>
            <token id="28" string="533,000" />
          </tokens>
        </entity>
        <entity id="6" string="Chicago Sun-Times" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="12" string="Chicago" />
            <token id="13" string="Sun-Times" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="11" has_coreference="true">
      <content>Maxwell, who publishes Britain&amp;apost;s Daily Mirror, recently bought three U.S. supermarket tabloids, the Globe, National Examiner and the Sun, for an estimated $100 million.</content>
      <tokens>
        <token id="1" string="Maxwell" lemma="Maxwell" stem="maxwel" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="publishes" lemma="publish" stem="publish" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="Britain" lemma="Britain" stem="britain" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="true" />
        <token id="6" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="Daily" lemma="Daily" stem="daili" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="8" string="Mirror" lemma="Mirror" stem="mirror" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="recently" lemma="recently" stem="recent" pos="RB" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="11" string="bought" lemma="buy" stem="bought" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="three" lemma="three" stem="three" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="13" string="U.S." lemma="U.S." stem="u.s." pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="14" string="supermarket" lemma="supermarket" stem="supermarket" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="tabloids" lemma="tabloid" stem="tabloid" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="Globe" lemma="Globe" stem="globe" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="National" lemma="National" stem="nation" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="21" string="Examiner" lemma="Examiner" stem="examin" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="22" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="Sun" lemma="Sun" stem="sun" pos="NNP" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="25" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="estimated" lemma="estimate" stem="estim" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="$" lemma="$" stem="$" pos="$" type="Symbol" isStopWord="true" ner="MONEY" is_referenced="false" is_refers="false" />
        <token id="30" string="100" lemma="100" stem="100" pos="CD" type="Number" isStopWord="false" ner="MONEY" is_referenced="false" is_refers="false" />
        <token id="31" string="million" lemma="million" stem="million" pos="CD" type="Word" isStopWord="false" ner="MONEY" is_referenced="false" is_refers="false" />
        <token id="32" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Maxwell)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBZ publishes) (NP (NP (NNP Britain) (POS 's)) (NNP Daily) (NNP Mirror))))) (, ,)) (ADVP (RB recently)) (VP (VBD bought) (NP (NP (CD three) (NNP U.S.) (NN supermarket) (NNS tabloids)) (, ,) (NP (DT the) (NNP Globe)) (, ,) (NP (NNP National) (NNP Examiner)) (CC and) (NP (DT the) (NNP Sun))) (, ,) (PP (IN for) (NP (DT an) (QP (VBN estimated) ($ $) (CD 100) (CD million))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="publishes Britain 's Daily Mirror" type="VP">
          <tokens>
            <token id="4" string="publishes" />
            <token id="5" string="Britain" />
            <token id="6" string="'s" />
            <token id="7" string="Daily" />
            <token id="8" string="Mirror" />
          </tokens>
        </chunking>
        <chunking id="2" string="Britain 's Daily Mirror" type="NP">
          <tokens>
            <token id="5" string="Britain" />
            <token id="6" string="'s" />
            <token id="7" string="Daily" />
            <token id="8" string="Mirror" />
          </tokens>
        </chunking>
        <chunking id="3" string="Maxwell" type="NP">
          <tokens>
            <token id="1" string="Maxwell" />
          </tokens>
        </chunking>
        <chunking id="4" string="bought three U.S. supermarket tabloids , the Globe , National Examiner and the Sun , for an estimated $ 100 million" type="VP">
          <tokens>
            <token id="11" string="bought" />
            <token id="12" string="three" />
            <token id="13" string="U.S." />
            <token id="14" string="supermarket" />
            <token id="15" string="tabloids" />
            <token id="16" string="," />
            <token id="17" string="the" />
            <token id="18" string="Globe" />
            <token id="19" string="," />
            <token id="20" string="National" />
            <token id="21" string="Examiner" />
            <token id="22" string="and" />
            <token id="23" string="the" />
            <token id="24" string="Sun" />
            <token id="25" string="," />
            <token id="26" string="for" />
            <token id="27" string="an" />
            <token id="28" string="estimated" />
            <token id="29" string="$" />
            <token id="30" string="100" />
            <token id="31" string="million" />
          </tokens>
        </chunking>
        <chunking id="5" string="National Examiner" type="NP">
          <tokens>
            <token id="20" string="National" />
            <token id="21" string="Examiner" />
          </tokens>
        </chunking>
        <chunking id="6" string="an estimated $ 100 million" type="NP">
          <tokens>
            <token id="27" string="an" />
            <token id="28" string="estimated" />
            <token id="29" string="$" />
            <token id="30" string="100" />
            <token id="31" string="million" />
          </tokens>
        </chunking>
        <chunking id="7" string="three U.S. supermarket tabloids , the Globe , National Examiner and the Sun" type="NP">
          <tokens>
            <token id="12" string="three" />
            <token id="13" string="U.S." />
            <token id="14" string="supermarket" />
            <token id="15" string="tabloids" />
            <token id="16" string="," />
            <token id="17" string="the" />
            <token id="18" string="Globe" />
            <token id="19" string="," />
            <token id="20" string="National" />
            <token id="21" string="Examiner" />
            <token id="22" string="and" />
            <token id="23" string="the" />
            <token id="24" string="Sun" />
          </tokens>
        </chunking>
        <chunking id="8" string="three U.S. supermarket tabloids" type="NP">
          <tokens>
            <token id="12" string="three" />
            <token id="13" string="U.S." />
            <token id="14" string="supermarket" />
            <token id="15" string="tabloids" />
          </tokens>
        </chunking>
        <chunking id="9" string="who publishes Britain 's Daily Mirror" type="SBAR">
          <tokens>
            <token id="3" string="who" />
            <token id="4" string="publishes" />
            <token id="5" string="Britain" />
            <token id="6" string="'s" />
            <token id="7" string="Daily" />
            <token id="8" string="Mirror" />
          </tokens>
        </chunking>
        <chunking id="10" string="the Globe" type="NP">
          <tokens>
            <token id="17" string="the" />
            <token id="18" string="Globe" />
          </tokens>
        </chunking>
        <chunking id="11" string="the Sun" type="NP">
          <tokens>
            <token id="23" string="the" />
            <token id="24" string="Sun" />
          </tokens>
        </chunking>
        <chunking id="12" string="Maxwell , who publishes Britain 's Daily Mirror ," type="NP">
          <tokens>
            <token id="1" string="Maxwell" />
            <token id="2" string="," />
            <token id="3" string="who" />
            <token id="4" string="publishes" />
            <token id="5" string="Britain" />
            <token id="6" string="'s" />
            <token id="7" string="Daily" />
            <token id="8" string="Mirror" />
            <token id="9" string="," />
          </tokens>
        </chunking>
        <chunking id="13" string="Britain 's" type="NP">
          <tokens>
            <token id="5" string="Britain" />
            <token id="6" string="'s" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="11">bought</governor>
          <dependent id="1">Maxwell</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">publishes</governor>
          <dependent id="3">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="1">Maxwell</governor>
          <dependent id="4">publishes</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="8">Mirror</governor>
          <dependent id="5">Britain</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">Britain</governor>
          <dependent id="6">'s</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">Mirror</governor>
          <dependent id="7">Daily</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">publishes</governor>
          <dependent id="8">Mirror</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">bought</governor>
          <dependent id="10">recently</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="11">bought</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="15">tabloids</governor>
          <dependent id="12">three</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">tabloids</governor>
          <dependent id="13">U.S.</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">tabloids</governor>
          <dependent id="14">supermarket</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">bought</governor>
          <dependent id="15">tabloids</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">Globe</governor>
          <dependent id="17">the</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="15">tabloids</governor>
          <dependent id="18">Globe</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="21">Examiner</governor>
          <dependent id="20">National</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="15">tabloids</governor>
          <dependent id="21">Examiner</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="15">tabloids</governor>
          <dependent id="22">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="24">Sun</governor>
          <dependent id="23">the</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="15">tabloids</governor>
          <dependent id="24">Sun</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">$</governor>
          <dependent id="26">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="29">$</governor>
          <dependent id="27">an</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="29">$</governor>
          <dependent id="28">estimated</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">bought</governor>
          <dependent id="29">$</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="31">million</governor>
          <dependent id="30">100</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="29">$</governor>
          <dependent id="31">million</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="$ 100 million" type="MONEY" score="0.0">
          <tokens>
            <token id="29" string="$" />
            <token id="30" string="100" />
            <token id="31" string="million" />
          </tokens>
        </entity>
        <entity id="2" string="U.S." type="LOCATION" score="0.0">
          <tokens>
            <token id="13" string="U.S." />
          </tokens>
        </entity>
        <entity id="3" string="Globe" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="18" string="Globe" />
          </tokens>
        </entity>
        <entity id="4" string="Daily Mirror" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="7" string="Daily" />
            <token id="8" string="Mirror" />
          </tokens>
        </entity>
        <entity id="5" string="Maxwell" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Maxwell" />
          </tokens>
        </entity>
        <entity id="6" string="recently" type="DATE" score="0.0">
          <tokens>
            <token id="10" string="recently" />
          </tokens>
        </entity>
        <entity id="7" string="Sun" type="MISC" score="0.0">
          <tokens>
            <token id="24" string="Sun" />
          </tokens>
        </entity>
        <entity id="8" string="Britain" type="LOCATION" score="0.0">
          <tokens>
            <token id="5" string="Britain" />
          </tokens>
        </entity>
        <entity id="9" string="National Examiner" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="20" string="National" />
            <token id="21" string="Examiner" />
          </tokens>
        </entity>
        <entity id="10" string="three" type="NUMBER" score="0.0">
          <tokens>
            <token id="12" string="three" />
          </tokens>
        </entity>
      </entities>
    </sentence>
  </sentences>
  <coreferences>
    <coreference id="1" type="PROPER">
      <referenced ids_tokens="3-4-5" string="entrepreneur Robert Maxwell" id_sentence="1" />
      <mentions>
        <mention ids_tokens="1" string="Maxwell" id_sentence="2" />
        <mention ids_tokens="9" string="his" id_sentence="2" />
        <mention ids_tokens="13" string="Maxwell" id_sentence="3" />
        <mention ids_tokens="1-2" string="Maxwell's" id_sentence="4" />
        <mention ids_tokens="16-17" string="Maxwell's" id_sentence="4" />
        <mention ids_tokens="12" string="Maxwell" id_sentence="5" />
        <mention ids_tokens="9" string="Maxwell" id_sentence="8" />
        <mention ids_tokens="23" string="his" id_sentence="8" />
        <mention ids_tokens="29" string="his" id_sentence="8" />
        <mention ids_tokens="1" string="Maxwell" id_sentence="9" />
        <mention ids_tokens="6" string="Maxwell" id_sentence="10" />
        <mention ids_tokens="1-8" string="Maxwell , who publishes Britain's Daily Mirror" id_sentence="11" />
        <mention ids_tokens="1" string="Maxwell" id_sentence="11" />
      </mentions>
    </coreference>
    <coreference id="2" type="PROPER">
      <referenced ids_tokens="18-19-20-21" string="Gannett 's USA Today" id_sentence="2" />
      <mentions>
        <mention ids_tokens="16-27" string="a U.S. newspaper valued &quot; in the hundreds of millions of dollars" id_sentence="1" />
        <mention ids_tokens="14-42" string="USA Today , since it would mesh well with his efforts to print and distribute his month-old &quot; pan-European &quot; English-language paper , The European , in this country" id_sentence="8" />
        <mention ids_tokens="14-15" string="USA Today" id_sentence="8" />
        <mention ids_tokens="18" string="it" id_sentence="8" />
        <mention ids_tokens="8-9" string="USA Today" id_sentence="9" />
      </mentions>
    </coreference>
    <coreference id="3" type="PROPER">
      <referenced ids_tokens="26-27-28-29" string="New York Daily News" id_sentence="2" />
      <mentions>
        <mention ids_tokens="1-3" string="The Daily News" id_sentence="6" />
      </mentions>
    </coreference>
    <coreference id="4" type="PROPER">
      <referenced ids_tokens="31-32-33" string="the Chicago Sun-Times" id_sentence="2" />
      <mentions>
        <mention ids_tokens="11-31" string="the Chicago Sun-Times , which has seen daily circulation slide more than 100,000 papers , to about 533,000 , since 1985" id_sentence="10" />
        <mention ids_tokens="12-13" string="Chicago Sun-Times" id_sentence="10" />
      </mentions>
    </coreference>
    <coreference id="7" type="NOMINAL">
      <referenced ids_tokens="25-26-27-28-29-30-31-32-33-34-35-36" string="a spokesman for Maxwell Communications , the investor 's London-based holding company" id_sentence="4" />
      <mentions>
        <mention ids_tokens="1-2" string="The spokesman" id_sentence="5" />
      </mentions>
    </coreference>
    <coreference id="8" type="PROPER">
      <referenced ids_tokens="15-16-17" string="the United States" id_sentence="5" />
      <mentions>
        <mention ids_tokens="8-9" string="United States" id_sentence="6" />
      </mentions>
    </coreference>
    <coreference id="11" type="NOMINAL">
      <referenced ids_tokens="1-2" string="The paper" id_sentence="7" />
      <mentions>
        <mention ids_tokens="4" string="it" id_sentence="8" />
      </mentions>
    </coreference>
  </coreferences>
</document>
