<?xml version="1.0" encoding="UTF-8"?>
<document id="0" name="AP881001-0104">
  <sentences>
    <sentence id="1" has_coreference="true">
      <content>Cynthia Lennon joins the throng denouncing the new, unauthorized biography of her late former husband, John Lennon, as written by a money-hungry author capitalizing on untruths.</content>
      <tokens>
        <token id="1" string="Cynthia" lemma="Cynthia" stem="cynthia" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="2" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="3" string="joins" lemma="join" stem="join" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="throng" lemma="throng" stem="throng" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="denouncing" lemma="denounce" stem="denounc" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="new" lemma="new" stem="new" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="unauthorized" lemma="unauthorized" stem="unauthor" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="biography" lemma="biography" stem="biographi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="late" lemma="late" stem="late" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="former" lemma="former" stem="former" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="husband" lemma="husband" stem="husband" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="19" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="written" lemma="write" stem="written" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="money-hungry" lemma="money-hungry" stem="money-hungri" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="author" lemma="author" stem="author" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="capitalizing" lemma="capitalize" stem="capit" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="untruths" lemma="untruth" stem="untruth" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Cynthia) (NNP Lennon)) (VP (VBZ joins) (NP (NP (DT the) (NN throng)) (VP (VBG denouncing) (NP (NP (DT the) (JJ new) (, ,) (JJ unauthorized) (NN biography)) (PP (IN of) (NP (NP (PRP$ her) (JJ late) (JJ former) (NN husband)) (, ,) (NP (NNP John) (NNP Lennon)) (, ,)))) (SBAR (IN as) (S (VP (VBN written) (PP (IN by) (NP (NP (DT a) (JJ money-hungry) (NN author)) (VP (VBG capitalizing) (PP (IN on) (NP (NNS untruths)))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a money-hungry author" type="NP">
          <tokens>
            <token id="24" string="a" />
            <token id="25" string="money-hungry" />
            <token id="26" string="author" />
          </tokens>
        </chunking>
        <chunking id="2" string="her late former husband , John Lennon ," type="NP">
          <tokens>
            <token id="13" string="her" />
            <token id="14" string="late" />
            <token id="15" string="former" />
            <token id="16" string="husband" />
            <token id="17" string="," />
            <token id="18" string="John" />
            <token id="19" string="Lennon" />
            <token id="20" string="," />
          </tokens>
        </chunking>
        <chunking id="3" string="written by a money-hungry author capitalizing on untruths" type="VP">
          <tokens>
            <token id="22" string="written" />
            <token id="23" string="by" />
            <token id="24" string="a" />
            <token id="25" string="money-hungry" />
            <token id="26" string="author" />
            <token id="27" string="capitalizing" />
            <token id="28" string="on" />
            <token id="29" string="untruths" />
          </tokens>
        </chunking>
        <chunking id="4" string="untruths" type="NP">
          <tokens>
            <token id="29" string="untruths" />
          </tokens>
        </chunking>
        <chunking id="5" string="the throng" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="throng" />
          </tokens>
        </chunking>
        <chunking id="6" string="capitalizing on untruths" type="VP">
          <tokens>
            <token id="27" string="capitalizing" />
            <token id="28" string="on" />
            <token id="29" string="untruths" />
          </tokens>
        </chunking>
        <chunking id="7" string="John Lennon" type="NP">
          <tokens>
            <token id="18" string="John" />
            <token id="19" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="8" string="her late former husband" type="NP">
          <tokens>
            <token id="13" string="her" />
            <token id="14" string="late" />
            <token id="15" string="former" />
            <token id="16" string="husband" />
          </tokens>
        </chunking>
        <chunking id="9" string="as written by a money-hungry author capitalizing on untruths" type="SBAR">
          <tokens>
            <token id="21" string="as" />
            <token id="22" string="written" />
            <token id="23" string="by" />
            <token id="24" string="a" />
            <token id="25" string="money-hungry" />
            <token id="26" string="author" />
            <token id="27" string="capitalizing" />
            <token id="28" string="on" />
            <token id="29" string="untruths" />
          </tokens>
        </chunking>
        <chunking id="10" string="Cynthia Lennon" type="NP">
          <tokens>
            <token id="1" string="Cynthia" />
            <token id="2" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="11" string="a money-hungry author capitalizing on untruths" type="NP">
          <tokens>
            <token id="24" string="a" />
            <token id="25" string="money-hungry" />
            <token id="26" string="author" />
            <token id="27" string="capitalizing" />
            <token id="28" string="on" />
            <token id="29" string="untruths" />
          </tokens>
        </chunking>
        <chunking id="12" string="the throng denouncing the new , unauthorized biography of her late former husband , John Lennon , as written by a money-hungry author capitalizing on untruths" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="throng" />
            <token id="6" string="denouncing" />
            <token id="7" string="the" />
            <token id="8" string="new" />
            <token id="9" string="," />
            <token id="10" string="unauthorized" />
            <token id="11" string="biography" />
            <token id="12" string="of" />
            <token id="13" string="her" />
            <token id="14" string="late" />
            <token id="15" string="former" />
            <token id="16" string="husband" />
            <token id="17" string="," />
            <token id="18" string="John" />
            <token id="19" string="Lennon" />
            <token id="20" string="," />
            <token id="21" string="as" />
            <token id="22" string="written" />
            <token id="23" string="by" />
            <token id="24" string="a" />
            <token id="25" string="money-hungry" />
            <token id="26" string="author" />
            <token id="27" string="capitalizing" />
            <token id="28" string="on" />
            <token id="29" string="untruths" />
          </tokens>
        </chunking>
        <chunking id="13" string="denouncing the new , unauthorized biography of her late former husband , John Lennon , as written by a money-hungry author capitalizing on untruths" type="VP">
          <tokens>
            <token id="6" string="denouncing" />
            <token id="7" string="the" />
            <token id="8" string="new" />
            <token id="9" string="," />
            <token id="10" string="unauthorized" />
            <token id="11" string="biography" />
            <token id="12" string="of" />
            <token id="13" string="her" />
            <token id="14" string="late" />
            <token id="15" string="former" />
            <token id="16" string="husband" />
            <token id="17" string="," />
            <token id="18" string="John" />
            <token id="19" string="Lennon" />
            <token id="20" string="," />
            <token id="21" string="as" />
            <token id="22" string="written" />
            <token id="23" string="by" />
            <token id="24" string="a" />
            <token id="25" string="money-hungry" />
            <token id="26" string="author" />
            <token id="27" string="capitalizing" />
            <token id="28" string="on" />
            <token id="29" string="untruths" />
          </tokens>
        </chunking>
        <chunking id="14" string="joins the throng denouncing the new , unauthorized biography of her late former husband , John Lennon , as written by a money-hungry author capitalizing on untruths" type="VP">
          <tokens>
            <token id="3" string="joins" />
            <token id="4" string="the" />
            <token id="5" string="throng" />
            <token id="6" string="denouncing" />
            <token id="7" string="the" />
            <token id="8" string="new" />
            <token id="9" string="," />
            <token id="10" string="unauthorized" />
            <token id="11" string="biography" />
            <token id="12" string="of" />
            <token id="13" string="her" />
            <token id="14" string="late" />
            <token id="15" string="former" />
            <token id="16" string="husband" />
            <token id="17" string="," />
            <token id="18" string="John" />
            <token id="19" string="Lennon" />
            <token id="20" string="," />
            <token id="21" string="as" />
            <token id="22" string="written" />
            <token id="23" string="by" />
            <token id="24" string="a" />
            <token id="25" string="money-hungry" />
            <token id="26" string="author" />
            <token id="27" string="capitalizing" />
            <token id="28" string="on" />
            <token id="29" string="untruths" />
          </tokens>
        </chunking>
        <chunking id="15" string="the new , unauthorized biography" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="new" />
            <token id="9" string="," />
            <token id="10" string="unauthorized" />
            <token id="11" string="biography" />
          </tokens>
        </chunking>
        <chunking id="16" string="the new , unauthorized biography of her late former husband , John Lennon ," type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="new" />
            <token id="9" string="," />
            <token id="10" string="unauthorized" />
            <token id="11" string="biography" />
            <token id="12" string="of" />
            <token id="13" string="her" />
            <token id="14" string="late" />
            <token id="15" string="former" />
            <token id="16" string="husband" />
            <token id="17" string="," />
            <token id="18" string="John" />
            <token id="19" string="Lennon" />
            <token id="20" string="," />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">Lennon</governor>
          <dependent id="1">Cynthia</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">joins</governor>
          <dependent id="2">Lennon</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">joins</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">throng</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">joins</governor>
          <dependent id="5">throng</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="5">throng</governor>
          <dependent id="6">denouncing</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">biography</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">biography</governor>
          <dependent id="8">new</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">biography</governor>
          <dependent id="10">unauthorized</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">denouncing</governor>
          <dependent id="11">biography</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">husband</governor>
          <dependent id="12">of</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="16">husband</governor>
          <dependent id="13">her</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="16">husband</governor>
          <dependent id="14">late</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="16">husband</governor>
          <dependent id="15">former</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">biography</governor>
          <dependent id="16">husband</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">Lennon</governor>
          <dependent id="18">John</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="16">husband</governor>
          <dependent id="19">Lennon</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="22">written</governor>
          <dependent id="21">as</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="6">denouncing</governor>
          <dependent id="22">written</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">author</governor>
          <dependent id="23">by</dependent>
        </dependency>
        <dependency type="det">
          <governor id="26">author</governor>
          <dependent id="24">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="26">author</governor>
          <dependent id="25">money-hungry</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="22">written</governor>
          <dependent id="26">author</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="26">author</governor>
          <dependent id="27">capitalizing</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">untruths</governor>
          <dependent id="28">on</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">capitalizing</governor>
          <dependent id="29">untruths</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="18" string="John" />
            <token id="19" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="Cynthia Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Cynthia" />
            <token id="2" string="Lennon" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="2" has_coreference="true">
      <content>``People who write books like this, it is for greed, and as far as I&amp;apost;m concerned, they&amp;apost;re gravediggers, no more, no less,&amp;apost;&amp;apost; Mrs. Lennon says on the CBS show ``60 Minutes&amp;apost;&amp;apost; scheduled for broadcast Sunday evening.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="People" lemma="people" stem="peopl" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="write" lemma="write" stem="write" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="books" lemma="book" stem="book" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="like" lemma="like" stem="like" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="greed" lemma="greed" stem="greed" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="far" lemma="far" stem="far" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="'m" lemma="be" stem="'m" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="concerned" lemma="concerned" stem="concern" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="23" string="'re" lemma="be" stem="'re" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="gravediggers" lemma="gravedigger" stem="gravedigg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="25" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="no" lemma="no" stem="no" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="more" lemma="more" stem="more" pos="RBR" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="no" lemma="no" stem="no" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="less" lemma="less" stem="less" pos="JJR" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="Mrs." lemma="Mrs." stem="mrs." pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="34" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="35" string="says" lemma="say" stem="sai" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="36" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="CBS" lemma="CBS" stem="cbs" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="39" string="show" lemma="show" stem="show" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="40" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="41" string="60" lemma="60" stem="60" pos="CD" type="Number" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="42" string="Minutes" lemma="Minutes" stem="minut" pos="NNPS" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="43" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="44" string="scheduled" lemma="schedule" stem="schedul" pos="VBN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="45" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="46" string="broadcast" lemma="broadcast" stem="broadcast" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="47" string="Sunday" lemma="Sunday" stem="sundai" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="48" string="evening" lemma="evening" stem="even" pos="NN" type="Word" isStopWord="false" ner="TIME" is_referenced="true" is_refers="false" />
        <token id="49" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (NP (NP (NNS People)) (SBAR (WHNP (WP who)) (S (VP (VBP write) (NP (NP (NNS books)) (PP (IN like) (NP (DT this)))))))) (, ,) (NP (PRP it)) (VP (VBZ is) (UCP (PP (IN for) (NP (NN greed))) (, ,) (CC and) (SBAR (IN as) (S (SBAR (ADVP (RB far)) (IN as) (S (NP (PRP I)) (VP (VBP 'm) (ADJP (JJ concerned))))) (, ,) (NP (PRP they)) (VP (VBP 're) (NP (NNS gravediggers)) (, ,) (ADVP (DT no) (RBR more)))))) (, ,) (ADVP (DT no) (JJR less)))) (, ,) ('' '') (NP (NNP Mrs.) (NNP Lennon)) (VP (VBZ says) (PP (IN on) (NP (DT the) (NNP CBS) (NN show))) (S (NP (`` ``) (NP (CD 60) (NNPS Minutes)) ('' '') (VP (VBN scheduled) (PP (IN for) (NP (NN broadcast))) (NP-TMP (NNP Sunday) (NN evening)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="'re gravediggers , no more" type="VP">
          <tokens>
            <token id="23" string="'re" />
            <token id="24" string="gravediggers" />
            <token id="25" string="," />
            <token id="26" string="no" />
            <token id="27" string="more" />
          </tokens>
        </chunking>
        <chunking id="2" string="People" type="NP">
          <tokens>
            <token id="2" string="People" />
          </tokens>
        </chunking>
        <chunking id="3" string="it" type="NP">
          <tokens>
            <token id="9" string="it" />
          </tokens>
        </chunking>
        <chunking id="4" string="this" type="NP">
          <tokens>
            <token id="7" string="this" />
          </tokens>
        </chunking>
        <chunking id="5" string="books" type="NP">
          <tokens>
            <token id="5" string="books" />
          </tokens>
        </chunking>
        <chunking id="6" string="`` 60 Minutes '' scheduled for broadcast Sunday evening" type="NP">
          <tokens>
            <token id="40" string="``" />
            <token id="41" string="60" />
            <token id="42" string="Minutes" />
            <token id="43" string="''" />
            <token id="44" string="scheduled" />
            <token id="45" string="for" />
            <token id="46" string="broadcast" />
            <token id="47" string="Sunday" />
            <token id="48" string="evening" />
          </tokens>
        </chunking>
        <chunking id="7" string="60 Minutes" type="NP">
          <tokens>
            <token id="41" string="60" />
            <token id="42" string="Minutes" />
          </tokens>
        </chunking>
        <chunking id="8" string="books like this" type="NP">
          <tokens>
            <token id="5" string="books" />
            <token id="6" string="like" />
            <token id="7" string="this" />
          </tokens>
        </chunking>
        <chunking id="9" string="broadcast" type="NP">
          <tokens>
            <token id="46" string="broadcast" />
          </tokens>
        </chunking>
        <chunking id="10" string="the CBS show" type="NP">
          <tokens>
            <token id="37" string="the" />
            <token id="38" string="CBS" />
            <token id="39" string="show" />
          </tokens>
        </chunking>
        <chunking id="11" string="is for greed , and as far as I 'm concerned , they 're gravediggers , no more , no less" type="VP">
          <tokens>
            <token id="10" string="is" />
            <token id="11" string="for" />
            <token id="12" string="greed" />
            <token id="13" string="," />
            <token id="14" string="and" />
            <token id="15" string="as" />
            <token id="16" string="far" />
            <token id="17" string="as" />
            <token id="18" string="I" />
            <token id="19" string="'m" />
            <token id="20" string="concerned" />
            <token id="21" string="," />
            <token id="22" string="they" />
            <token id="23" string="'re" />
            <token id="24" string="gravediggers" />
            <token id="25" string="," />
            <token id="26" string="no" />
            <token id="27" string="more" />
            <token id="28" string="," />
            <token id="29" string="no" />
            <token id="30" string="less" />
          </tokens>
        </chunking>
        <chunking id="12" string="concerned" type="ADJP">
          <tokens>
            <token id="20" string="concerned" />
          </tokens>
        </chunking>
        <chunking id="13" string="gravediggers" type="NP">
          <tokens>
            <token id="24" string="gravediggers" />
          </tokens>
        </chunking>
        <chunking id="14" string="People who write books like this" type="NP">
          <tokens>
            <token id="2" string="People" />
            <token id="3" string="who" />
            <token id="4" string="write" />
            <token id="5" string="books" />
            <token id="6" string="like" />
            <token id="7" string="this" />
          </tokens>
        </chunking>
        <chunking id="15" string="I" type="NP">
          <tokens>
            <token id="18" string="I" />
          </tokens>
        </chunking>
        <chunking id="16" string="far as I 'm concerned" type="SBAR">
          <tokens>
            <token id="16" string="far" />
            <token id="17" string="as" />
            <token id="18" string="I" />
            <token id="19" string="'m" />
            <token id="20" string="concerned" />
          </tokens>
        </chunking>
        <chunking id="17" string="scheduled for broadcast Sunday evening" type="VP">
          <tokens>
            <token id="44" string="scheduled" />
            <token id="45" string="for" />
            <token id="46" string="broadcast" />
            <token id="47" string="Sunday" />
            <token id="48" string="evening" />
          </tokens>
        </chunking>
        <chunking id="18" string="who write books like this" type="SBAR">
          <tokens>
            <token id="3" string="who" />
            <token id="4" string="write" />
            <token id="5" string="books" />
            <token id="6" string="like" />
            <token id="7" string="this" />
          </tokens>
        </chunking>
        <chunking id="19" string="they" type="NP">
          <tokens>
            <token id="22" string="they" />
          </tokens>
        </chunking>
        <chunking id="20" string="greed" type="NP">
          <tokens>
            <token id="12" string="greed" />
          </tokens>
        </chunking>
        <chunking id="21" string="Mrs. Lennon" type="NP">
          <tokens>
            <token id="33" string="Mrs." />
            <token id="34" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="22" string="says on the CBS show `` 60 Minutes '' scheduled for broadcast Sunday evening" type="VP">
          <tokens>
            <token id="35" string="says" />
            <token id="36" string="on" />
            <token id="37" string="the" />
            <token id="38" string="CBS" />
            <token id="39" string="show" />
            <token id="40" string="``" />
            <token id="41" string="60" />
            <token id="42" string="Minutes" />
            <token id="43" string="''" />
            <token id="44" string="scheduled" />
            <token id="45" string="for" />
            <token id="46" string="broadcast" />
            <token id="47" string="Sunday" />
            <token id="48" string="evening" />
          </tokens>
        </chunking>
        <chunking id="23" string="write books like this" type="VP">
          <tokens>
            <token id="4" string="write" />
            <token id="5" string="books" />
            <token id="6" string="like" />
            <token id="7" string="this" />
          </tokens>
        </chunking>
        <chunking id="24" string="as far as I 'm concerned , they 're gravediggers , no more" type="SBAR">
          <tokens>
            <token id="15" string="as" />
            <token id="16" string="far" />
            <token id="17" string="as" />
            <token id="18" string="I" />
            <token id="19" string="'m" />
            <token id="20" string="concerned" />
            <token id="21" string="," />
            <token id="22" string="they" />
            <token id="23" string="'re" />
            <token id="24" string="gravediggers" />
            <token id="25" string="," />
            <token id="26" string="no" />
            <token id="27" string="more" />
          </tokens>
        </chunking>
        <chunking id="25" string="'m concerned" type="VP">
          <tokens>
            <token id="19" string="'m" />
            <token id="20" string="concerned" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="12">greed</governor>
          <dependent id="2">People</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">write</governor>
          <dependent id="3">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="2">People</governor>
          <dependent id="4">write</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">write</governor>
          <dependent id="5">books</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">this</governor>
          <dependent id="6">like</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">books</governor>
          <dependent id="7">this</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">greed</governor>
          <dependent id="9">it</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="12">greed</governor>
          <dependent id="10">is</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">greed</governor>
          <dependent id="11">for</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="35">says</governor>
          <dependent id="12">greed</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="12">greed</governor>
          <dependent id="14">and</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="24">gravediggers</governor>
          <dependent id="15">as</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="20">concerned</governor>
          <dependent id="16">far</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="20">concerned</governor>
          <dependent id="17">as</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="20">concerned</governor>
          <dependent id="18">I</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="20">concerned</governor>
          <dependent id="19">'m</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="24">gravediggers</governor>
          <dependent id="20">concerned</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="24">gravediggers</governor>
          <dependent id="22">they</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="24">gravediggers</governor>
          <dependent id="23">'re</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="12">greed</governor>
          <dependent id="24">gravediggers</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="27">more</governor>
          <dependent id="26">no</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="24">gravediggers</governor>
          <dependent id="27">more</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="30">less</governor>
          <dependent id="29">no</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="12">greed</governor>
          <dependent id="30">less</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="34">Lennon</governor>
          <dependent id="33">Mrs.</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="35">says</governor>
          <dependent id="34">Lennon</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="35">says</dependent>
        </dependency>
        <dependency type="case">
          <governor id="39">show</governor>
          <dependent id="36">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="39">show</governor>
          <dependent id="37">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="39">show</governor>
          <dependent id="38">CBS</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="35">says</governor>
          <dependent id="39">show</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="42">Minutes</governor>
          <dependent id="41">60</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="35">says</governor>
          <dependent id="42">Minutes</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="42">Minutes</governor>
          <dependent id="44">scheduled</dependent>
        </dependency>
        <dependency type="case">
          <governor id="46">broadcast</governor>
          <dependent id="45">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="44">scheduled</governor>
          <dependent id="46">broadcast</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="48">evening</governor>
          <dependent id="47">Sunday</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="44">scheduled</governor>
          <dependent id="48">evening</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="60 Minutes" type="DURATION" score="0.0">
          <tokens>
            <token id="41" string="60" />
            <token id="42" string="Minutes" />
          </tokens>
        </entity>
        <entity id="2" string="CBS" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="38" string="CBS" />
          </tokens>
        </entity>
        <entity id="3" string="evening" type="TIME" score="0.0">
          <tokens>
            <token id="48" string="evening" />
          </tokens>
        </entity>
        <entity id="4" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="34" string="Lennon" />
          </tokens>
        </entity>
        <entity id="5" string="Sunday" type="DATE" score="0.0">
          <tokens>
            <token id="47" string="Sunday" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="3" has_coreference="true">
      <content>``Grave robbers, body snatchers, you name it, they are it.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="Grave" lemma="grave" stem="grave" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="robbers" lemma="robber" stem="robber" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="body" lemma="body" stem="bodi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="snatchers" lemma="snatcher" stem="snatcher" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="you" lemma="you" stem="you" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="name" lemma="name" stem="name" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="are" lemma="be" stem="ar" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (NP (NN Grave) (NNS robbers)) (, ,) (NP (NN body) (NNS snatchers)) (PRN (, ,) (S (NP (PRP you)) (VP (VBP name) (NP (PRP it)))) (, ,))) (NP (PRP they)) (VP (VBP are) (NP (PRP it))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="Grave robbers" type="NP">
          <tokens>
            <token id="2" string="Grave" />
            <token id="3" string="robbers" />
          </tokens>
        </chunking>
        <chunking id="2" string="they" type="NP">
          <tokens>
            <token id="12" string="they" />
          </tokens>
        </chunking>
        <chunking id="3" string="body snatchers" type="NP">
          <tokens>
            <token id="5" string="body" />
            <token id="6" string="snatchers" />
          </tokens>
        </chunking>
        <chunking id="4" string="it" type="NP">
          <tokens>
            <token id="10" string="it" />
          </tokens>
        </chunking>
        <chunking id="5" string="are it" type="VP">
          <tokens>
            <token id="13" string="are" />
            <token id="14" string="it" />
          </tokens>
        </chunking>
        <chunking id="6" string="name it" type="VP">
          <tokens>
            <token id="9" string="name" />
            <token id="10" string="it" />
          </tokens>
        </chunking>
        <chunking id="7" string="Grave robbers , body snatchers , you name it ," type="NP">
          <tokens>
            <token id="2" string="Grave" />
            <token id="3" string="robbers" />
            <token id="4" string="," />
            <token id="5" string="body" />
            <token id="6" string="snatchers" />
            <token id="7" string="," />
            <token id="8" string="you" />
            <token id="9" string="name" />
            <token id="10" string="it" />
            <token id="11" string="," />
          </tokens>
        </chunking>
        <chunking id="8" string="you" type="NP">
          <tokens>
            <token id="8" string="you" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="3">robbers</governor>
          <dependent id="2">Grave</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">it</governor>
          <dependent id="3">robbers</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">snatchers</governor>
          <dependent id="5">body</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="3">robbers</governor>
          <dependent id="6">snatchers</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">name</governor>
          <dependent id="8">you</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="3">robbers</governor>
          <dependent id="9">name</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="9">name</governor>
          <dependent id="10">it</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">it</governor>
          <dependent id="12">they</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="14">it</governor>
          <dependent id="13">are</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="14">it</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="4" has_coreference="true">
      <content>The biography by Albert Goldman, ``The Lives of John Lennon,&amp;apost;&amp;apost; depicts the former Beatle as an anorexic bisexual addled by drugs who raged his way from Liverpool to New York.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="biography" lemma="biography" stem="biographi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="Albert" lemma="Albert" stem="albert" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="5" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="Lives" lemma="life" stem="live" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="12" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="depicts" lemma="depict" stem="depict" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="former" lemma="former" stem="former" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="Beatle" lemma="Beatle" stem="beatl" pos="NNP" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="19" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="21" string="anorexic" lemma="anorexic" stem="anorex" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="22" string="bisexual" lemma="bisexual" stem="bisexu" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="23" string="addled" lemma="addled" stem="addl" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="24" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="drugs" lemma="drug" stem="drug" pos="NNS" type="Word" isStopWord="false" ner="CAUSE_OF_DEATH" is_referenced="false" is_refers="false" />
        <token id="26" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="raged" lemma="rage" stem="rage" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="way" lemma="way" stem="wai" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="Liverpool" lemma="Liverpool" stem="liverpool" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="32" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="New" lemma="New" stem="new" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="34" string="York" lemma="York" stem="york" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="35" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP-TMP (NP (DT The) (NN biography)) (PP (IN by) (NP (NNP Albert) (NNP Goldman)))) (, ,) (NP (`` ``) (NP (NP (DT The) (NNS Lives)) (PP (IN of) (NP (NNP John) (NNP Lennon)))) (, ,) ('' '')) (VP (VBZ depicts) (NP (DT the) (JJ former) (NNP Beatle)) (PP (IN as) (NP (DT an) (JJ anorexic) (JJ bisexual) (JJ addled))) (PP (IN by) (NP (NP (NNS drugs)) (SBAR (WHNP (WP who)) (S (VP (VBD raged) (NP (PRP$ his) (NN way)) (PP (IN from) (NP (NNP Liverpool))) (PP (TO to) (NP (NNP New) (NNP York))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="New York" type="NP">
          <tokens>
            <token id="33" string="New" />
            <token id="34" string="York" />
          </tokens>
        </chunking>
        <chunking id="2" string="Liverpool" type="NP">
          <tokens>
            <token id="31" string="Liverpool" />
          </tokens>
        </chunking>
        <chunking id="3" string="drugs" type="NP">
          <tokens>
            <token id="25" string="drugs" />
          </tokens>
        </chunking>
        <chunking id="4" string="raged his way from Liverpool to New York" type="VP">
          <tokens>
            <token id="27" string="raged" />
            <token id="28" string="his" />
            <token id="29" string="way" />
            <token id="30" string="from" />
            <token id="31" string="Liverpool" />
            <token id="32" string="to" />
            <token id="33" string="New" />
            <token id="34" string="York" />
          </tokens>
        </chunking>
        <chunking id="5" string="The Lives" type="NP">
          <tokens>
            <token id="8" string="The" />
            <token id="9" string="Lives" />
          </tokens>
        </chunking>
        <chunking id="6" string="drugs who raged his way from Liverpool to New York" type="NP">
          <tokens>
            <token id="25" string="drugs" />
            <token id="26" string="who" />
            <token id="27" string="raged" />
            <token id="28" string="his" />
            <token id="29" string="way" />
            <token id="30" string="from" />
            <token id="31" string="Liverpool" />
            <token id="32" string="to" />
            <token id="33" string="New" />
            <token id="34" string="York" />
          </tokens>
        </chunking>
        <chunking id="7" string="The biography" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="biography" />
          </tokens>
        </chunking>
        <chunking id="8" string="the former Beatle" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="former" />
            <token id="18" string="Beatle" />
          </tokens>
        </chunking>
        <chunking id="9" string="an anorexic bisexual addled" type="NP">
          <tokens>
            <token id="20" string="an" />
            <token id="21" string="anorexic" />
            <token id="22" string="bisexual" />
            <token id="23" string="addled" />
          </tokens>
        </chunking>
        <chunking id="10" string="who raged his way from Liverpool to New York" type="SBAR">
          <tokens>
            <token id="26" string="who" />
            <token id="27" string="raged" />
            <token id="28" string="his" />
            <token id="29" string="way" />
            <token id="30" string="from" />
            <token id="31" string="Liverpool" />
            <token id="32" string="to" />
            <token id="33" string="New" />
            <token id="34" string="York" />
          </tokens>
        </chunking>
        <chunking id="11" string="`` The Lives of John Lennon , ''" type="NP">
          <tokens>
            <token id="7" string="``" />
            <token id="8" string="The" />
            <token id="9" string="Lives" />
            <token id="10" string="of" />
            <token id="11" string="John" />
            <token id="12" string="Lennon" />
            <token id="13" string="," />
            <token id="14" string="''" />
          </tokens>
        </chunking>
        <chunking id="12" string="John Lennon" type="NP">
          <tokens>
            <token id="11" string="John" />
            <token id="12" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="13" string="The Lives of John Lennon" type="NP">
          <tokens>
            <token id="8" string="The" />
            <token id="9" string="Lives" />
            <token id="10" string="of" />
            <token id="11" string="John" />
            <token id="12" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="14" string="his way" type="NP">
          <tokens>
            <token id="28" string="his" />
            <token id="29" string="way" />
          </tokens>
        </chunking>
        <chunking id="15" string="depicts the former Beatle as an anorexic bisexual addled by drugs who raged his way from Liverpool to New York" type="VP">
          <tokens>
            <token id="15" string="depicts" />
            <token id="16" string="the" />
            <token id="17" string="former" />
            <token id="18" string="Beatle" />
            <token id="19" string="as" />
            <token id="20" string="an" />
            <token id="21" string="anorexic" />
            <token id="22" string="bisexual" />
            <token id="23" string="addled" />
            <token id="24" string="by" />
            <token id="25" string="drugs" />
            <token id="26" string="who" />
            <token id="27" string="raged" />
            <token id="28" string="his" />
            <token id="29" string="way" />
            <token id="30" string="from" />
            <token id="31" string="Liverpool" />
            <token id="32" string="to" />
            <token id="33" string="New" />
            <token id="34" string="York" />
          </tokens>
        </chunking>
        <chunking id="16" string="Albert Goldman" type="NP">
          <tokens>
            <token id="4" string="Albert" />
            <token id="5" string="Goldman" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">biography</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="15">depicts</governor>
          <dependent id="2">biography</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">Goldman</governor>
          <dependent id="3">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">Goldman</governor>
          <dependent id="4">Albert</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">biography</governor>
          <dependent id="5">Goldman</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">Lives</governor>
          <dependent id="8">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">depicts</governor>
          <dependent id="9">Lives</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">Lennon</governor>
          <dependent id="10">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">Lennon</governor>
          <dependent id="11">John</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">Lives</governor>
          <dependent id="12">Lennon</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="15">depicts</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">Beatle</governor>
          <dependent id="16">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="18">Beatle</governor>
          <dependent id="17">former</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">depicts</governor>
          <dependent id="18">Beatle</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">addled</governor>
          <dependent id="19">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="23">addled</governor>
          <dependent id="20">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="23">addled</governor>
          <dependent id="21">anorexic</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="23">addled</governor>
          <dependent id="22">bisexual</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">depicts</governor>
          <dependent id="23">addled</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">drugs</governor>
          <dependent id="24">by</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">depicts</governor>
          <dependent id="25">drugs</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="27">raged</governor>
          <dependent id="26">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="25">drugs</governor>
          <dependent id="27">raged</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="29">way</governor>
          <dependent id="28">his</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="27">raged</governor>
          <dependent id="29">way</dependent>
        </dependency>
        <dependency type="case">
          <governor id="31">Liverpool</governor>
          <dependent id="30">from</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">raged</governor>
          <dependent id="31">Liverpool</dependent>
        </dependency>
        <dependency type="case">
          <governor id="34">York</governor>
          <dependent id="32">to</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="34">York</governor>
          <dependent id="33">New</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">raged</governor>
          <dependent id="34">York</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="11" string="John" />
            <token id="12" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="New York" type="LOCATION" score="0.0">
          <tokens>
            <token id="33" string="New" />
            <token id="34" string="York" />
          </tokens>
        </entity>
        <entity id="3" string="Liverpool" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="31" string="Liverpool" />
          </tokens>
        </entity>
        <entity id="4" string="Beatle" type="MISC" score="0.0">
          <tokens>
            <token id="18" string="Beatle" />
          </tokens>
        </entity>
        <entity id="5" string="drugs" type="CAUSE_OF_DEATH" score="0.0">
          <tokens>
            <token id="25" string="drugs" />
          </tokens>
        </entity>
        <entity id="6" string="Albert Goldman" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="Albert" />
            <token id="5" string="Goldman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="5" has_coreference="true">
      <content>It was released Sept. 14.</content>
      <tokens>
        <token id="1" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="released" lemma="release" stem="releas" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="Sept." lemma="Sept." stem="sept." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="14" lemma="14" stem="14" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="6" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP It)) (VP (VBD was) (VP (VBN released) (NP-TMP (NNP Sept.) (CD 14)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="released Sept. 14" type="VP">
          <tokens>
            <token id="3" string="released" />
            <token id="4" string="Sept." />
            <token id="5" string="14" />
          </tokens>
        </chunking>
        <chunking id="2" string="was released Sept. 14" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="released" />
            <token id="4" string="Sept." />
            <token id="5" string="14" />
          </tokens>
        </chunking>
        <chunking id="3" string="It" type="NP">
          <tokens>
            <token id="1" string="It" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="3">released</governor>
          <dependent id="1">It</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="3">released</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">released</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="3">released</governor>
          <dependent id="4">Sept.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="4">Sept.</governor>
          <dependent id="5">14</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Sept. 14" type="DATE" score="0.0">
          <tokens>
            <token id="4" string="Sept." />
            <token id="5" string="14" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="6" has_coreference="true">
      <content>Goldman, who defends his Lennon book and said it is based on 1,200 interviews, also has written controversial biographies of comedian Lenny Bruce and Elvis Presley.</content>
      <tokens>
        <token id="1" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="defends" lemma="defend" stem="defend" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="6" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="7" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="8" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="10" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="based" lemma="base" stem="base" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="13" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="1,200" lemma="1,200" stem="1,200" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="true" />
        <token id="15" string="interviews" lemma="interview" stem="interview" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="written" lemma="write" stem="written" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="controversial" lemma="controversial" stem="controversi" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="biographies" lemma="biography" stem="biographi" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="comedian" lemma="comedian" stem="comedian" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="24" string="Lenny" lemma="Lenny" stem="lenni" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="25" string="Bruce" lemma="Bruce" stem="bruce" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="26" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="27" string="Elvis" lemma="Elvis" stem="elvi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="28" string="Presley" lemma="Presley" stem="preslei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="29" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Goldman)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VP (VBZ defends) (NP (PRP$ his) (NNP Lennon) (NN book))) (CC and) (VP (VBD said) (SBAR (S (NP (PRP it)) (VP (VBZ is) (VP (VBN based) (PP (IN on) (NP (CD 1,200) (NNS interviews))))))))))) (, ,)) (ADVP (RB also)) (VP (VBZ has) (VP (VBN written) (NP (NP (JJ controversial) (NNS biographies)) (PP (IN of) (NP (NN comedian) (NNP Lenny) (NNP Bruce) (CC and) (NNP Elvis) (NNP Presley)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="who defends his Lennon book and said it is based on 1,200 interviews" type="SBAR">
          <tokens>
            <token id="3" string="who" />
            <token id="4" string="defends" />
            <token id="5" string="his" />
            <token id="6" string="Lennon" />
            <token id="7" string="book" />
            <token id="8" string="and" />
            <token id="9" string="said" />
            <token id="10" string="it" />
            <token id="11" string="is" />
            <token id="12" string="based" />
            <token id="13" string="on" />
            <token id="14" string="1,200" />
            <token id="15" string="interviews" />
          </tokens>
        </chunking>
        <chunking id="2" string="based on 1,200 interviews" type="VP">
          <tokens>
            <token id="12" string="based" />
            <token id="13" string="on" />
            <token id="14" string="1,200" />
            <token id="15" string="interviews" />
          </tokens>
        </chunking>
        <chunking id="3" string="defends his Lennon book and said it is based on 1,200 interviews" type="VP">
          <tokens>
            <token id="4" string="defends" />
            <token id="5" string="his" />
            <token id="6" string="Lennon" />
            <token id="7" string="book" />
            <token id="8" string="and" />
            <token id="9" string="said" />
            <token id="10" string="it" />
            <token id="11" string="is" />
            <token id="12" string="based" />
            <token id="13" string="on" />
            <token id="14" string="1,200" />
            <token id="15" string="interviews" />
          </tokens>
        </chunking>
        <chunking id="4" string="is based on 1,200 interviews" type="VP">
          <tokens>
            <token id="11" string="is" />
            <token id="12" string="based" />
            <token id="13" string="on" />
            <token id="14" string="1,200" />
            <token id="15" string="interviews" />
          </tokens>
        </chunking>
        <chunking id="5" string="written controversial biographies of comedian Lenny Bruce and Elvis Presley" type="VP">
          <tokens>
            <token id="19" string="written" />
            <token id="20" string="controversial" />
            <token id="21" string="biographies" />
            <token id="22" string="of" />
            <token id="23" string="comedian" />
            <token id="24" string="Lenny" />
            <token id="25" string="Bruce" />
            <token id="26" string="and" />
            <token id="27" string="Elvis" />
            <token id="28" string="Presley" />
          </tokens>
        </chunking>
        <chunking id="6" string="defends his Lennon book" type="VP">
          <tokens>
            <token id="4" string="defends" />
            <token id="5" string="his" />
            <token id="6" string="Lennon" />
            <token id="7" string="book" />
          </tokens>
        </chunking>
        <chunking id="7" string="it" type="NP">
          <tokens>
            <token id="10" string="it" />
          </tokens>
        </chunking>
        <chunking id="8" string="controversial biographies of comedian Lenny Bruce and Elvis Presley" type="NP">
          <tokens>
            <token id="20" string="controversial" />
            <token id="21" string="biographies" />
            <token id="22" string="of" />
            <token id="23" string="comedian" />
            <token id="24" string="Lenny" />
            <token id="25" string="Bruce" />
            <token id="26" string="and" />
            <token id="27" string="Elvis" />
            <token id="28" string="Presley" />
          </tokens>
        </chunking>
        <chunking id="9" string="controversial biographies" type="NP">
          <tokens>
            <token id="20" string="controversial" />
            <token id="21" string="biographies" />
          </tokens>
        </chunking>
        <chunking id="10" string="his Lennon book" type="NP">
          <tokens>
            <token id="5" string="his" />
            <token id="6" string="Lennon" />
            <token id="7" string="book" />
          </tokens>
        </chunking>
        <chunking id="11" string="comedian Lenny Bruce and Elvis Presley" type="NP">
          <tokens>
            <token id="23" string="comedian" />
            <token id="24" string="Lenny" />
            <token id="25" string="Bruce" />
            <token id="26" string="and" />
            <token id="27" string="Elvis" />
            <token id="28" string="Presley" />
          </tokens>
        </chunking>
        <chunking id="12" string="Goldman , who defends his Lennon book and said it is based on 1,200 interviews ," type="NP">
          <tokens>
            <token id="1" string="Goldman" />
            <token id="2" string="," />
            <token id="3" string="who" />
            <token id="4" string="defends" />
            <token id="5" string="his" />
            <token id="6" string="Lennon" />
            <token id="7" string="book" />
            <token id="8" string="and" />
            <token id="9" string="said" />
            <token id="10" string="it" />
            <token id="11" string="is" />
            <token id="12" string="based" />
            <token id="13" string="on" />
            <token id="14" string="1,200" />
            <token id="15" string="interviews" />
            <token id="16" string="," />
          </tokens>
        </chunking>
        <chunking id="13" string="has written controversial biographies of comedian Lenny Bruce and Elvis Presley" type="VP">
          <tokens>
            <token id="18" string="has" />
            <token id="19" string="written" />
            <token id="20" string="controversial" />
            <token id="21" string="biographies" />
            <token id="22" string="of" />
            <token id="23" string="comedian" />
            <token id="24" string="Lenny" />
            <token id="25" string="Bruce" />
            <token id="26" string="and" />
            <token id="27" string="Elvis" />
            <token id="28" string="Presley" />
          </tokens>
        </chunking>
        <chunking id="14" string="said it is based on 1,200 interviews" type="VP">
          <tokens>
            <token id="9" string="said" />
            <token id="10" string="it" />
            <token id="11" string="is" />
            <token id="12" string="based" />
            <token id="13" string="on" />
            <token id="14" string="1,200" />
            <token id="15" string="interviews" />
          </tokens>
        </chunking>
        <chunking id="15" string="Goldman" type="NP">
          <tokens>
            <token id="1" string="Goldman" />
          </tokens>
        </chunking>
        <chunking id="16" string="it is based on 1,200 interviews" type="SBAR">
          <tokens>
            <token id="10" string="it" />
            <token id="11" string="is" />
            <token id="12" string="based" />
            <token id="13" string="on" />
            <token id="14" string="1,200" />
            <token id="15" string="interviews" />
          </tokens>
        </chunking>
        <chunking id="17" string="1,200 interviews" type="NP">
          <tokens>
            <token id="14" string="1,200" />
            <token id="15" string="interviews" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="19">written</governor>
          <dependent id="1">Goldman</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">defends</governor>
          <dependent id="3">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="1">Goldman</governor>
          <dependent id="4">defends</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="7">book</governor>
          <dependent id="5">his</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">book</governor>
          <dependent id="6">Lennon</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">defends</governor>
          <dependent id="7">book</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">defends</governor>
          <dependent id="8">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">defends</governor>
          <dependent id="9">said</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="12">based</governor>
          <dependent id="10">it</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="12">based</governor>
          <dependent id="11">is</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="9">said</governor>
          <dependent id="12">based</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">interviews</governor>
          <dependent id="13">on</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="15">interviews</governor>
          <dependent id="14">1,200</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">based</governor>
          <dependent id="15">interviews</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="19">written</governor>
          <dependent id="17">also</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="19">written</governor>
          <dependent id="18">has</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="19">written</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="21">biographies</governor>
          <dependent id="20">controversial</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="19">written</governor>
          <dependent id="21">biographies</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">Bruce</governor>
          <dependent id="22">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">Bruce</governor>
          <dependent id="23">comedian</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">Bruce</governor>
          <dependent id="24">Lenny</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="21">biographies</governor>
          <dependent id="25">Bruce</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="25">Bruce</governor>
          <dependent id="26">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="28">Presley</governor>
          <dependent id="27">Elvis</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="25">Bruce</governor>
          <dependent id="28">Presley</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="1,200" type="NUMBER" score="0.0">
          <tokens>
            <token id="14" string="1,200" />
          </tokens>
        </entity>
        <entity id="2" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Lennon" />
          </tokens>
        </entity>
        <entity id="3" string="Elvis Presley" type="PERSON" score="0.0">
          <tokens>
            <token id="27" string="Elvis" />
            <token id="28" string="Presley" />
          </tokens>
        </entity>
        <entity id="4" string="Lenny Bruce" type="PERSON" score="0.0">
          <tokens>
            <token id="24" string="Lenny" />
            <token id="25" string="Bruce" />
          </tokens>
        </entity>
        <entity id="5" string="Goldman" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Goldman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="7" has_coreference="true">
      <content>Lennon was murdered outside his Upper West Side home on Dec. 8, 1980, by Mark David Chapman.</content>
      <tokens>
        <token id="1" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="murdered" lemma="murder" stem="murder" pos="VBN" type="Word" isStopWord="false" ner="CAUSE_OF_DEATH" is_referenced="false" is_refers="false" />
        <token id="4" string="outside" lemma="outside" stem="outsid" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="Upper" lemma="Upper" stem="upper" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="7" string="West" lemma="West" stem="west" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="8" string="Side" lemma="Side" stem="side" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="9" string="home" lemma="home" stem="home" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="Dec." lemma="Dec." stem="dec." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="12" string="8" lemma="8" stem="8" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="14" string="1980" lemma="1980" stem="1980" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="15" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="Mark" lemma="Mark" stem="mark" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="18" string="David" lemma="David" stem="david" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="19" string="Chapman" lemma="Chapman" stem="chapman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="20" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Lennon)) (VP (VBD was) (VP (VBN murdered) (PP (IN outside) (NP (PRP$ his) (NNP Upper) (NNP West) (NNP Side) (NN home))) (PP (IN on) (NP (NNP Dec.) (CD 8) (, ,) (CD 1980) (, ,))) (PP (IN by) (NP (NNP Mark) (NNP David) (NNP Chapman))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Dec. 8 , 1980 ," type="NP">
          <tokens>
            <token id="11" string="Dec." />
            <token id="12" string="8" />
            <token id="13" string="," />
            <token id="14" string="1980" />
            <token id="15" string="," />
          </tokens>
        </chunking>
        <chunking id="2" string="his Upper West Side home" type="NP">
          <tokens>
            <token id="5" string="his" />
            <token id="6" string="Upper" />
            <token id="7" string="West" />
            <token id="8" string="Side" />
            <token id="9" string="home" />
          </tokens>
        </chunking>
        <chunking id="3" string="Lennon" type="NP">
          <tokens>
            <token id="1" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="4" string="murdered outside his Upper West Side home on Dec. 8 , 1980 , by Mark David Chapman" type="VP">
          <tokens>
            <token id="3" string="murdered" />
            <token id="4" string="outside" />
            <token id="5" string="his" />
            <token id="6" string="Upper" />
            <token id="7" string="West" />
            <token id="8" string="Side" />
            <token id="9" string="home" />
            <token id="10" string="on" />
            <token id="11" string="Dec." />
            <token id="12" string="8" />
            <token id="13" string="," />
            <token id="14" string="1980" />
            <token id="15" string="," />
            <token id="16" string="by" />
            <token id="17" string="Mark" />
            <token id="18" string="David" />
            <token id="19" string="Chapman" />
          </tokens>
        </chunking>
        <chunking id="5" string="Mark David Chapman" type="NP">
          <tokens>
            <token id="17" string="Mark" />
            <token id="18" string="David" />
            <token id="19" string="Chapman" />
          </tokens>
        </chunking>
        <chunking id="6" string="was murdered outside his Upper West Side home on Dec. 8 , 1980 , by Mark David Chapman" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="murdered" />
            <token id="4" string="outside" />
            <token id="5" string="his" />
            <token id="6" string="Upper" />
            <token id="7" string="West" />
            <token id="8" string="Side" />
            <token id="9" string="home" />
            <token id="10" string="on" />
            <token id="11" string="Dec." />
            <token id="12" string="8" />
            <token id="13" string="," />
            <token id="14" string="1980" />
            <token id="15" string="," />
            <token id="16" string="by" />
            <token id="17" string="Mark" />
            <token id="18" string="David" />
            <token id="19" string="Chapman" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="3">murdered</governor>
          <dependent id="1">Lennon</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="3">murdered</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">murdered</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">home</governor>
          <dependent id="4">outside</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="9">home</governor>
          <dependent id="5">his</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">home</governor>
          <dependent id="6">Upper</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">home</governor>
          <dependent id="7">West</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">home</governor>
          <dependent id="8">Side</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">murdered</governor>
          <dependent id="9">home</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">Dec.</governor>
          <dependent id="10">on</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">murdered</governor>
          <dependent id="11">Dec.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="11">Dec.</governor>
          <dependent id="12">8</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="11">Dec.</governor>
          <dependent id="14">1980</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">Chapman</governor>
          <dependent id="16">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">Chapman</governor>
          <dependent id="17">Mark</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">Chapman</governor>
          <dependent id="18">David</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">murdered</governor>
          <dependent id="19">Chapman</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Dec. 8 , 1980" type="DATE" score="0.0">
          <tokens>
            <token id="11" string="Dec." />
            <token id="12" string="8" />
            <token id="13" string="," />
            <token id="14" string="1980" />
          </tokens>
        </entity>
        <entity id="2" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Lennon" />
          </tokens>
        </entity>
        <entity id="3" string="murdered" type="CAUSE_OF_DEATH" score="0.0">
          <tokens>
            <token id="3" string="murdered" />
          </tokens>
        </entity>
        <entity id="4" string="Mark David Chapman" type="PERSON" score="0.0">
          <tokens>
            <token id="17" string="Mark" />
            <token id="18" string="David" />
            <token id="19" string="Chapman" />
          </tokens>
        </entity>
        <entity id="5" string="Upper West Side" type="LOCATION" score="0.0">
          <tokens>
            <token id="6" string="Upper" />
            <token id="7" string="West" />
            <token id="8" string="Side" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="8" has_coreference="true">
      <content>``All I know and all the people that love John know that we had our years (and) that nobody, no film, no book will ever complete what went on in our lives.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="All" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="know" lemma="know" stem="know" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="all" lemma="all" stem="all" pos="PDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="people" lemma="people" stem="peopl" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="love" lemma="love" stem="love" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="12" string="know" lemma="know" stem="know" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="we" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="our" lemma="we" stem="our" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="17" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="true" is_refers="false" />
        <token id="18" string="(" lemma="-lrb-" stem="(" pos="-LRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string=")" lemma="-rrb-" stem=")" pos="-RRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="nobody" lemma="nobody" stem="nobodi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="no" lemma="no" stem="no" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="film" lemma="film" stem="film" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="no" lemma="no" stem="no" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="29" string="will" lemma="will" stem="will" pos="MD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="ever" lemma="ever" stem="ever" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="complete" lemma="complete" stem="complet" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="what" lemma="what" stem="what" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="went" lemma="go" stem="went" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="our" lemma="we" stem="our" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="37" string="lives" lemma="life" stem="live" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="38" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (SINV (`` ``) (S (S (NP (DT All) (PRP I)) (VP (VP (VBP know)) (CC and) (VP (NP (NP (PDT all) (DT the) (NNS people)) (SBAR (WHNP (WDT that)) (S (VP (VBP love) (S (NP (NNP John)) (VP (VB know) (SBAR (IN that) (S (NP (PRP we)) (VP (VBD had) (NP (PRP$ our) (NNS years)))))))))))))) (-LRB- -LRB-) (CC and) (-RRB- -RRB-) (S (PP (IN that) (NP (NP (NN nobody)) (, ,) (NP (DT no) (NN film)))) (, ,) (NP (DT no) (NN book)) (VP (MD will) (ADVP (RB ever)) (VP (VB complete) (NP (WP what)))))) (VP (VBD went) (PP (IN on) (PP (IN in)))) (NP (PRP$ our) (NNS lives)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="that love John know that we had our years" type="SBAR">
          <tokens>
            <token id="9" string="that" />
            <token id="10" string="love" />
            <token id="11" string="John" />
            <token id="12" string="know" />
            <token id="13" string="that" />
            <token id="14" string="we" />
            <token id="15" string="had" />
            <token id="16" string="our" />
            <token id="17" string="years" />
          </tokens>
        </chunking>
        <chunking id="2" string="no film" type="NP">
          <tokens>
            <token id="24" string="no" />
            <token id="25" string="film" />
          </tokens>
        </chunking>
        <chunking id="3" string="our lives" type="NP">
          <tokens>
            <token id="36" string="our" />
            <token id="37" string="lives" />
          </tokens>
        </chunking>
        <chunking id="4" string="John" type="NP">
          <tokens>
            <token id="11" string="John" />
          </tokens>
        </chunking>
        <chunking id="5" string="know that we had our years" type="VP">
          <tokens>
            <token id="12" string="know" />
            <token id="13" string="that" />
            <token id="14" string="we" />
            <token id="15" string="had" />
            <token id="16" string="our" />
            <token id="17" string="years" />
          </tokens>
        </chunking>
        <chunking id="6" string="we" type="NP">
          <tokens>
            <token id="14" string="we" />
          </tokens>
        </chunking>
        <chunking id="7" string="our years" type="NP">
          <tokens>
            <token id="16" string="our" />
            <token id="17" string="years" />
          </tokens>
        </chunking>
        <chunking id="8" string="All I" type="NP">
          <tokens>
            <token id="2" string="All" />
            <token id="3" string="I" />
          </tokens>
        </chunking>
        <chunking id="9" string="all the people that love John know that we had our years" type="VP">
          <tokens>
            <token id="6" string="all" />
            <token id="7" string="the" />
            <token id="8" string="people" />
            <token id="9" string="that" />
            <token id="10" string="love" />
            <token id="11" string="John" />
            <token id="12" string="know" />
            <token id="13" string="that" />
            <token id="14" string="we" />
            <token id="15" string="had" />
            <token id="16" string="our" />
            <token id="17" string="years" />
          </tokens>
        </chunking>
        <chunking id="10" string="no book" type="NP">
          <tokens>
            <token id="27" string="no" />
            <token id="28" string="book" />
          </tokens>
        </chunking>
        <chunking id="11" string="all the people" type="NP">
          <tokens>
            <token id="6" string="all" />
            <token id="7" string="the" />
            <token id="8" string="people" />
          </tokens>
        </chunking>
        <chunking id="12" string="had our years" type="VP">
          <tokens>
            <token id="15" string="had" />
            <token id="16" string="our" />
            <token id="17" string="years" />
          </tokens>
        </chunking>
        <chunking id="13" string="will ever complete what" type="VP">
          <tokens>
            <token id="29" string="will" />
            <token id="30" string="ever" />
            <token id="31" string="complete" />
            <token id="32" string="what" />
          </tokens>
        </chunking>
        <chunking id="14" string="know and all the people that love John know that we had our years" type="VP">
          <tokens>
            <token id="4" string="know" />
            <token id="5" string="and" />
            <token id="6" string="all" />
            <token id="7" string="the" />
            <token id="8" string="people" />
            <token id="9" string="that" />
            <token id="10" string="love" />
            <token id="11" string="John" />
            <token id="12" string="know" />
            <token id="13" string="that" />
            <token id="14" string="we" />
            <token id="15" string="had" />
            <token id="16" string="our" />
            <token id="17" string="years" />
          </tokens>
        </chunking>
        <chunking id="15" string="nobody , no film" type="NP">
          <tokens>
            <token id="22" string="nobody" />
            <token id="23" string="," />
            <token id="24" string="no" />
            <token id="25" string="film" />
          </tokens>
        </chunking>
        <chunking id="16" string="what" type="NP">
          <tokens>
            <token id="32" string="what" />
          </tokens>
        </chunking>
        <chunking id="17" string="love John know that we had our years" type="VP">
          <tokens>
            <token id="10" string="love" />
            <token id="11" string="John" />
            <token id="12" string="know" />
            <token id="13" string="that" />
            <token id="14" string="we" />
            <token id="15" string="had" />
            <token id="16" string="our" />
            <token id="17" string="years" />
          </tokens>
        </chunking>
        <chunking id="18" string="know" type="VP">
          <tokens>
            <token id="4" string="know" />
          </tokens>
        </chunking>
        <chunking id="19" string="went on in" type="VP">
          <tokens>
            <token id="33" string="went" />
            <token id="34" string="on" />
            <token id="35" string="in" />
          </tokens>
        </chunking>
        <chunking id="20" string="that we had our years" type="SBAR">
          <tokens>
            <token id="13" string="that" />
            <token id="14" string="we" />
            <token id="15" string="had" />
            <token id="16" string="our" />
            <token id="17" string="years" />
          </tokens>
        </chunking>
        <chunking id="21" string="complete what" type="VP">
          <tokens>
            <token id="31" string="complete" />
            <token id="32" string="what" />
          </tokens>
        </chunking>
        <chunking id="22" string="nobody" type="NP">
          <tokens>
            <token id="22" string="nobody" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">I</governor>
          <dependent id="2">All</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">know</governor>
          <dependent id="3">I</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="33">went</governor>
          <dependent id="4">know</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">know</governor>
          <dependent id="5">and</dependent>
        </dependency>
        <dependency type="det:predet">
          <governor id="8">people</governor>
          <dependent id="6">all</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">people</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">know</governor>
          <dependent id="8">people</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">love</governor>
          <dependent id="9">that</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="8">people</governor>
          <dependent id="10">love</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">know</governor>
          <dependent id="11">John</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="10">love</governor>
          <dependent id="12">know</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="15">had</governor>
          <dependent id="13">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">had</governor>
          <dependent id="14">we</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="12">know</governor>
          <dependent id="15">had</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="17">years</governor>
          <dependent id="16">our</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="15">had</governor>
          <dependent id="17">years</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">know</governor>
          <dependent id="19">and</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">nobody</governor>
          <dependent id="21">that</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="31">complete</governor>
          <dependent id="22">nobody</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="25">film</governor>
          <dependent id="24">no</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="22">nobody</governor>
          <dependent id="25">film</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="28">book</governor>
          <dependent id="27">no</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="31">complete</governor>
          <dependent id="28">book</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="31">complete</governor>
          <dependent id="29">will</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="31">complete</governor>
          <dependent id="30">ever</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">know</governor>
          <dependent id="31">complete</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="31">complete</governor>
          <dependent id="32">what</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="33">went</dependent>
        </dependency>
        <dependency type="case">
          <governor id="35">in</governor>
          <dependent id="34">on</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="33">went</governor>
          <dependent id="35">in</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="37">lives</governor>
          <dependent id="36">our</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="33">went</governor>
          <dependent id="37">lives</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="11" string="John" />
          </tokens>
        </entity>
        <entity id="2" string="years" type="DURATION" score="0.0">
          <tokens>
            <token id="17" string="years" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="9" has_coreference="true">
      <content>That&amp;apost;s our part of our hearts that has to stay with us, must not be public,&amp;apost;&amp;apost; Mrs. Lennon said.</content>
      <tokens>
        <token id="1" string="That" lemma="that" stem="that" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="our" lemma="we" stem="our" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="part" lemma="part" stem="part" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="our" lemma="we" stem="our" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="hearts" lemma="heart" stem="heart" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="stay" lemma="stay" stem="stai" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="12" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="us" lemma="we" stem="u" pos="PRP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="must" lemma="must" stem="must" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="16" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="be" lemma="be" stem="be" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="public" lemma="public" stem="public" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="Mrs." lemma="Mrs." stem="mrs." pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="22" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="23" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (DT That)) (VP (VBZ 's) (NP (NP (PRP$ our) (NN part)) (PP (IN of) (NP (PRP$ our) (NNS hearts))) (SBAR (WHNP (WDT that)) (S (VP (VBZ has) (VP (S (VP (TO to) (VP (VB stay) (PP (IN with) (NP (PRP us)))))) (, ,) (VP (MD must) (RB not) (VP (VB be) (ADJP (JJ public))))))))))) (, ,) ('' '') (NP (NNP Mrs.) (NNP Lennon)) (VP (VBD said)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="That" type="NP">
          <tokens>
            <token id="1" string="That" />
          </tokens>
        </chunking>
        <chunking id="2" string="our hearts" type="NP">
          <tokens>
            <token id="6" string="our" />
            <token id="7" string="hearts" />
          </tokens>
        </chunking>
        <chunking id="3" string="to stay with us , must not be public" type="VP">
          <tokens>
            <token id="10" string="to" />
            <token id="11" string="stay" />
            <token id="12" string="with" />
            <token id="13" string="us" />
            <token id="14" string="," />
            <token id="15" string="must" />
            <token id="16" string="not" />
            <token id="17" string="be" />
            <token id="18" string="public" />
          </tokens>
        </chunking>
        <chunking id="4" string="to stay with us" type="VP">
          <tokens>
            <token id="10" string="to" />
            <token id="11" string="stay" />
            <token id="12" string="with" />
            <token id="13" string="us" />
          </tokens>
        </chunking>
        <chunking id="5" string="be public" type="VP">
          <tokens>
            <token id="17" string="be" />
            <token id="18" string="public" />
          </tokens>
        </chunking>
        <chunking id="6" string="has to stay with us , must not be public" type="VP">
          <tokens>
            <token id="9" string="has" />
            <token id="10" string="to" />
            <token id="11" string="stay" />
            <token id="12" string="with" />
            <token id="13" string="us" />
            <token id="14" string="," />
            <token id="15" string="must" />
            <token id="16" string="not" />
            <token id="17" string="be" />
            <token id="18" string="public" />
          </tokens>
        </chunking>
        <chunking id="7" string="stay with us" type="VP">
          <tokens>
            <token id="11" string="stay" />
            <token id="12" string="with" />
            <token id="13" string="us" />
          </tokens>
        </chunking>
        <chunking id="8" string="public" type="ADJP">
          <tokens>
            <token id="18" string="public" />
          </tokens>
        </chunking>
        <chunking id="9" string="'s our part of our hearts that has to stay with us , must not be public" type="VP">
          <tokens>
            <token id="2" string="'s" />
            <token id="3" string="our" />
            <token id="4" string="part" />
            <token id="5" string="of" />
            <token id="6" string="our" />
            <token id="7" string="hearts" />
            <token id="8" string="that" />
            <token id="9" string="has" />
            <token id="10" string="to" />
            <token id="11" string="stay" />
            <token id="12" string="with" />
            <token id="13" string="us" />
            <token id="14" string="," />
            <token id="15" string="must" />
            <token id="16" string="not" />
            <token id="17" string="be" />
            <token id="18" string="public" />
          </tokens>
        </chunking>
        <chunking id="10" string="that has to stay with us , must not be public" type="SBAR">
          <tokens>
            <token id="8" string="that" />
            <token id="9" string="has" />
            <token id="10" string="to" />
            <token id="11" string="stay" />
            <token id="12" string="with" />
            <token id="13" string="us" />
            <token id="14" string="," />
            <token id="15" string="must" />
            <token id="16" string="not" />
            <token id="17" string="be" />
            <token id="18" string="public" />
          </tokens>
        </chunking>
        <chunking id="11" string="Mrs. Lennon" type="NP">
          <tokens>
            <token id="21" string="Mrs." />
            <token id="22" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="12" string="our part" type="NP">
          <tokens>
            <token id="3" string="our" />
            <token id="4" string="part" />
          </tokens>
        </chunking>
        <chunking id="13" string="us" type="NP">
          <tokens>
            <token id="13" string="us" />
          </tokens>
        </chunking>
        <chunking id="14" string="must not be public" type="VP">
          <tokens>
            <token id="15" string="must" />
            <token id="16" string="not" />
            <token id="17" string="be" />
            <token id="18" string="public" />
          </tokens>
        </chunking>
        <chunking id="15" string="said" type="VP">
          <tokens>
            <token id="23" string="said" />
          </tokens>
        </chunking>
        <chunking id="16" string="our part of our hearts that has to stay with us , must not be public" type="NP">
          <tokens>
            <token id="3" string="our" />
            <token id="4" string="part" />
            <token id="5" string="of" />
            <token id="6" string="our" />
            <token id="7" string="hearts" />
            <token id="8" string="that" />
            <token id="9" string="has" />
            <token id="10" string="to" />
            <token id="11" string="stay" />
            <token id="12" string="with" />
            <token id="13" string="us" />
            <token id="14" string="," />
            <token id="15" string="must" />
            <token id="16" string="not" />
            <token id="17" string="be" />
            <token id="18" string="public" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">part</governor>
          <dependent id="1">That</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="4">part</governor>
          <dependent id="2">'s</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="4">part</governor>
          <dependent id="3">our</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="23">said</governor>
          <dependent id="4">part</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">hearts</governor>
          <dependent id="5">of</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="7">hearts</governor>
          <dependent id="6">our</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">part</governor>
          <dependent id="7">hearts</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="18">public</governor>
          <dependent id="8">that</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="18">public</governor>
          <dependent id="9">has</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="11">stay</governor>
          <dependent id="10">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="18">public</governor>
          <dependent id="11">stay</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">us</governor>
          <dependent id="12">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">stay</governor>
          <dependent id="13">us</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="18">public</governor>
          <dependent id="15">must</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="18">public</governor>
          <dependent id="16">not</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="18">public</governor>
          <dependent id="17">be</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="4">part</governor>
          <dependent id="18">public</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="22">Lennon</governor>
          <dependent id="21">Mrs.</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="23">said</governor>
          <dependent id="22">Lennon</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="23">said</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="22" string="Lennon" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="10" has_coreference="true">
      <content>``I love John.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="love" lemma="love" stem="love" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="5" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (PRP I)) (VP (VBP love) (NP (NNP John))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="I" type="NP">
          <tokens>
            <token id="2" string="I" />
          </tokens>
        </chunking>
        <chunking id="2" string="love John" type="VP">
          <tokens>
            <token id="3" string="love" />
            <token id="4" string="John" />
          </tokens>
        </chunking>
        <chunking id="3" string="John" type="NP">
          <tokens>
            <token id="4" string="John" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">love</governor>
          <dependent id="2">I</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">love</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="3">love</governor>
          <dependent id="4">John</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="John" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="11" has_coreference="true">
      <content>I&amp;apost;ve always loved John because he was my first love.</content>
      <tokens>
        <token id="1" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="'ve" lemma="have" stem="'ve" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="always" lemma="always" stem="alwai" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="loved" lemma="love" stem="love" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="6" string="because" lemma="because" stem="becaus" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="my" lemma="my" stem="my" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="first" lemma="first" stem="first" pos="JJ" type="Word" isStopWord="false" ner="ORDINAL" is_referenced="false" is_refers="false" />
        <token id="11" string="love" lemma="love" stem="love" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP I)) (VP (VBP 've) (ADVP (RB always)) (VP (VBN loved) (NP (NNP John)) (SBAR (IN because) (S (NP (PRP he)) (VP (VBD was) (NP (PRP$ my) (JJ first) (NN love))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="because he was my first love" type="SBAR">
          <tokens>
            <token id="6" string="because" />
            <token id="7" string="he" />
            <token id="8" string="was" />
            <token id="9" string="my" />
            <token id="10" string="first" />
            <token id="11" string="love" />
          </tokens>
        </chunking>
        <chunking id="2" string="my first love" type="NP">
          <tokens>
            <token id="9" string="my" />
            <token id="10" string="first" />
            <token id="11" string="love" />
          </tokens>
        </chunking>
        <chunking id="3" string="loved John because he was my first love" type="VP">
          <tokens>
            <token id="4" string="loved" />
            <token id="5" string="John" />
            <token id="6" string="because" />
            <token id="7" string="he" />
            <token id="8" string="was" />
            <token id="9" string="my" />
            <token id="10" string="first" />
            <token id="11" string="love" />
          </tokens>
        </chunking>
        <chunking id="4" string="was my first love" type="VP">
          <tokens>
            <token id="8" string="was" />
            <token id="9" string="my" />
            <token id="10" string="first" />
            <token id="11" string="love" />
          </tokens>
        </chunking>
        <chunking id="5" string="I" type="NP">
          <tokens>
            <token id="1" string="I" />
          </tokens>
        </chunking>
        <chunking id="6" string="John" type="NP">
          <tokens>
            <token id="5" string="John" />
          </tokens>
        </chunking>
        <chunking id="7" string="he" type="NP">
          <tokens>
            <token id="7" string="he" />
          </tokens>
        </chunking>
        <chunking id="8" string="'ve always loved John because he was my first love" type="VP">
          <tokens>
            <token id="2" string="'ve" />
            <token id="3" string="always" />
            <token id="4" string="loved" />
            <token id="5" string="John" />
            <token id="6" string="because" />
            <token id="7" string="he" />
            <token id="8" string="was" />
            <token id="9" string="my" />
            <token id="10" string="first" />
            <token id="11" string="love" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">loved</governor>
          <dependent id="1">I</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="4">loved</governor>
          <dependent id="2">'ve</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">loved</governor>
          <dependent id="3">always</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">loved</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">loved</governor>
          <dependent id="5">John</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="11">love</governor>
          <dependent id="6">because</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">love</governor>
          <dependent id="7">he</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="11">love</governor>
          <dependent id="8">was</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="11">love</governor>
          <dependent id="9">my</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">love</governor>
          <dependent id="10">first</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="4">loved</governor>
          <dependent id="11">love</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="first" type="ORDINAL" score="0.0">
          <tokens>
            <token id="10" string="first" />
          </tokens>
        </entity>
        <entity id="2" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="John" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="12" has_coreference="true">
      <content>But, I mean, I&amp;apost;ve loved him in different ways.</content>
      <tokens>
        <token id="1" string="But" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="mean" lemma="mean" stem="mean" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="'ve" lemma="have" stem="'ve" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="loved" lemma="love" stem="love" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="him" lemma="he" stem="him" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="different" lemma="different" stem="differ" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="ways" lemma="way" stem="wai" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (CC But) (PRN (, ,) (S (NP (PRP I)) (VP (VBP mean))) (, ,)) (NP (PRP I)) (VP (VBP 've) (VP (VBD loved) (NP (PRP him)) (PP (IN in) (NP (JJ different) (NNS ways))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="'ve loved him in different ways" type="VP">
          <tokens>
            <token id="7" string="'ve" />
            <token id="8" string="loved" />
            <token id="9" string="him" />
            <token id="10" string="in" />
            <token id="11" string="different" />
            <token id="12" string="ways" />
          </tokens>
        </chunking>
        <chunking id="2" string="different ways" type="NP">
          <tokens>
            <token id="11" string="different" />
            <token id="12" string="ways" />
          </tokens>
        </chunking>
        <chunking id="3" string="mean" type="VP">
          <tokens>
            <token id="4" string="mean" />
          </tokens>
        </chunking>
        <chunking id="4" string="loved him in different ways" type="VP">
          <tokens>
            <token id="8" string="loved" />
            <token id="9" string="him" />
            <token id="10" string="in" />
            <token id="11" string="different" />
            <token id="12" string="ways" />
          </tokens>
        </chunking>
        <chunking id="5" string="I" type="NP">
          <tokens>
            <token id="3" string="I" />
          </tokens>
        </chunking>
        <chunking id="6" string="him" type="NP">
          <tokens>
            <token id="9" string="him" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="cc">
          <governor id="8">loved</governor>
          <dependent id="1">But</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">mean</governor>
          <dependent id="3">I</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="8">loved</governor>
          <dependent id="4">mean</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">loved</governor>
          <dependent id="6">I</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="8">loved</governor>
          <dependent id="7">'ve</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="8">loved</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="8">loved</governor>
          <dependent id="9">him</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">ways</governor>
          <dependent id="10">in</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">ways</governor>
          <dependent id="11">different</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">loved</governor>
          <dependent id="12">ways</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="13" has_coreference="true">
      <content>I love his memory.</content>
      <tokens>
        <token id="1" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="love" lemma="love" stem="love" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="memory" lemma="memory" stem="memori" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP I)) (VP (VBP love) (NP (PRP$ his) (NN memory))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="his memory" type="NP">
          <tokens>
            <token id="3" string="his" />
            <token id="4" string="memory" />
          </tokens>
        </chunking>
        <chunking id="2" string="I" type="NP">
          <tokens>
            <token id="1" string="I" />
          </tokens>
        </chunking>
        <chunking id="3" string="love his memory" type="VP">
          <tokens>
            <token id="2" string="love" />
            <token id="3" string="his" />
            <token id="4" string="memory" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">love</governor>
          <dependent id="1">I</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">love</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="4">memory</governor>
          <dependent id="3">his</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">love</governor>
          <dependent id="4">memory</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="14" has_coreference="true">
      <content>I loved what he did.</content>
      <tokens>
        <token id="1" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="loved" lemma="love" stem="love" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="what" lemma="what" stem="what" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="did" lemma="do" stem="did" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP I)) (VP (VBD loved) (SBAR (WHNP (WP what)) (S (NP (PRP he)) (VP (VBD did))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="loved what he did" type="VP">
          <tokens>
            <token id="2" string="loved" />
            <token id="3" string="what" />
            <token id="4" string="he" />
            <token id="5" string="did" />
          </tokens>
        </chunking>
        <chunking id="2" string="I" type="NP">
          <tokens>
            <token id="1" string="I" />
          </tokens>
        </chunking>
        <chunking id="3" string="he" type="NP">
          <tokens>
            <token id="4" string="he" />
          </tokens>
        </chunking>
        <chunking id="4" string="what he did" type="SBAR">
          <tokens>
            <token id="3" string="what" />
            <token id="4" string="he" />
            <token id="5" string="did" />
          </tokens>
        </chunking>
        <chunking id="5" string="did" type="VP">
          <tokens>
            <token id="5" string="did" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">loved</governor>
          <dependent id="1">I</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">loved</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">did</governor>
          <dependent id="3">what</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">did</governor>
          <dependent id="4">he</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="2">loved</governor>
          <dependent id="5">did</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="15" has_coreference="true">
      <content>I understood his madness, his phases.</content>
      <tokens>
        <token id="1" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="understood" lemma="understand" stem="understood" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="madness" lemma="madness" stem="mad" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="phases" lemma="phase" stem="phase" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP I)) (VP (VBD understood) (NP (NP (PRP$ his) (NN madness)) (, ,) (NP (PRP$ his) (NNS phases)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="his madness" type="NP">
          <tokens>
            <token id="3" string="his" />
            <token id="4" string="madness" />
          </tokens>
        </chunking>
        <chunking id="2" string="his madness , his phases" type="NP">
          <tokens>
            <token id="3" string="his" />
            <token id="4" string="madness" />
            <token id="5" string="," />
            <token id="6" string="his" />
            <token id="7" string="phases" />
          </tokens>
        </chunking>
        <chunking id="3" string="I" type="NP">
          <tokens>
            <token id="1" string="I" />
          </tokens>
        </chunking>
        <chunking id="4" string="his phases" type="NP">
          <tokens>
            <token id="6" string="his" />
            <token id="7" string="phases" />
          </tokens>
        </chunking>
        <chunking id="5" string="understood his madness , his phases" type="VP">
          <tokens>
            <token id="2" string="understood" />
            <token id="3" string="his" />
            <token id="4" string="madness" />
            <token id="5" string="," />
            <token id="6" string="his" />
            <token id="7" string="phases" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">understood</governor>
          <dependent id="1">I</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">understood</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="4">madness</governor>
          <dependent id="3">his</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">understood</governor>
          <dependent id="4">madness</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="7">phases</governor>
          <dependent id="6">his</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="4">madness</governor>
          <dependent id="7">phases</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="16" has_coreference="true">
      <content>I just feel that I understood.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="just" lemma="just" stem="just" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="feel" lemma="feel" stem="feel" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="understood" lemma="understand" stem="understood" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP I)) (ADVP (RB just)) (VP (VBP feel) (SBAR (IN that) (S (NP (PRP I)) (VP (VBD understood))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="I" type="NP">
          <tokens>
            <token id="1" string="I" />
          </tokens>
        </chunking>
        <chunking id="2" string="that I understood" type="SBAR">
          <tokens>
            <token id="4" string="that" />
            <token id="5" string="I" />
            <token id="6" string="understood" />
          </tokens>
        </chunking>
        <chunking id="3" string="feel that I understood" type="VP">
          <tokens>
            <token id="3" string="feel" />
            <token id="4" string="that" />
            <token id="5" string="I" />
            <token id="6" string="understood" />
          </tokens>
        </chunking>
        <chunking id="4" string="understood" type="VP">
          <tokens>
            <token id="6" string="understood" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">feel</governor>
          <dependent id="1">I</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="3">feel</governor>
          <dependent id="2">just</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">feel</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="6">understood</governor>
          <dependent id="4">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">understood</governor>
          <dependent id="5">I</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">feel</governor>
          <dependent id="6">understood</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="17" has_coreference="true">
      <content>The couple divorced a few months before Lennon married Yoko Ono in 1969.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="couple" lemma="couple" stem="coupl" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="divorced" lemma="divorce" stem="divorc" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="5" string="few" lemma="few" stem="few" pos="JJ" type="Word" isStopWord="true" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="6" string="months" lemma="month" stem="month" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="7" string="before" lemma="before" stem="befor" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="9" string="married" lemma="married" stem="marri" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="Yoko" lemma="Yoko" stem="yoko" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="11" string="Ono" lemma="Ono" stem="ono" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="12" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="1969" lemma="1969" stem="1969" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN couple)) (VP (VBD divorced) (NP (DT a) (JJ few) (NNS months)) (SBAR (IN before) (S (NP (NNP Lennon)) (VP (JJ married) (NP (NP (NNP Yoko) (NNP Ono)) (PP (IN in) (NP (CD 1969)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="before Lennon married Yoko Ono in 1969" type="SBAR">
          <tokens>
            <token id="7" string="before" />
            <token id="8" string="Lennon" />
            <token id="9" string="married" />
            <token id="10" string="Yoko" />
            <token id="11" string="Ono" />
            <token id="12" string="in" />
            <token id="13" string="1969" />
          </tokens>
        </chunking>
        <chunking id="2" string="Lennon" type="NP">
          <tokens>
            <token id="8" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="3" string="a few months" type="NP">
          <tokens>
            <token id="4" string="a" />
            <token id="5" string="few" />
            <token id="6" string="months" />
          </tokens>
        </chunking>
        <chunking id="4" string="Yoko Ono" type="NP">
          <tokens>
            <token id="10" string="Yoko" />
            <token id="11" string="Ono" />
          </tokens>
        </chunking>
        <chunking id="5" string="The couple" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="couple" />
          </tokens>
        </chunking>
        <chunking id="6" string="married Yoko Ono in 1969" type="VP">
          <tokens>
            <token id="9" string="married" />
            <token id="10" string="Yoko" />
            <token id="11" string="Ono" />
            <token id="12" string="in" />
            <token id="13" string="1969" />
          </tokens>
        </chunking>
        <chunking id="7" string="1969" type="NP">
          <tokens>
            <token id="13" string="1969" />
          </tokens>
        </chunking>
        <chunking id="8" string="Yoko Ono in 1969" type="NP">
          <tokens>
            <token id="10" string="Yoko" />
            <token id="11" string="Ono" />
            <token id="12" string="in" />
            <token id="13" string="1969" />
          </tokens>
        </chunking>
        <chunking id="9" string="divorced a few months before Lennon married Yoko Ono in 1969" type="VP">
          <tokens>
            <token id="3" string="divorced" />
            <token id="4" string="a" />
            <token id="5" string="few" />
            <token id="6" string="months" />
            <token id="7" string="before" />
            <token id="8" string="Lennon" />
            <token id="9" string="married" />
            <token id="10" string="Yoko" />
            <token id="11" string="Ono" />
            <token id="12" string="in" />
            <token id="13" string="1969" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">couple</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">divorced</governor>
          <dependent id="2">couple</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">divorced</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">months</governor>
          <dependent id="4">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">months</governor>
          <dependent id="5">few</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="3">divorced</governor>
          <dependent id="6">months</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="9">married</governor>
          <dependent id="7">before</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">married</governor>
          <dependent id="8">Lennon</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="3">divorced</governor>
          <dependent id="9">married</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">Ono</governor>
          <dependent id="10">Yoko</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="9">married</governor>
          <dependent id="11">Ono</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">1969</governor>
          <dependent id="12">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">Ono</governor>
          <dependent id="13">1969</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="a few months" type="DURATION" score="0.0">
          <tokens>
            <token id="4" string="a" />
            <token id="5" string="few" />
            <token id="6" string="months" />
          </tokens>
        </entity>
        <entity id="3" string="Yoko Ono" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="Yoko" />
            <token id="11" string="Ono" />
          </tokens>
        </entity>
        <entity id="4" string="1969" type="DATE" score="0.0">
          <tokens>
            <token id="13" string="1969" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="18" has_coreference="true">
      <content>Ms. Ono already has trashed the book, calling it ``totally fiction&amp;apost;&amp;apost; in a nationwide broadcast that coincided with the release of the book.</content>
      <tokens>
        <token id="1" string="Ms." lemma="Ms." stem="ms." pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="2" string="Ono" lemma="Ono" stem="ono" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="already" lemma="already" stem="alreadi" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="trashed" lemma="trash" stem="trash" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="calling" lemma="call" stem="call" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="totally" lemma="totally" stem="total" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="fiction" lemma="fiction" stem="fiction" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="nationwide" lemma="nationwide" stem="nationwid" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="broadcast" lemma="broadcast" stem="broadcast" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="coincided" lemma="coincide" stem="coincid" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="release" lemma="release" stem="releas" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="26" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="27" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Ms.) (NNP Ono)) (ADVP (RB already)) (VP (VBZ has) (VP (VBN trashed) (NP (DT the) (NN book)) (, ,) (S (VP (VBG calling) (NP (PRP it)) (NP (`` ``) (NP (RB totally) (NN fiction)) ('' '') (PP (IN in) (NP (NP (DT a) (JJ nationwide) (NN broadcast)) (SBAR (WHNP (WDT that)) (S (VP (VBD coincided) (PP (IN with) (NP (NP (DT the) (NN release)) (PP (IN of) (NP (DT the) (NN book))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="that coincided with the release of the book" type="SBAR">
          <tokens>
            <token id="19" string="that" />
            <token id="20" string="coincided" />
            <token id="21" string="with" />
            <token id="22" string="the" />
            <token id="23" string="release" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="book" />
          </tokens>
        </chunking>
        <chunking id="2" string="Ms. Ono" type="NP">
          <tokens>
            <token id="1" string="Ms." />
            <token id="2" string="Ono" />
          </tokens>
        </chunking>
        <chunking id="3" string="calling it `` totally fiction '' in a nationwide broadcast that coincided with the release of the book" type="VP">
          <tokens>
            <token id="9" string="calling" />
            <token id="10" string="it" />
            <token id="11" string="``" />
            <token id="12" string="totally" />
            <token id="13" string="fiction" />
            <token id="14" string="''" />
            <token id="15" string="in" />
            <token id="16" string="a" />
            <token id="17" string="nationwide" />
            <token id="18" string="broadcast" />
            <token id="19" string="that" />
            <token id="20" string="coincided" />
            <token id="21" string="with" />
            <token id="22" string="the" />
            <token id="23" string="release" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="book" />
          </tokens>
        </chunking>
        <chunking id="4" string="totally fiction" type="NP">
          <tokens>
            <token id="12" string="totally" />
            <token id="13" string="fiction" />
          </tokens>
        </chunking>
        <chunking id="5" string="it" type="NP">
          <tokens>
            <token id="10" string="it" />
          </tokens>
        </chunking>
        <chunking id="6" string="the release" type="NP">
          <tokens>
            <token id="22" string="the" />
            <token id="23" string="release" />
          </tokens>
        </chunking>
        <chunking id="7" string="the book" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="book" />
          </tokens>
        </chunking>
        <chunking id="8" string="`` totally fiction '' in a nationwide broadcast that coincided with the release of the book" type="NP">
          <tokens>
            <token id="11" string="``" />
            <token id="12" string="totally" />
            <token id="13" string="fiction" />
            <token id="14" string="''" />
            <token id="15" string="in" />
            <token id="16" string="a" />
            <token id="17" string="nationwide" />
            <token id="18" string="broadcast" />
            <token id="19" string="that" />
            <token id="20" string="coincided" />
            <token id="21" string="with" />
            <token id="22" string="the" />
            <token id="23" string="release" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="book" />
          </tokens>
        </chunking>
        <chunking id="9" string="a nationwide broadcast" type="NP">
          <tokens>
            <token id="16" string="a" />
            <token id="17" string="nationwide" />
            <token id="18" string="broadcast" />
          </tokens>
        </chunking>
        <chunking id="10" string="has trashed the book , calling it `` totally fiction '' in a nationwide broadcast that coincided with the release of the book" type="VP">
          <tokens>
            <token id="4" string="has" />
            <token id="5" string="trashed" />
            <token id="6" string="the" />
            <token id="7" string="book" />
            <token id="8" string="," />
            <token id="9" string="calling" />
            <token id="10" string="it" />
            <token id="11" string="``" />
            <token id="12" string="totally" />
            <token id="13" string="fiction" />
            <token id="14" string="''" />
            <token id="15" string="in" />
            <token id="16" string="a" />
            <token id="17" string="nationwide" />
            <token id="18" string="broadcast" />
            <token id="19" string="that" />
            <token id="20" string="coincided" />
            <token id="21" string="with" />
            <token id="22" string="the" />
            <token id="23" string="release" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="book" />
          </tokens>
        </chunking>
        <chunking id="11" string="coincided with the release of the book" type="VP">
          <tokens>
            <token id="20" string="coincided" />
            <token id="21" string="with" />
            <token id="22" string="the" />
            <token id="23" string="release" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="book" />
          </tokens>
        </chunking>
        <chunking id="12" string="trashed the book , calling it `` totally fiction '' in a nationwide broadcast that coincided with the release of the book" type="VP">
          <tokens>
            <token id="5" string="trashed" />
            <token id="6" string="the" />
            <token id="7" string="book" />
            <token id="8" string="," />
            <token id="9" string="calling" />
            <token id="10" string="it" />
            <token id="11" string="``" />
            <token id="12" string="totally" />
            <token id="13" string="fiction" />
            <token id="14" string="''" />
            <token id="15" string="in" />
            <token id="16" string="a" />
            <token id="17" string="nationwide" />
            <token id="18" string="broadcast" />
            <token id="19" string="that" />
            <token id="20" string="coincided" />
            <token id="21" string="with" />
            <token id="22" string="the" />
            <token id="23" string="release" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="book" />
          </tokens>
        </chunking>
        <chunking id="13" string="the release of the book" type="NP">
          <tokens>
            <token id="22" string="the" />
            <token id="23" string="release" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="book" />
          </tokens>
        </chunking>
        <chunking id="14" string="a nationwide broadcast that coincided with the release of the book" type="NP">
          <tokens>
            <token id="16" string="a" />
            <token id="17" string="nationwide" />
            <token id="18" string="broadcast" />
            <token id="19" string="that" />
            <token id="20" string="coincided" />
            <token id="21" string="with" />
            <token id="22" string="the" />
            <token id="23" string="release" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="book" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">Ono</governor>
          <dependent id="1">Ms.</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">trashed</governor>
          <dependent id="2">Ono</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">trashed</governor>
          <dependent id="3">already</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="5">trashed</governor>
          <dependent id="4">has</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">trashed</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">book</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">trashed</governor>
          <dependent id="7">book</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="5">trashed</governor>
          <dependent id="9">calling</dependent>
        </dependency>
        <dependency type="iobj">
          <governor id="9">calling</governor>
          <dependent id="10">it</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="13">fiction</governor>
          <dependent id="12">totally</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="9">calling</governor>
          <dependent id="13">fiction</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">broadcast</governor>
          <dependent id="15">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">broadcast</governor>
          <dependent id="16">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="18">broadcast</governor>
          <dependent id="17">nationwide</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">fiction</governor>
          <dependent id="18">broadcast</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="20">coincided</governor>
          <dependent id="19">that</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="18">broadcast</governor>
          <dependent id="20">coincided</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">release</governor>
          <dependent id="21">with</dependent>
        </dependency>
        <dependency type="det">
          <governor id="23">release</governor>
          <dependent id="22">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">coincided</governor>
          <dependent id="23">release</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">book</governor>
          <dependent id="24">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="26">book</governor>
          <dependent id="25">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">release</governor>
          <dependent id="26">book</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Ono" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Ono" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="19" has_coreference="true">
      <content>She too was interviewed on the ``60 Minutes&amp;apost;&amp;apost; show, entitled ``The Two Mrs. Lennons.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="She" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="too" lemma="too" stem="too" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="interviewed" lemma="interview" stem="interview" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="60" lemma="60" stem="60" pos="CD" type="Number" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="true" />
        <token id="9" string="Minutes" lemma="Minutes" stem="minut" pos="NNPS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="true" />
        <token id="10" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="show" lemma="show" stem="show" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="entitled" lemma="entitle" stem="entitl" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="Two" lemma="two" stem="two" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="17" string="Mrs." lemma="Mrs." stem="mrs." pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="Lennons" lemma="Lennons" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="19" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP She)) (ADVP (RB too)) (VP (VBD was) (VP (VBN interviewed) (PP (IN on) (NP (NP (DT the) (`` ``) (NP (CD 60) (NNPS Minutes)) ('' '') (NN show)) (, ,) (VP (VBN entitled) (`` ``) (NP (DT The) (CD Two) (NNP Mrs.) (NNP Lennons))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="60 Minutes" type="NP">
          <tokens>
            <token id="8" string="60" />
            <token id="9" string="Minutes" />
          </tokens>
        </chunking>
        <chunking id="2" string="The Two Mrs. Lennons" type="NP">
          <tokens>
            <token id="15" string="The" />
            <token id="16" string="Two" />
            <token id="17" string="Mrs." />
            <token id="18" string="Lennons" />
          </tokens>
        </chunking>
        <chunking id="3" string="entitled `` The Two Mrs. Lennons" type="VP">
          <tokens>
            <token id="13" string="entitled" />
            <token id="14" string="``" />
            <token id="15" string="The" />
            <token id="16" string="Two" />
            <token id="17" string="Mrs." />
            <token id="18" string="Lennons" />
          </tokens>
        </chunking>
        <chunking id="4" string="interviewed on the `` 60 Minutes '' show , entitled `` The Two Mrs. Lennons" type="VP">
          <tokens>
            <token id="4" string="interviewed" />
            <token id="5" string="on" />
            <token id="6" string="the" />
            <token id="7" string="``" />
            <token id="8" string="60" />
            <token id="9" string="Minutes" />
            <token id="10" string="''" />
            <token id="11" string="show" />
            <token id="12" string="," />
            <token id="13" string="entitled" />
            <token id="14" string="``" />
            <token id="15" string="The" />
            <token id="16" string="Two" />
            <token id="17" string="Mrs." />
            <token id="18" string="Lennons" />
          </tokens>
        </chunking>
        <chunking id="5" string="the `` 60 Minutes '' show , entitled `` The Two Mrs. Lennons" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="``" />
            <token id="8" string="60" />
            <token id="9" string="Minutes" />
            <token id="10" string="''" />
            <token id="11" string="show" />
            <token id="12" string="," />
            <token id="13" string="entitled" />
            <token id="14" string="``" />
            <token id="15" string="The" />
            <token id="16" string="Two" />
            <token id="17" string="Mrs." />
            <token id="18" string="Lennons" />
          </tokens>
        </chunking>
        <chunking id="6" string="the `` 60 Minutes '' show" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="``" />
            <token id="8" string="60" />
            <token id="9" string="Minutes" />
            <token id="10" string="''" />
            <token id="11" string="show" />
          </tokens>
        </chunking>
        <chunking id="7" string="was interviewed on the `` 60 Minutes '' show , entitled `` The Two Mrs. Lennons" type="VP">
          <tokens>
            <token id="3" string="was" />
            <token id="4" string="interviewed" />
            <token id="5" string="on" />
            <token id="6" string="the" />
            <token id="7" string="``" />
            <token id="8" string="60" />
            <token id="9" string="Minutes" />
            <token id="10" string="''" />
            <token id="11" string="show" />
            <token id="12" string="," />
            <token id="13" string="entitled" />
            <token id="14" string="``" />
            <token id="15" string="The" />
            <token id="16" string="Two" />
            <token id="17" string="Mrs." />
            <token id="18" string="Lennons" />
          </tokens>
        </chunking>
        <chunking id="8" string="She" type="NP">
          <tokens>
            <token id="1" string="She" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="4">interviewed</governor>
          <dependent id="1">She</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">interviewed</governor>
          <dependent id="2">too</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="4">interviewed</governor>
          <dependent id="3">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">interviewed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">show</governor>
          <dependent id="5">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">show</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="9">Minutes</governor>
          <dependent id="8">60</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">show</governor>
          <dependent id="9">Minutes</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">interviewed</governor>
          <dependent id="11">show</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="11">show</governor>
          <dependent id="13">entitled</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">Lennons</governor>
          <dependent id="15">The</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="18">Lennons</governor>
          <dependent id="16">Two</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="18">Lennons</governor>
          <dependent id="17">Mrs.</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="13">entitled</governor>
          <dependent id="18">Lennons</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="60 Minutes" type="DURATION" score="0.0">
          <tokens>
            <token id="8" string="60" />
            <token id="9" string="Minutes" />
          </tokens>
        </entity>
        <entity id="2" string="Two" type="NUMBER" score="0.0">
          <tokens>
            <token id="16" string="Two" />
          </tokens>
        </entity>
        <entity id="3" string="Lennons" type="PERSON" score="0.0">
          <tokens>
            <token id="18" string="Lennons" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="20" has_coreference="true">
      <content>``These people in this book are not us.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="These" lemma="these" stem="these" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="3" string="people" lemma="people" stem="peopl" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="6" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="7" string="are" lemma="be" stem="ar" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="us" lemma="we" stem="u" pos="PRP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (NP (DT These) (NNS people)) (PP (IN in) (NP (DT this) (NN book)))) (VP (VBP are) (RB not) (NP (PRP us))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="These people" type="NP">
          <tokens>
            <token id="2" string="These" />
            <token id="3" string="people" />
          </tokens>
        </chunking>
        <chunking id="2" string="this book" type="NP">
          <tokens>
            <token id="5" string="this" />
            <token id="6" string="book" />
          </tokens>
        </chunking>
        <chunking id="3" string="are not us" type="VP">
          <tokens>
            <token id="7" string="are" />
            <token id="8" string="not" />
            <token id="9" string="us" />
          </tokens>
        </chunking>
        <chunking id="4" string="These people in this book" type="NP">
          <tokens>
            <token id="2" string="These" />
            <token id="3" string="people" />
            <token id="4" string="in" />
            <token id="5" string="this" />
            <token id="6" string="book" />
          </tokens>
        </chunking>
        <chunking id="5" string="us" type="NP">
          <tokens>
            <token id="9" string="us" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">people</governor>
          <dependent id="2">These</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">us</governor>
          <dependent id="3">people</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">book</governor>
          <dependent id="4">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">book</governor>
          <dependent id="5">this</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">people</governor>
          <dependent id="6">book</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="9">us</governor>
          <dependent id="7">are</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="9">us</governor>
          <dependent id="8">not</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="9">us</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="21" has_coreference="true">
      <content>It&amp;apost;s not John and me,&amp;apost;&amp;apost; Ono said in the taped radio broadcast.</content>
      <tokens>
        <token id="1" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="5" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="me" lemma="I" stem="me" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="Ono" lemma="Ono" stem="ono" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="10" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="taped" lemma="tape" stem="tape" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="radio" lemma="radio" stem="radio" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="broadcast" lemma="broadcast" stem="broadcast" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (PRP It)) (VP (VBZ 's) (RB not) (NP (NP (NNP John)) (CC and) (NP (PRP me))))) (, ,) ('' '') (NP (NNP Ono)) (VP (VBD said) (PP (IN in) (NP (DT the) (VBN taped) (NN radio) (NN broadcast)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="'s not John and me" type="VP">
          <tokens>
            <token id="2" string="'s" />
            <token id="3" string="not" />
            <token id="4" string="John" />
            <token id="5" string="and" />
            <token id="6" string="me" />
          </tokens>
        </chunking>
        <chunking id="2" string="said in the taped radio broadcast" type="VP">
          <tokens>
            <token id="10" string="said" />
            <token id="11" string="in" />
            <token id="12" string="the" />
            <token id="13" string="taped" />
            <token id="14" string="radio" />
            <token id="15" string="broadcast" />
          </tokens>
        </chunking>
        <chunking id="3" string="me" type="NP">
          <tokens>
            <token id="6" string="me" />
          </tokens>
        </chunking>
        <chunking id="4" string="John" type="NP">
          <tokens>
            <token id="4" string="John" />
          </tokens>
        </chunking>
        <chunking id="5" string="It" type="NP">
          <tokens>
            <token id="1" string="It" />
          </tokens>
        </chunking>
        <chunking id="6" string="the taped radio broadcast" type="NP">
          <tokens>
            <token id="12" string="the" />
            <token id="13" string="taped" />
            <token id="14" string="radio" />
            <token id="15" string="broadcast" />
          </tokens>
        </chunking>
        <chunking id="7" string="John and me" type="NP">
          <tokens>
            <token id="4" string="John" />
            <token id="5" string="and" />
            <token id="6" string="me" />
          </tokens>
        </chunking>
        <chunking id="8" string="Ono" type="NP">
          <tokens>
            <token id="9" string="Ono" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">John</governor>
          <dependent id="1">It</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="4">John</governor>
          <dependent id="2">'s</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="4">John</governor>
          <dependent id="3">not</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="10">said</governor>
          <dependent id="4">John</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">John</governor>
          <dependent id="5">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">John</governor>
          <dependent id="6">me</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">said</governor>
          <dependent id="9">Ono</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="10">said</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">broadcast</governor>
          <dependent id="11">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">broadcast</governor>
          <dependent id="12">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">broadcast</governor>
          <dependent id="13">taped</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">broadcast</governor>
          <dependent id="14">radio</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">said</governor>
          <dependent id="15">broadcast</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="John" />
          </tokens>
        </entity>
        <entity id="2" string="Ono" type="PERSON" score="0.0">
          <tokens>
            <token id="9" string="Ono" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="22" has_coreference="true">
      <content>Cynthia and John&amp;apost;s son, Julian Lennon, also denounced the book on the radio taping, calling it ``lies, untruths,&amp;apost;&amp;apost; and adding that ``the whole thing is just sickening.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="Cynthia" lemma="Cynthia" stem="cynthia" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="4" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="son" lemma="son" stem="son" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="Julian" lemma="Julian" stem="julian" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="8" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="denounced" lemma="denounce" stem="denounc" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="14" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="radio" lemma="radio" stem="radio" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="taping" lemma="taping" stem="tape" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="calling" lemma="call" stem="call" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="21" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="lies" lemma="lie" stem="li" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="untruths" lemma="untruth" stem="untruth" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="adding" lemma="add" stem="ad" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="whole" lemma="whole" stem="whole" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="thing" lemma="thing" stem="thing" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="just" lemma="just" stem="just" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="sickening" lemma="sickening" stem="sicken" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="37" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NP (NNP Cynthia) (CC and) (NNP John) (POS 's)) (NN son)) (, ,) (NP (NNP Julian) (NNP Lennon)) (, ,)) (ADVP (RB also)) (VP (VBD denounced) (NP (DT the) (NN book)) (PP (IN on) (NP (DT the) (NN radio) (NN taping))) (, ,) (S (VP (VP (VBG calling) (SBAR (S (NP (PRP it)) (`` ``) (VP (VP (VBZ lies)) (, ,) (VP (VBZ untruths))) (, ,) ('' '')))) (CC and) (VP (VBG adding) (SBAR (IN that) (`` ``) (S (NP (DT the) (JJ whole) (NN thing)) (VP (VBZ is) (ADJP (RB just) (JJ sickening))))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="Cynthia and John 's son , Julian Lennon ," type="NP">
          <tokens>
            <token id="1" string="Cynthia" />
            <token id="2" string="and" />
            <token id="3" string="John" />
            <token id="4" string="'s" />
            <token id="5" string="son" />
            <token id="6" string="," />
            <token id="7" string="Julian" />
            <token id="8" string="Lennon" />
            <token id="9" string="," />
          </tokens>
        </chunking>
        <chunking id="2" string="untruths" type="VP">
          <tokens>
            <token id="24" string="untruths" />
          </tokens>
        </chunking>
        <chunking id="3" string="denounced the book on the radio taping , calling it `` lies , untruths , '' and adding that `` the whole thing is just sickening" type="VP">
          <tokens>
            <token id="11" string="denounced" />
            <token id="12" string="the" />
            <token id="13" string="book" />
            <token id="14" string="on" />
            <token id="15" string="the" />
            <token id="16" string="radio" />
            <token id="17" string="taping" />
            <token id="18" string="," />
            <token id="19" string="calling" />
            <token id="20" string="it" />
            <token id="21" string="``" />
            <token id="22" string="lies" />
            <token id="23" string="," />
            <token id="24" string="untruths" />
            <token id="25" string="," />
            <token id="26" string="''" />
            <token id="27" string="and" />
            <token id="28" string="adding" />
            <token id="29" string="that" />
            <token id="30" string="``" />
            <token id="31" string="the" />
            <token id="32" string="whole" />
            <token id="33" string="thing" />
            <token id="34" string="is" />
            <token id="35" string="just" />
            <token id="36" string="sickening" />
          </tokens>
        </chunking>
        <chunking id="4" string="calling it `` lies , untruths , ''" type="VP">
          <tokens>
            <token id="19" string="calling" />
            <token id="20" string="it" />
            <token id="21" string="``" />
            <token id="22" string="lies" />
            <token id="23" string="," />
            <token id="24" string="untruths" />
            <token id="25" string="," />
            <token id="26" string="''" />
          </tokens>
        </chunking>
        <chunking id="5" string="just sickening" type="ADJP">
          <tokens>
            <token id="35" string="just" />
            <token id="36" string="sickening" />
          </tokens>
        </chunking>
        <chunking id="6" string="it `` lies , untruths , ''" type="SBAR">
          <tokens>
            <token id="20" string="it" />
            <token id="21" string="``" />
            <token id="22" string="lies" />
            <token id="23" string="," />
            <token id="24" string="untruths" />
            <token id="25" string="," />
            <token id="26" string="''" />
          </tokens>
        </chunking>
        <chunking id="7" string="it" type="NP">
          <tokens>
            <token id="20" string="it" />
          </tokens>
        </chunking>
        <chunking id="8" string="Cynthia and John 's" type="NP">
          <tokens>
            <token id="1" string="Cynthia" />
            <token id="2" string="and" />
            <token id="3" string="John" />
            <token id="4" string="'s" />
          </tokens>
        </chunking>
        <chunking id="9" string="the book" type="NP">
          <tokens>
            <token id="12" string="the" />
            <token id="13" string="book" />
          </tokens>
        </chunking>
        <chunking id="10" string="adding that `` the whole thing is just sickening" type="VP">
          <tokens>
            <token id="28" string="adding" />
            <token id="29" string="that" />
            <token id="30" string="``" />
            <token id="31" string="the" />
            <token id="32" string="whole" />
            <token id="33" string="thing" />
            <token id="34" string="is" />
            <token id="35" string="just" />
            <token id="36" string="sickening" />
          </tokens>
        </chunking>
        <chunking id="11" string="is just sickening" type="VP">
          <tokens>
            <token id="34" string="is" />
            <token id="35" string="just" />
            <token id="36" string="sickening" />
          </tokens>
        </chunking>
        <chunking id="12" string="the whole thing" type="NP">
          <tokens>
            <token id="31" string="the" />
            <token id="32" string="whole" />
            <token id="33" string="thing" />
          </tokens>
        </chunking>
        <chunking id="13" string="Julian Lennon" type="NP">
          <tokens>
            <token id="7" string="Julian" />
            <token id="8" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="14" string="lies , untruths" type="VP">
          <tokens>
            <token id="22" string="lies" />
            <token id="23" string="," />
            <token id="24" string="untruths" />
          </tokens>
        </chunking>
        <chunking id="15" string="lies" type="VP">
          <tokens>
            <token id="22" string="lies" />
          </tokens>
        </chunking>
        <chunking id="16" string="that `` the whole thing is just sickening" type="SBAR">
          <tokens>
            <token id="29" string="that" />
            <token id="30" string="``" />
            <token id="31" string="the" />
            <token id="32" string="whole" />
            <token id="33" string="thing" />
            <token id="34" string="is" />
            <token id="35" string="just" />
            <token id="36" string="sickening" />
          </tokens>
        </chunking>
        <chunking id="17" string="the radio taping" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="radio" />
            <token id="17" string="taping" />
          </tokens>
        </chunking>
        <chunking id="18" string="Cynthia and John 's son" type="NP">
          <tokens>
            <token id="1" string="Cynthia" />
            <token id="2" string="and" />
            <token id="3" string="John" />
            <token id="4" string="'s" />
            <token id="5" string="son" />
          </tokens>
        </chunking>
        <chunking id="19" string="calling it `` lies , untruths , '' and adding that `` the whole thing is just sickening" type="VP">
          <tokens>
            <token id="19" string="calling" />
            <token id="20" string="it" />
            <token id="21" string="``" />
            <token id="22" string="lies" />
            <token id="23" string="," />
            <token id="24" string="untruths" />
            <token id="25" string="," />
            <token id="26" string="''" />
            <token id="27" string="and" />
            <token id="28" string="adding" />
            <token id="29" string="that" />
            <token id="30" string="``" />
            <token id="31" string="the" />
            <token id="32" string="whole" />
            <token id="33" string="thing" />
            <token id="34" string="is" />
            <token id="35" string="just" />
            <token id="36" string="sickening" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="5">son</governor>
          <dependent id="1">Cynthia</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="1">Cynthia</governor>
          <dependent id="2">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="1">Cynthia</governor>
          <dependent id="3">John</dependent>
        </dependency>
        <dependency type="case">
          <governor id="1">Cynthia</governor>
          <dependent id="4">'s</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">denounced</governor>
          <dependent id="5">son</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">Lennon</governor>
          <dependent id="7">Julian</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="5">son</governor>
          <dependent id="8">Lennon</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">denounced</governor>
          <dependent id="10">also</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="11">denounced</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">book</governor>
          <dependent id="12">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">denounced</governor>
          <dependent id="13">book</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">taping</governor>
          <dependent id="14">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="17">taping</governor>
          <dependent id="15">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="17">taping</governor>
          <dependent id="16">radio</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">denounced</governor>
          <dependent id="17">taping</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="11">denounced</governor>
          <dependent id="19">calling</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">lies</governor>
          <dependent id="20">it</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="19">calling</governor>
          <dependent id="22">lies</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="22">lies</governor>
          <dependent id="24">untruths</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="19">calling</governor>
          <dependent id="27">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="19">calling</governor>
          <dependent id="28">adding</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="36">sickening</governor>
          <dependent id="29">that</dependent>
        </dependency>
        <dependency type="det">
          <governor id="33">thing</governor>
          <dependent id="31">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="33">thing</governor>
          <dependent id="32">whole</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="36">sickening</governor>
          <dependent id="33">thing</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="36">sickening</governor>
          <dependent id="34">is</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="36">sickening</governor>
          <dependent id="35">just</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="28">adding</governor>
          <dependent id="36">sickening</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Cynthia" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Cynthia" />
          </tokens>
        </entity>
        <entity id="2" string="Julian Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="7" string="Julian" />
            <token id="8" string="Lennon" />
          </tokens>
        </entity>
        <entity id="3" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="3" string="John" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="23" has_coreference="true">
      <content>Sean Lennon, the son of Yoko and John who is now 12, added his voice on the CBS broadcast, rebutting charges that his father was inattentive to parenting.</content>
      <tokens>
        <token id="1" string="Sean" lemma="Sean" stem="sean" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="son" lemma="son" stem="son" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="6" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="Yoko" lemma="Yoko" stem="yoko" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="8" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="10" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="now" lemma="now" stem="now" pos="RB" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="13" string="12" lemma="12" stem="12" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="true" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="added" lemma="add" stem="ad" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="voice" lemma="voice" stem="voic" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="CBS" lemma="CBS" stem="cbs" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="21" string="broadcast" lemma="broadcast" stem="broadcast" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="rebutting" lemma="rebut" stem="rebut" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="charges" lemma="charge" stem="charg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="27" string="father" lemma="father" stem="father" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="28" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="inattentive" lemma="inattentive" stem="inattent" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="parenting" lemma="parenting" stem="parent" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Sean) (NNP Lennon)) (, ,) (NP (NP (DT the) (NN son)) (PP (IN of) (NP (NNP Yoko) (CC and) (NNP John))) (SBAR (WHNP (WP who)) (S (VP (VBZ is) (ADVP (RB now)) (NP (CD 12)))))) (, ,)) (VP (VBD added) (NP (PRP$ his) (NN voice)) (PP (IN on) (NP (DT the) (NNP CBS) (NN broadcast))) (, ,) (S (VP (VBG rebutting) (NP (NNS charges)) (SBAR (IN that) (S (NP (PRP$ his) (NN father)) (VP (VBD was) (ADJP (JJ inattentive) (PP (TO to) (NP (NN parenting)))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="12" type="NP">
          <tokens>
            <token id="13" string="12" />
          </tokens>
        </chunking>
        <chunking id="2" string="his voice" type="NP">
          <tokens>
            <token id="16" string="his" />
            <token id="17" string="voice" />
          </tokens>
        </chunking>
        <chunking id="3" string="Yoko and John" type="NP">
          <tokens>
            <token id="7" string="Yoko" />
            <token id="8" string="and" />
            <token id="9" string="John" />
          </tokens>
        </chunking>
        <chunking id="4" string="was inattentive to parenting" type="VP">
          <tokens>
            <token id="28" string="was" />
            <token id="29" string="inattentive" />
            <token id="30" string="to" />
            <token id="31" string="parenting" />
          </tokens>
        </chunking>
        <chunking id="5" string="the son of Yoko and John who is now 12" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="son" />
            <token id="6" string="of" />
            <token id="7" string="Yoko" />
            <token id="8" string="and" />
            <token id="9" string="John" />
            <token id="10" string="who" />
            <token id="11" string="is" />
            <token id="12" string="now" />
            <token id="13" string="12" />
          </tokens>
        </chunking>
        <chunking id="6" string="is now 12" type="VP">
          <tokens>
            <token id="11" string="is" />
            <token id="12" string="now" />
            <token id="13" string="12" />
          </tokens>
        </chunking>
        <chunking id="7" string="the son" type="NP">
          <tokens>
            <token id="4" string="the" />
            <token id="5" string="son" />
          </tokens>
        </chunking>
        <chunking id="8" string="who is now 12" type="SBAR">
          <tokens>
            <token id="10" string="who" />
            <token id="11" string="is" />
            <token id="12" string="now" />
            <token id="13" string="12" />
          </tokens>
        </chunking>
        <chunking id="9" string="Sean Lennon , the son of Yoko and John who is now 12 ," type="NP">
          <tokens>
            <token id="1" string="Sean" />
            <token id="2" string="Lennon" />
            <token id="3" string="," />
            <token id="4" string="the" />
            <token id="5" string="son" />
            <token id="6" string="of" />
            <token id="7" string="Yoko" />
            <token id="8" string="and" />
            <token id="9" string="John" />
            <token id="10" string="who" />
            <token id="11" string="is" />
            <token id="12" string="now" />
            <token id="13" string="12" />
            <token id="14" string="," />
          </tokens>
        </chunking>
        <chunking id="10" string="inattentive to parenting" type="ADJP">
          <tokens>
            <token id="29" string="inattentive" />
            <token id="30" string="to" />
            <token id="31" string="parenting" />
          </tokens>
        </chunking>
        <chunking id="11" string="Sean Lennon" type="NP">
          <tokens>
            <token id="1" string="Sean" />
            <token id="2" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="12" string="parenting" type="NP">
          <tokens>
            <token id="31" string="parenting" />
          </tokens>
        </chunking>
        <chunking id="13" string="that his father was inattentive to parenting" type="SBAR">
          <tokens>
            <token id="25" string="that" />
            <token id="26" string="his" />
            <token id="27" string="father" />
            <token id="28" string="was" />
            <token id="29" string="inattentive" />
            <token id="30" string="to" />
            <token id="31" string="parenting" />
          </tokens>
        </chunking>
        <chunking id="14" string="charges" type="NP">
          <tokens>
            <token id="24" string="charges" />
          </tokens>
        </chunking>
        <chunking id="15" string="added his voice on the CBS broadcast , rebutting charges that his father was inattentive to parenting" type="VP">
          <tokens>
            <token id="15" string="added" />
            <token id="16" string="his" />
            <token id="17" string="voice" />
            <token id="18" string="on" />
            <token id="19" string="the" />
            <token id="20" string="CBS" />
            <token id="21" string="broadcast" />
            <token id="22" string="," />
            <token id="23" string="rebutting" />
            <token id="24" string="charges" />
            <token id="25" string="that" />
            <token id="26" string="his" />
            <token id="27" string="father" />
            <token id="28" string="was" />
            <token id="29" string="inattentive" />
            <token id="30" string="to" />
            <token id="31" string="parenting" />
          </tokens>
        </chunking>
        <chunking id="16" string="the CBS broadcast" type="NP">
          <tokens>
            <token id="19" string="the" />
            <token id="20" string="CBS" />
            <token id="21" string="broadcast" />
          </tokens>
        </chunking>
        <chunking id="17" string="his father" type="NP">
          <tokens>
            <token id="26" string="his" />
            <token id="27" string="father" />
          </tokens>
        </chunking>
        <chunking id="18" string="rebutting charges that his father was inattentive to parenting" type="VP">
          <tokens>
            <token id="23" string="rebutting" />
            <token id="24" string="charges" />
            <token id="25" string="that" />
            <token id="26" string="his" />
            <token id="27" string="father" />
            <token id="28" string="was" />
            <token id="29" string="inattentive" />
            <token id="30" string="to" />
            <token id="31" string="parenting" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="compound">
          <governor id="2">Lennon</governor>
          <dependent id="1">Sean</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">added</governor>
          <dependent id="2">Lennon</dependent>
        </dependency>
        <dependency type="det">
          <governor id="5">son</governor>
          <dependent id="4">the</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="2">Lennon</governor>
          <dependent id="5">son</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">Yoko</governor>
          <dependent id="6">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">son</governor>
          <dependent id="7">Yoko</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">Yoko</governor>
          <dependent id="8">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">Yoko</governor>
          <dependent id="9">John</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="13">12</governor>
          <dependent id="10">who</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="13">12</governor>
          <dependent id="11">is</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="13">12</governor>
          <dependent id="12">now</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="5">son</governor>
          <dependent id="13">12</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="15">added</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="17">voice</governor>
          <dependent id="16">his</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">added</governor>
          <dependent id="17">voice</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">broadcast</governor>
          <dependent id="18">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="21">broadcast</governor>
          <dependent id="19">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="21">broadcast</governor>
          <dependent id="20">CBS</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">added</governor>
          <dependent id="21">broadcast</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="15">added</governor>
          <dependent id="23">rebutting</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="23">rebutting</governor>
          <dependent id="24">charges</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="29">inattentive</governor>
          <dependent id="25">that</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="27">father</governor>
          <dependent id="26">his</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="29">inattentive</governor>
          <dependent id="27">father</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="29">inattentive</governor>
          <dependent id="28">was</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="23">rebutting</governor>
          <dependent id="29">inattentive</dependent>
        </dependency>
        <dependency type="case">
          <governor id="31">parenting</governor>
          <dependent id="30">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="29">inattentive</governor>
          <dependent id="31">parenting</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="12" type="NUMBER" score="0.0">
          <tokens>
            <token id="13" string="12" />
          </tokens>
        </entity>
        <entity id="2" string="Sean Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Sean" />
            <token id="2" string="Lennon" />
          </tokens>
        </entity>
        <entity id="3" string="CBS" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="20" string="CBS" />
          </tokens>
        </entity>
        <entity id="4" string="Yoko" type="PERSON" score="0.0">
          <tokens>
            <token id="7" string="Yoko" />
          </tokens>
        </entity>
        <entity id="5" string="now" type="DATE" score="0.0">
          <tokens>
            <token id="12" string="now" />
          </tokens>
        </entity>
        <entity id="6" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="9" string="John" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="24" has_coreference="true">
      <content>``I think he did a lot of fathering in the five years that I knew him,&amp;apost;&amp;apost; Sean says.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="think" lemma="think" stem="think" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="did" lemma="do" stem="did" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="lot" lemma="lot" stem="lot" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="fathering" lemma="fathering" stem="father" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="12" string="five" lemma="five" stem="five" pos="CD" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="13" string="years" lemma="year" stem="year" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="14" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="16" string="knew" lemma="know" stem="knew" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="him" lemma="he" stem="him" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="Sean" lemma="Sean" stem="sean" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="21" string="says" lemma="say" stem="sai" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (NP (PRP I)) (VP (VBP think) (SBAR (S (NP (PRP he)) (VP (VBD did) (NP (NP (DT a) (NN lot)) (PP (IN of) (NP (NN fathering)))) (PP (IN in) (NP (DT the) (CD five) (NNS years))) (SBAR (IN that) (S (NP (PRP I)) (VP (VBD knew) (NP (PRP him)))))))))) (, ,) ('' '') (NP (NNP Sean)) (VP (VBZ says)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="fathering" type="NP">
          <tokens>
            <token id="9" string="fathering" />
          </tokens>
        </chunking>
        <chunking id="2" string="knew him" type="VP">
          <tokens>
            <token id="16" string="knew" />
            <token id="17" string="him" />
          </tokens>
        </chunking>
        <chunking id="3" string="think he did a lot of fathering in the five years that I knew him" type="VP">
          <tokens>
            <token id="3" string="think" />
            <token id="4" string="he" />
            <token id="5" string="did" />
            <token id="6" string="a" />
            <token id="7" string="lot" />
            <token id="8" string="of" />
            <token id="9" string="fathering" />
            <token id="10" string="in" />
            <token id="11" string="the" />
            <token id="12" string="five" />
            <token id="13" string="years" />
            <token id="14" string="that" />
            <token id="15" string="I" />
            <token id="16" string="knew" />
            <token id="17" string="him" />
          </tokens>
        </chunking>
        <chunking id="4" string="a lot of fathering" type="NP">
          <tokens>
            <token id="6" string="a" />
            <token id="7" string="lot" />
            <token id="8" string="of" />
            <token id="9" string="fathering" />
          </tokens>
        </chunking>
        <chunking id="5" string="I" type="NP">
          <tokens>
            <token id="2" string="I" />
          </tokens>
        </chunking>
        <chunking id="6" string="him" type="NP">
          <tokens>
            <token id="17" string="him" />
          </tokens>
        </chunking>
        <chunking id="7" string="he did a lot of fathering in the five years that I knew him" type="SBAR">
          <tokens>
            <token id="4" string="he" />
            <token id="5" string="did" />
            <token id="6" string="a" />
            <token id="7" string="lot" />
            <token id="8" string="of" />
            <token id="9" string="fathering" />
            <token id="10" string="in" />
            <token id="11" string="the" />
            <token id="12" string="five" />
            <token id="13" string="years" />
            <token id="14" string="that" />
            <token id="15" string="I" />
            <token id="16" string="knew" />
            <token id="17" string="him" />
          </tokens>
        </chunking>
        <chunking id="8" string="that I knew him" type="SBAR">
          <tokens>
            <token id="14" string="that" />
            <token id="15" string="I" />
            <token id="16" string="knew" />
            <token id="17" string="him" />
          </tokens>
        </chunking>
        <chunking id="9" string="says" type="VP">
          <tokens>
            <token id="21" string="says" />
          </tokens>
        </chunking>
        <chunking id="10" string="a lot" type="NP">
          <tokens>
            <token id="6" string="a" />
            <token id="7" string="lot" />
          </tokens>
        </chunking>
        <chunking id="11" string="did a lot of fathering in the five years that I knew him" type="VP">
          <tokens>
            <token id="5" string="did" />
            <token id="6" string="a" />
            <token id="7" string="lot" />
            <token id="8" string="of" />
            <token id="9" string="fathering" />
            <token id="10" string="in" />
            <token id="11" string="the" />
            <token id="12" string="five" />
            <token id="13" string="years" />
            <token id="14" string="that" />
            <token id="15" string="I" />
            <token id="16" string="knew" />
            <token id="17" string="him" />
          </tokens>
        </chunking>
        <chunking id="12" string="the five years" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="five" />
            <token id="13" string="years" />
          </tokens>
        </chunking>
        <chunking id="13" string="Sean" type="NP">
          <tokens>
            <token id="20" string="Sean" />
          </tokens>
        </chunking>
        <chunking id="14" string="he" type="NP">
          <tokens>
            <token id="4" string="he" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">think</governor>
          <dependent id="2">I</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="21">says</governor>
          <dependent id="3">think</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">did</governor>
          <dependent id="4">he</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">think</governor>
          <dependent id="5">did</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">lot</governor>
          <dependent id="6">a</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">did</governor>
          <dependent id="7">lot</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">fathering</governor>
          <dependent id="8">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">lot</governor>
          <dependent id="9">fathering</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">years</governor>
          <dependent id="10">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">years</governor>
          <dependent id="11">the</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="13">years</governor>
          <dependent id="12">five</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">did</governor>
          <dependent id="13">years</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="16">knew</governor>
          <dependent id="14">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">knew</governor>
          <dependent id="15">I</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="5">did</governor>
          <dependent id="16">knew</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="16">knew</governor>
          <dependent id="17">him</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">says</governor>
          <dependent id="20">Sean</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="21">says</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="the five years" type="DURATION" score="0.0">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="five" />
            <token id="13" string="years" />
          </tokens>
        </entity>
        <entity id="2" string="Sean" type="PERSON" score="0.0">
          <tokens>
            <token id="20" string="Sean" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="25" has_coreference="true">
      <content>``He spent so much time with me not just at home, but everywhere, you know.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="He" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="spent" lemma="spend" stem="spent" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="so" lemma="so" stem="so" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="much" lemma="much" stem="much" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="time" lemma="time" stem="time" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="me" lemma="I" stem="me" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="9" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="just" lemma="just" stem="just" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="home" lemma="home" stem="home" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="everywhere" lemma="everywhere" stem="everywher" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="you" lemma="you" stem="you" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="know" lemma="know" stem="know" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (NP (PRP He)) (VP (VBD spent) (ADVP (RB so)) (NP-TMP (JJ much) (NN time)) (PP (PP (IN with) (NP (PRP me))) (CONJP (RB not) (RB just)) (PP (IN at) (NP (NN home)))))) (, ,) (CC but) (S (ADVP (RB everywhere)) (, ,) (NP (PRP you)) (VP (VBP know))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="me" type="NP">
          <tokens>
            <token id="8" string="me" />
          </tokens>
        </chunking>
        <chunking id="2" string="know" type="VP">
          <tokens>
            <token id="18" string="know" />
          </tokens>
        </chunking>
        <chunking id="3" string="He" type="NP">
          <tokens>
            <token id="2" string="He" />
          </tokens>
        </chunking>
        <chunking id="4" string="spent so much time with me not just at home" type="VP">
          <tokens>
            <token id="3" string="spent" />
            <token id="4" string="so" />
            <token id="5" string="much" />
            <token id="6" string="time" />
            <token id="7" string="with" />
            <token id="8" string="me" />
            <token id="9" string="not" />
            <token id="10" string="just" />
            <token id="11" string="at" />
            <token id="12" string="home" />
          </tokens>
        </chunking>
        <chunking id="5" string="home" type="NP">
          <tokens>
            <token id="12" string="home" />
          </tokens>
        </chunking>
        <chunking id="6" string="you" type="NP">
          <tokens>
            <token id="17" string="you" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">spent</governor>
          <dependent id="2">He</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">spent</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="3">spent</governor>
          <dependent id="4">so</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="6">time</governor>
          <dependent id="5">much</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="3">spent</governor>
          <dependent id="6">time</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">me</governor>
          <dependent id="7">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">spent</governor>
          <dependent id="8">me</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="10">just</governor>
          <dependent id="9">not</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="8">me</governor>
          <dependent id="10">just</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">home</governor>
          <dependent id="11">at</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="8">me</governor>
          <dependent id="12">home</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="3">spent</governor>
          <dependent id="14">but</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="18">know</governor>
          <dependent id="15">everywhere</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="18">know</governor>
          <dependent id="17">you</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="3">spent</governor>
          <dependent id="18">know</dependent>
        </dependency>
      </dependencies>
    </sentence>
  </sentences>
  <coreferences>
    <coreference id="1" type="PROPER">
      <referenced ids_tokens="1-2" string="Cynthia Lennon" id_sentence="1" />
      <mentions>
        <mention ids_tokens="18" string="I" id_sentence="2" />
        <mention ids_tokens="22" string="they" id_sentence="2" />
        <mention ids_tokens="24" string="gravediggers" id_sentence="2" />
        <mention ids_tokens="33-34" string="Mrs. Lennon" id_sentence="2" />
        <mention ids_tokens="21-22" string="Mrs. Lennon" id_sentence="9" />
        <mention ids_tokens="1" string="Cynthia" id_sentence="22" />
      </mentions>
    </coreference>
    <coreference id="2" type="PROPER">
      <referenced ids_tokens="18-19" string="John Lennon" id_sentence="1" />
      <mentions>
        <mention ids_tokens="6" string="Lennon" id_sentence="6" />
        <mention ids_tokens="1" string="Lennon" id_sentence="7" />
        <mention ids_tokens="5" string="his" id_sentence="7" />
        <mention ids_tokens="3" string="I" id_sentence="8" />
        <mention ids_tokens="6" string="our" id_sentence="9" />
        <mention ids_tokens="2" string="I" id_sentence="10" />
        <mention ids_tokens="1" string="I" id_sentence="11" />
        <mention ids_tokens="9" string="my" id_sentence="11" />
        <mention ids_tokens="3" string="I" id_sentence="12" />
        <mention ids_tokens="6" string="I" id_sentence="12" />
        <mention ids_tokens="1" string="I" id_sentence="13" />
        <mention ids_tokens="1" string="I" id_sentence="14" />
        <mention ids_tokens="1" string="I" id_sentence="15" />
        <mention ids_tokens="1" string="I" id_sentence="16" />
        <mention ids_tokens="5" string="I" id_sentence="16" />
        <mention ids_tokens="8" string="Lennon" id_sentence="17" />
        <mention ids_tokens="7-8" string="Julian Lennon" id_sentence="22" />
        <mention ids_tokens="1-13" string="Sean Lennon , the son of Yoko and John who is now 12" id_sentence="23" />
        <mention ids_tokens="1-2" string="Sean Lennon" id_sentence="23" />
        <mention ids_tokens="4-13" string="the son of Yoko and John who is now 12" id_sentence="23" />
        <mention ids_tokens="16" string="his" id_sentence="23" />
        <mention ids_tokens="26" string="his" id_sentence="23" />
        <mention ids_tokens="4" string="he" id_sentence="24" />
        <mention ids_tokens="17" string="him" id_sentence="24" />
        <mention ids_tokens="20" string="Sean" id_sentence="24" />
        <mention ids_tokens="2" string="He" id_sentence="25" />
      </mentions>
    </coreference>
    <coreference id="4" type="NOMINAL">
      <referenced ids_tokens="7" string="this" id_sentence="2" />
      <mentions>
        <mention ids_tokens="10" string="it" id_sentence="3" />
        <mention ids_tokens="14" string="it" id_sentence="3" />
      </mentions>
    </coreference>
    <coreference id="5" type="PROPER">
      <referenced ids_tokens="40-41-42-43-44-45-46-47-48" string="`` 60 Minutes '' scheduled for broadcast Sunday evening" id_sentence="2" />
      <mentions>
        <mention ids_tokens="8-9" string="60 Minutes" id_sentence="19" />
      </mentions>
    </coreference>
    <coreference id="7" type="NOMINAL">
      <referenced ids_tokens="26-27" string="his father" id_sentence="23" />
      <mentions>
        <mention ids_tokens="2" string="I" id_sentence="24" />
        <mention ids_tokens="15" string="I" id_sentence="24" />
        <mention ids_tokens="8" string="me" id_sentence="25" />
      </mentions>
    </coreference>
    <coreference id="8" type="PROPER">
      <referenced ids_tokens="4-5" string="Albert Goldman" id_sentence="4" />
      <mentions>
        <mention ids_tokens="1-15" string="Goldman , who defends his Lennon book and said it is based on 1,200 interviews" id_sentence="6" />
        <mention ids_tokens="1" string="Goldman" id_sentence="6" />
        <mention ids_tokens="5" string="his" id_sentence="6" />
      </mentions>
    </coreference>
    <coreference id="9" type="NOMINAL">
      <referenced ids_tokens="7-8-9-10-11-12-13-14" string="`` The Lives of John Lennon , ''" id_sentence="4" />
      <mentions>
        <mention ids_tokens="36-37" string="our lives" id_sentence="8" />
      </mentions>
    </coreference>
    <coreference id="10" type="NOMINAL">
      <referenced ids_tokens="20-21-22-23" string="an anorexic bisexual addled" id_sentence="4" />
      <mentions>
        <mention ids_tokens="1" string="It" id_sentence="5" />
      </mentions>
    </coreference>
    <coreference id="11" type="NOMINAL">
      <referenced ids_tokens="5-6-7" string="his Lennon book" id_sentence="6" />
      <mentions>
        <mention ids_tokens="27-28" string="no book" id_sentence="8" />
        <mention ids_tokens="6-7" string="the book" id_sentence="18" />
        <mention ids_tokens="10" string="it" id_sentence="18" />
        <mention ids_tokens="25-26" string="the book" id_sentence="18" />
        <mention ids_tokens="5-6" string="this book" id_sentence="20" />
        <mention ids_tokens="1" string="It" id_sentence="21" />
        <mention ids_tokens="12-13" string="the book" id_sentence="22" />
        <mention ids_tokens="20" string="it" id_sentence="22" />
      </mentions>
    </coreference>
    <coreference id="12" type="LIST">
      <referenced ids_tokens="23-24-25-26-27-28" string="comedian Lenny Bruce and Elvis Presley" id_sentence="6" />
      <mentions>
        <mention ids_tokens="14" string="we" id_sentence="8" />
        <mention ids_tokens="16" string="our" id_sentence="8" />
        <mention ids_tokens="36" string="our" id_sentence="8" />
        <mention ids_tokens="3" string="our" id_sentence="9" />
        <mention ids_tokens="13" string="us" id_sentence="9" />
      </mentions>
    </coreference>
    <coreference id="13" type="PROPER">
      <referenced ids_tokens="11" string="John" id_sentence="8" />
      <mentions>
        <mention ids_tokens="7" string="he" id_sentence="11" />
        <mention ids_tokens="9" string="him" id_sentence="12" />
        <mention ids_tokens="3" string="his" id_sentence="13" />
        <mention ids_tokens="4" string="he" id_sentence="14" />
        <mention ids_tokens="3" string="his" id_sentence="15" />
        <mention ids_tokens="6" string="his" id_sentence="15" />
        <mention ids_tokens="4-6" string="John and me" id_sentence="21" />
        <mention ids_tokens="3-4" string="John's" id_sentence="22" />
        <mention ids_tokens="7-9" string="Yoko and John" id_sentence="23" />
      </mentions>
    </coreference>
    <coreference id="14" type="PROPER">
      <referenced ids_tokens="16-17" string="our years" id_sentence="8" />
      <mentions>
        <mention ids_tokens="1" string="That" id_sentence="9" />
        <mention ids_tokens="3-18" string="our part of our hearts that has to stay with us , must not be public" id_sentence="9" />
      </mentions>
    </coreference>
    <coreference id="15" type="PROPER">
      <referenced ids_tokens="10-11-12-13" string="Yoko Ono in 1969" id_sentence="17" />
      <mentions>
        <mention ids_tokens="1-2" string="Ms. Ono" id_sentence="18" />
        <mention ids_tokens="1" string="She" id_sentence="19" />
        <mention ids_tokens="9" string="Ono" id_sentence="21" />
        <mention ids_tokens="7" string="Yoko" id_sentence="23" />
      </mentions>
    </coreference>
    <coreference id="16" type="NOMINAL">
      <referenced ids_tokens="2-3-4-5-6" string="These people in this book" id_sentence="20" />
      <mentions>
        <mention ids_tokens="6" string="me" id_sentence="21" />
      </mentions>
    </coreference>
  </coreferences>
</document>
