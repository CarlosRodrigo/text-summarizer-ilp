<?xml version="1.0" encoding="UTF-8"?>
<document id="0" name="FT923-3822">
  <sentences>
    <sentence id="1" has_coreference="true">
      <content>THE annual Booker ballyhoo got going again yesterday when the judges for this year&amp;apost;s Pounds 20,000 prize for British (and Commonwealth) fiction announced the shortlist.</content>
      <tokens>
        <token id="1" string="THE" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="annual" lemma="annual" stem="annual" pos="JJ" type="Word" isStopWord="false" ner="SET" is_referenced="false" is_refers="false" />
        <token id="3" string="Booker" lemma="Booker" stem="booker" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="4" string="ballyhoo" lemma="ballyhoo" stem="ballyhoo" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="got" lemma="get" stem="got" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="going" lemma="go" stem="go" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="again" lemma="again" stem="again" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="yesterday" lemma="yesterday" stem="yesterdai" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="9" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="judges" lemma="judge" stem="judg" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" ner="DATE" is_referenced="true" is_refers="true" />
        <token id="14" string="year" lemma="year" stem="year" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="true" />
        <token id="15" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="16" string="Pounds" lemma="Pounds" stem="pound" pos="NNPS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="17" string="20,000" lemma="20,000" stem="20,000" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="18" string="prize" lemma="prize" stem="prize" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="British" lemma="british" stem="british" pos="JJ" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="false" is_refers="false" />
        <token id="21" string="(" lemma="-lrb-" stem="(" pos="-LRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="Commonwealth" lemma="Commonwealth" stem="commonwealth" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="24" string=")" lemma="-rrb-" stem=")" pos="-RRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="fiction" lemma="fiction" stem="fiction" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="announced" lemma="announce" stem="announc" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="shortlist" lemma="shortlist" stem="shortlist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="29" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT THE) (JJ annual) (NNP Booker) (NN ballyhoo)) (VP (VBD got) (S (VP (VBG going) (ADVP (RB again)) (NP-TMP (NN yesterday)))) (SBAR (WHADVP (WRB when) (NP (NP (DT the) (NNS judges)) (PP (IN for) (NP (NP (DT this) (NN year) (POS 's)) (NNPS Pounds))))) (S (NP (NP (CD 20,000) (NN prize)) (PP (IN for) (NP (ADJP (ADJP (JJ British)) (PRN (-LRB- -LRB-) (PP (CC and) (NP (NNP Commonwealth))) (-RRB- -RRB-))) (NN fiction)))) (VP (VBD announced) (NP (DT the) (NN shortlist)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="British" type="ADJP">
          <tokens>
            <token id="20" string="British" />
          </tokens>
        </chunking>
        <chunking id="2" string="20,000 prize for British -LRB- and Commonwealth -RRB- fiction" type="NP">
          <tokens>
            <token id="17" string="20,000" />
            <token id="18" string="prize" />
            <token id="19" string="for" />
            <token id="20" string="British" />
            <token id="21" string="(" />
            <token id="22" string="and" />
            <token id="23" string="Commonwealth" />
            <token id="24" string=")" />
            <token id="25" string="fiction" />
          </tokens>
        </chunking>
        <chunking id="3" string="British -LRB- and Commonwealth -RRB-" type="ADJP">
          <tokens>
            <token id="20" string="British" />
            <token id="21" string="(" />
            <token id="22" string="and" />
            <token id="23" string="Commonwealth" />
            <token id="24" string=")" />
          </tokens>
        </chunking>
        <chunking id="4" string="this year 's" type="NP">
          <tokens>
            <token id="13" string="this" />
            <token id="14" string="year" />
            <token id="15" string="'s" />
          </tokens>
        </chunking>
        <chunking id="5" string="THE annual Booker ballyhoo" type="NP">
          <tokens>
            <token id="1" string="THE" />
            <token id="2" string="annual" />
            <token id="3" string="Booker" />
            <token id="4" string="ballyhoo" />
          </tokens>
        </chunking>
        <chunking id="6" string="20,000 prize" type="NP">
          <tokens>
            <token id="17" string="20,000" />
            <token id="18" string="prize" />
          </tokens>
        </chunking>
        <chunking id="7" string="when the judges for this year 's Pounds 20,000 prize for British -LRB- and Commonwealth -RRB- fiction announced the shortlist" type="SBAR">
          <tokens>
            <token id="9" string="when" />
            <token id="10" string="the" />
            <token id="11" string="judges" />
            <token id="12" string="for" />
            <token id="13" string="this" />
            <token id="14" string="year" />
            <token id="15" string="'s" />
            <token id="16" string="Pounds" />
            <token id="17" string="20,000" />
            <token id="18" string="prize" />
            <token id="19" string="for" />
            <token id="20" string="British" />
            <token id="21" string="(" />
            <token id="22" string="and" />
            <token id="23" string="Commonwealth" />
            <token id="24" string=")" />
            <token id="25" string="fiction" />
            <token id="26" string="announced" />
            <token id="27" string="the" />
            <token id="28" string="shortlist" />
          </tokens>
        </chunking>
        <chunking id="8" string="the shortlist" type="NP">
          <tokens>
            <token id="27" string="the" />
            <token id="28" string="shortlist" />
          </tokens>
        </chunking>
        <chunking id="9" string="got going again yesterday when the judges for this year 's Pounds 20,000 prize for British -LRB- and Commonwealth -RRB- fiction announced the shortlist" type="VP">
          <tokens>
            <token id="5" string="got" />
            <token id="6" string="going" />
            <token id="7" string="again" />
            <token id="8" string="yesterday" />
            <token id="9" string="when" />
            <token id="10" string="the" />
            <token id="11" string="judges" />
            <token id="12" string="for" />
            <token id="13" string="this" />
            <token id="14" string="year" />
            <token id="15" string="'s" />
            <token id="16" string="Pounds" />
            <token id="17" string="20,000" />
            <token id="18" string="prize" />
            <token id="19" string="for" />
            <token id="20" string="British" />
            <token id="21" string="(" />
            <token id="22" string="and" />
            <token id="23" string="Commonwealth" />
            <token id="24" string=")" />
            <token id="25" string="fiction" />
            <token id="26" string="announced" />
            <token id="27" string="the" />
            <token id="28" string="shortlist" />
          </tokens>
        </chunking>
        <chunking id="10" string="going again yesterday" type="VP">
          <tokens>
            <token id="6" string="going" />
            <token id="7" string="again" />
            <token id="8" string="yesterday" />
          </tokens>
        </chunking>
        <chunking id="11" string="British -LRB- and Commonwealth -RRB- fiction" type="NP">
          <tokens>
            <token id="20" string="British" />
            <token id="21" string="(" />
            <token id="22" string="and" />
            <token id="23" string="Commonwealth" />
            <token id="24" string=")" />
            <token id="25" string="fiction" />
          </tokens>
        </chunking>
        <chunking id="12" string="announced the shortlist" type="VP">
          <tokens>
            <token id="26" string="announced" />
            <token id="27" string="the" />
            <token id="28" string="shortlist" />
          </tokens>
        </chunking>
        <chunking id="13" string="the judges" type="NP">
          <tokens>
            <token id="10" string="the" />
            <token id="11" string="judges" />
          </tokens>
        </chunking>
        <chunking id="14" string="Commonwealth" type="NP">
          <tokens>
            <token id="23" string="Commonwealth" />
          </tokens>
        </chunking>
        <chunking id="15" string="the judges for this year 's Pounds" type="NP">
          <tokens>
            <token id="10" string="the" />
            <token id="11" string="judges" />
            <token id="12" string="for" />
            <token id="13" string="this" />
            <token id="14" string="year" />
            <token id="15" string="'s" />
            <token id="16" string="Pounds" />
          </tokens>
        </chunking>
        <chunking id="16" string="this year 's Pounds" type="NP">
          <tokens>
            <token id="13" string="this" />
            <token id="14" string="year" />
            <token id="15" string="'s" />
            <token id="16" string="Pounds" />
          </tokens>
        </chunking>
        <chunking id="17" string="when the judges for this year 's Pounds" type="WHADVP">
          <tokens>
            <token id="9" string="when" />
            <token id="10" string="the" />
            <token id="11" string="judges" />
            <token id="12" string="for" />
            <token id="13" string="this" />
            <token id="14" string="year" />
            <token id="15" string="'s" />
            <token id="16" string="Pounds" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="4">ballyhoo</governor>
          <dependent id="1">THE</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="4">ballyhoo</governor>
          <dependent id="2">annual</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">ballyhoo</governor>
          <dependent id="3">Booker</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">got</governor>
          <dependent id="4">ballyhoo</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">got</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="5">got</governor>
          <dependent id="6">going</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="6">going</governor>
          <dependent id="7">again</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="6">going</governor>
          <dependent id="8">yesterday</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="26">announced</governor>
          <dependent id="9">when</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">judges</governor>
          <dependent id="10">the</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="9">when</governor>
          <dependent id="11">judges</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">Pounds</governor>
          <dependent id="12">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">year</governor>
          <dependent id="13">this</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="16">Pounds</governor>
          <dependent id="14">year</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">year</governor>
          <dependent id="15">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="11">judges</governor>
          <dependent id="16">Pounds</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="18">prize</governor>
          <dependent id="17">20,000</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="26">announced</governor>
          <dependent id="18">prize</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">fiction</governor>
          <dependent id="19">for</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="25">fiction</governor>
          <dependent id="20">British</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">Commonwealth</governor>
          <dependent id="22">and</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="20">British</governor>
          <dependent id="23">Commonwealth</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="18">prize</governor>
          <dependent id="25">fiction</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="5">got</governor>
          <dependent id="26">announced</dependent>
        </dependency>
        <dependency type="det">
          <governor id="28">shortlist</governor>
          <dependent id="27">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="26">announced</governor>
          <dependent id="28">shortlist</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="yesterday" type="DATE" score="0.0">
          <tokens>
            <token id="8" string="yesterday" />
          </tokens>
        </entity>
        <entity id="2" string="British" type="NATIONALITY" score="0.0">
          <tokens>
            <token id="20" string="British" />
          </tokens>
        </entity>
        <entity id="3" string="Booker" type="PERSON" score="0.0">
          <tokens>
            <token id="3" string="Booker" />
          </tokens>
        </entity>
        <entity id="4" string="annual" type="SET" score="0.0">
          <tokens>
            <token id="2" string="annual" />
          </tokens>
        </entity>
        <entity id="5" string="20,000" type="NUMBER" score="0.0">
          <tokens>
            <token id="17" string="20,000" />
          </tokens>
        </entity>
        <entity id="6" string="Commonwealth" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="23" string="Commonwealth" />
          </tokens>
        </entity>
        <entity id="7" string="this year" type="DATE" score="0.0">
          <tokens>
            <token id="13" string="this" />
            <token id="14" string="year" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="2" has_coreference="true">
      <content>The Prize (courtesy of Booker plc, the food and agri-business group) is no longer the richest literary award but it guarantees great prestige, and sales, for the winner, and, say its admirers, helpfully brings The Novel to public attention.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="Prize" lemma="Prize" stem="prize" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="(" lemma="-lrb-" stem="(" pos="-LRB-" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="4" string="courtesy" lemma="courtesy" stem="courtesi" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="6" string="Booker" lemma="Booker" stem="booker" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="7" string="plc" lemma="plc" stem="plc" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="food" lemma="food" stem="food" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="12" string="agri-business" lemma="agri-business" stem="agri-busi" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="13" string="group" lemma="group" stem="group" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string=")" lemma="-rrb-" stem=")" pos="-RRB-" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="no" lemma="no" stem="no" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="longer" lemma="longer" stem="longer" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="richest" lemma="richest" stem="richest" pos="JJS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="literary" lemma="literary" stem="literari" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="award" lemma="award" stem="award" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="guarantees" lemma="guarantee" stem="guarante" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="great" lemma="great" stem="great" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="prestige" lemma="prestige" stem="prestig" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="sales" lemma="sale" stem="sale" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="33" string="winner" lemma="winner" stem="winner" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="34" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="say" lemma="say" stem="sai" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="38" string="its" lemma="its" stem="it" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="39" string="admirers" lemma="admirer" stem="admir" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="40" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="41" string="helpfully" lemma="helpfully" stem="helpfulli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="42" string="brings" lemma="bring" stem="bring" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="43" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="44" string="Novel" lemma="novel" stem="novel" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="45" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="46" string="public" lemma="public" stem="public" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="47" string="attention" lemma="attention" stem="attent" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="48" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (NP (DT The) (NNP Prize)) (PRN (-LRB- -LRB-) (NP (NP (NN courtesy)) (PP (IN of) (NP (NP (NNP Booker) (NN plc)) (, ,) (NP (DT the) (NN food)) (CC and) (NP (NN agri-business) (NN group))))) (-RRB- -RRB-))) (VP (VBZ is) (ADVP (RB no) (RB longer)) (NP (DT the) (ADJP (JJS richest) (JJ literary)) (NN award)))) (CC but) (S (NP (PRP it)) (VP (VP (VBZ guarantees) (NP (NP (NP (JJ great) (NN prestige)) (, ,) (CC and) (NP (NNS sales)) (, ,)) (PP (IN for) (NP (DT the) (NN winner)))) (, ,)) (CC and) (PRN (, ,) (S (VP (VB say) (NP (PRP$ its) (NNS admirers)))) (, ,)) (VP (ADVP (RB helpfully)) (VBZ brings) (NP (DT The) (JJ Novel)) (PP (TO to) (NP (JJ public) (NN attention)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="agri-business group" type="NP">
          <tokens>
            <token id="12" string="agri-business" />
            <token id="13" string="group" />
          </tokens>
        </chunking>
        <chunking id="2" string="great prestige" type="NP">
          <tokens>
            <token id="25" string="great" />
            <token id="26" string="prestige" />
          </tokens>
        </chunking>
        <chunking id="3" string="is no longer the richest literary award" type="VP">
          <tokens>
            <token id="15" string="is" />
            <token id="16" string="no" />
            <token id="17" string="longer" />
            <token id="18" string="the" />
            <token id="19" string="richest" />
            <token id="20" string="literary" />
            <token id="21" string="award" />
          </tokens>
        </chunking>
        <chunking id="4" string="richest literary" type="ADJP">
          <tokens>
            <token id="19" string="richest" />
            <token id="20" string="literary" />
          </tokens>
        </chunking>
        <chunking id="5" string="sales" type="NP">
          <tokens>
            <token id="29" string="sales" />
          </tokens>
        </chunking>
        <chunking id="6" string="the richest literary award" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="richest" />
            <token id="20" string="literary" />
            <token id="21" string="award" />
          </tokens>
        </chunking>
        <chunking id="7" string="it" type="NP">
          <tokens>
            <token id="23" string="it" />
          </tokens>
        </chunking>
        <chunking id="8" string="The Prize -LRB- courtesy of Booker plc , the food and agri-business group -RRB-" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Prize" />
            <token id="3" string="(" />
            <token id="4" string="courtesy" />
            <token id="5" string="of" />
            <token id="6" string="Booker" />
            <token id="7" string="plc" />
            <token id="8" string="," />
            <token id="9" string="the" />
            <token id="10" string="food" />
            <token id="11" string="and" />
            <token id="12" string="agri-business" />
            <token id="13" string="group" />
            <token id="14" string=")" />
          </tokens>
        </chunking>
        <chunking id="9" string="great prestige , and sales , for the winner" type="NP">
          <tokens>
            <token id="25" string="great" />
            <token id="26" string="prestige" />
            <token id="27" string="," />
            <token id="28" string="and" />
            <token id="29" string="sales" />
            <token id="30" string="," />
            <token id="31" string="for" />
            <token id="32" string="the" />
            <token id="33" string="winner" />
          </tokens>
        </chunking>
        <chunking id="10" string="its admirers" type="NP">
          <tokens>
            <token id="38" string="its" />
            <token id="39" string="admirers" />
          </tokens>
        </chunking>
        <chunking id="11" string="guarantees great prestige , and sales , for the winner , and , say its admirers , helpfully brings The Novel to public attention" type="VP">
          <tokens>
            <token id="24" string="guarantees" />
            <token id="25" string="great" />
            <token id="26" string="prestige" />
            <token id="27" string="," />
            <token id="28" string="and" />
            <token id="29" string="sales" />
            <token id="30" string="," />
            <token id="31" string="for" />
            <token id="32" string="the" />
            <token id="33" string="winner" />
            <token id="34" string="," />
            <token id="35" string="and" />
            <token id="36" string="," />
            <token id="37" string="say" />
            <token id="38" string="its" />
            <token id="39" string="admirers" />
            <token id="40" string="," />
            <token id="41" string="helpfully" />
            <token id="42" string="brings" />
            <token id="43" string="The" />
            <token id="44" string="Novel" />
            <token id="45" string="to" />
            <token id="46" string="public" />
            <token id="47" string="attention" />
          </tokens>
        </chunking>
        <chunking id="12" string="helpfully brings The Novel to public attention" type="VP">
          <tokens>
            <token id="41" string="helpfully" />
            <token id="42" string="brings" />
            <token id="43" string="The" />
            <token id="44" string="Novel" />
            <token id="45" string="to" />
            <token id="46" string="public" />
            <token id="47" string="attention" />
          </tokens>
        </chunking>
        <chunking id="13" string="public attention" type="NP">
          <tokens>
            <token id="46" string="public" />
            <token id="47" string="attention" />
          </tokens>
        </chunking>
        <chunking id="14" string="the food" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="food" />
          </tokens>
        </chunking>
        <chunking id="15" string="The Novel" type="NP">
          <tokens>
            <token id="43" string="The" />
            <token id="44" string="Novel" />
          </tokens>
        </chunking>
        <chunking id="16" string="the winner" type="NP">
          <tokens>
            <token id="32" string="the" />
            <token id="33" string="winner" />
          </tokens>
        </chunking>
        <chunking id="17" string="courtesy" type="NP">
          <tokens>
            <token id="4" string="courtesy" />
          </tokens>
        </chunking>
        <chunking id="18" string="Booker plc , the food and agri-business group" type="NP">
          <tokens>
            <token id="6" string="Booker" />
            <token id="7" string="plc" />
            <token id="8" string="," />
            <token id="9" string="the" />
            <token id="10" string="food" />
            <token id="11" string="and" />
            <token id="12" string="agri-business" />
            <token id="13" string="group" />
          </tokens>
        </chunking>
        <chunking id="19" string="Booker plc" type="NP">
          <tokens>
            <token id="6" string="Booker" />
            <token id="7" string="plc" />
          </tokens>
        </chunking>
        <chunking id="20" string="guarantees great prestige , and sales , for the winner ," type="VP">
          <tokens>
            <token id="24" string="guarantees" />
            <token id="25" string="great" />
            <token id="26" string="prestige" />
            <token id="27" string="," />
            <token id="28" string="and" />
            <token id="29" string="sales" />
            <token id="30" string="," />
            <token id="31" string="for" />
            <token id="32" string="the" />
            <token id="33" string="winner" />
            <token id="34" string="," />
          </tokens>
        </chunking>
        <chunking id="21" string="The Prize" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Prize" />
          </tokens>
        </chunking>
        <chunking id="22" string="say its admirers" type="VP">
          <tokens>
            <token id="37" string="say" />
            <token id="38" string="its" />
            <token id="39" string="admirers" />
          </tokens>
        </chunking>
        <chunking id="23" string="courtesy of Booker plc , the food and agri-business group" type="NP">
          <tokens>
            <token id="4" string="courtesy" />
            <token id="5" string="of" />
            <token id="6" string="Booker" />
            <token id="7" string="plc" />
            <token id="8" string="," />
            <token id="9" string="the" />
            <token id="10" string="food" />
            <token id="11" string="and" />
            <token id="12" string="agri-business" />
            <token id="13" string="group" />
          </tokens>
        </chunking>
        <chunking id="24" string="great prestige , and sales ," type="NP">
          <tokens>
            <token id="25" string="great" />
            <token id="26" string="prestige" />
            <token id="27" string="," />
            <token id="28" string="and" />
            <token id="29" string="sales" />
            <token id="30" string="," />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">Prize</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">award</governor>
          <dependent id="2">Prize</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="2">Prize</governor>
          <dependent id="4">courtesy</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">plc</governor>
          <dependent id="5">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">plc</governor>
          <dependent id="6">Booker</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">courtesy</governor>
          <dependent id="7">plc</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">food</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">plc</governor>
          <dependent id="10">food</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">plc</governor>
          <dependent id="11">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="13">group</governor>
          <dependent id="12">agri-business</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">plc</governor>
          <dependent id="13">group</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="21">award</governor>
          <dependent id="15">is</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="17">longer</governor>
          <dependent id="16">no</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="21">award</governor>
          <dependent id="17">longer</dependent>
        </dependency>
        <dependency type="det">
          <governor id="21">award</governor>
          <dependent id="18">the</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="20">literary</governor>
          <dependent id="19">richest</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="21">award</governor>
          <dependent id="20">literary</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="21">award</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="21">award</governor>
          <dependent id="22">but</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="24">guarantees</governor>
          <dependent id="23">it</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="21">award</governor>
          <dependent id="24">guarantees</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="26">prestige</governor>
          <dependent id="25">great</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="24">guarantees</governor>
          <dependent id="26">prestige</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="26">prestige</governor>
          <dependent id="28">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="26">prestige</governor>
          <dependent id="29">sales</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">winner</governor>
          <dependent id="31">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="33">winner</governor>
          <dependent id="32">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">prestige</governor>
          <dependent id="33">winner</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="24">guarantees</governor>
          <dependent id="35">and</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="24">guarantees</governor>
          <dependent id="37">say</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="39">admirers</governor>
          <dependent id="38">its</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="37">say</governor>
          <dependent id="39">admirers</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="42">brings</governor>
          <dependent id="41">helpfully</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="24">guarantees</governor>
          <dependent id="42">brings</dependent>
        </dependency>
        <dependency type="det">
          <governor id="44">Novel</governor>
          <dependent id="43">The</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="42">brings</governor>
          <dependent id="44">Novel</dependent>
        </dependency>
        <dependency type="case">
          <governor id="47">attention</governor>
          <dependent id="45">to</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="47">attention</governor>
          <dependent id="46">public</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="42">brings</governor>
          <dependent id="47">attention</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Booker" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Booker" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="3" has_coreference="true">
      <content>Its critics say that the autumn brouhaha distorts the annual fiction industry and the intense media hype of the shortlisted authors does a disservice to the hordes of other gifted but unlisted writers.</content>
      <tokens>
        <token id="1" string="Its" lemma="its" stem="it" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="critics" lemma="critic" stem="critic" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="say" lemma="say" stem="sai" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="autumn" lemma="autumn" stem="autumn" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="7" string="brouhaha" lemma="brouhaha" stem="brouhaha" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="distorts" lemma="distort" stem="distort" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="annual" lemma="annual" stem="annual" pos="JJ" type="Word" isStopWord="false" ner="SET" is_referenced="false" is_refers="false" />
        <token id="11" string="fiction" lemma="fiction" stem="fiction" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="industry" lemma="industry" stem="industri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="intense" lemma="intense" stem="intens" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="media" lemma="media" stem="media" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="hype" lemma="hype" stem="hype" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="shortlisted" lemma="shortlisted" stem="shortlist" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="authors" lemma="author" stem="author" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="does" lemma="do" stem="doe" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="disservice" lemma="disservice" stem="disservic" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="hordes" lemma="horde" stem="hord" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="other" lemma="other" stem="other" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="gifted" lemma="gifted" stem="gift" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="unlisted" lemma="unlisted" stem="unlist" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="writers" lemma="writer" stem="writer" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (PRP$ Its) (NNS critics)) (VP (VBP say) (SBAR (IN that) (S (NP (DT the) (NN autumn) (NN brouhaha)) (VP (VBZ distorts) (NP (DT the) (JJ annual) (NN fiction) (NN industry))))))) (CC and) (S (NP (NP (DT the) (JJ intense) (NNS media) (NN hype)) (PP (IN of) (NP (DT the) (JJ shortlisted) (NNS authors)))) (VP (VBZ does) (NP (DT a) (NN disservice)) (PP (TO to) (NP (NP (DT the) (NNS hordes)) (PP (IN of) (NP (JJ other) (JJ gifted) (CC but) (JJ unlisted) (NNS writers))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="that the autumn brouhaha distorts the annual fiction industry" type="SBAR">
          <tokens>
            <token id="4" string="that" />
            <token id="5" string="the" />
            <token id="6" string="autumn" />
            <token id="7" string="brouhaha" />
            <token id="8" string="distorts" />
            <token id="9" string="the" />
            <token id="10" string="annual" />
            <token id="11" string="fiction" />
            <token id="12" string="industry" />
          </tokens>
        </chunking>
        <chunking id="2" string="does a disservice to the hordes of other gifted but unlisted writers" type="VP">
          <tokens>
            <token id="22" string="does" />
            <token id="23" string="a" />
            <token id="24" string="disservice" />
            <token id="25" string="to" />
            <token id="26" string="the" />
            <token id="27" string="hordes" />
            <token id="28" string="of" />
            <token id="29" string="other" />
            <token id="30" string="gifted" />
            <token id="31" string="but" />
            <token id="32" string="unlisted" />
            <token id="33" string="writers" />
          </tokens>
        </chunking>
        <chunking id="3" string="say that the autumn brouhaha distorts the annual fiction industry" type="VP">
          <tokens>
            <token id="3" string="say" />
            <token id="4" string="that" />
            <token id="5" string="the" />
            <token id="6" string="autumn" />
            <token id="7" string="brouhaha" />
            <token id="8" string="distorts" />
            <token id="9" string="the" />
            <token id="10" string="annual" />
            <token id="11" string="fiction" />
            <token id="12" string="industry" />
          </tokens>
        </chunking>
        <chunking id="4" string="the autumn brouhaha" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="autumn" />
            <token id="7" string="brouhaha" />
          </tokens>
        </chunking>
        <chunking id="5" string="the hordes" type="NP">
          <tokens>
            <token id="26" string="the" />
            <token id="27" string="hordes" />
          </tokens>
        </chunking>
        <chunking id="6" string="the intense media hype of the shortlisted authors" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="intense" />
            <token id="16" string="media" />
            <token id="17" string="hype" />
            <token id="18" string="of" />
            <token id="19" string="the" />
            <token id="20" string="shortlisted" />
            <token id="21" string="authors" />
          </tokens>
        </chunking>
        <chunking id="7" string="the intense media hype" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="intense" />
            <token id="16" string="media" />
            <token id="17" string="hype" />
          </tokens>
        </chunking>
        <chunking id="8" string="the hordes of other gifted but unlisted writers" type="NP">
          <tokens>
            <token id="26" string="the" />
            <token id="27" string="hordes" />
            <token id="28" string="of" />
            <token id="29" string="other" />
            <token id="30" string="gifted" />
            <token id="31" string="but" />
            <token id="32" string="unlisted" />
            <token id="33" string="writers" />
          </tokens>
        </chunking>
        <chunking id="9" string="the shortlisted authors" type="NP">
          <tokens>
            <token id="19" string="the" />
            <token id="20" string="shortlisted" />
            <token id="21" string="authors" />
          </tokens>
        </chunking>
        <chunking id="10" string="Its critics" type="NP">
          <tokens>
            <token id="1" string="Its" />
            <token id="2" string="critics" />
          </tokens>
        </chunking>
        <chunking id="11" string="a disservice" type="NP">
          <tokens>
            <token id="23" string="a" />
            <token id="24" string="disservice" />
          </tokens>
        </chunking>
        <chunking id="12" string="distorts the annual fiction industry" type="VP">
          <tokens>
            <token id="8" string="distorts" />
            <token id="9" string="the" />
            <token id="10" string="annual" />
            <token id="11" string="fiction" />
            <token id="12" string="industry" />
          </tokens>
        </chunking>
        <chunking id="13" string="other gifted but unlisted writers" type="NP">
          <tokens>
            <token id="29" string="other" />
            <token id="30" string="gifted" />
            <token id="31" string="but" />
            <token id="32" string="unlisted" />
            <token id="33" string="writers" />
          </tokens>
        </chunking>
        <chunking id="14" string="the annual fiction industry" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="annual" />
            <token id="11" string="fiction" />
            <token id="12" string="industry" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="2">critics</governor>
          <dependent id="1">Its</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="3">say</governor>
          <dependent id="2">critics</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">say</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="8">distorts</governor>
          <dependent id="4">that</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">brouhaha</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">brouhaha</governor>
          <dependent id="6">autumn</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">distorts</governor>
          <dependent id="7">brouhaha</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">say</governor>
          <dependent id="8">distorts</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">industry</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">industry</governor>
          <dependent id="10">annual</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">industry</governor>
          <dependent id="11">fiction</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="8">distorts</governor>
          <dependent id="12">industry</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="3">say</governor>
          <dependent id="13">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="17">hype</governor>
          <dependent id="14">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="17">hype</governor>
          <dependent id="15">intense</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="17">hype</governor>
          <dependent id="16">media</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">does</governor>
          <dependent id="17">hype</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">authors</governor>
          <dependent id="18">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="21">authors</governor>
          <dependent id="19">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="21">authors</governor>
          <dependent id="20">shortlisted</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">hype</governor>
          <dependent id="21">authors</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="3">say</governor>
          <dependent id="22">does</dependent>
        </dependency>
        <dependency type="det">
          <governor id="24">disservice</governor>
          <dependent id="23">a</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="22">does</governor>
          <dependent id="24">disservice</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">hordes</governor>
          <dependent id="25">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="27">hordes</governor>
          <dependent id="26">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="22">does</governor>
          <dependent id="27">hordes</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">gifted</governor>
          <dependent id="28">of</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="30">gifted</governor>
          <dependent id="29">other</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">hordes</governor>
          <dependent id="30">gifted</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="30">gifted</governor>
          <dependent id="31">but</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="33">writers</governor>
          <dependent id="32">unlisted</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="30">gifted</governor>
          <dependent id="33">writers</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="autumn" type="DATE" score="0.0">
          <tokens>
            <token id="6" string="autumn" />
          </tokens>
        </entity>
        <entity id="2" string="annual" type="SET" score="0.0">
          <tokens>
            <token id="10" string="annual" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="4" has_coreference="true">
      <content>This year&amp;apost;s shortlist contains no surprises.</content>
      <tokens>
        <token id="1" string="This" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="2" string="year" lemma="year" stem="year" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="3" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="shortlist" lemma="shortlist" stem="shortlist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="contains" lemma="contain" stem="contain" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="no" lemma="no" stem="no" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="surprises" lemma="surprise" stem="surpris" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT This) (NN year) (POS 's)) (NN shortlist)) (VP (VBZ contains) (NP (DT no) (NNS surprises))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="This year 's" type="NP">
          <tokens>
            <token id="1" string="This" />
            <token id="2" string="year" />
            <token id="3" string="'s" />
          </tokens>
        </chunking>
        <chunking id="2" string="contains no surprises" type="VP">
          <tokens>
            <token id="5" string="contains" />
            <token id="6" string="no" />
            <token id="7" string="surprises" />
          </tokens>
        </chunking>
        <chunking id="3" string="This year 's shortlist" type="NP">
          <tokens>
            <token id="1" string="This" />
            <token id="2" string="year" />
            <token id="3" string="'s" />
            <token id="4" string="shortlist" />
          </tokens>
        </chunking>
        <chunking id="4" string="no surprises" type="NP">
          <tokens>
            <token id="6" string="no" />
            <token id="7" string="surprises" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">year</governor>
          <dependent id="1">This</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="4">shortlist</governor>
          <dependent id="2">year</dependent>
        </dependency>
        <dependency type="case">
          <governor id="2">year</governor>
          <dependent id="3">'s</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">contains</governor>
          <dependent id="4">shortlist</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">contains</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="7">surprises</governor>
          <dependent id="6">no</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">contains</governor>
          <dependent id="7">surprises</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="This year" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="This" />
            <token id="2" string="year" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="5" has_coreference="false">
      <content>London&amp;apost;s litterati have for weeks been betting on Michael Ondaatje&amp;apost;s The English Patient - those of them who have seen a proof copy because it is not officially published until next week.</content>
      <tokens>
        <token id="1" string="London" lemma="London" stem="london" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="2" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="3" string="litterati" lemma="litteratus" stem="litterati" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="have" lemma="have" stem="have" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="weeks" lemma="week" stem="week" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="7" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="betting" lemma="bet" stem="bet" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="Michael" lemma="Michael" stem="michael" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="11" string="Ondaatje" lemma="Ondaatje" stem="ondaatj" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="12" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="English" lemma="english" stem="english" pos="JJ" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="15" string="Patient" lemma="patient" stem="patient" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="-" lemma="-" stem="-" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="those" lemma="those" stem="those" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="them" lemma="they" stem="them" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="have" lemma="have" stem="have" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="seen" lemma="see" stem="seen" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="24" string="proof" lemma="proof" stem="proof" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="25" string="copy" lemma="copy" stem="copi" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="26" string="because" lemma="because" stem="becaus" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="officially" lemma="officially" stem="offici" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="published" lemma="publish" stem="publish" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="until" lemma="until" stem="until" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="next" lemma="next" stem="next" pos="JJ" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="34" string="week" lemma="week" stem="week" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="35" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP London) (POS 's)) (NNS litterati)) (VP (VBP have) (PP (IN for) (NP (NNS weeks))) (VP (VBN been) (VP (VBG betting) (PP (IN on) (NP (NP (NNP Michael) (NNP Ondaatje) (POS 's)) (NP (NP (DT The) (JJ English) (NN Patient)) (: -) (NP (NP (DT those)) (PP (IN of) (NP (PRP them))) (SBAR (WHNP (WP who)) (S (VP (VBP have) (VP (VBN seen) (NP (DT a) (NN proof) (NN copy)) (SBAR (IN because) (S (NP (PRP it)) (VP (VBZ is) (RB not) (VP (ADVP (RB officially)) (VBN published) (PP (IN until) (NP (JJ next) (NN week)))))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Michael Ondaatje 's The English Patient - those of them who have seen a proof copy because it is not officially published until next week" type="NP">
          <tokens>
            <token id="10" string="Michael" />
            <token id="11" string="Ondaatje" />
            <token id="12" string="'s" />
            <token id="13" string="The" />
            <token id="14" string="English" />
            <token id="15" string="Patient" />
            <token id="16" string="-" />
            <token id="17" string="those" />
            <token id="18" string="of" />
            <token id="19" string="them" />
            <token id="20" string="who" />
            <token id="21" string="have" />
            <token id="22" string="seen" />
            <token id="23" string="a" />
            <token id="24" string="proof" />
            <token id="25" string="copy" />
            <token id="26" string="because" />
            <token id="27" string="it" />
            <token id="28" string="is" />
            <token id="29" string="not" />
            <token id="30" string="officially" />
            <token id="31" string="published" />
            <token id="32" string="until" />
            <token id="33" string="next" />
            <token id="34" string="week" />
          </tokens>
        </chunking>
        <chunking id="2" string="Michael Ondaatje 's" type="NP">
          <tokens>
            <token id="10" string="Michael" />
            <token id="11" string="Ondaatje" />
            <token id="12" string="'s" />
          </tokens>
        </chunking>
        <chunking id="3" string="London 's litterati" type="NP">
          <tokens>
            <token id="1" string="London" />
            <token id="2" string="'s" />
            <token id="3" string="litterati" />
          </tokens>
        </chunking>
        <chunking id="4" string="weeks" type="NP">
          <tokens>
            <token id="6" string="weeks" />
          </tokens>
        </chunking>
        <chunking id="5" string="seen a proof copy because it is not officially published until next week" type="VP">
          <tokens>
            <token id="22" string="seen" />
            <token id="23" string="a" />
            <token id="24" string="proof" />
            <token id="25" string="copy" />
            <token id="26" string="because" />
            <token id="27" string="it" />
            <token id="28" string="is" />
            <token id="29" string="not" />
            <token id="30" string="officially" />
            <token id="31" string="published" />
            <token id="32" string="until" />
            <token id="33" string="next" />
            <token id="34" string="week" />
          </tokens>
        </chunking>
        <chunking id="6" string="London 's" type="NP">
          <tokens>
            <token id="1" string="London" />
            <token id="2" string="'s" />
          </tokens>
        </chunking>
        <chunking id="7" string="it" type="NP">
          <tokens>
            <token id="27" string="it" />
          </tokens>
        </chunking>
        <chunking id="8" string="who have seen a proof copy because it is not officially published until next week" type="SBAR">
          <tokens>
            <token id="20" string="who" />
            <token id="21" string="have" />
            <token id="22" string="seen" />
            <token id="23" string="a" />
            <token id="24" string="proof" />
            <token id="25" string="copy" />
            <token id="26" string="because" />
            <token id="27" string="it" />
            <token id="28" string="is" />
            <token id="29" string="not" />
            <token id="30" string="officially" />
            <token id="31" string="published" />
            <token id="32" string="until" />
            <token id="33" string="next" />
            <token id="34" string="week" />
          </tokens>
        </chunking>
        <chunking id="9" string="because it is not officially published until next week" type="SBAR">
          <tokens>
            <token id="26" string="because" />
            <token id="27" string="it" />
            <token id="28" string="is" />
            <token id="29" string="not" />
            <token id="30" string="officially" />
            <token id="31" string="published" />
            <token id="32" string="until" />
            <token id="33" string="next" />
            <token id="34" string="week" />
          </tokens>
        </chunking>
        <chunking id="10" string="them" type="NP">
          <tokens>
            <token id="19" string="them" />
          </tokens>
        </chunking>
        <chunking id="11" string="officially published until next week" type="VP">
          <tokens>
            <token id="30" string="officially" />
            <token id="31" string="published" />
            <token id="32" string="until" />
            <token id="33" string="next" />
            <token id="34" string="week" />
          </tokens>
        </chunking>
        <chunking id="12" string="have for weeks been betting on Michael Ondaatje 's The English Patient - those of them who have seen a proof copy because it is not officially published until next week" type="VP">
          <tokens>
            <token id="4" string="have" />
            <token id="5" string="for" />
            <token id="6" string="weeks" />
            <token id="7" string="been" />
            <token id="8" string="betting" />
            <token id="9" string="on" />
            <token id="10" string="Michael" />
            <token id="11" string="Ondaatje" />
            <token id="12" string="'s" />
            <token id="13" string="The" />
            <token id="14" string="English" />
            <token id="15" string="Patient" />
            <token id="16" string="-" />
            <token id="17" string="those" />
            <token id="18" string="of" />
            <token id="19" string="them" />
            <token id="20" string="who" />
            <token id="21" string="have" />
            <token id="22" string="seen" />
            <token id="23" string="a" />
            <token id="24" string="proof" />
            <token id="25" string="copy" />
            <token id="26" string="because" />
            <token id="27" string="it" />
            <token id="28" string="is" />
            <token id="29" string="not" />
            <token id="30" string="officially" />
            <token id="31" string="published" />
            <token id="32" string="until" />
            <token id="33" string="next" />
            <token id="34" string="week" />
          </tokens>
        </chunking>
        <chunking id="13" string="been betting on Michael Ondaatje 's The English Patient - those of them who have seen a proof copy because it is not officially published until next week" type="VP">
          <tokens>
            <token id="7" string="been" />
            <token id="8" string="betting" />
            <token id="9" string="on" />
            <token id="10" string="Michael" />
            <token id="11" string="Ondaatje" />
            <token id="12" string="'s" />
            <token id="13" string="The" />
            <token id="14" string="English" />
            <token id="15" string="Patient" />
            <token id="16" string="-" />
            <token id="17" string="those" />
            <token id="18" string="of" />
            <token id="19" string="them" />
            <token id="20" string="who" />
            <token id="21" string="have" />
            <token id="22" string="seen" />
            <token id="23" string="a" />
            <token id="24" string="proof" />
            <token id="25" string="copy" />
            <token id="26" string="because" />
            <token id="27" string="it" />
            <token id="28" string="is" />
            <token id="29" string="not" />
            <token id="30" string="officially" />
            <token id="31" string="published" />
            <token id="32" string="until" />
            <token id="33" string="next" />
            <token id="34" string="week" />
          </tokens>
        </chunking>
        <chunking id="14" string="The English Patient" type="NP">
          <tokens>
            <token id="13" string="The" />
            <token id="14" string="English" />
            <token id="15" string="Patient" />
          </tokens>
        </chunking>
        <chunking id="15" string="next week" type="NP">
          <tokens>
            <token id="33" string="next" />
            <token id="34" string="week" />
          </tokens>
        </chunking>
        <chunking id="16" string="a proof copy" type="NP">
          <tokens>
            <token id="23" string="a" />
            <token id="24" string="proof" />
            <token id="25" string="copy" />
          </tokens>
        </chunking>
        <chunking id="17" string="The English Patient - those of them who have seen a proof copy because it is not officially published until next week" type="NP">
          <tokens>
            <token id="13" string="The" />
            <token id="14" string="English" />
            <token id="15" string="Patient" />
            <token id="16" string="-" />
            <token id="17" string="those" />
            <token id="18" string="of" />
            <token id="19" string="them" />
            <token id="20" string="who" />
            <token id="21" string="have" />
            <token id="22" string="seen" />
            <token id="23" string="a" />
            <token id="24" string="proof" />
            <token id="25" string="copy" />
            <token id="26" string="because" />
            <token id="27" string="it" />
            <token id="28" string="is" />
            <token id="29" string="not" />
            <token id="30" string="officially" />
            <token id="31" string="published" />
            <token id="32" string="until" />
            <token id="33" string="next" />
            <token id="34" string="week" />
          </tokens>
        </chunking>
        <chunking id="18" string="have seen a proof copy because it is not officially published until next week" type="VP">
          <tokens>
            <token id="21" string="have" />
            <token id="22" string="seen" />
            <token id="23" string="a" />
            <token id="24" string="proof" />
            <token id="25" string="copy" />
            <token id="26" string="because" />
            <token id="27" string="it" />
            <token id="28" string="is" />
            <token id="29" string="not" />
            <token id="30" string="officially" />
            <token id="31" string="published" />
            <token id="32" string="until" />
            <token id="33" string="next" />
            <token id="34" string="week" />
          </tokens>
        </chunking>
        <chunking id="19" string="those of them who have seen a proof copy because it is not officially published until next week" type="NP">
          <tokens>
            <token id="17" string="those" />
            <token id="18" string="of" />
            <token id="19" string="them" />
            <token id="20" string="who" />
            <token id="21" string="have" />
            <token id="22" string="seen" />
            <token id="23" string="a" />
            <token id="24" string="proof" />
            <token id="25" string="copy" />
            <token id="26" string="because" />
            <token id="27" string="it" />
            <token id="28" string="is" />
            <token id="29" string="not" />
            <token id="30" string="officially" />
            <token id="31" string="published" />
            <token id="32" string="until" />
            <token id="33" string="next" />
            <token id="34" string="week" />
          </tokens>
        </chunking>
        <chunking id="20" string="betting on Michael Ondaatje 's The English Patient - those of them who have seen a proof copy because it is not officially published until next week" type="VP">
          <tokens>
            <token id="8" string="betting" />
            <token id="9" string="on" />
            <token id="10" string="Michael" />
            <token id="11" string="Ondaatje" />
            <token id="12" string="'s" />
            <token id="13" string="The" />
            <token id="14" string="English" />
            <token id="15" string="Patient" />
            <token id="16" string="-" />
            <token id="17" string="those" />
            <token id="18" string="of" />
            <token id="19" string="them" />
            <token id="20" string="who" />
            <token id="21" string="have" />
            <token id="22" string="seen" />
            <token id="23" string="a" />
            <token id="24" string="proof" />
            <token id="25" string="copy" />
            <token id="26" string="because" />
            <token id="27" string="it" />
            <token id="28" string="is" />
            <token id="29" string="not" />
            <token id="30" string="officially" />
            <token id="31" string="published" />
            <token id="32" string="until" />
            <token id="33" string="next" />
            <token id="34" string="week" />
          </tokens>
        </chunking>
        <chunking id="21" string="is not officially published until next week" type="VP">
          <tokens>
            <token id="28" string="is" />
            <token id="29" string="not" />
            <token id="30" string="officially" />
            <token id="31" string="published" />
            <token id="32" string="until" />
            <token id="33" string="next" />
            <token id="34" string="week" />
          </tokens>
        </chunking>
        <chunking id="22" string="those" type="NP">
          <tokens>
            <token id="17" string="those" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="3">litterati</governor>
          <dependent id="1">London</dependent>
        </dependency>
        <dependency type="case">
          <governor id="1">London</governor>
          <dependent id="2">'s</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">betting</governor>
          <dependent id="3">litterati</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="8">betting</governor>
          <dependent id="4">have</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">weeks</governor>
          <dependent id="5">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">betting</governor>
          <dependent id="6">weeks</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="8">betting</governor>
          <dependent id="7">been</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="8">betting</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">Ondaatje</governor>
          <dependent id="9">on</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">Ondaatje</governor>
          <dependent id="10">Michael</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">betting</governor>
          <dependent id="11">Ondaatje</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">Ondaatje</governor>
          <dependent id="12">'s</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">Patient</governor>
          <dependent id="13">The</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">Patient</governor>
          <dependent id="14">English</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="11">Ondaatje</governor>
          <dependent id="15">Patient</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="15">Patient</governor>
          <dependent id="17">those</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">them</governor>
          <dependent id="18">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">those</governor>
          <dependent id="19">them</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">seen</governor>
          <dependent id="20">who</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="22">seen</governor>
          <dependent id="21">have</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="17">those</governor>
          <dependent id="22">seen</dependent>
        </dependency>
        <dependency type="det">
          <governor id="25">copy</governor>
          <dependent id="23">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">copy</governor>
          <dependent id="24">proof</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="22">seen</governor>
          <dependent id="25">copy</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="31">published</governor>
          <dependent id="26">because</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="31">published</governor>
          <dependent id="27">it</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="31">published</governor>
          <dependent id="28">is</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="31">published</governor>
          <dependent id="29">not</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="31">published</governor>
          <dependent id="30">officially</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="22">seen</governor>
          <dependent id="31">published</dependent>
        </dependency>
        <dependency type="case">
          <governor id="34">week</governor>
          <dependent id="32">until</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="34">week</governor>
          <dependent id="33">next</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="31">published</governor>
          <dependent id="34">week</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="weeks" type="DURATION" score="0.0">
          <tokens>
            <token id="6" string="weeks" />
          </tokens>
        </entity>
        <entity id="2" string="next week" type="DATE" score="0.0">
          <tokens>
            <token id="33" string="next" />
            <token id="34" string="week" />
          </tokens>
        </entity>
        <entity id="3" string="London" type="LOCATION" score="0.0">
          <tokens>
            <token id="1" string="London" />
          </tokens>
        </entity>
        <entity id="4" string="Michael Ondaatje" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="Michael" />
            <token id="11" string="Ondaatje" />
          </tokens>
        </entity>
        <entity id="5" string="English" type="MISC" score="0.0">
          <tokens>
            <token id="14" string="English" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="6" has_coreference="false">
      <content>Last night the bookmakers William Hill made it the 5-2 favourite.</content>
      <tokens>
        <token id="1" string="Last" lemma="last" stem="last" pos="JJ" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="2" string="night" lemma="night" stem="night" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="bookmakers" lemma="bookmaker" stem="bookmak" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="William" lemma="William" stem="william" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="6" string="Hill" lemma="Hill" stem="hill" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="7" string="made" lemma="make" stem="made" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="5-2" lemma="5-2" stem="5-2" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="11" string="favourite" lemma="favourite" stem="favourit" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP-TMP (JJ Last) (NN night)) (NP (NP (DT the) (NNS bookmakers)) (NP (NNP William) (NNP Hill))) (VP (VBD made) (S (NP (PRP it)) (NP (DT the) (CD 5-2) (NN favourite)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the bookmakers" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="bookmakers" />
          </tokens>
        </chunking>
        <chunking id="2" string="made it the 5-2 favourite" type="VP">
          <tokens>
            <token id="7" string="made" />
            <token id="8" string="it" />
            <token id="9" string="the" />
            <token id="10" string="5-2" />
            <token id="11" string="favourite" />
          </tokens>
        </chunking>
        <chunking id="3" string="the bookmakers William Hill" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="bookmakers" />
            <token id="5" string="William" />
            <token id="6" string="Hill" />
          </tokens>
        </chunking>
        <chunking id="4" string="William Hill" type="NP">
          <tokens>
            <token id="5" string="William" />
            <token id="6" string="Hill" />
          </tokens>
        </chunking>
        <chunking id="5" string="it" type="NP">
          <tokens>
            <token id="8" string="it" />
          </tokens>
        </chunking>
        <chunking id="6" string="the 5-2 favourite" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="5-2" />
            <token id="11" string="favourite" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="amod">
          <governor id="2">night</governor>
          <dependent id="1">Last</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="7">made</governor>
          <dependent id="2">night</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">bookmakers</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">made</governor>
          <dependent id="4">bookmakers</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">Hill</governor>
          <dependent id="5">William</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="4">bookmakers</governor>
          <dependent id="6">Hill</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">made</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">favourite</governor>
          <dependent id="8">it</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">favourite</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="11">favourite</governor>
          <dependent id="10">5-2</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="7">made</governor>
          <dependent id="11">favourite</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="William Hill" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="William" />
            <token id="6" string="Hill" />
          </tokens>
        </entity>
        <entity id="2" string="5-2" type="NUMBER" score="0.0">
          <tokens>
            <token id="10" string="5-2" />
          </tokens>
        </entity>
        <entity id="3" string="Last night" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Last" />
            <token id="2" string="night" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="7" has_coreference="true">
      <content>The shortlist is: Serenity House by Christopher Hope (MacMillan Pounds 14.99).</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="shortlist" lemma="shortlist" stem="shortlist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="Serenity" lemma="Serenity" stem="seren" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="6" string="House" lemma="House" stem="hous" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="Christopher" lemma="Christopher" stem="christoph" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="9" string="Hope" lemma="Hope" stem="hope" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="10" string="(" lemma="-lrb-" stem="(" pos="-LRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="MacMillan" lemma="MacMillan" stem="macmillan" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="12" string="Pounds" lemma="Pounds" stem="pound" pos="NNPS" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="13" string="14.99" lemma="14.99" stem="14.99" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="true" />
        <token id="14" string=")" lemma="-rrb-" stem=")" pos="-RRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN shortlist)) (VP (VBZ is) (: :) (NP (NP (NNP Serenity) (NNP House)) (PP (IN by) (NP (NP (NNP Christopher) (NNP Hope)) (PRN (-LRB- -LRB-) (NP (NP (NNP MacMillan) (NNPS Pounds)) (NP (CD 14.99))) (-RRB- -RRB-)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="14.99" type="NP">
          <tokens>
            <token id="13" string="14.99" />
          </tokens>
        </chunking>
        <chunking id="2" string="The shortlist" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="shortlist" />
          </tokens>
        </chunking>
        <chunking id="3" string="Christopher Hope" type="NP">
          <tokens>
            <token id="8" string="Christopher" />
            <token id="9" string="Hope" />
          </tokens>
        </chunking>
        <chunking id="4" string="MacMillan Pounds 14.99" type="NP">
          <tokens>
            <token id="11" string="MacMillan" />
            <token id="12" string="Pounds" />
            <token id="13" string="14.99" />
          </tokens>
        </chunking>
        <chunking id="5" string="is : Serenity House by Christopher Hope -LRB- MacMillan Pounds 14.99 -RRB-" type="VP">
          <tokens>
            <token id="3" string="is" />
            <token id="4" string=":" />
            <token id="5" string="Serenity" />
            <token id="6" string="House" />
            <token id="7" string="by" />
            <token id="8" string="Christopher" />
            <token id="9" string="Hope" />
            <token id="10" string="(" />
            <token id="11" string="MacMillan" />
            <token id="12" string="Pounds" />
            <token id="13" string="14.99" />
            <token id="14" string=")" />
          </tokens>
        </chunking>
        <chunking id="6" string="Serenity House" type="NP">
          <tokens>
            <token id="5" string="Serenity" />
            <token id="6" string="House" />
          </tokens>
        </chunking>
        <chunking id="7" string="Christopher Hope -LRB- MacMillan Pounds 14.99 -RRB-" type="NP">
          <tokens>
            <token id="8" string="Christopher" />
            <token id="9" string="Hope" />
            <token id="10" string="(" />
            <token id="11" string="MacMillan" />
            <token id="12" string="Pounds" />
            <token id="13" string="14.99" />
            <token id="14" string=")" />
          </tokens>
        </chunking>
        <chunking id="8" string="Serenity House by Christopher Hope -LRB- MacMillan Pounds 14.99 -RRB-" type="NP">
          <tokens>
            <token id="5" string="Serenity" />
            <token id="6" string="House" />
            <token id="7" string="by" />
            <token id="8" string="Christopher" />
            <token id="9" string="Hope" />
            <token id="10" string="(" />
            <token id="11" string="MacMillan" />
            <token id="12" string="Pounds" />
            <token id="13" string="14.99" />
            <token id="14" string=")" />
          </tokens>
        </chunking>
        <chunking id="9" string="MacMillan Pounds" type="NP">
          <tokens>
            <token id="11" string="MacMillan" />
            <token id="12" string="Pounds" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">shortlist</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">House</governor>
          <dependent id="2">shortlist</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="6">House</governor>
          <dependent id="3">is</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">House</governor>
          <dependent id="5">Serenity</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">House</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">Hope</governor>
          <dependent id="7">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">Hope</governor>
          <dependent id="8">Christopher</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">House</governor>
          <dependent id="9">Hope</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">Pounds</governor>
          <dependent id="11">MacMillan</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="9">Hope</governor>
          <dependent id="12">Pounds</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="12">Pounds</governor>
          <dependent id="13">14.99</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="14.99" type="NUMBER" score="0.0">
          <tokens>
            <token id="13" string="14.99" />
          </tokens>
        </entity>
        <entity id="2" string="Christopher Hope" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Christopher" />
            <token id="9" string="Hope" />
          </tokens>
        </entity>
        <entity id="3" string="MacMillan Pounds" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="11" string="MacMillan" />
            <token id="12" string="Pounds" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="8" has_coreference="false">
      <content>Black comedy by a master South Africa-born specialist in the trade.</content>
      <tokens>
        <token id="1" string="Black" lemma="black" stem="black" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="comedy" lemma="comedy" stem="comedi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="master" lemma="master" stem="master" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="South" lemma="South" stem="south" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="7" string="Africa-born" lemma="Africa-born" stem="africa-born" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="8" string="specialist" lemma="specialist" stem="specialist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="trade" lemma="trade" stem="trade" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (JJ Black) (NN comedy)) (PP (IN by) (NP (DT a) (NN master) (NNP South) (NNP Africa-born) (NN specialist))) (PP (IN in) (NP (DT the) (NN trade))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Black comedy by a master South Africa-born specialist in the trade ." type="NP">
          <tokens>
            <token id="1" string="Black" />
            <token id="2" string="comedy" />
            <token id="3" string="by" />
            <token id="4" string="a" />
            <token id="5" string="master" />
            <token id="6" string="South" />
            <token id="7" string="Africa-born" />
            <token id="8" string="specialist" />
            <token id="9" string="in" />
            <token id="10" string="the" />
            <token id="11" string="trade" />
            <token id="12" string="." />
          </tokens>
        </chunking>
        <chunking id="2" string="the trade" type="NP">
          <tokens>
            <token id="10" string="the" />
            <token id="11" string="trade" />
          </tokens>
        </chunking>
        <chunking id="3" string="a master South Africa-born specialist" type="NP">
          <tokens>
            <token id="4" string="a" />
            <token id="5" string="master" />
            <token id="6" string="South" />
            <token id="7" string="Africa-born" />
            <token id="8" string="specialist" />
          </tokens>
        </chunking>
        <chunking id="4" string="Black comedy" type="NP">
          <tokens>
            <token id="1" string="Black" />
            <token id="2" string="comedy" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="amod">
          <governor id="2">comedy</governor>
          <dependent id="1">Black</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">comedy</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">specialist</governor>
          <dependent id="3">by</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">specialist</governor>
          <dependent id="4">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">specialist</governor>
          <dependent id="5">master</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">specialist</governor>
          <dependent id="6">South</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">specialist</governor>
          <dependent id="7">Africa-born</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">comedy</governor>
          <dependent id="8">specialist</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">trade</governor>
          <dependent id="9">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">trade</governor>
          <dependent id="10">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">comedy</governor>
          <dependent id="11">trade</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="South Africa-born" type="LOCATION" score="0.0">
          <tokens>
            <token id="6" string="South" />
            <token id="7" string="Africa-born" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="9" has_coreference="false">
      <content>Set in a Highgate &amp;apost;eventide refuge&amp;apost; where euthanasia is an unspoken word and its senior citizen, Max Montfalcon, turns out to be a geriatric Nazi scientist.</content>
      <tokens>
        <token id="1" string="Set" lemma="set" stem="set" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="Highgate" lemma="Highgate" stem="highgat" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="5" string="'" lemma="`" stem="'" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="eventide" lemma="eventide" stem="eventid" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="refuge" lemma="refuge" stem="refug" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="where" lemma="where" stem="where" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="euthanasia" lemma="euthanasia" stem="euthanasia" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="unspoken" lemma="unspoken" stem="unspoken" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string="word" lemma="word" stem="word" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="its" lemma="its" stem="it" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="senior" lemma="senior" stem="senior" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="citizen" lemma="citizen" stem="citizen" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="Max" lemma="Max" stem="max" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="21" string="Montfalcon" lemma="Montfalcon" stem="montfalcon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="22" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="turns" lemma="turn" stem="turn" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="out" lemma="out" stem="out" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="be" lemma="be" stem="be" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="geriatric" lemma="geriatric" stem="geriatr" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="Nazi" lemma="nazi" stem="nazi" pos="JJ" type="Word" isStopWord="false" ner="IDEOLOGY" is_referenced="false" is_refers="false" />
        <token id="30" string="scientist" lemma="scientist" stem="scientist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (VP (VB Set) (PP (IN in) (NP (NP (DT a) (NNP Highgate) (`` `) (NN eventide) (NN refuge) ('' ')) (SBAR (WHADVP (WRB where)) (S (S (NP (NN euthanasia)) (VP (VBZ is) (NP (DT an) (JJ unspoken) (NN word)))) (CC and) (S (NP (NP (PRP$ its) (JJ senior) (NN citizen)) (, ,) (NP (NNP Max) (NNP Montfalcon)) (, ,)) (VP (VBZ turns) (PRT (RP out)) (S (VP (TO to) (VP (VB be) (NP (DT a) (JJ geriatric) (JJ Nazi) (NN scientist)))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="where euthanasia is an unspoken word and its senior citizen , Max Montfalcon , turns out to be a geriatric Nazi scientist" type="SBAR">
          <tokens>
            <token id="9" string="where" />
            <token id="10" string="euthanasia" />
            <token id="11" string="is" />
            <token id="12" string="an" />
            <token id="13" string="unspoken" />
            <token id="14" string="word" />
            <token id="15" string="and" />
            <token id="16" string="its" />
            <token id="17" string="senior" />
            <token id="18" string="citizen" />
            <token id="19" string="," />
            <token id="20" string="Max" />
            <token id="21" string="Montfalcon" />
            <token id="22" string="," />
            <token id="23" string="turns" />
            <token id="24" string="out" />
            <token id="25" string="to" />
            <token id="26" string="be" />
            <token id="27" string="a" />
            <token id="28" string="geriatric" />
            <token id="29" string="Nazi" />
            <token id="30" string="scientist" />
          </tokens>
        </chunking>
        <chunking id="2" string="is an unspoken word" type="VP">
          <tokens>
            <token id="11" string="is" />
            <token id="12" string="an" />
            <token id="13" string="unspoken" />
            <token id="14" string="word" />
          </tokens>
        </chunking>
        <chunking id="3" string="to be a geriatric Nazi scientist" type="VP">
          <tokens>
            <token id="25" string="to" />
            <token id="26" string="be" />
            <token id="27" string="a" />
            <token id="28" string="geriatric" />
            <token id="29" string="Nazi" />
            <token id="30" string="scientist" />
          </tokens>
        </chunking>
        <chunking id="4" string="its senior citizen , Max Montfalcon ," type="NP">
          <tokens>
            <token id="16" string="its" />
            <token id="17" string="senior" />
            <token id="18" string="citizen" />
            <token id="19" string="," />
            <token id="20" string="Max" />
            <token id="21" string="Montfalcon" />
            <token id="22" string="," />
          </tokens>
        </chunking>
        <chunking id="5" string="euthanasia" type="NP">
          <tokens>
            <token id="10" string="euthanasia" />
          </tokens>
        </chunking>
        <chunking id="6" string="turns out to be a geriatric Nazi scientist" type="VP">
          <tokens>
            <token id="23" string="turns" />
            <token id="24" string="out" />
            <token id="25" string="to" />
            <token id="26" string="be" />
            <token id="27" string="a" />
            <token id="28" string="geriatric" />
            <token id="29" string="Nazi" />
            <token id="30" string="scientist" />
          </tokens>
        </chunking>
        <chunking id="7" string="a geriatric Nazi scientist" type="NP">
          <tokens>
            <token id="27" string="a" />
            <token id="28" string="geriatric" />
            <token id="29" string="Nazi" />
            <token id="30" string="scientist" />
          </tokens>
        </chunking>
        <chunking id="8" string="an unspoken word" type="NP">
          <tokens>
            <token id="12" string="an" />
            <token id="13" string="unspoken" />
            <token id="14" string="word" />
          </tokens>
        </chunking>
        <chunking id="9" string="Max Montfalcon" type="NP">
          <tokens>
            <token id="20" string="Max" />
            <token id="21" string="Montfalcon" />
          </tokens>
        </chunking>
        <chunking id="10" string="a Highgate ` eventide refuge ' where euthanasia is an unspoken word and its senior citizen , Max Montfalcon , turns out to be a geriatric Nazi scientist" type="NP">
          <tokens>
            <token id="3" string="a" />
            <token id="4" string="Highgate" />
            <token id="5" string="'" />
            <token id="6" string="eventide" />
            <token id="7" string="refuge" />
            <token id="8" string="'" />
            <token id="9" string="where" />
            <token id="10" string="euthanasia" />
            <token id="11" string="is" />
            <token id="12" string="an" />
            <token id="13" string="unspoken" />
            <token id="14" string="word" />
            <token id="15" string="and" />
            <token id="16" string="its" />
            <token id="17" string="senior" />
            <token id="18" string="citizen" />
            <token id="19" string="," />
            <token id="20" string="Max" />
            <token id="21" string="Montfalcon" />
            <token id="22" string="," />
            <token id="23" string="turns" />
            <token id="24" string="out" />
            <token id="25" string="to" />
            <token id="26" string="be" />
            <token id="27" string="a" />
            <token id="28" string="geriatric" />
            <token id="29" string="Nazi" />
            <token id="30" string="scientist" />
          </tokens>
        </chunking>
        <chunking id="11" string="its senior citizen" type="NP">
          <tokens>
            <token id="16" string="its" />
            <token id="17" string="senior" />
            <token id="18" string="citizen" />
          </tokens>
        </chunking>
        <chunking id="12" string="be a geriatric Nazi scientist" type="VP">
          <tokens>
            <token id="26" string="be" />
            <token id="27" string="a" />
            <token id="28" string="geriatric" />
            <token id="29" string="Nazi" />
            <token id="30" string="scientist" />
          </tokens>
        </chunking>
        <chunking id="13" string="where" type="WHADVP">
          <tokens>
            <token id="9" string="where" />
          </tokens>
        </chunking>
        <chunking id="14" string="Set in a Highgate ` eventide refuge ' where euthanasia is an unspoken word and its senior citizen , Max Montfalcon , turns out to be a geriatric Nazi scientist" type="VP">
          <tokens>
            <token id="1" string="Set" />
            <token id="2" string="in" />
            <token id="3" string="a" />
            <token id="4" string="Highgate" />
            <token id="5" string="'" />
            <token id="6" string="eventide" />
            <token id="7" string="refuge" />
            <token id="8" string="'" />
            <token id="9" string="where" />
            <token id="10" string="euthanasia" />
            <token id="11" string="is" />
            <token id="12" string="an" />
            <token id="13" string="unspoken" />
            <token id="14" string="word" />
            <token id="15" string="and" />
            <token id="16" string="its" />
            <token id="17" string="senior" />
            <token id="18" string="citizen" />
            <token id="19" string="," />
            <token id="20" string="Max" />
            <token id="21" string="Montfalcon" />
            <token id="22" string="," />
            <token id="23" string="turns" />
            <token id="24" string="out" />
            <token id="25" string="to" />
            <token id="26" string="be" />
            <token id="27" string="a" />
            <token id="28" string="geriatric" />
            <token id="29" string="Nazi" />
            <token id="30" string="scientist" />
          </tokens>
        </chunking>
        <chunking id="15" string="a Highgate ` eventide refuge '" type="NP">
          <tokens>
            <token id="3" string="a" />
            <token id="4" string="Highgate" />
            <token id="5" string="'" />
            <token id="6" string="eventide" />
            <token id="7" string="refuge" />
            <token id="8" string="'" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Set</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">refuge</governor>
          <dependent id="2">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">refuge</governor>
          <dependent id="3">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">refuge</governor>
          <dependent id="4">Highgate</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">refuge</governor>
          <dependent id="6">eventide</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="1">Set</governor>
          <dependent id="7">refuge</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="14">word</governor>
          <dependent id="9">where</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">word</governor>
          <dependent id="10">euthanasia</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="14">word</governor>
          <dependent id="11">is</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">word</governor>
          <dependent id="12">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="14">word</governor>
          <dependent id="13">unspoken</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="7">refuge</governor>
          <dependent id="14">word</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="14">word</governor>
          <dependent id="15">and</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="18">citizen</governor>
          <dependent id="16">its</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="18">citizen</governor>
          <dependent id="17">senior</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="23">turns</governor>
          <dependent id="18">citizen</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="21">Montfalcon</governor>
          <dependent id="20">Max</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="18">citizen</governor>
          <dependent id="21">Montfalcon</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="14">word</governor>
          <dependent id="23">turns</dependent>
        </dependency>
        <dependency type="compound:prt">
          <governor id="23">turns</governor>
          <dependent id="24">out</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="30">scientist</governor>
          <dependent id="25">to</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="30">scientist</governor>
          <dependent id="26">be</dependent>
        </dependency>
        <dependency type="det">
          <governor id="30">scientist</governor>
          <dependent id="27">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="30">scientist</governor>
          <dependent id="28">geriatric</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="30">scientist</governor>
          <dependent id="29">Nazi</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="23">turns</governor>
          <dependent id="30">scientist</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Max Montfalcon" type="PERSON" score="0.0">
          <tokens>
            <token id="20" string="Max" />
            <token id="21" string="Montfalcon" />
          </tokens>
        </entity>
        <entity id="2" string="Highgate" type="LOCATION" score="0.0">
          <tokens>
            <token id="4" string="Highgate" />
          </tokens>
        </entity>
        <entity id="3" string="Nazi" type="IDEOLOGY" score="0.0">
          <tokens>
            <token id="29" string="Nazi" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="10" has_coreference="true">
      <content>The FT review next Saturday will describe it as &amp;apost;surreal farce .</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="FT" lemma="FT" stem="ft" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="review" lemma="review" stem="review" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="next" lemma="next" stem="next" pos="JJ" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="5" string="Saturday" lemma="Saturday" stem="saturdai" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="6" string="will" lemma="will" stem="will" pos="MD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="describe" lemma="describe" stem="describ" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="'" lemma="`" stem="'" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="surreal" lemma="surreal" stem="surreal" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="farce" lemma="farce" stem="farc" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NNP FT) (NN review)) (NP-TMP (JJ next) (NNP Saturday)) (VP (MD will) (VP (VB describe) (NP (PRP it)) (PP (IN as) (`` `) (NP (JJ surreal) (NN farce))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="will describe it as ` surreal farce" type="VP">
          <tokens>
            <token id="6" string="will" />
            <token id="7" string="describe" />
            <token id="8" string="it" />
            <token id="9" string="as" />
            <token id="10" string="'" />
            <token id="11" string="surreal" />
            <token id="12" string="farce" />
          </tokens>
        </chunking>
        <chunking id="2" string="surreal farce" type="NP">
          <tokens>
            <token id="11" string="surreal" />
            <token id="12" string="farce" />
          </tokens>
        </chunking>
        <chunking id="3" string="describe it as ` surreal farce" type="VP">
          <tokens>
            <token id="7" string="describe" />
            <token id="8" string="it" />
            <token id="9" string="as" />
            <token id="10" string="'" />
            <token id="11" string="surreal" />
            <token id="12" string="farce" />
          </tokens>
        </chunking>
        <chunking id="4" string="it" type="NP">
          <tokens>
            <token id="8" string="it" />
          </tokens>
        </chunking>
        <chunking id="5" string="The FT review" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="FT" />
            <token id="3" string="review" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">review</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">review</governor>
          <dependent id="2">FT</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">describe</governor>
          <dependent id="3">review</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="5">Saturday</governor>
          <dependent id="4">next</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="7">describe</governor>
          <dependent id="5">Saturday</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="7">describe</governor>
          <dependent id="6">will</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">describe</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">describe</governor>
          <dependent id="8">it</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">farce</governor>
          <dependent id="9">as</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">farce</governor>
          <dependent id="11">surreal</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">describe</governor>
          <dependent id="12">farce</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="next Saturday" type="DATE" score="0.0">
          <tokens>
            <token id="4" string="next" />
            <token id="5" string="Saturday" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="11" has_coreference="false">
      <content>.</content>
      <tokens>
        <token id="1" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="." type="NP">
          <tokens>
            <token id="1" string="." />
          </tokens>
        </chunking>
      </chunkings>
    </sentence>
    <sentence id="12" has_coreference="false">
      <content>scabrously enjoyable .</content>
      <tokens>
        <token id="1" string="scabrously" lemma="scabrously" stem="scabrous" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="enjoyable" lemma="enjoyable" stem="enjoy" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (FRAG (ADJP (RB scabrously) (JJ enjoyable)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="scabrously enjoyable" type="ADJP">
          <tokens>
            <token id="1" string="scabrously" />
            <token id="2" string="enjoyable" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="2">enjoyable</governor>
          <dependent id="1">scabrously</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">enjoyable</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="13" has_coreference="false">
      <content>.&amp;apost;</content>
      <tokens>
        <token id="1" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (VP (. .) ('' '))))</syntactictree>
      <chunkings>
        <chunking id="1" string=". '" type="VP">
          <tokens>
            <token id="1" string="." />
            <token id="2" string="'" />
          </tokens>
        </chunking>
      </chunkings>
    </sentence>
    <sentence id="14" has_coreference="false">
      <content>The Butcher&amp;apost;s Boy by Patrick McCabe (Picador Pounds 14.99).</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="Butcher" lemma="Butcher" stem="butcher" pos="NNP" type="Word" isStopWord="false" ner="TITLE" is_referenced="false" is_refers="false" />
        <token id="3" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="Boy" lemma="boy" stem="boi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Patrick" lemma="Patrick" stem="patrick" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="7" string="McCabe" lemma="McCabe" stem="mccabe" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="8" string="(" lemma="-lrb-" stem="(" pos="-LRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="Picador" lemma="Picador" stem="picador" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="Pounds" lemma="Pounds" stem="pound" pos="NNPS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="14.99" lemma="14.99" stem="14.99" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="12" string=")" lemma="-rrb-" stem=")" pos="-RRB-" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NP (NP (DT The) (NNP Butcher) (POS 's)) (NN Boy)) (PP (IN by) (NP (NP (NNP Patrick) (NNP McCabe)) (PRN (-LRB- -LRB-) (NP (NP (NNP Picador) (NNPS Pounds)) (NP (CD 14.99))) (-RRB- -RRB-))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="14.99" type="NP">
          <tokens>
            <token id="11" string="14.99" />
          </tokens>
        </chunking>
        <chunking id="2" string="Picador Pounds" type="NP">
          <tokens>
            <token id="9" string="Picador" />
            <token id="10" string="Pounds" />
          </tokens>
        </chunking>
        <chunking id="3" string="Patrick McCabe -LRB- Picador Pounds 14.99 -RRB-" type="NP">
          <tokens>
            <token id="6" string="Patrick" />
            <token id="7" string="McCabe" />
            <token id="8" string="(" />
            <token id="9" string="Picador" />
            <token id="10" string="Pounds" />
            <token id="11" string="14.99" />
            <token id="12" string=")" />
          </tokens>
        </chunking>
        <chunking id="4" string="The Butcher 's Boy by Patrick McCabe -LRB- Picador Pounds 14.99 -RRB- ." type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Butcher" />
            <token id="3" string="'s" />
            <token id="4" string="Boy" />
            <token id="5" string="by" />
            <token id="6" string="Patrick" />
            <token id="7" string="McCabe" />
            <token id="8" string="(" />
            <token id="9" string="Picador" />
            <token id="10" string="Pounds" />
            <token id="11" string="14.99" />
            <token id="12" string=")" />
            <token id="13" string="." />
          </tokens>
        </chunking>
        <chunking id="5" string="The Butcher 's Boy by Patrick McCabe -LRB- Picador Pounds 14.99 -RRB-" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Butcher" />
            <token id="3" string="'s" />
            <token id="4" string="Boy" />
            <token id="5" string="by" />
            <token id="6" string="Patrick" />
            <token id="7" string="McCabe" />
            <token id="8" string="(" />
            <token id="9" string="Picador" />
            <token id="10" string="Pounds" />
            <token id="11" string="14.99" />
            <token id="12" string=")" />
          </tokens>
        </chunking>
        <chunking id="6" string="The Butcher 's" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Butcher" />
            <token id="3" string="'s" />
          </tokens>
        </chunking>
        <chunking id="7" string="Patrick McCabe" type="NP">
          <tokens>
            <token id="6" string="Patrick" />
            <token id="7" string="McCabe" />
          </tokens>
        </chunking>
        <chunking id="8" string="The Butcher 's Boy" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Butcher" />
            <token id="3" string="'s" />
            <token id="4" string="Boy" />
          </tokens>
        </chunking>
        <chunking id="9" string="Picador Pounds 14.99" type="NP">
          <tokens>
            <token id="9" string="Picador" />
            <token id="10" string="Pounds" />
            <token id="11" string="14.99" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">Butcher</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="4">Boy</governor>
          <dependent id="2">Butcher</dependent>
        </dependency>
        <dependency type="case">
          <governor id="2">Butcher</governor>
          <dependent id="3">'s</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">Boy</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">McCabe</governor>
          <dependent id="5">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">McCabe</governor>
          <dependent id="6">Patrick</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">Boy</governor>
          <dependent id="7">McCabe</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">Pounds</governor>
          <dependent id="9">Picador</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="7">McCabe</governor>
          <dependent id="10">Pounds</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="10">Pounds</governor>
          <dependent id="11">14.99</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="14.99" type="NUMBER" score="0.0">
          <tokens>
            <token id="11" string="14.99" />
          </tokens>
        </entity>
        <entity id="2" string="Butcher" type="TITLE" score="0.0">
          <tokens>
            <token id="2" string="Butcher" />
          </tokens>
        </entity>
        <entity id="3" string="Patrick McCabe" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Patrick" />
            <token id="7" string="McCabe" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="15" has_coreference="false">
      <content>An Irish murderer, a soft-in-the-head orphan jailed for life, recalls his life in grisly detail.</content>
      <tokens>
        <token id="1" string="An" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="Irish" lemma="irish" stem="irish" pos="JJ" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="true" is_refers="false" />
        <token id="3" string="murderer" lemma="murderer" stem="murder" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="soft-in-the-head" lemma="soft-in-the-head" stem="soft-in-the-head" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="orphan" lemma="orphan" stem="orphan" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="jailed" lemma="jail" stem="jail" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="life" lemma="life" stem="life" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="recalls" lemma="recall" stem="recal" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="life" lemma="life" stem="life" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="grisly" lemma="grisly" stem="grisli" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="detail" lemma="detail" stem="detail" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT An) (JJ Irish) (NN murderer)) (, ,) (NP (NP (DT a) (JJ soft-in-the-head) (NN orphan)) (VP (VBN jailed) (PP (IN for) (NP (NN life))))) (, ,)) (VP (VBZ recalls) (NP (PRP$ his) (NN life)) (PP (IN in) (NP (JJ grisly) (NN detail)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a soft-in-the-head orphan" type="NP">
          <tokens>
            <token id="5" string="a" />
            <token id="6" string="soft-in-the-head" />
            <token id="7" string="orphan" />
          </tokens>
        </chunking>
        <chunking id="2" string="his life" type="NP">
          <tokens>
            <token id="13" string="his" />
            <token id="14" string="life" />
          </tokens>
        </chunking>
        <chunking id="3" string="grisly detail" type="NP">
          <tokens>
            <token id="16" string="grisly" />
            <token id="17" string="detail" />
          </tokens>
        </chunking>
        <chunking id="4" string="An Irish murderer , a soft-in-the-head orphan jailed for life ," type="NP">
          <tokens>
            <token id="1" string="An" />
            <token id="2" string="Irish" />
            <token id="3" string="murderer" />
            <token id="4" string="," />
            <token id="5" string="a" />
            <token id="6" string="soft-in-the-head" />
            <token id="7" string="orphan" />
            <token id="8" string="jailed" />
            <token id="9" string="for" />
            <token id="10" string="life" />
            <token id="11" string="," />
          </tokens>
        </chunking>
        <chunking id="5" string="An Irish murderer" type="NP">
          <tokens>
            <token id="1" string="An" />
            <token id="2" string="Irish" />
            <token id="3" string="murderer" />
          </tokens>
        </chunking>
        <chunking id="6" string="a soft-in-the-head orphan jailed for life" type="NP">
          <tokens>
            <token id="5" string="a" />
            <token id="6" string="soft-in-the-head" />
            <token id="7" string="orphan" />
            <token id="8" string="jailed" />
            <token id="9" string="for" />
            <token id="10" string="life" />
          </tokens>
        </chunking>
        <chunking id="7" string="recalls his life in grisly detail" type="VP">
          <tokens>
            <token id="12" string="recalls" />
            <token id="13" string="his" />
            <token id="14" string="life" />
            <token id="15" string="in" />
            <token id="16" string="grisly" />
            <token id="17" string="detail" />
          </tokens>
        </chunking>
        <chunking id="8" string="life" type="NP">
          <tokens>
            <token id="10" string="life" />
          </tokens>
        </chunking>
        <chunking id="9" string="jailed for life" type="VP">
          <tokens>
            <token id="8" string="jailed" />
            <token id="9" string="for" />
            <token id="10" string="life" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">murderer</governor>
          <dependent id="1">An</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">murderer</governor>
          <dependent id="2">Irish</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">recalls</governor>
          <dependent id="3">murderer</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">orphan</governor>
          <dependent id="5">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">orphan</governor>
          <dependent id="6">soft-in-the-head</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="3">murderer</governor>
          <dependent id="7">orphan</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="7">orphan</governor>
          <dependent id="8">jailed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">life</governor>
          <dependent id="9">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">jailed</governor>
          <dependent id="10">life</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="12">recalls</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="14">life</governor>
          <dependent id="13">his</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="12">recalls</governor>
          <dependent id="14">life</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">detail</governor>
          <dependent id="15">in</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="17">detail</governor>
          <dependent id="16">grisly</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">recalls</governor>
          <dependent id="17">detail</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Irish" type="NATIONALITY" score="0.0">
          <tokens>
            <token id="2" string="Irish" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="16" has_coreference="false">
      <content>The FT reviewer said that &amp;apost;the misery and deprivation of working class life have never been so brutally evoked .</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="FT" lemma="FT" stem="ft" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="reviewer" lemma="reviewer" stem="review" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="'" lemma="`" stem="'" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="misery" lemma="misery" stem="miseri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="deprivation" lemma="deprivation" stem="depriv" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="working" lemma="work" stem="work" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="class" lemma="class" stem="class" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="life" lemma="life" stem="life" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="have" lemma="have" stem="have" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="never" lemma="never" stem="never" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="so" lemma="so" stem="so" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="brutally" lemma="brutally" stem="brutal" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="evoked" lemma="evoke" stem="evok" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NNP FT) (NN reviewer)) (VP (VBD said) (SBAR (IN that) (S (NP (NP (`` `) (DT the) (NN misery) (CC and) (NN deprivation)) (PP (IN of) (NP (VBG working) (NN class) (NN life)))) (VP (VBP have) (ADVP (RB never)) (VP (VBN been) (VP (ADVP (RB so) (RB brutally)) (VBN evoked))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="so brutally evoked" type="VP">
          <tokens>
            <token id="18" string="so" />
            <token id="19" string="brutally" />
            <token id="20" string="evoked" />
          </tokens>
        </chunking>
        <chunking id="2" string="have never been so brutally evoked" type="VP">
          <tokens>
            <token id="15" string="have" />
            <token id="16" string="never" />
            <token id="17" string="been" />
            <token id="18" string="so" />
            <token id="19" string="brutally" />
            <token id="20" string="evoked" />
          </tokens>
        </chunking>
        <chunking id="3" string="been so brutally evoked" type="VP">
          <tokens>
            <token id="17" string="been" />
            <token id="18" string="so" />
            <token id="19" string="brutally" />
            <token id="20" string="evoked" />
          </tokens>
        </chunking>
        <chunking id="4" string="that ` the misery and deprivation of working class life have never been so brutally evoked" type="SBAR">
          <tokens>
            <token id="5" string="that" />
            <token id="6" string="'" />
            <token id="7" string="the" />
            <token id="8" string="misery" />
            <token id="9" string="and" />
            <token id="10" string="deprivation" />
            <token id="11" string="of" />
            <token id="12" string="working" />
            <token id="13" string="class" />
            <token id="14" string="life" />
            <token id="15" string="have" />
            <token id="16" string="never" />
            <token id="17" string="been" />
            <token id="18" string="so" />
            <token id="19" string="brutally" />
            <token id="20" string="evoked" />
          </tokens>
        </chunking>
        <chunking id="5" string="said that ` the misery and deprivation of working class life have never been so brutally evoked" type="VP">
          <tokens>
            <token id="4" string="said" />
            <token id="5" string="that" />
            <token id="6" string="'" />
            <token id="7" string="the" />
            <token id="8" string="misery" />
            <token id="9" string="and" />
            <token id="10" string="deprivation" />
            <token id="11" string="of" />
            <token id="12" string="working" />
            <token id="13" string="class" />
            <token id="14" string="life" />
            <token id="15" string="have" />
            <token id="16" string="never" />
            <token id="17" string="been" />
            <token id="18" string="so" />
            <token id="19" string="brutally" />
            <token id="20" string="evoked" />
          </tokens>
        </chunking>
        <chunking id="6" string="` the misery and deprivation of working class life" type="NP">
          <tokens>
            <token id="6" string="'" />
            <token id="7" string="the" />
            <token id="8" string="misery" />
            <token id="9" string="and" />
            <token id="10" string="deprivation" />
            <token id="11" string="of" />
            <token id="12" string="working" />
            <token id="13" string="class" />
            <token id="14" string="life" />
          </tokens>
        </chunking>
        <chunking id="7" string="The FT reviewer" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="FT" />
            <token id="3" string="reviewer" />
          </tokens>
        </chunking>
        <chunking id="8" string="` the misery and deprivation" type="NP">
          <tokens>
            <token id="6" string="'" />
            <token id="7" string="the" />
            <token id="8" string="misery" />
            <token id="9" string="and" />
            <token id="10" string="deprivation" />
          </tokens>
        </chunking>
        <chunking id="9" string="working class life" type="NP">
          <tokens>
            <token id="12" string="working" />
            <token id="13" string="class" />
            <token id="14" string="life" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">reviewer</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">reviewer</governor>
          <dependent id="2">FT</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">said</governor>
          <dependent id="3">reviewer</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">said</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="20">evoked</governor>
          <dependent id="5">that</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">misery</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="20">evoked</governor>
          <dependent id="8">misery</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="8">misery</governor>
          <dependent id="9">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="8">misery</governor>
          <dependent id="10">deprivation</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">life</governor>
          <dependent id="11">of</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="14">life</governor>
          <dependent id="12">working</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">life</governor>
          <dependent id="13">class</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">misery</governor>
          <dependent id="14">life</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="20">evoked</governor>
          <dependent id="15">have</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="20">evoked</governor>
          <dependent id="16">never</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="20">evoked</governor>
          <dependent id="17">been</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="19">brutally</governor>
          <dependent id="18">so</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="20">evoked</governor>
          <dependent id="19">brutally</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="4">said</governor>
          <dependent id="20">evoked</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="17" has_coreference="false">
      <content>.&amp;apost;</content>
      <tokens>
        <token id="1" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (VP (. .) ('' '))))</syntactictree>
      <chunkings>
        <chunking id="1" string=". '" type="VP">
          <tokens>
            <token id="1" string="." />
            <token id="2" string="'" />
          </tokens>
        </chunking>
      </chunkings>
    </sentence>
    <sentence id="18" has_coreference="true">
      <content>Black Dogs by Ian McEwan (Secker &amp;amp;amp; Warburg Pounds 14.99).</content>
      <tokens>
        <token id="1" string="Black" lemma="black" stem="black" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="Dogs" lemma="dog" stem="dog" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="Ian" lemma="Ian" stem="ian" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="5" string="McEwan" lemma="McEwan" stem="mcewan" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="6" string="(" lemma="-lrb-" stem="(" pos="-LRB-" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="Secker" lemma="Secker" stem="secker" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="8" string="&amp;amp;" lemma="&amp;" stem="&amp;amp;" pos="CC" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="9" string="Warburg" lemma="Warburg" stem="warburg" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="10" string="Pounds" lemma="Pounds" stem="pound" pos="NNPS" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="11" string="14.99" lemma="14.99" stem="14.99" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="12" string=")" lemma="-rrb-" stem=")" pos="-RRB-" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (JJ Black) (NNS Dogs)) (PP (IN by) (NP (NP (NNP Ian) (NNP McEwan)) (PRN (-LRB- -LRB-) (NP (NP (NNP Secker) (CC &amp;) (NP (NNP Warburg) (NNPS Pounds))) (NP (CD 14.99))) (-RRB- -RRB-)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="14.99" type="NP">
          <tokens>
            <token id="11" string="14.99" />
          </tokens>
        </chunking>
        <chunking id="2" string="Black Dogs by Ian McEwan -LRB- Secker &amp; Warburg Pounds 14.99 -RRB- ." type="NP">
          <tokens>
            <token id="1" string="Black" />
            <token id="2" string="Dogs" />
            <token id="3" string="by" />
            <token id="4" string="Ian" />
            <token id="5" string="McEwan" />
            <token id="6" string="(" />
            <token id="7" string="Secker" />
            <token id="8" string="&amp;amp;" />
            <token id="9" string="Warburg" />
            <token id="10" string="Pounds" />
            <token id="11" string="14.99" />
            <token id="12" string=")" />
            <token id="13" string="." />
          </tokens>
        </chunking>
        <chunking id="3" string="Secker &amp; Warburg Pounds 14.99" type="NP">
          <tokens>
            <token id="7" string="Secker" />
            <token id="8" string="&amp;amp;" />
            <token id="9" string="Warburg" />
            <token id="10" string="Pounds" />
            <token id="11" string="14.99" />
          </tokens>
        </chunking>
        <chunking id="4" string="Black Dogs" type="NP">
          <tokens>
            <token id="1" string="Black" />
            <token id="2" string="Dogs" />
          </tokens>
        </chunking>
        <chunking id="5" string="Warburg Pounds" type="NP">
          <tokens>
            <token id="9" string="Warburg" />
            <token id="10" string="Pounds" />
          </tokens>
        </chunking>
        <chunking id="6" string="Secker &amp; Warburg Pounds" type="NP">
          <tokens>
            <token id="7" string="Secker" />
            <token id="8" string="&amp;amp;" />
            <token id="9" string="Warburg" />
            <token id="10" string="Pounds" />
          </tokens>
        </chunking>
        <chunking id="7" string="Ian McEwan -LRB- Secker &amp; Warburg Pounds 14.99 -RRB-" type="NP">
          <tokens>
            <token id="4" string="Ian" />
            <token id="5" string="McEwan" />
            <token id="6" string="(" />
            <token id="7" string="Secker" />
            <token id="8" string="&amp;amp;" />
            <token id="9" string="Warburg" />
            <token id="10" string="Pounds" />
            <token id="11" string="14.99" />
            <token id="12" string=")" />
          </tokens>
        </chunking>
        <chunking id="8" string="Ian McEwan" type="NP">
          <tokens>
            <token id="4" string="Ian" />
            <token id="5" string="McEwan" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="amod">
          <governor id="2">Dogs</governor>
          <dependent id="1">Black</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">Dogs</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">McEwan</governor>
          <dependent id="3">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">McEwan</governor>
          <dependent id="4">Ian</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">Dogs</governor>
          <dependent id="5">McEwan</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="5">McEwan</governor>
          <dependent id="7">Secker</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">Secker</governor>
          <dependent id="8">&amp;</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">Pounds</governor>
          <dependent id="9">Warburg</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">Secker</governor>
          <dependent id="10">Pounds</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="7">Secker</governor>
          <dependent id="11">14.99</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="14.99" type="NUMBER" score="0.0">
          <tokens>
            <token id="11" string="14.99" />
          </tokens>
        </entity>
        <entity id="2" string="Secker &amp;amp; Warburg Pounds" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="7" string="Secker" />
            <token id="8" string="&amp;amp;" />
            <token id="9" string="Warburg" />
            <token id="10" string="Pounds" />
          </tokens>
        </entity>
        <entity id="3" string="Ian McEwan" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="Ian" />
            <token id="5" string="McEwan" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="19" has_coreference="true">
      <content>McEwan&amp;apost;s fiction is uniquely disturbing - some would say deliberately offensive.</content>
      <tokens>
        <token id="1" string="McEwan" lemma="McEwan" stem="mcewan" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="fiction" lemma="fiction" stem="fiction" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="uniquely" lemma="uniquely" stem="uniqu" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="disturbing" lemma="disturbing" stem="disturb" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="-" lemma="-" stem="-" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="some" lemma="some" stem="some" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="would" lemma="would" stem="would" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="say" lemma="say" stem="sai" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="deliberately" lemma="deliberately" stem="deliber" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="offensive" lemma="offensive" stem="offens" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (NP (NNP McEwan) (POS 's)) (NN fiction)) (VP (VBZ is) (ADJP (RB uniquely) (JJ disturbing)))) (: -) (S (NP (DT some)) (VP (MD would) (VP (VB say) (ADJP (RB deliberately) (JJ offensive))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="would say deliberately offensive" type="VP">
          <tokens>
            <token id="9" string="would" />
            <token id="10" string="say" />
            <token id="11" string="deliberately" />
            <token id="12" string="offensive" />
          </tokens>
        </chunking>
        <chunking id="2" string="deliberately offensive" type="ADJP">
          <tokens>
            <token id="11" string="deliberately" />
            <token id="12" string="offensive" />
          </tokens>
        </chunking>
        <chunking id="3" string="uniquely disturbing" type="ADJP">
          <tokens>
            <token id="5" string="uniquely" />
            <token id="6" string="disturbing" />
          </tokens>
        </chunking>
        <chunking id="4" string="is uniquely disturbing" type="VP">
          <tokens>
            <token id="4" string="is" />
            <token id="5" string="uniquely" />
            <token id="6" string="disturbing" />
          </tokens>
        </chunking>
        <chunking id="5" string="some" type="NP">
          <tokens>
            <token id="8" string="some" />
          </tokens>
        </chunking>
        <chunking id="6" string="McEwan 's" type="NP">
          <tokens>
            <token id="1" string="McEwan" />
            <token id="2" string="'s" />
          </tokens>
        </chunking>
        <chunking id="7" string="say deliberately offensive" type="VP">
          <tokens>
            <token id="10" string="say" />
            <token id="11" string="deliberately" />
            <token id="12" string="offensive" />
          </tokens>
        </chunking>
        <chunking id="8" string="McEwan 's fiction" type="NP">
          <tokens>
            <token id="1" string="McEwan" />
            <token id="2" string="'s" />
            <token id="3" string="fiction" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="3">fiction</governor>
          <dependent id="1">McEwan</dependent>
        </dependency>
        <dependency type="case">
          <governor id="1">McEwan</governor>
          <dependent id="2">'s</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">disturbing</governor>
          <dependent id="3">fiction</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="6">disturbing</governor>
          <dependent id="4">is</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="6">disturbing</governor>
          <dependent id="5">uniquely</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">disturbing</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">say</governor>
          <dependent id="8">some</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="10">say</governor>
          <dependent id="9">would</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="6">disturbing</governor>
          <dependent id="10">say</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="12">offensive</governor>
          <dependent id="11">deliberately</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="10">say</governor>
          <dependent id="12">offensive</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="McEwan" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="McEwan" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="20" has_coreference="true">
      <content>He is the novelist of misanthropy.</content>
      <tokens>
        <token id="1" string="He" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="novelist" lemma="novelist" stem="novelist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="misanthropy" lemma="misanthropy" stem="misanthropi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP He)) (VP (VBZ is) (NP (NP (DT the) (NN novelist)) (PP (IN of) (NP (NN misanthropy))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="is the novelist of misanthropy" type="VP">
          <tokens>
            <token id="2" string="is" />
            <token id="3" string="the" />
            <token id="4" string="novelist" />
            <token id="5" string="of" />
            <token id="6" string="misanthropy" />
          </tokens>
        </chunking>
        <chunking id="2" string="misanthropy" type="NP">
          <tokens>
            <token id="6" string="misanthropy" />
          </tokens>
        </chunking>
        <chunking id="3" string="He" type="NP">
          <tokens>
            <token id="1" string="He" />
          </tokens>
        </chunking>
        <chunking id="4" string="the novelist" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="novelist" />
          </tokens>
        </chunking>
        <chunking id="5" string="the novelist of misanthropy" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="novelist" />
            <token id="5" string="of" />
            <token id="6" string="misanthropy" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">novelist</governor>
          <dependent id="1">He</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="4">novelist</governor>
          <dependent id="2">is</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">novelist</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">novelist</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">misanthropy</governor>
          <dependent id="5">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">novelist</governor>
          <dependent id="6">misanthropy</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="21" has_coreference="false">
      <content>This is the tale of a failed marriage, episodically told, and the black dogs of evil are both metaphorical and literal.</content>
      <tokens>
        <token id="1" string="This" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="tale" lemma="tale" stem="tale" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="failed" lemma="fail" stem="fail" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="marriage" lemma="marriage" stem="marriag" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="episodically" lemma="episodically" stem="episod" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="told" lemma="tell" stem="told" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="black" lemma="black" stem="black" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="dogs" lemma="dog" stem="dog" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="evil" lemma="evil" stem="evil" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="are" lemma="be" stem="ar" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="both" lemma="both" stem="both" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="metaphorical" lemma="metaphorical" stem="metaphor" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="literal" lemma="literal" stem="liter" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (DT This)) (VP (VP (VBZ is) (NP (NP (DT the) (NN tale)) (PP (IN of) (NP (DT a) (VBN failed) (NN marriage))))) (, ,) (ADVP (RB episodically)) (VP (VBD told)))) (, ,) (CC and) (S (NP (NP (DT the) (JJ black) (NNS dogs)) (PP (IN of) (NP (NN evil)))) (VP (VBP are) (ADJP (DT both) (JJ metaphorical) (CC and) (JJ literal)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="is the tale of a failed marriage , episodically told" type="VP">
          <tokens>
            <token id="2" string="is" />
            <token id="3" string="the" />
            <token id="4" string="tale" />
            <token id="5" string="of" />
            <token id="6" string="a" />
            <token id="7" string="failed" />
            <token id="8" string="marriage" />
            <token id="9" string="," />
            <token id="10" string="episodically" />
            <token id="11" string="told" />
          </tokens>
        </chunking>
        <chunking id="2" string="the black dogs" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="black" />
            <token id="16" string="dogs" />
          </tokens>
        </chunking>
        <chunking id="3" string="are both metaphorical and literal" type="VP">
          <tokens>
            <token id="19" string="are" />
            <token id="20" string="both" />
            <token id="21" string="metaphorical" />
            <token id="22" string="and" />
            <token id="23" string="literal" />
          </tokens>
        </chunking>
        <chunking id="4" string="told" type="VP">
          <tokens>
            <token id="11" string="told" />
          </tokens>
        </chunking>
        <chunking id="5" string="is the tale of a failed marriage" type="VP">
          <tokens>
            <token id="2" string="is" />
            <token id="3" string="the" />
            <token id="4" string="tale" />
            <token id="5" string="of" />
            <token id="6" string="a" />
            <token id="7" string="failed" />
            <token id="8" string="marriage" />
          </tokens>
        </chunking>
        <chunking id="6" string="the tale of a failed marriage" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="tale" />
            <token id="5" string="of" />
            <token id="6" string="a" />
            <token id="7" string="failed" />
            <token id="8" string="marriage" />
          </tokens>
        </chunking>
        <chunking id="7" string="the tale" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="tale" />
          </tokens>
        </chunking>
        <chunking id="8" string="the black dogs of evil" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="black" />
            <token id="16" string="dogs" />
            <token id="17" string="of" />
            <token id="18" string="evil" />
          </tokens>
        </chunking>
        <chunking id="9" string="This" type="NP">
          <tokens>
            <token id="1" string="This" />
          </tokens>
        </chunking>
        <chunking id="10" string="evil" type="NP">
          <tokens>
            <token id="18" string="evil" />
          </tokens>
        </chunking>
        <chunking id="11" string="both metaphorical and literal" type="ADJP">
          <tokens>
            <token id="20" string="both" />
            <token id="21" string="metaphorical" />
            <token id="22" string="and" />
            <token id="23" string="literal" />
          </tokens>
        </chunking>
        <chunking id="12" string="a failed marriage" type="NP">
          <tokens>
            <token id="6" string="a" />
            <token id="7" string="failed" />
            <token id="8" string="marriage" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">tale</governor>
          <dependent id="1">This</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="4">tale</governor>
          <dependent id="2">is</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">tale</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">tale</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">marriage</governor>
          <dependent id="5">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">marriage</governor>
          <dependent id="6">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">marriage</governor>
          <dependent id="7">failed</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">tale</governor>
          <dependent id="8">marriage</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="11">told</governor>
          <dependent id="10">episodically</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="4">tale</governor>
          <dependent id="11">told</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">tale</governor>
          <dependent id="13">and</dependent>
        </dependency>
        <dependency type="det">
          <governor id="16">dogs</governor>
          <dependent id="14">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="16">dogs</governor>
          <dependent id="15">black</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">metaphorical</governor>
          <dependent id="16">dogs</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">evil</governor>
          <dependent id="17">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">dogs</governor>
          <dependent id="18">evil</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="21">metaphorical</governor>
          <dependent id="19">are</dependent>
        </dependency>
        <dependency type="cc:preconj">
          <governor id="21">metaphorical</governor>
          <dependent id="20">both</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">tale</governor>
          <dependent id="21">metaphorical</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="21">metaphorical</governor>
          <dependent id="22">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="21">metaphorical</governor>
          <dependent id="23">literal</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="22" has_coreference="true">
      <content>The English Patient by Michael Ondaatje (Bloomsbury Pounds 14.99).</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="English" lemma="English" stem="english" pos="NNP" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="true" />
        <token id="3" string="Patient" lemma="patient" stem="patient" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="4" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="Michael" lemma="Michael" stem="michael" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="6" string="Ondaatje" lemma="Ondaatje" stem="ondaatj" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="7" string="(" lemma="-lrb-" stem="(" pos="-LRB-" type="Symbol" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="8" string="Bloomsbury" lemma="Bloomsbury" stem="bloomsburi" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="9" string="Pounds" lemma="Pounds" stem="pound" pos="NNPS" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="10" string="14.99" lemma="14.99" stem="14.99" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="true" />
        <token id="11" string=")" lemma="-rrb-" stem=")" pos="-RRB-" type="Symbol" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (DT The) (NNP English) (NN Patient)) (PP (IN by) (NP (NP (NNP Michael) (NNP Ondaatje)) (PRN (-LRB- -LRB-) (NP (NP (NNP Bloomsbury) (NNPS Pounds)) (NP (CD 14.99))) (-RRB- -RRB-)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Bloomsbury Pounds" type="NP">
          <tokens>
            <token id="8" string="Bloomsbury" />
            <token id="9" string="Pounds" />
          </tokens>
        </chunking>
        <chunking id="2" string="14.99" type="NP">
          <tokens>
            <token id="10" string="14.99" />
          </tokens>
        </chunking>
        <chunking id="3" string="The English Patient by Michael Ondaatje -LRB- Bloomsbury Pounds 14.99 -RRB- ." type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="English" />
            <token id="3" string="Patient" />
            <token id="4" string="by" />
            <token id="5" string="Michael" />
            <token id="6" string="Ondaatje" />
            <token id="7" string="(" />
            <token id="8" string="Bloomsbury" />
            <token id="9" string="Pounds" />
            <token id="10" string="14.99" />
            <token id="11" string=")" />
            <token id="12" string="." />
          </tokens>
        </chunking>
        <chunking id="4" string="Michael Ondaatje -LRB- Bloomsbury Pounds 14.99 -RRB-" type="NP">
          <tokens>
            <token id="5" string="Michael" />
            <token id="6" string="Ondaatje" />
            <token id="7" string="(" />
            <token id="8" string="Bloomsbury" />
            <token id="9" string="Pounds" />
            <token id="10" string="14.99" />
            <token id="11" string=")" />
          </tokens>
        </chunking>
        <chunking id="5" string="The English Patient" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="English" />
            <token id="3" string="Patient" />
          </tokens>
        </chunking>
        <chunking id="6" string="Michael Ondaatje" type="NP">
          <tokens>
            <token id="5" string="Michael" />
            <token id="6" string="Ondaatje" />
          </tokens>
        </chunking>
        <chunking id="7" string="Bloomsbury Pounds 14.99" type="NP">
          <tokens>
            <token id="8" string="Bloomsbury" />
            <token id="9" string="Pounds" />
            <token id="10" string="14.99" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">Patient</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">Patient</governor>
          <dependent id="2">English</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">Patient</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">Ondaatje</governor>
          <dependent id="4">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">Ondaatje</governor>
          <dependent id="5">Michael</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">Patient</governor>
          <dependent id="6">Ondaatje</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">Pounds</governor>
          <dependent id="8">Bloomsbury</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="6">Ondaatje</governor>
          <dependent id="9">Pounds</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="9">Pounds</governor>
          <dependent id="10">14.99</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="14.99" type="NUMBER" score="0.0">
          <tokens>
            <token id="10" string="14.99" />
          </tokens>
        </entity>
        <entity id="2" string="Michael Ondaatje" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="Michael" />
            <token id="6" string="Ondaatje" />
          </tokens>
        </entity>
        <entity id="3" string="English" type="MISC" score="0.0">
          <tokens>
            <token id="2" string="English" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="23" has_coreference="true">
      <content>The Sri Lankan poet and novelist Ondaatje, who lives in Canada, has followed &amp;apost;in the skin of a lion&amp;apost; with an extraordinary book about four characters in a Tuscan villa in the closing months of the last war.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="Sri" lemma="Sri" stem="sri" pos="NNP" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="false" is_refers="false" />
        <token id="3" string="Lankan" lemma="Lankan" stem="lankan" pos="NNP" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="false" is_refers="false" />
        <token id="4" string="poet" lemma="poet" stem="poet" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="novelist" lemma="novelist" stem="novelist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="Ondaatje" lemma="Ondaatje" stem="ondaatj" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="lives" lemma="live" stem="live" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="Canada" lemma="Canada" stem="canada" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="followed" lemma="follow" stem="follow" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="'" lemma="`" stem="'" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="skin" lemma="skin" stem="skin" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="lion" lemma="lion" stem="lion" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="extraordinary" lemma="extraordinary" stem="extraordinari" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="four" lemma="four" stem="four" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="30" string="characters" lemma="character" stem="charact" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="Tuscan" lemma="tuscan" stem="tuscan" pos="JJ" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="34" string="villa" lemma="villa" stem="villa" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="closing" lemma="closing" stem="close" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="38" string="months" lemma="month" stem="month" pos="NNS" type="Word" isStopWord="false" ner="DURATION" is_referenced="false" is_refers="false" />
        <token id="39" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="40" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="41" string="last" lemma="last" stem="last" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="42" string="war" lemma="war" stem="war" pos="NN" type="Word" isStopWord="false" ner="CAUSE_OF_DEATH" is_referenced="false" is_refers="false" />
        <token id="43" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NP (DT The) (NNP Sri) (NNP Lankan) (NN poet)) (CC and) (NP (NN novelist) (NNP Ondaatje))) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBZ lives) (PP (IN in) (NP (NNP Canada)))))) (, ,)) (VP (VBZ has) (VP (VBN followed) (`` `) (PP (IN in) (NP (NP (DT the) (NN skin)) (PP (IN of) (NP (DT a) (NN lion))))) ('' ') (PP (IN with) (NP (NP (DT an) (JJ extraordinary) (NN book)) (PP (IN about) (NP (NP (CD four) (NNS characters)) (PP (IN in) (NP (NP (DT a) (JJ Tuscan) (NN villa)) (PP (IN in) (NP (NP (DT the) (NN closing) (NNS months)) (PP (IN of) (NP (DT the) (JJ last) (NN war))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a lion" type="NP">
          <tokens>
            <token id="21" string="a" />
            <token id="22" string="lion" />
          </tokens>
        </chunking>
        <chunking id="2" string="The Sri Lankan poet" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Sri" />
            <token id="3" string="Lankan" />
            <token id="4" string="poet" />
          </tokens>
        </chunking>
        <chunking id="3" string="four characters in a Tuscan villa in the closing months of the last war" type="NP">
          <tokens>
            <token id="29" string="four" />
            <token id="30" string="characters" />
            <token id="31" string="in" />
            <token id="32" string="a" />
            <token id="33" string="Tuscan" />
            <token id="34" string="villa" />
            <token id="35" string="in" />
            <token id="36" string="the" />
            <token id="37" string="closing" />
            <token id="38" string="months" />
            <token id="39" string="of" />
            <token id="40" string="the" />
            <token id="41" string="last" />
            <token id="42" string="war" />
          </tokens>
        </chunking>
        <chunking id="4" string="four characters" type="NP">
          <tokens>
            <token id="29" string="four" />
            <token id="30" string="characters" />
          </tokens>
        </chunking>
        <chunking id="5" string="lives in Canada" type="VP">
          <tokens>
            <token id="10" string="lives" />
            <token id="11" string="in" />
            <token id="12" string="Canada" />
          </tokens>
        </chunking>
        <chunking id="6" string="the skin" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="skin" />
          </tokens>
        </chunking>
        <chunking id="7" string="who lives in Canada" type="SBAR">
          <tokens>
            <token id="9" string="who" />
            <token id="10" string="lives" />
            <token id="11" string="in" />
            <token id="12" string="Canada" />
          </tokens>
        </chunking>
        <chunking id="8" string="an extraordinary book about four characters in a Tuscan villa in the closing months of the last war" type="NP">
          <tokens>
            <token id="25" string="an" />
            <token id="26" string="extraordinary" />
            <token id="27" string="book" />
            <token id="28" string="about" />
            <token id="29" string="four" />
            <token id="30" string="characters" />
            <token id="31" string="in" />
            <token id="32" string="a" />
            <token id="33" string="Tuscan" />
            <token id="34" string="villa" />
            <token id="35" string="in" />
            <token id="36" string="the" />
            <token id="37" string="closing" />
            <token id="38" string="months" />
            <token id="39" string="of" />
            <token id="40" string="the" />
            <token id="41" string="last" />
            <token id="42" string="war" />
          </tokens>
        </chunking>
        <chunking id="9" string="The Sri Lankan poet and novelist Ondaatje , who lives in Canada ," type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Sri" />
            <token id="3" string="Lankan" />
            <token id="4" string="poet" />
            <token id="5" string="and" />
            <token id="6" string="novelist" />
            <token id="7" string="Ondaatje" />
            <token id="8" string="," />
            <token id="9" string="who" />
            <token id="10" string="lives" />
            <token id="11" string="in" />
            <token id="12" string="Canada" />
            <token id="13" string="," />
          </tokens>
        </chunking>
        <chunking id="10" string="the closing months" type="NP">
          <tokens>
            <token id="36" string="the" />
            <token id="37" string="closing" />
            <token id="38" string="months" />
          </tokens>
        </chunking>
        <chunking id="11" string="Canada" type="NP">
          <tokens>
            <token id="12" string="Canada" />
          </tokens>
        </chunking>
        <chunking id="12" string="novelist Ondaatje" type="NP">
          <tokens>
            <token id="6" string="novelist" />
            <token id="7" string="Ondaatje" />
          </tokens>
        </chunking>
        <chunking id="13" string="a Tuscan villa" type="NP">
          <tokens>
            <token id="32" string="a" />
            <token id="33" string="Tuscan" />
            <token id="34" string="villa" />
          </tokens>
        </chunking>
        <chunking id="14" string="an extraordinary book" type="NP">
          <tokens>
            <token id="25" string="an" />
            <token id="26" string="extraordinary" />
            <token id="27" string="book" />
          </tokens>
        </chunking>
        <chunking id="15" string="the closing months of the last war" type="NP">
          <tokens>
            <token id="36" string="the" />
            <token id="37" string="closing" />
            <token id="38" string="months" />
            <token id="39" string="of" />
            <token id="40" string="the" />
            <token id="41" string="last" />
            <token id="42" string="war" />
          </tokens>
        </chunking>
        <chunking id="16" string="the last war" type="NP">
          <tokens>
            <token id="40" string="the" />
            <token id="41" string="last" />
            <token id="42" string="war" />
          </tokens>
        </chunking>
        <chunking id="17" string="has followed ` in the skin of a lion ' with an extraordinary book about four characters in a Tuscan villa in the closing months of the last war" type="VP">
          <tokens>
            <token id="14" string="has" />
            <token id="15" string="followed" />
            <token id="16" string="'" />
            <token id="17" string="in" />
            <token id="18" string="the" />
            <token id="19" string="skin" />
            <token id="20" string="of" />
            <token id="21" string="a" />
            <token id="22" string="lion" />
            <token id="23" string="'" />
            <token id="24" string="with" />
            <token id="25" string="an" />
            <token id="26" string="extraordinary" />
            <token id="27" string="book" />
            <token id="28" string="about" />
            <token id="29" string="four" />
            <token id="30" string="characters" />
            <token id="31" string="in" />
            <token id="32" string="a" />
            <token id="33" string="Tuscan" />
            <token id="34" string="villa" />
            <token id="35" string="in" />
            <token id="36" string="the" />
            <token id="37" string="closing" />
            <token id="38" string="months" />
            <token id="39" string="of" />
            <token id="40" string="the" />
            <token id="41" string="last" />
            <token id="42" string="war" />
          </tokens>
        </chunking>
        <chunking id="18" string="The Sri Lankan poet and novelist Ondaatje" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Sri" />
            <token id="3" string="Lankan" />
            <token id="4" string="poet" />
            <token id="5" string="and" />
            <token id="6" string="novelist" />
            <token id="7" string="Ondaatje" />
          </tokens>
        </chunking>
        <chunking id="19" string="the skin of a lion" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="skin" />
            <token id="20" string="of" />
            <token id="21" string="a" />
            <token id="22" string="lion" />
          </tokens>
        </chunking>
        <chunking id="20" string="a Tuscan villa in the closing months of the last war" type="NP">
          <tokens>
            <token id="32" string="a" />
            <token id="33" string="Tuscan" />
            <token id="34" string="villa" />
            <token id="35" string="in" />
            <token id="36" string="the" />
            <token id="37" string="closing" />
            <token id="38" string="months" />
            <token id="39" string="of" />
            <token id="40" string="the" />
            <token id="41" string="last" />
            <token id="42" string="war" />
          </tokens>
        </chunking>
        <chunking id="21" string="followed ` in the skin of a lion ' with an extraordinary book about four characters in a Tuscan villa in the closing months of the last war" type="VP">
          <tokens>
            <token id="15" string="followed" />
            <token id="16" string="'" />
            <token id="17" string="in" />
            <token id="18" string="the" />
            <token id="19" string="skin" />
            <token id="20" string="of" />
            <token id="21" string="a" />
            <token id="22" string="lion" />
            <token id="23" string="'" />
            <token id="24" string="with" />
            <token id="25" string="an" />
            <token id="26" string="extraordinary" />
            <token id="27" string="book" />
            <token id="28" string="about" />
            <token id="29" string="four" />
            <token id="30" string="characters" />
            <token id="31" string="in" />
            <token id="32" string="a" />
            <token id="33" string="Tuscan" />
            <token id="34" string="villa" />
            <token id="35" string="in" />
            <token id="36" string="the" />
            <token id="37" string="closing" />
            <token id="38" string="months" />
            <token id="39" string="of" />
            <token id="40" string="the" />
            <token id="41" string="last" />
            <token id="42" string="war" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="4">poet</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">poet</governor>
          <dependent id="2">Sri</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">poet</governor>
          <dependent id="3">Lankan</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">followed</governor>
          <dependent id="4">poet</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">poet</governor>
          <dependent id="5">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">Ondaatje</governor>
          <dependent id="6">novelist</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">poet</governor>
          <dependent id="7">Ondaatje</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">lives</governor>
          <dependent id="9">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="4">poet</governor>
          <dependent id="10">lives</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">Canada</governor>
          <dependent id="11">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">lives</governor>
          <dependent id="12">Canada</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="15">followed</governor>
          <dependent id="14">has</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="15">followed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">skin</governor>
          <dependent id="17">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">skin</governor>
          <dependent id="18">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">followed</governor>
          <dependent id="19">skin</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">lion</governor>
          <dependent id="20">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="22">lion</governor>
          <dependent id="21">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">skin</governor>
          <dependent id="22">lion</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">book</governor>
          <dependent id="24">with</dependent>
        </dependency>
        <dependency type="det">
          <governor id="27">book</governor>
          <dependent id="25">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="27">book</governor>
          <dependent id="26">extraordinary</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">followed</governor>
          <dependent id="27">book</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">characters</governor>
          <dependent id="28">about</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="30">characters</governor>
          <dependent id="29">four</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">book</governor>
          <dependent id="30">characters</dependent>
        </dependency>
        <dependency type="case">
          <governor id="34">villa</governor>
          <dependent id="31">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="34">villa</governor>
          <dependent id="32">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="34">villa</governor>
          <dependent id="33">Tuscan</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="30">characters</governor>
          <dependent id="34">villa</dependent>
        </dependency>
        <dependency type="case">
          <governor id="38">months</governor>
          <dependent id="35">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="38">months</governor>
          <dependent id="36">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="38">months</governor>
          <dependent id="37">closing</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="34">villa</governor>
          <dependent id="38">months</dependent>
        </dependency>
        <dependency type="case">
          <governor id="42">war</governor>
          <dependent id="39">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="42">war</governor>
          <dependent id="40">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="42">war</governor>
          <dependent id="41">last</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="38">months</governor>
          <dependent id="42">war</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Tuscan" type="MISC" score="0.0">
          <tokens>
            <token id="33" string="Tuscan" />
          </tokens>
        </entity>
        <entity id="2" string="Sri Lankan" type="NATIONALITY" score="0.0">
          <tokens>
            <token id="2" string="Sri" />
            <token id="3" string="Lankan" />
          </tokens>
        </entity>
        <entity id="3" string="Canada" type="LOCATION" score="0.0">
          <tokens>
            <token id="12" string="Canada" />
          </tokens>
        </entity>
        <entity id="4" string="months" type="DURATION" score="0.0">
          <tokens>
            <token id="38" string="months" />
          </tokens>
        </entity>
        <entity id="5" string="four" type="NUMBER" score="0.0">
          <tokens>
            <token id="29" string="four" />
          </tokens>
        </entity>
        <entity id="6" string="war" type="CAUSE_OF_DEATH" score="0.0">
          <tokens>
            <token id="42" string="war" />
          </tokens>
        </entity>
        <entity id="7" string="Ondaatje" type="PERSON" score="0.0">
          <tokens>
            <token id="7" string="Ondaatje" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="24" has_coreference="false">
      <content>The central metaphor is bombs - disposal of, and then Hiroshima.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="central" lemma="central" stem="central" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="metaphor" lemma="metaphor" stem="metaphor" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="bombs" lemma="bomb" stem="bomb" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="-" lemma="-" stem="-" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="disposal" lemma="disposal" stem="dispos" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="then" lemma="then" stem="then" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="Hiroshima" lemma="Hiroshima" stem="hiroshima" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (JJ central) (NN metaphor)) (VP (VBZ is) (NP (NP (NNS bombs)) (: -) (NP (NP (NN disposal)) (PP (IN of) (PRN (, ,) (CC and) (ADVP (RB then))) (NP (NNP Hiroshima)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="bombs - disposal of , and then Hiroshima" type="NP">
          <tokens>
            <token id="5" string="bombs" />
            <token id="6" string="-" />
            <token id="7" string="disposal" />
            <token id="8" string="of" />
            <token id="9" string="," />
            <token id="10" string="and" />
            <token id="11" string="then" />
            <token id="12" string="Hiroshima" />
          </tokens>
        </chunking>
        <chunking id="2" string="The central metaphor" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="central" />
            <token id="3" string="metaphor" />
          </tokens>
        </chunking>
        <chunking id="3" string="bombs" type="NP">
          <tokens>
            <token id="5" string="bombs" />
          </tokens>
        </chunking>
        <chunking id="4" string="disposal of , and then Hiroshima" type="NP">
          <tokens>
            <token id="7" string="disposal" />
            <token id="8" string="of" />
            <token id="9" string="," />
            <token id="10" string="and" />
            <token id="11" string="then" />
            <token id="12" string="Hiroshima" />
          </tokens>
        </chunking>
        <chunking id="5" string="Hiroshima" type="NP">
          <tokens>
            <token id="12" string="Hiroshima" />
          </tokens>
        </chunking>
        <chunking id="6" string="is bombs - disposal of , and then Hiroshima" type="VP">
          <tokens>
            <token id="4" string="is" />
            <token id="5" string="bombs" />
            <token id="6" string="-" />
            <token id="7" string="disposal" />
            <token id="8" string="of" />
            <token id="9" string="," />
            <token id="10" string="and" />
            <token id="11" string="then" />
            <token id="12" string="Hiroshima" />
          </tokens>
        </chunking>
        <chunking id="7" string="disposal" type="NP">
          <tokens>
            <token id="7" string="disposal" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">metaphor</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="3">metaphor</governor>
          <dependent id="2">central</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">bombs</governor>
          <dependent id="3">metaphor</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="5">bombs</governor>
          <dependent id="4">is</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">bombs</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="5">bombs</governor>
          <dependent id="7">disposal</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">Hiroshima</governor>
          <dependent id="8">of</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="11">then</governor>
          <dependent id="10">and</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="12">Hiroshima</governor>
          <dependent id="11">then</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">disposal</governor>
          <dependent id="12">Hiroshima</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Hiroshima" type="LOCATION" score="0.0">
          <tokens>
            <token id="12" string="Hiroshima" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="25" has_coreference="true">
      <content>The &amp;apost;English patient&amp;apost; is a central mystery around which the others&amp;apost; stories orbit: he fell out of the sky in flames; who is he?</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="'" lemma="`" stem="'" pos="``" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="3" string="English" lemma="english" stem="english" pos="JJ" type="Word" isStopWord="false" ner="MISC" is_referenced="true" is_refers="false" />
        <token id="4" string="patient" lemma="patient" stem="patient" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="'" lemma="'" stem="'" pos="''" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="6" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="central" lemma="central" stem="central" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="mystery" lemma="mystery" stem="mysteri" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="around" lemma="around" stem="around" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="which" lemma="which" stem="which" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="others" lemma="other" stem="other" pos="NNS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="'" lemma="'" stem="'" pos="POS" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="stories" lemma="story" stem="stori" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="orbit" lemma="orbit" stem="orbit" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="fell" lemma="fall" stem="fell" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="out" lemma="out" stem="out" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="sky" lemma="sky" stem="sky" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="flames" lemma="flame" stem="flame" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string=";" lemma=";" stem=";" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="?" lemma="?" stem="?" pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (`` `) (JJ English) (NN patient) ('' ')) (VP (VBZ is) (NP (NP (DT a) (JJ central) (NN mystery)) (SBAR (WHPP (IN around) (WHNP (WDT which))) (S (NP (NP (DT the) (NNS others) (POS ')) (NNS stories) (NN orbit)) (: :) (NP (PRP he)) (VP (VBD fell) (ADVP (RB out)) (PP (IN of) (NP (NP (DT the) (NN sky)) (PP (IN in) (NP (NP (NNS flames)) (: ;) (SBARQ (WHNP (WP who)) (SQ (VBZ is) (NP (PRP he))) (. ?)))))))))))))</syntactictree>
      <chunkings>
        <chunking id="1" string="flames ; who is he ?" type="NP">
          <tokens>
            <token id="25" string="flames" />
            <token id="26" string=";" />
            <token id="27" string="who" />
            <token id="28" string="is" />
            <token id="29" string="he" />
            <token id="30" string="?" />
          </tokens>
        </chunking>
        <chunking id="2" string="The ` English patient '" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="'" />
            <token id="3" string="English" />
            <token id="4" string="patient" />
            <token id="5" string="'" />
          </tokens>
        </chunking>
        <chunking id="3" string="a central mystery" type="NP">
          <tokens>
            <token id="7" string="a" />
            <token id="8" string="central" />
            <token id="9" string="mystery" />
          </tokens>
        </chunking>
        <chunking id="4" string="flames" type="NP">
          <tokens>
            <token id="25" string="flames" />
          </tokens>
        </chunking>
        <chunking id="5" string="around which the others ' stories orbit : he fell out of the sky in flames ; who is he ?" type="SBAR">
          <tokens>
            <token id="10" string="around" />
            <token id="11" string="which" />
            <token id="12" string="the" />
            <token id="13" string="others" />
            <token id="14" string="'" />
            <token id="15" string="stories" />
            <token id="16" string="orbit" />
            <token id="17" string=":" />
            <token id="18" string="he" />
            <token id="19" string="fell" />
            <token id="20" string="out" />
            <token id="21" string="of" />
            <token id="22" string="the" />
            <token id="23" string="sky" />
            <token id="24" string="in" />
            <token id="25" string="flames" />
            <token id="26" string=";" />
            <token id="27" string="who" />
            <token id="28" string="is" />
            <token id="29" string="he" />
            <token id="30" string="?" />
          </tokens>
        </chunking>
        <chunking id="6" string="the sky in flames ; who is he ?" type="NP">
          <tokens>
            <token id="22" string="the" />
            <token id="23" string="sky" />
            <token id="24" string="in" />
            <token id="25" string="flames" />
            <token id="26" string=";" />
            <token id="27" string="who" />
            <token id="28" string="is" />
            <token id="29" string="he" />
            <token id="30" string="?" />
          </tokens>
        </chunking>
        <chunking id="7" string="a central mystery around which the others ' stories orbit : he fell out of the sky in flames ; who is he ?" type="NP">
          <tokens>
            <token id="7" string="a" />
            <token id="8" string="central" />
            <token id="9" string="mystery" />
            <token id="10" string="around" />
            <token id="11" string="which" />
            <token id="12" string="the" />
            <token id="13" string="others" />
            <token id="14" string="'" />
            <token id="15" string="stories" />
            <token id="16" string="orbit" />
            <token id="17" string=":" />
            <token id="18" string="he" />
            <token id="19" string="fell" />
            <token id="20" string="out" />
            <token id="21" string="of" />
            <token id="22" string="the" />
            <token id="23" string="sky" />
            <token id="24" string="in" />
            <token id="25" string="flames" />
            <token id="26" string=";" />
            <token id="27" string="who" />
            <token id="28" string="is" />
            <token id="29" string="he" />
            <token id="30" string="?" />
          </tokens>
        </chunking>
        <chunking id="8" string="the others ' stories orbit" type="NP">
          <tokens>
            <token id="12" string="the" />
            <token id="13" string="others" />
            <token id="14" string="'" />
            <token id="15" string="stories" />
            <token id="16" string="orbit" />
          </tokens>
        </chunking>
        <chunking id="9" string="the others '" type="NP">
          <tokens>
            <token id="12" string="the" />
            <token id="13" string="others" />
            <token id="14" string="'" />
          </tokens>
        </chunking>
        <chunking id="10" string="the sky" type="NP">
          <tokens>
            <token id="22" string="the" />
            <token id="23" string="sky" />
          </tokens>
        </chunking>
        <chunking id="11" string="is a central mystery around which the others ' stories orbit : he fell out of the sky in flames ; who is he ?" type="VP">
          <tokens>
            <token id="6" string="is" />
            <token id="7" string="a" />
            <token id="8" string="central" />
            <token id="9" string="mystery" />
            <token id="10" string="around" />
            <token id="11" string="which" />
            <token id="12" string="the" />
            <token id="13" string="others" />
            <token id="14" string="'" />
            <token id="15" string="stories" />
            <token id="16" string="orbit" />
            <token id="17" string=":" />
            <token id="18" string="he" />
            <token id="19" string="fell" />
            <token id="20" string="out" />
            <token id="21" string="of" />
            <token id="22" string="the" />
            <token id="23" string="sky" />
            <token id="24" string="in" />
            <token id="25" string="flames" />
            <token id="26" string=";" />
            <token id="27" string="who" />
            <token id="28" string="is" />
            <token id="29" string="he" />
            <token id="30" string="?" />
          </tokens>
        </chunking>
        <chunking id="12" string="he" type="NP">
          <tokens>
            <token id="18" string="he" />
          </tokens>
        </chunking>
        <chunking id="13" string="fell out of the sky in flames ; who is he ?" type="VP">
          <tokens>
            <token id="19" string="fell" />
            <token id="20" string="out" />
            <token id="21" string="of" />
            <token id="22" string="the" />
            <token id="23" string="sky" />
            <token id="24" string="in" />
            <token id="25" string="flames" />
            <token id="26" string=";" />
            <token id="27" string="who" />
            <token id="28" string="is" />
            <token id="29" string="he" />
            <token id="30" string="?" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="4">patient</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="4">patient</governor>
          <dependent id="3">English</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">mystery</governor>
          <dependent id="4">patient</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="9">mystery</governor>
          <dependent id="6">is</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">mystery</governor>
          <dependent id="7">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">mystery</governor>
          <dependent id="8">central</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="9">mystery</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">which</governor>
          <dependent id="10">around</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">fell</governor>
          <dependent id="11">which</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">others</governor>
          <dependent id="12">the</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="16">orbit</governor>
          <dependent id="13">others</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">others</governor>
          <dependent id="14">'</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="16">orbit</governor>
          <dependent id="15">stories</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">fell</governor>
          <dependent id="16">orbit</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">fell</governor>
          <dependent id="18">he</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="9">mystery</governor>
          <dependent id="19">fell</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="19">fell</governor>
          <dependent id="20">out</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">sky</governor>
          <dependent id="21">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="23">sky</governor>
          <dependent id="22">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">fell</governor>
          <dependent id="23">sky</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">flames</governor>
          <dependent id="24">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">sky</governor>
          <dependent id="25">flames</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="25">flames</governor>
          <dependent id="27">who</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="27">who</governor>
          <dependent id="28">is</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="27">who</governor>
          <dependent id="29">he</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="English" type="MISC" score="0.0">
          <tokens>
            <token id="3" string="English" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="26" has_coreference="true">
      <content>The FT review on Saturday will hail it as a masterpiece.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="FT" lemma="FT" stem="ft" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="review" lemma="review" stem="review" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="Saturday" lemma="Saturday" stem="saturdai" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="true" />
        <token id="6" string="will" lemma="will" stem="will" pos="MD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="hail" lemma="hail" stem="hail" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="masterpiece" lemma="masterpiece" stem="masterpiec" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT The) (NNP FT) (NN review)) (PP (IN on) (NP (NNP Saturday)))) (VP (MD will) (VP (VB hail) (NP (PRP it)) (PP (IN as) (NP (DT a) (NN masterpiece))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="hail it as a masterpiece" type="VP">
          <tokens>
            <token id="7" string="hail" />
            <token id="8" string="it" />
            <token id="9" string="as" />
            <token id="10" string="a" />
            <token id="11" string="masterpiece" />
          </tokens>
        </chunking>
        <chunking id="2" string="a masterpiece" type="NP">
          <tokens>
            <token id="10" string="a" />
            <token id="11" string="masterpiece" />
          </tokens>
        </chunking>
        <chunking id="3" string="will hail it as a masterpiece" type="VP">
          <tokens>
            <token id="6" string="will" />
            <token id="7" string="hail" />
            <token id="8" string="it" />
            <token id="9" string="as" />
            <token id="10" string="a" />
            <token id="11" string="masterpiece" />
          </tokens>
        </chunking>
        <chunking id="4" string="The FT review on Saturday" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="FT" />
            <token id="3" string="review" />
            <token id="4" string="on" />
            <token id="5" string="Saturday" />
          </tokens>
        </chunking>
        <chunking id="5" string="it" type="NP">
          <tokens>
            <token id="8" string="it" />
          </tokens>
        </chunking>
        <chunking id="6" string="The FT review" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="FT" />
            <token id="3" string="review" />
          </tokens>
        </chunking>
        <chunking id="7" string="Saturday" type="NP">
          <tokens>
            <token id="5" string="Saturday" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">review</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">review</governor>
          <dependent id="2">FT</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">hail</governor>
          <dependent id="3">review</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">Saturday</governor>
          <dependent id="4">on</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">review</governor>
          <dependent id="5">Saturday</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="7">hail</governor>
          <dependent id="6">will</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">hail</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">hail</governor>
          <dependent id="8">it</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">masterpiece</governor>
          <dependent id="9">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">masterpiece</governor>
          <dependent id="10">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">hail</governor>
          <dependent id="11">masterpiece</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Saturday" type="DATE" score="0.0">
          <tokens>
            <token id="5" string="Saturday" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="27" has_coreference="true">
      <content>Daughters of the House by Michele Roberts (Virago Pounds 14.99).</content>
      <tokens>
        <token id="1" string="Daughters" lemma="daughter" stem="daughter" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="4" string="House" lemma="House" stem="hous" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="5" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Michele" lemma="Michele" stem="michel" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="7" string="Roberts" lemma="Roberts" stem="robert" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="8" string="(" lemma="-lrb-" stem="(" pos="-LRB-" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="Virago" lemma="Virago" stem="virago" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="Pounds" lemma="Pounds" stem="pound" pos="NNPS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="11" string="14.99" lemma="14.99" stem="14.99" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="12" string=")" lemma="-rrb-" stem=")" pos="-RRB-" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNS Daughters)) (PP (IN of) (NP (DT the) (NNP House))) (PP (IN by) (NP (NP (NNP Michele) (NNP Roberts)) (PRN (-LRB- -LRB-) (NP (NP (NNP Virago) (NNPS Pounds)) (NP (CD 14.99))) (-RRB- -RRB-)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="14.99" type="NP">
          <tokens>
            <token id="11" string="14.99" />
          </tokens>
        </chunking>
        <chunking id="2" string="Daughters" type="NP">
          <tokens>
            <token id="1" string="Daughters" />
          </tokens>
        </chunking>
        <chunking id="3" string="Daughters of the House by Michele Roberts -LRB- Virago Pounds 14.99 -RRB- ." type="NP">
          <tokens>
            <token id="1" string="Daughters" />
            <token id="2" string="of" />
            <token id="3" string="the" />
            <token id="4" string="House" />
            <token id="5" string="by" />
            <token id="6" string="Michele" />
            <token id="7" string="Roberts" />
            <token id="8" string="(" />
            <token id="9" string="Virago" />
            <token id="10" string="Pounds" />
            <token id="11" string="14.99" />
            <token id="12" string=")" />
            <token id="13" string="." />
          </tokens>
        </chunking>
        <chunking id="4" string="the House" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="House" />
          </tokens>
        </chunking>
        <chunking id="5" string="Virago Pounds" type="NP">
          <tokens>
            <token id="9" string="Virago" />
            <token id="10" string="Pounds" />
          </tokens>
        </chunking>
        <chunking id="6" string="Virago Pounds 14.99" type="NP">
          <tokens>
            <token id="9" string="Virago" />
            <token id="10" string="Pounds" />
            <token id="11" string="14.99" />
          </tokens>
        </chunking>
        <chunking id="7" string="Michele Roberts" type="NP">
          <tokens>
            <token id="6" string="Michele" />
            <token id="7" string="Roberts" />
          </tokens>
        </chunking>
        <chunking id="8" string="Michele Roberts -LRB- Virago Pounds 14.99 -RRB-" type="NP">
          <tokens>
            <token id="6" string="Michele" />
            <token id="7" string="Roberts" />
            <token id="8" string="(" />
            <token id="9" string="Virago" />
            <token id="10" string="Pounds" />
            <token id="11" string="14.99" />
            <token id="12" string=")" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Daughters</dependent>
        </dependency>
        <dependency type="case">
          <governor id="4">House</governor>
          <dependent id="2">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">House</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="1">Daughters</governor>
          <dependent id="4">House</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">Roberts</governor>
          <dependent id="5">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">Roberts</governor>
          <dependent id="6">Michele</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="1">Daughters</governor>
          <dependent id="7">Roberts</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="10">Pounds</governor>
          <dependent id="9">Virago</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="7">Roberts</governor>
          <dependent id="10">Pounds</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="10">Pounds</governor>
          <dependent id="11">14.99</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="14.99" type="NUMBER" score="0.0">
          <tokens>
            <token id="11" string="14.99" />
          </tokens>
        </entity>
        <entity id="2" string="House" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="4" string="House" />
          </tokens>
        </entity>
        <entity id="3" string="Michele Roberts" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Michele" />
            <token id="7" string="Roberts" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="28" has_coreference="true">
      <content>The surprise contender because it is not yet published.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="surprise" lemma="surprise" stem="surpris" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="contender" lemma="contender" stem="contend" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="because" lemma="because" stem="becaus" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="yet" lemma="yet" stem="yet" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="published" lemma="publish" stem="publish" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT The) (NN surprise) (NN contender)) (PP (IN because) (NP (PRP it)))) (VP (VBZ is) (RB not) (VP (ADVP (RB yet)) (VBN published))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="is not yet published" type="VP">
          <tokens>
            <token id="6" string="is" />
            <token id="7" string="not" />
            <token id="8" string="yet" />
            <token id="9" string="published" />
          </tokens>
        </chunking>
        <chunking id="2" string="The surprise contender" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="surprise" />
            <token id="3" string="contender" />
          </tokens>
        </chunking>
        <chunking id="3" string="it" type="NP">
          <tokens>
            <token id="5" string="it" />
          </tokens>
        </chunking>
        <chunking id="4" string="yet published" type="VP">
          <tokens>
            <token id="8" string="yet" />
            <token id="9" string="published" />
          </tokens>
        </chunking>
        <chunking id="5" string="The surprise contender because it" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="surprise" />
            <token id="3" string="contender" />
            <token id="4" string="because" />
            <token id="5" string="it" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">contender</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">contender</governor>
          <dependent id="2">surprise</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="9">published</governor>
          <dependent id="3">contender</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">it</governor>
          <dependent id="4">because</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">contender</governor>
          <dependent id="5">it</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="9">published</governor>
          <dependent id="6">is</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="9">published</governor>
          <dependent id="7">not</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="9">published</governor>
          <dependent id="8">yet</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="9">published</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="29" has_coreference="true">
      <content>Roberts comes from an Anglo-French Catholic background and this informs her new novel with a strong rural-Normandy atmosphere.</content>
      <tokens>
        <token id="1" string="Roberts" lemma="Roberts" stem="robert" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="comes" lemma="come" stem="come" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="Anglo-French" lemma="anglo-french" stem="anglo-french" pos="JJ" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="6" string="Catholic" lemma="catholic" stem="cathol" pos="JJ" type="Word" isStopWord="false" ner="IDEOLOGY" is_referenced="false" is_refers="false" />
        <token id="7" string="background" lemma="background" stem="background" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="informs" lemma="inform" stem="inform" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="new" lemma="new" stem="new" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="novel" lemma="novel" stem="novel" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="strong" lemma="strong" stem="strong" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="rural-Normandy" lemma="rural-normandy" stem="rural-normandi" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="atmosphere" lemma="atmosphere" stem="atmospher" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (NNP Roberts)) (VP (VBZ comes) (PP (IN from) (NP (DT an) (JJ Anglo-French) (JJ Catholic) (NN background))))) (CC and) (S (NP (DT this)) (VP (VBZ informs) (NP (NP (PRP$ her) (JJ new) (JJ novel)) (PP (IN with) (NP (DT a) (JJ strong) (JJ rural-Normandy) (NN atmosphere)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="informs her new novel with a strong rural-Normandy atmosphere" type="VP">
          <tokens>
            <token id="10" string="informs" />
            <token id="11" string="her" />
            <token id="12" string="new" />
            <token id="13" string="novel" />
            <token id="14" string="with" />
            <token id="15" string="a" />
            <token id="16" string="strong" />
            <token id="17" string="rural-Normandy" />
            <token id="18" string="atmosphere" />
          </tokens>
        </chunking>
        <chunking id="2" string="an Anglo-French Catholic background" type="NP">
          <tokens>
            <token id="4" string="an" />
            <token id="5" string="Anglo-French" />
            <token id="6" string="Catholic" />
            <token id="7" string="background" />
          </tokens>
        </chunking>
        <chunking id="3" string="her new novel" type="NP">
          <tokens>
            <token id="11" string="her" />
            <token id="12" string="new" />
            <token id="13" string="novel" />
          </tokens>
        </chunking>
        <chunking id="4" string="this" type="NP">
          <tokens>
            <token id="9" string="this" />
          </tokens>
        </chunking>
        <chunking id="5" string="her new novel with a strong rural-Normandy atmosphere" type="NP">
          <tokens>
            <token id="11" string="her" />
            <token id="12" string="new" />
            <token id="13" string="novel" />
            <token id="14" string="with" />
            <token id="15" string="a" />
            <token id="16" string="strong" />
            <token id="17" string="rural-Normandy" />
            <token id="18" string="atmosphere" />
          </tokens>
        </chunking>
        <chunking id="6" string="Roberts" type="NP">
          <tokens>
            <token id="1" string="Roberts" />
          </tokens>
        </chunking>
        <chunking id="7" string="comes from an Anglo-French Catholic background" type="VP">
          <tokens>
            <token id="2" string="comes" />
            <token id="3" string="from" />
            <token id="4" string="an" />
            <token id="5" string="Anglo-French" />
            <token id="6" string="Catholic" />
            <token id="7" string="background" />
          </tokens>
        </chunking>
        <chunking id="8" string="a strong rural-Normandy atmosphere" type="NP">
          <tokens>
            <token id="15" string="a" />
            <token id="16" string="strong" />
            <token id="17" string="rural-Normandy" />
            <token id="18" string="atmosphere" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">comes</governor>
          <dependent id="1">Roberts</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">comes</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">background</governor>
          <dependent id="3">from</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">background</governor>
          <dependent id="4">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">background</governor>
          <dependent id="5">Anglo-French</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">background</governor>
          <dependent id="6">Catholic</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">comes</governor>
          <dependent id="7">background</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="2">comes</governor>
          <dependent id="8">and</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">informs</governor>
          <dependent id="9">this</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="2">comes</governor>
          <dependent id="10">informs</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="13">novel</governor>
          <dependent id="11">her</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">novel</governor>
          <dependent id="12">new</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="10">informs</governor>
          <dependent id="13">novel</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">atmosphere</governor>
          <dependent id="14">with</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">atmosphere</governor>
          <dependent id="15">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="18">atmosphere</governor>
          <dependent id="16">strong</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="18">atmosphere</governor>
          <dependent id="17">rural-Normandy</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">novel</governor>
          <dependent id="18">atmosphere</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Catholic" type="IDEOLOGY" score="0.0">
          <tokens>
            <token id="6" string="Catholic" />
          </tokens>
        </entity>
        <entity id="2" string="Anglo-French" type="MISC" score="0.0">
          <tokens>
            <token id="5" string="Anglo-French" />
          </tokens>
        </entity>
        <entity id="3" string="Roberts" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Roberts" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="30" has_coreference="false">
      <content>Two cousins, one pious, one worldly.</content>
      <tokens>
        <token id="1" string="Two" lemma="two" stem="two" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="2" string="cousins" lemma="cousin" stem="cousin" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="one" lemma="one" stem="on" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="5" string="pious" lemma="pious" stem="piou" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="one" lemma="one" stem="on" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="8" string="worldly" lemma="worldly" stem="worldli" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (CD Two) (NNS cousins)) (PRN (, ,) (NP-TMP (CD one)) (ADJP (JJ pious)) (, ,) (NP-TMP (CD one)) (ADJP (JJ worldly))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Two cousins" type="NP">
          <tokens>
            <token id="1" string="Two" />
            <token id="2" string="cousins" />
          </tokens>
        </chunking>
        <chunking id="2" string="worldly" type="ADJP">
          <tokens>
            <token id="8" string="worldly" />
          </tokens>
        </chunking>
        <chunking id="3" string="pious" type="ADJP">
          <tokens>
            <token id="5" string="pious" />
          </tokens>
        </chunking>
        <chunking id="4" string="Two cousins , one pious , one worldly ." type="NP">
          <tokens>
            <token id="1" string="Two" />
            <token id="2" string="cousins" />
            <token id="3" string="," />
            <token id="4" string="one" />
            <token id="5" string="pious" />
            <token id="6" string="," />
            <token id="7" string="one" />
            <token id="8" string="worldly" />
            <token id="9" string="." />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nummod">
          <governor id="2">cousins</governor>
          <dependent id="1">Two</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">cousins</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="2">cousins</governor>
          <dependent id="4">one</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="4">one</governor>
          <dependent id="5">pious</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="4">one</governor>
          <dependent id="7">one</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="4">one</governor>
          <dependent id="8">worldly</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="one" type="NUMBER" score="0.0">
          <tokens>
            <token id="4" string="one" />
          </tokens>
        </entity>
        <entity id="2" string="Two" type="NUMBER" score="0.0">
          <tokens>
            <token id="1" string="Two" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="31" has_coreference="false">
      <content>Shades of Saint Therese of Lisieux.</content>
      <tokens>
        <token id="1" string="Shades" lemma="shades" stem="shade" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="Saint" lemma="Saint" stem="saint" pos="NNP" type="Word" isStopWord="false" ner="TITLE" is_referenced="false" is_refers="false" />
        <token id="4" string="Therese" lemma="Therese" stem="theres" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="5" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Lisieux" lemma="Lisieux" stem="lisieux" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="7" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (NNS Shades)) (PP (IN of) (NP (NNP Saint) (NNP Therese))) (PP (IN of) (NP (NNP Lisieux))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Saint Therese" type="NP">
          <tokens>
            <token id="3" string="Saint" />
            <token id="4" string="Therese" />
          </tokens>
        </chunking>
        <chunking id="2" string="Lisieux" type="NP">
          <tokens>
            <token id="6" string="Lisieux" />
          </tokens>
        </chunking>
        <chunking id="3" string="Shades of Saint Therese of Lisieux ." type="NP">
          <tokens>
            <token id="1" string="Shades" />
            <token id="2" string="of" />
            <token id="3" string="Saint" />
            <token id="4" string="Therese" />
            <token id="5" string="of" />
            <token id="6" string="Lisieux" />
            <token id="7" string="." />
          </tokens>
        </chunking>
        <chunking id="4" string="Shades" type="NP">
          <tokens>
            <token id="1" string="Shades" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="1">Shades</dependent>
        </dependency>
        <dependency type="case">
          <governor id="4">Therese</governor>
          <dependent id="2">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">Therese</governor>
          <dependent id="3">Saint</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="1">Shades</governor>
          <dependent id="4">Therese</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">Lisieux</governor>
          <dependent id="5">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="1">Shades</governor>
          <dependent id="6">Lisieux</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Lisieux" type="LOCATION" score="0.0">
          <tokens>
            <token id="6" string="Lisieux" />
          </tokens>
        </entity>
        <entity id="2" string="Therese" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="Therese" />
          </tokens>
        </entity>
        <entity id="3" string="Saint" type="TITLE" score="0.0">
          <tokens>
            <token id="3" string="Saint" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="32" has_coreference="false">
      <content>Lots of action in the kitchen; something is hidden in the cellar.</content>
      <tokens>
        <token id="1" string="Lots" lemma="lot" stem="lot" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="action" lemma="action" stem="action" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="kitchen" lemma="kitchen" stem="kitchen" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string=";" lemma=";" stem=";" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="something" lemma="something" stem="someth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="hidden" lemma="hide" stem="hidden" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="cellar" lemma="cellar" stem="cellar" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNS Lots)) (PP (IN of) (NP (NP (NN action)) (PP (IN in) (NP (DT the) (NN kitchen) (: ;) (NN something)))))) (VP (VBZ is) (VP (VBN hidden) (PP (IN in) (NP (DT the) (NN cellar))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="is hidden in the cellar" type="VP">
          <tokens>
            <token id="9" string="is" />
            <token id="10" string="hidden" />
            <token id="11" string="in" />
            <token id="12" string="the" />
            <token id="13" string="cellar" />
          </tokens>
        </chunking>
        <chunking id="2" string="the kitchen ; something" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="kitchen" />
            <token id="7" string=";" />
            <token id="8" string="something" />
          </tokens>
        </chunking>
        <chunking id="3" string="hidden in the cellar" type="VP">
          <tokens>
            <token id="10" string="hidden" />
            <token id="11" string="in" />
            <token id="12" string="the" />
            <token id="13" string="cellar" />
          </tokens>
        </chunking>
        <chunking id="4" string="Lots" type="NP">
          <tokens>
            <token id="1" string="Lots" />
          </tokens>
        </chunking>
        <chunking id="5" string="action in the kitchen ; something" type="NP">
          <tokens>
            <token id="3" string="action" />
            <token id="4" string="in" />
            <token id="5" string="the" />
            <token id="6" string="kitchen" />
            <token id="7" string=";" />
            <token id="8" string="something" />
          </tokens>
        </chunking>
        <chunking id="6" string="the cellar" type="NP">
          <tokens>
            <token id="12" string="the" />
            <token id="13" string="cellar" />
          </tokens>
        </chunking>
        <chunking id="7" string="Lots of action in the kitchen ; something" type="NP">
          <tokens>
            <token id="1" string="Lots" />
            <token id="2" string="of" />
            <token id="3" string="action" />
            <token id="4" string="in" />
            <token id="5" string="the" />
            <token id="6" string="kitchen" />
            <token id="7" string=";" />
            <token id="8" string="something" />
          </tokens>
        </chunking>
        <chunking id="8" string="action" type="NP">
          <tokens>
            <token id="3" string="action" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="10">hidden</governor>
          <dependent id="1">Lots</dependent>
        </dependency>
        <dependency type="case">
          <governor id="3">action</governor>
          <dependent id="2">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="1">Lots</governor>
          <dependent id="3">action</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">something</governor>
          <dependent id="4">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">something</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">something</governor>
          <dependent id="6">kitchen</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">action</governor>
          <dependent id="8">something</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="10">hidden</governor>
          <dependent id="9">is</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="10">hidden</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">cellar</governor>
          <dependent id="11">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">cellar</governor>
          <dependent id="12">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">hidden</governor>
          <dependent id="13">cellar</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="33" has_coreference="true">
      <content>Sacred Hunger by Barry Unsworth (Hamish Hamilton Pounds 15.99).</content>
      <tokens>
        <token id="1" string="Sacred" lemma="sacred" stem="sacr" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="Hunger" lemma="hunger" stem="hunger" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="Barry" lemma="Barry" stem="barri" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="5" string="Unsworth" lemma="Unsworth" stem="unsworth" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="6" string="(" lemma="-lrb-" stem="(" pos="-LRB-" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="Hamish" lemma="Hamish" stem="hamish" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="8" string="Hamilton" lemma="Hamilton" stem="hamilton" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="9" string="Pounds" lemma="Pounds" stem="pound" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="15.99" lemma="15.99" stem="15.99" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="true" is_refers="false" />
        <token id="11" string=")" lemma="-rrb-" stem=")" pos="-RRB-" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (NP (NP (JJ Sacred) (NN Hunger)) (PP (IN by) (NP (NP (NNP Barry) (NNP Unsworth)) (PRN (-LRB- -LRB-) (NP (NP (NNP Hamish) (NNP Hamilton) (NNP Pounds)) (NP (CD 15.99))) (-RRB- -RRB-)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Hamish Hamilton Pounds" type="NP">
          <tokens>
            <token id="7" string="Hamish" />
            <token id="8" string="Hamilton" />
            <token id="9" string="Pounds" />
          </tokens>
        </chunking>
        <chunking id="2" string="15.99" type="NP">
          <tokens>
            <token id="10" string="15.99" />
          </tokens>
        </chunking>
        <chunking id="3" string="Barry Unsworth" type="NP">
          <tokens>
            <token id="4" string="Barry" />
            <token id="5" string="Unsworth" />
          </tokens>
        </chunking>
        <chunking id="4" string="Sacred Hunger" type="NP">
          <tokens>
            <token id="1" string="Sacred" />
            <token id="2" string="Hunger" />
          </tokens>
        </chunking>
        <chunking id="5" string="Sacred Hunger by Barry Unsworth -LRB- Hamish Hamilton Pounds 15.99 -RRB- ." type="NP">
          <tokens>
            <token id="1" string="Sacred" />
            <token id="2" string="Hunger" />
            <token id="3" string="by" />
            <token id="4" string="Barry" />
            <token id="5" string="Unsworth" />
            <token id="6" string="(" />
            <token id="7" string="Hamish" />
            <token id="8" string="Hamilton" />
            <token id="9" string="Pounds" />
            <token id="10" string="15.99" />
            <token id="11" string=")" />
            <token id="12" string="." />
          </tokens>
        </chunking>
        <chunking id="6" string="Barry Unsworth -LRB- Hamish Hamilton Pounds 15.99 -RRB-" type="NP">
          <tokens>
            <token id="4" string="Barry" />
            <token id="5" string="Unsworth" />
            <token id="6" string="(" />
            <token id="7" string="Hamish" />
            <token id="8" string="Hamilton" />
            <token id="9" string="Pounds" />
            <token id="10" string="15.99" />
            <token id="11" string=")" />
          </tokens>
        </chunking>
        <chunking id="7" string="Hamish Hamilton Pounds 15.99" type="NP">
          <tokens>
            <token id="7" string="Hamish" />
            <token id="8" string="Hamilton" />
            <token id="9" string="Pounds" />
            <token id="10" string="15.99" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="amod">
          <governor id="2">Hunger</governor>
          <dependent id="1">Sacred</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">Hunger</dependent>
        </dependency>
        <dependency type="case">
          <governor id="5">Unsworth</governor>
          <dependent id="3">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="5">Unsworth</governor>
          <dependent id="4">Barry</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">Hunger</governor>
          <dependent id="5">Unsworth</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">Pounds</governor>
          <dependent id="7">Hamish</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">Pounds</governor>
          <dependent id="8">Hamilton</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="5">Unsworth</governor>
          <dependent id="9">Pounds</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="9">Pounds</governor>
          <dependent id="10">15.99</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Hamish Hamilton" type="PERSON" score="0.0">
          <tokens>
            <token id="7" string="Hamish" />
            <token id="8" string="Hamilton" />
          </tokens>
        </entity>
        <entity id="2" string="15.99" type="NUMBER" score="0.0">
          <tokens>
            <token id="10" string="15.99" />
          </tokens>
        </entity>
        <entity id="3" string="Barry Unsworth" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="Barry" />
            <token id="5" string="Unsworth" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="34" has_coreference="true">
      <content>Unsworth&amp;apost;s massive novel about the 1750s slave trade came out early this year to widespread critical praise.</content>
      <tokens>
        <token id="1" string="Unsworth" lemma="Unsworth" stem="unsworth" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="massive" lemma="massive" stem="massiv" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="novel" lemma="novel" stem="novel" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="7" string="1750s" lemma="1750s" stem="1750" pos="CD" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="8" string="slave" lemma="slave" stem="slave" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="trade" lemma="trade" stem="trade" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="came" lemma="come" stem="came" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="out" lemma="out" stem="out" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="early" lemma="early" stem="earli" pos="RB" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="13" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="14" string="year" lemma="year" stem="year" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="15" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="widespread" lemma="widespread" stem="widespread" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="critical" lemma="critical" stem="critic" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="praise" lemma="praise" stem="prais" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Unsworth) (POS 's)) (JJ massive) (ADJP (JJ novel) (PP (IN about) (NP (DT the) (CD 1750s) (NN slave)))) (NN trade)) (VP (VBD came) (ADVP (RB out)) (NP-TMP (RB early) (DT this) (NN year)) (PP (TO to) (NP (JJ widespread) (JJ critical) (NN praise)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Unsworth 's" type="NP">
          <tokens>
            <token id="1" string="Unsworth" />
            <token id="2" string="'s" />
          </tokens>
        </chunking>
        <chunking id="2" string="the 1750s slave" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="1750s" />
            <token id="8" string="slave" />
          </tokens>
        </chunking>
        <chunking id="3" string="came out early this year to widespread critical praise" type="VP">
          <tokens>
            <token id="10" string="came" />
            <token id="11" string="out" />
            <token id="12" string="early" />
            <token id="13" string="this" />
            <token id="14" string="year" />
            <token id="15" string="to" />
            <token id="16" string="widespread" />
            <token id="17" string="critical" />
            <token id="18" string="praise" />
          </tokens>
        </chunking>
        <chunking id="4" string="novel about the 1750s slave" type="ADJP">
          <tokens>
            <token id="4" string="novel" />
            <token id="5" string="about" />
            <token id="6" string="the" />
            <token id="7" string="1750s" />
            <token id="8" string="slave" />
          </tokens>
        </chunking>
        <chunking id="5" string="Unsworth 's massive novel about the 1750s slave trade" type="NP">
          <tokens>
            <token id="1" string="Unsworth" />
            <token id="2" string="'s" />
            <token id="3" string="massive" />
            <token id="4" string="novel" />
            <token id="5" string="about" />
            <token id="6" string="the" />
            <token id="7" string="1750s" />
            <token id="8" string="slave" />
            <token id="9" string="trade" />
          </tokens>
        </chunking>
        <chunking id="6" string="widespread critical praise" type="NP">
          <tokens>
            <token id="16" string="widespread" />
            <token id="17" string="critical" />
            <token id="18" string="praise" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nmod:poss">
          <governor id="9">trade</governor>
          <dependent id="1">Unsworth</dependent>
        </dependency>
        <dependency type="case">
          <governor id="1">Unsworth</governor>
          <dependent id="2">'s</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">trade</governor>
          <dependent id="3">massive</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="9">trade</governor>
          <dependent id="4">novel</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">slave</governor>
          <dependent id="5">about</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">slave</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="8">slave</governor>
          <dependent id="7">1750s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">novel</governor>
          <dependent id="8">slave</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">came</governor>
          <dependent id="9">trade</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="10">came</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="10">came</governor>
          <dependent id="11">out</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="14">year</governor>
          <dependent id="12">early</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">year</governor>
          <dependent id="13">this</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="10">came</governor>
          <dependent id="14">year</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">praise</governor>
          <dependent id="15">to</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="18">praise</governor>
          <dependent id="16">widespread</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="18">praise</governor>
          <dependent id="17">critical</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">came</governor>
          <dependent id="18">praise</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Unsworth" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Unsworth" />
          </tokens>
        </entity>
        <entity id="2" string="about the 1750s" type="DATE" score="0.0">
          <tokens>
            <token id="5" string="about" />
            <token id="6" string="the" />
            <token id="7" string="1750s" />
          </tokens>
        </entity>
        <entity id="3" string="early this year" type="DATE" score="0.0">
          <tokens>
            <token id="12" string="early" />
            <token id="13" string="this" />
            <token id="14" string="year" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="35" has_coreference="false">
      <content>The shipboard detail - the horrors of the trade -are brilliantly done.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="shipboard" lemma="shipboard" stem="shipboard" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="detail" lemma="detail" stem="detail" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="-" lemma="-" stem="-" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="horrors" lemma="horror" stem="horror" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="trade" lemma="trade" stem="trade" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="-" lemma="-" stem="-" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="are" lemma="be" stem="ar" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="brilliantly" lemma="brilliantly" stem="brilliantli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="done" lemma="do" stem="done" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (DT The) (NN shipboard) (NN detail)) (: -) (NP (NP (DT the) (NNS horrors)) (PP (IN of) (NP (DT the) (NN trade)))) (: -)) (VP (VBP are) (ADVP (RB brilliantly)) (VP (VBN done))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="The shipboard detail - the horrors of the trade -" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="shipboard" />
            <token id="3" string="detail" />
            <token id="4" string="-" />
            <token id="5" string="the" />
            <token id="6" string="horrors" />
            <token id="7" string="of" />
            <token id="8" string="the" />
            <token id="9" string="trade" />
            <token id="10" string="-" />
          </tokens>
        </chunking>
        <chunking id="2" string="the trade" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="trade" />
          </tokens>
        </chunking>
        <chunking id="3" string="the horrors" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="horrors" />
          </tokens>
        </chunking>
        <chunking id="4" string="the horrors of the trade" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="horrors" />
            <token id="7" string="of" />
            <token id="8" string="the" />
            <token id="9" string="trade" />
          </tokens>
        </chunking>
        <chunking id="5" string="The shipboard detail" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="shipboard" />
            <token id="3" string="detail" />
          </tokens>
        </chunking>
        <chunking id="6" string="done" type="VP">
          <tokens>
            <token id="13" string="done" />
          </tokens>
        </chunking>
        <chunking id="7" string="are brilliantly done" type="VP">
          <tokens>
            <token id="11" string="are" />
            <token id="12" string="brilliantly" />
            <token id="13" string="done" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">detail</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">detail</governor>
          <dependent id="2">shipboard</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="13">done</governor>
          <dependent id="3">detail</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">horrors</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="3">detail</governor>
          <dependent id="6">horrors</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">trade</governor>
          <dependent id="7">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">trade</governor>
          <dependent id="8">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">horrors</governor>
          <dependent id="9">trade</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="13">done</governor>
          <dependent id="11">are</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="13">done</governor>
          <dependent id="12">brilliantly</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="13">done</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="36" has_coreference="true">
      <content>The judges will have to rule on the characterisation of the European protagonists who go off to found a Utopia in Florida.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="judges" lemma="judge" stem="judg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="will" lemma="will" stem="will" pos="MD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="have" lemma="have" stem="have" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="rule" lemma="rule" stem="rule" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="characterisation" lemma="characterisation" stem="characteris" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="European" lemma="european" stem="european" pos="JJ" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="13" string="protagonists" lemma="protagonist" stem="protagonist" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="go" lemma="go" stem="go" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="off" lemma="off" stem="off" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="found" lemma="find" stem="found" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="Utopia" lemma="Utopia" stem="utopia" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="Florida" lemma="Florida" stem="florida" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="23" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NNS judges)) (VP (MD will) (VP (VB have) (S (VP (TO to) (VP (VB rule) (PP (IN on) (NP (NP (DT the) (NN characterisation)) (PP (IN of) (NP (NP (DT the) (JJ European) (NNS protagonists)) (SBAR (WHNP (WP who)) (S (VP (VBP go) (PRT (RP off)) (S (VP (TO to) (VP (VBN found) (NP (NP (DT a) (NNP Utopia)) (PP (IN in) (NP (NNP Florida))))))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="to found a Utopia in Florida" type="VP">
          <tokens>
            <token id="17" string="to" />
            <token id="18" string="found" />
            <token id="19" string="a" />
            <token id="20" string="Utopia" />
            <token id="21" string="in" />
            <token id="22" string="Florida" />
          </tokens>
        </chunking>
        <chunking id="2" string="the European protagonists" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="European" />
            <token id="13" string="protagonists" />
          </tokens>
        </chunking>
        <chunking id="3" string="have to rule on the characterisation of the European protagonists who go off to found a Utopia in Florida" type="VP">
          <tokens>
            <token id="4" string="have" />
            <token id="5" string="to" />
            <token id="6" string="rule" />
            <token id="7" string="on" />
            <token id="8" string="the" />
            <token id="9" string="characterisation" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="European" />
            <token id="13" string="protagonists" />
            <token id="14" string="who" />
            <token id="15" string="go" />
            <token id="16" string="off" />
            <token id="17" string="to" />
            <token id="18" string="found" />
            <token id="19" string="a" />
            <token id="20" string="Utopia" />
            <token id="21" string="in" />
            <token id="22" string="Florida" />
          </tokens>
        </chunking>
        <chunking id="4" string="a Utopia" type="NP">
          <tokens>
            <token id="19" string="a" />
            <token id="20" string="Utopia" />
          </tokens>
        </chunking>
        <chunking id="5" string="the characterisation of the European protagonists who go off to found a Utopia in Florida" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="characterisation" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="European" />
            <token id="13" string="protagonists" />
            <token id="14" string="who" />
            <token id="15" string="go" />
            <token id="16" string="off" />
            <token id="17" string="to" />
            <token id="18" string="found" />
            <token id="19" string="a" />
            <token id="20" string="Utopia" />
            <token id="21" string="in" />
            <token id="22" string="Florida" />
          </tokens>
        </chunking>
        <chunking id="6" string="rule on the characterisation of the European protagonists who go off to found a Utopia in Florida" type="VP">
          <tokens>
            <token id="6" string="rule" />
            <token id="7" string="on" />
            <token id="8" string="the" />
            <token id="9" string="characterisation" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="European" />
            <token id="13" string="protagonists" />
            <token id="14" string="who" />
            <token id="15" string="go" />
            <token id="16" string="off" />
            <token id="17" string="to" />
            <token id="18" string="found" />
            <token id="19" string="a" />
            <token id="20" string="Utopia" />
            <token id="21" string="in" />
            <token id="22" string="Florida" />
          </tokens>
        </chunking>
        <chunking id="7" string="Florida" type="NP">
          <tokens>
            <token id="22" string="Florida" />
          </tokens>
        </chunking>
        <chunking id="8" string="The judges" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="judges" />
          </tokens>
        </chunking>
        <chunking id="9" string="the European protagonists who go off to found a Utopia in Florida" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="European" />
            <token id="13" string="protagonists" />
            <token id="14" string="who" />
            <token id="15" string="go" />
            <token id="16" string="off" />
            <token id="17" string="to" />
            <token id="18" string="found" />
            <token id="19" string="a" />
            <token id="20" string="Utopia" />
            <token id="21" string="in" />
            <token id="22" string="Florida" />
          </tokens>
        </chunking>
        <chunking id="10" string="who go off to found a Utopia in Florida" type="SBAR">
          <tokens>
            <token id="14" string="who" />
            <token id="15" string="go" />
            <token id="16" string="off" />
            <token id="17" string="to" />
            <token id="18" string="found" />
            <token id="19" string="a" />
            <token id="20" string="Utopia" />
            <token id="21" string="in" />
            <token id="22" string="Florida" />
          </tokens>
        </chunking>
        <chunking id="11" string="found a Utopia in Florida" type="VP">
          <tokens>
            <token id="18" string="found" />
            <token id="19" string="a" />
            <token id="20" string="Utopia" />
            <token id="21" string="in" />
            <token id="22" string="Florida" />
          </tokens>
        </chunking>
        <chunking id="12" string="will have to rule on the characterisation of the European protagonists who go off to found a Utopia in Florida" type="VP">
          <tokens>
            <token id="3" string="will" />
            <token id="4" string="have" />
            <token id="5" string="to" />
            <token id="6" string="rule" />
            <token id="7" string="on" />
            <token id="8" string="the" />
            <token id="9" string="characterisation" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="European" />
            <token id="13" string="protagonists" />
            <token id="14" string="who" />
            <token id="15" string="go" />
            <token id="16" string="off" />
            <token id="17" string="to" />
            <token id="18" string="found" />
            <token id="19" string="a" />
            <token id="20" string="Utopia" />
            <token id="21" string="in" />
            <token id="22" string="Florida" />
          </tokens>
        </chunking>
        <chunking id="13" string="to rule on the characterisation of the European protagonists who go off to found a Utopia in Florida" type="VP">
          <tokens>
            <token id="5" string="to" />
            <token id="6" string="rule" />
            <token id="7" string="on" />
            <token id="8" string="the" />
            <token id="9" string="characterisation" />
            <token id="10" string="of" />
            <token id="11" string="the" />
            <token id="12" string="European" />
            <token id="13" string="protagonists" />
            <token id="14" string="who" />
            <token id="15" string="go" />
            <token id="16" string="off" />
            <token id="17" string="to" />
            <token id="18" string="found" />
            <token id="19" string="a" />
            <token id="20" string="Utopia" />
            <token id="21" string="in" />
            <token id="22" string="Florida" />
          </tokens>
        </chunking>
        <chunking id="14" string="the characterisation" type="NP">
          <tokens>
            <token id="8" string="the" />
            <token id="9" string="characterisation" />
          </tokens>
        </chunking>
        <chunking id="15" string="a Utopia in Florida" type="NP">
          <tokens>
            <token id="19" string="a" />
            <token id="20" string="Utopia" />
            <token id="21" string="in" />
            <token id="22" string="Florida" />
          </tokens>
        </chunking>
        <chunking id="16" string="go off to found a Utopia in Florida" type="VP">
          <tokens>
            <token id="15" string="go" />
            <token id="16" string="off" />
            <token id="17" string="to" />
            <token id="18" string="found" />
            <token id="19" string="a" />
            <token id="20" string="Utopia" />
            <token id="21" string="in" />
            <token id="22" string="Florida" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">judges</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">have</governor>
          <dependent id="2">judges</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="4">have</governor>
          <dependent id="3">will</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">have</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="6">rule</governor>
          <dependent id="5">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="4">have</governor>
          <dependent id="6">rule</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">characterisation</governor>
          <dependent id="7">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">characterisation</governor>
          <dependent id="8">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">rule</governor>
          <dependent id="9">characterisation</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">protagonists</governor>
          <dependent id="10">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">protagonists</governor>
          <dependent id="11">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">protagonists</governor>
          <dependent id="12">European</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">characterisation</governor>
          <dependent id="13">protagonists</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">go</governor>
          <dependent id="14">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="13">protagonists</governor>
          <dependent id="15">go</dependent>
        </dependency>
        <dependency type="compound:prt">
          <governor id="15">go</governor>
          <dependent id="16">off</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="18">found</governor>
          <dependent id="17">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="15">go</governor>
          <dependent id="18">found</dependent>
        </dependency>
        <dependency type="det">
          <governor id="20">Utopia</governor>
          <dependent id="19">a</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="18">found</governor>
          <dependent id="20">Utopia</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">Florida</governor>
          <dependent id="21">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">Utopia</governor>
          <dependent id="22">Florida</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="European" type="MISC" score="0.0">
          <tokens>
            <token id="12" string="European" />
          </tokens>
        </entity>
        <entity id="2" string="Florida" type="LOCATION" score="0.0">
          <tokens>
            <token id="22" string="Florida" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="37" has_coreference="true">
      <content>The judges this year, who will include Independent journalist Mark Lawson and John Coldstream, literary editor of The Daily Telegraph, will be chaired by the biographer Victoria Glendinning.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="judges" lemma="judge" stem="judg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="4" string="year" lemma="year" stem="year" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="will" lemma="will" stem="will" pos="MD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="include" lemma="include" stem="includ" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="Independent" lemma="Independent" stem="independ" pos="NNP" type="Word" isStopWord="false" ner="IDEOLOGY" is_referenced="false" is_refers="false" />
        <token id="10" string="journalist" lemma="journalist" stem="journalist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="Mark" lemma="Mark" stem="mark" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="12" string="Lawson" lemma="Lawson" stem="lawson" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="13" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="15" string="Coldstream" lemma="Coldstream" stem="coldstream" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="literary" lemma="literary" stem="literari" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="editor" lemma="editor" stem="editor" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="Daily" lemma="Daily" stem="daili" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="22" string="Telegraph" lemma="Telegraph" stem="telegraph" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="23" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="will" lemma="will" stem="will" pos="MD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="be" lemma="be" stem="be" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="chaired" lemma="chair" stem="chair" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="biographer" lemma="biographer" stem="biograph" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="Victoria" lemma="Victoria" stem="victoria" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="31" string="Glendinning" lemma="Glendinning" stem="glendin" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="32" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NP (DT The) (NNS judges)) (NP (DT this) (NN year))) (, ,) (SBAR (WHNP (WP who)) (S (VP (MD will) (VP (VB include) (NP (NP (NP (NNP Independent) (NN journalist) (NNP Mark) (NNP Lawson)) (CC and) (NP (NNP John) (NNP Coldstream))) (, ,) (NP (NP (JJ literary) (NN editor)) (PP (IN of) (NP (DT The) (NNP Daily) (NNP Telegraph))))))))) (, ,)) (VP (MD will) (VP (VB be) (VP (VBN chaired) (PP (IN by) (NP (DT the) (NN biographer) (NNP Victoria) (NNP Glendinning)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="be chaired by the biographer Victoria Glendinning" type="VP">
          <tokens>
            <token id="25" string="be" />
            <token id="26" string="chaired" />
            <token id="27" string="by" />
            <token id="28" string="the" />
            <token id="29" string="biographer" />
            <token id="30" string="Victoria" />
            <token id="31" string="Glendinning" />
          </tokens>
        </chunking>
        <chunking id="2" string="The judges this year" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="judges" />
            <token id="3" string="this" />
            <token id="4" string="year" />
          </tokens>
        </chunking>
        <chunking id="3" string="chaired by the biographer Victoria Glendinning" type="VP">
          <tokens>
            <token id="26" string="chaired" />
            <token id="27" string="by" />
            <token id="28" string="the" />
            <token id="29" string="biographer" />
            <token id="30" string="Victoria" />
            <token id="31" string="Glendinning" />
          </tokens>
        </chunking>
        <chunking id="4" string="literary editor of The Daily Telegraph" type="NP">
          <tokens>
            <token id="17" string="literary" />
            <token id="18" string="editor" />
            <token id="19" string="of" />
            <token id="20" string="The" />
            <token id="21" string="Daily" />
            <token id="22" string="Telegraph" />
          </tokens>
        </chunking>
        <chunking id="5" string="who will include Independent journalist Mark Lawson and John Coldstream , literary editor of The Daily Telegraph" type="SBAR">
          <tokens>
            <token id="6" string="who" />
            <token id="7" string="will" />
            <token id="8" string="include" />
            <token id="9" string="Independent" />
            <token id="10" string="journalist" />
            <token id="11" string="Mark" />
            <token id="12" string="Lawson" />
            <token id="13" string="and" />
            <token id="14" string="John" />
            <token id="15" string="Coldstream" />
            <token id="16" string="," />
            <token id="17" string="literary" />
            <token id="18" string="editor" />
            <token id="19" string="of" />
            <token id="20" string="The" />
            <token id="21" string="Daily" />
            <token id="22" string="Telegraph" />
          </tokens>
        </chunking>
        <chunking id="6" string="Independent journalist Mark Lawson" type="NP">
          <tokens>
            <token id="9" string="Independent" />
            <token id="10" string="journalist" />
            <token id="11" string="Mark" />
            <token id="12" string="Lawson" />
          </tokens>
        </chunking>
        <chunking id="7" string="literary editor" type="NP">
          <tokens>
            <token id="17" string="literary" />
            <token id="18" string="editor" />
          </tokens>
        </chunking>
        <chunking id="8" string="The judges this year , who will include Independent journalist Mark Lawson and John Coldstream , literary editor of The Daily Telegraph ," type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="judges" />
            <token id="3" string="this" />
            <token id="4" string="year" />
            <token id="5" string="," />
            <token id="6" string="who" />
            <token id="7" string="will" />
            <token id="8" string="include" />
            <token id="9" string="Independent" />
            <token id="10" string="journalist" />
            <token id="11" string="Mark" />
            <token id="12" string="Lawson" />
            <token id="13" string="and" />
            <token id="14" string="John" />
            <token id="15" string="Coldstream" />
            <token id="16" string="," />
            <token id="17" string="literary" />
            <token id="18" string="editor" />
            <token id="19" string="of" />
            <token id="20" string="The" />
            <token id="21" string="Daily" />
            <token id="22" string="Telegraph" />
            <token id="23" string="," />
          </tokens>
        </chunking>
        <chunking id="9" string="will include Independent journalist Mark Lawson and John Coldstream , literary editor of The Daily Telegraph" type="VP">
          <tokens>
            <token id="7" string="will" />
            <token id="8" string="include" />
            <token id="9" string="Independent" />
            <token id="10" string="journalist" />
            <token id="11" string="Mark" />
            <token id="12" string="Lawson" />
            <token id="13" string="and" />
            <token id="14" string="John" />
            <token id="15" string="Coldstream" />
            <token id="16" string="," />
            <token id="17" string="literary" />
            <token id="18" string="editor" />
            <token id="19" string="of" />
            <token id="20" string="The" />
            <token id="21" string="Daily" />
            <token id="22" string="Telegraph" />
          </tokens>
        </chunking>
        <chunking id="10" string="The judges" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="judges" />
          </tokens>
        </chunking>
        <chunking id="11" string="Independent journalist Mark Lawson and John Coldstream , literary editor of The Daily Telegraph" type="NP">
          <tokens>
            <token id="9" string="Independent" />
            <token id="10" string="journalist" />
            <token id="11" string="Mark" />
            <token id="12" string="Lawson" />
            <token id="13" string="and" />
            <token id="14" string="John" />
            <token id="15" string="Coldstream" />
            <token id="16" string="," />
            <token id="17" string="literary" />
            <token id="18" string="editor" />
            <token id="19" string="of" />
            <token id="20" string="The" />
            <token id="21" string="Daily" />
            <token id="22" string="Telegraph" />
          </tokens>
        </chunking>
        <chunking id="12" string="will be chaired by the biographer Victoria Glendinning" type="VP">
          <tokens>
            <token id="24" string="will" />
            <token id="25" string="be" />
            <token id="26" string="chaired" />
            <token id="27" string="by" />
            <token id="28" string="the" />
            <token id="29" string="biographer" />
            <token id="30" string="Victoria" />
            <token id="31" string="Glendinning" />
          </tokens>
        </chunking>
        <chunking id="13" string="include Independent journalist Mark Lawson and John Coldstream , literary editor of The Daily Telegraph" type="VP">
          <tokens>
            <token id="8" string="include" />
            <token id="9" string="Independent" />
            <token id="10" string="journalist" />
            <token id="11" string="Mark" />
            <token id="12" string="Lawson" />
            <token id="13" string="and" />
            <token id="14" string="John" />
            <token id="15" string="Coldstream" />
            <token id="16" string="," />
            <token id="17" string="literary" />
            <token id="18" string="editor" />
            <token id="19" string="of" />
            <token id="20" string="The" />
            <token id="21" string="Daily" />
            <token id="22" string="Telegraph" />
          </tokens>
        </chunking>
        <chunking id="14" string="The Daily Telegraph" type="NP">
          <tokens>
            <token id="20" string="The" />
            <token id="21" string="Daily" />
            <token id="22" string="Telegraph" />
          </tokens>
        </chunking>
        <chunking id="15" string="this year" type="NP">
          <tokens>
            <token id="3" string="this" />
            <token id="4" string="year" />
          </tokens>
        </chunking>
        <chunking id="16" string="Independent journalist Mark Lawson and John Coldstream" type="NP">
          <tokens>
            <token id="9" string="Independent" />
            <token id="10" string="journalist" />
            <token id="11" string="Mark" />
            <token id="12" string="Lawson" />
            <token id="13" string="and" />
            <token id="14" string="John" />
            <token id="15" string="Coldstream" />
          </tokens>
        </chunking>
        <chunking id="17" string="John Coldstream" type="NP">
          <tokens>
            <token id="14" string="John" />
            <token id="15" string="Coldstream" />
          </tokens>
        </chunking>
        <chunking id="18" string="the biographer Victoria Glendinning" type="NP">
          <tokens>
            <token id="28" string="the" />
            <token id="29" string="biographer" />
            <token id="30" string="Victoria" />
            <token id="31" string="Glendinning" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">judges</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="26">chaired</governor>
          <dependent id="2">judges</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">year</governor>
          <dependent id="3">this</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="2">judges</governor>
          <dependent id="4">year</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">include</governor>
          <dependent id="6">who</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="8">include</governor>
          <dependent id="7">will</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="2">judges</governor>
          <dependent id="8">include</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">Lawson</governor>
          <dependent id="9">Independent</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">Lawson</governor>
          <dependent id="10">journalist</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">Lawson</governor>
          <dependent id="11">Mark</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="8">include</governor>
          <dependent id="12">Lawson</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="12">Lawson</governor>
          <dependent id="13">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="15">Coldstream</governor>
          <dependent id="14">John</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="12">Lawson</governor>
          <dependent id="15">Coldstream</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="18">editor</governor>
          <dependent id="17">literary</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="12">Lawson</governor>
          <dependent id="18">editor</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">Telegraph</governor>
          <dependent id="19">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="22">Telegraph</governor>
          <dependent id="20">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="22">Telegraph</governor>
          <dependent id="21">Daily</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="18">editor</governor>
          <dependent id="22">Telegraph</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="26">chaired</governor>
          <dependent id="24">will</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="26">chaired</governor>
          <dependent id="25">be</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="26">chaired</dependent>
        </dependency>
        <dependency type="case">
          <governor id="31">Glendinning</governor>
          <dependent id="27">by</dependent>
        </dependency>
        <dependency type="det">
          <governor id="31">Glendinning</governor>
          <dependent id="28">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="31">Glendinning</governor>
          <dependent id="29">biographer</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="31">Glendinning</governor>
          <dependent id="30">Victoria</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">chaired</governor>
          <dependent id="31">Glendinning</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Independent" type="IDEOLOGY" score="0.0">
          <tokens>
            <token id="9" string="Independent" />
          </tokens>
        </entity>
        <entity id="2" string="Daily Telegraph" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="21" string="Daily" />
            <token id="22" string="Telegraph" />
          </tokens>
        </entity>
        <entity id="3" string="Mark Lawson" type="PERSON" score="0.0">
          <tokens>
            <token id="11" string="Mark" />
            <token id="12" string="Lawson" />
          </tokens>
        </entity>
        <entity id="4" string="Victoria Glendinning" type="PERSON" score="0.0">
          <tokens>
            <token id="30" string="Victoria" />
            <token id="31" string="Glendinning" />
          </tokens>
        </entity>
        <entity id="5" string="this year" type="DATE" score="0.0">
          <tokens>
            <token id="3" string="this" />
            <token id="4" string="year" />
          </tokens>
        </entity>
        <entity id="6" string="John Coldstream" type="PERSON" score="0.0">
          <tokens>
            <token id="14" string="John" />
            <token id="15" string="Coldstream" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="38" has_coreference="false">
      <content>The winner will be named at a Guildhall dinner on Tuesday October 13.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="winner" lemma="winner" stem="winner" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="will" lemma="will" stem="will" pos="MD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="be" lemma="be" stem="be" pos="VB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="named" lemma="name" stem="name" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Guildhall" lemma="Guildhall" stem="guildhal" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="dinner" lemma="dinner" stem="dinner" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="Tuesday" lemma="Tuesday" stem="tuesdai" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="12" string="October" lemma="October" stem="october" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="13" string="13" lemma="13" stem="13" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN winner)) (VP (MD will) (VP (VB be) (VP (VBN named) (PP (IN at) (NP (NP (DT a) (NNP Guildhall) (NN dinner)) (PP (IN on) (NP (NNP Tuesday))))) (NP-TMP (NNP October) (CD 13))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="will be named at a Guildhall dinner on Tuesday October 13" type="VP">
          <tokens>
            <token id="3" string="will" />
            <token id="4" string="be" />
            <token id="5" string="named" />
            <token id="6" string="at" />
            <token id="7" string="a" />
            <token id="8" string="Guildhall" />
            <token id="9" string="dinner" />
            <token id="10" string="on" />
            <token id="11" string="Tuesday" />
            <token id="12" string="October" />
            <token id="13" string="13" />
          </tokens>
        </chunking>
        <chunking id="2" string="a Guildhall dinner" type="NP">
          <tokens>
            <token id="7" string="a" />
            <token id="8" string="Guildhall" />
            <token id="9" string="dinner" />
          </tokens>
        </chunking>
        <chunking id="3" string="be named at a Guildhall dinner on Tuesday October 13" type="VP">
          <tokens>
            <token id="4" string="be" />
            <token id="5" string="named" />
            <token id="6" string="at" />
            <token id="7" string="a" />
            <token id="8" string="Guildhall" />
            <token id="9" string="dinner" />
            <token id="10" string="on" />
            <token id="11" string="Tuesday" />
            <token id="12" string="October" />
            <token id="13" string="13" />
          </tokens>
        </chunking>
        <chunking id="4" string="named at a Guildhall dinner on Tuesday October 13" type="VP">
          <tokens>
            <token id="5" string="named" />
            <token id="6" string="at" />
            <token id="7" string="a" />
            <token id="8" string="Guildhall" />
            <token id="9" string="dinner" />
            <token id="10" string="on" />
            <token id="11" string="Tuesday" />
            <token id="12" string="October" />
            <token id="13" string="13" />
          </tokens>
        </chunking>
        <chunking id="5" string="a Guildhall dinner on Tuesday" type="NP">
          <tokens>
            <token id="7" string="a" />
            <token id="8" string="Guildhall" />
            <token id="9" string="dinner" />
            <token id="10" string="on" />
            <token id="11" string="Tuesday" />
          </tokens>
        </chunking>
        <chunking id="6" string="Tuesday" type="NP">
          <tokens>
            <token id="11" string="Tuesday" />
          </tokens>
        </chunking>
        <chunking id="7" string="The winner" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="winner" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">winner</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="5">named</governor>
          <dependent id="2">winner</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="5">named</governor>
          <dependent id="3">will</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="5">named</governor>
          <dependent id="4">be</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">named</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">dinner</governor>
          <dependent id="6">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">dinner</governor>
          <dependent id="7">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">dinner</governor>
          <dependent id="8">Guildhall</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">named</governor>
          <dependent id="9">dinner</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">Tuesday</governor>
          <dependent id="10">on</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">dinner</governor>
          <dependent id="11">Tuesday</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="5">named</governor>
          <dependent id="12">October</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="12">October</governor>
          <dependent id="13">13</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Tuesday October 13" type="DATE" score="0.0">
          <tokens>
            <token id="11" string="Tuesday" />
            <token id="12" string="October" />
            <token id="13" string="13" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="39" has_coreference="true">
      <content>Last year the prize was won by Ben Okri, for his book The Famished Road.</content>
      <tokens>
        <token id="1" string="Last" lemma="last" stem="last" pos="JJ" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="2" string="year" lemma="year" stem="year" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="prize" lemma="prize" stem="prize" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="won" lemma="win" stem="won" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Ben" lemma="Ben" stem="ben" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="9" string="Okri" lemma="Okri" stem="okri" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="Famished" lemma="Famished" stem="famish" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="Road" lemma="Road" stem="road" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP-TMP (JJ Last) (NN year)) (NP (DT the) (NN prize)) (VP (VBD was) (VP (VBN won) (PP (IN by) (NP (NNP Ben) (NNP Okri))) (, ,) (PP (IN for) (NP (NP (PRP$ his) (NN book)) (NP (DT The) (NNP Famished) (NNP Road)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="his book" type="NP">
          <tokens>
            <token id="12" string="his" />
            <token id="13" string="book" />
          </tokens>
        </chunking>
        <chunking id="2" string="his book The Famished Road" type="NP">
          <tokens>
            <token id="12" string="his" />
            <token id="13" string="book" />
            <token id="14" string="The" />
            <token id="15" string="Famished" />
            <token id="16" string="Road" />
          </tokens>
        </chunking>
        <chunking id="3" string="Ben Okri" type="NP">
          <tokens>
            <token id="8" string="Ben" />
            <token id="9" string="Okri" />
          </tokens>
        </chunking>
        <chunking id="4" string="was won by Ben Okri , for his book The Famished Road" type="VP">
          <tokens>
            <token id="5" string="was" />
            <token id="6" string="won" />
            <token id="7" string="by" />
            <token id="8" string="Ben" />
            <token id="9" string="Okri" />
            <token id="10" string="," />
            <token id="11" string="for" />
            <token id="12" string="his" />
            <token id="13" string="book" />
            <token id="14" string="The" />
            <token id="15" string="Famished" />
            <token id="16" string="Road" />
          </tokens>
        </chunking>
        <chunking id="5" string="won by Ben Okri , for his book The Famished Road" type="VP">
          <tokens>
            <token id="6" string="won" />
            <token id="7" string="by" />
            <token id="8" string="Ben" />
            <token id="9" string="Okri" />
            <token id="10" string="," />
            <token id="11" string="for" />
            <token id="12" string="his" />
            <token id="13" string="book" />
            <token id="14" string="The" />
            <token id="15" string="Famished" />
            <token id="16" string="Road" />
          </tokens>
        </chunking>
        <chunking id="6" string="The Famished Road" type="NP">
          <tokens>
            <token id="14" string="The" />
            <token id="15" string="Famished" />
            <token id="16" string="Road" />
          </tokens>
        </chunking>
        <chunking id="7" string="the prize" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="prize" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="amod">
          <governor id="2">year</governor>
          <dependent id="1">Last</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="6">won</governor>
          <dependent id="2">year</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">prize</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="6">won</governor>
          <dependent id="4">prize</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="6">won</governor>
          <dependent id="5">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">won</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">Okri</governor>
          <dependent id="7">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">Okri</governor>
          <dependent id="8">Ben</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">won</governor>
          <dependent id="9">Okri</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">book</governor>
          <dependent id="11">for</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="13">book</governor>
          <dependent id="12">his</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">won</governor>
          <dependent id="13">book</dependent>
        </dependency>
        <dependency type="det">
          <governor id="16">Road</governor>
          <dependent id="14">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="16">Road</governor>
          <dependent id="15">Famished</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="13">book</governor>
          <dependent id="16">Road</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Ben Okri" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Ben" />
            <token id="9" string="Okri" />
          </tokens>
        </entity>
        <entity id="2" string="Last year" type="DATE" score="0.0">
          <tokens>
            <token id="1" string="Last" />
            <token id="2" string="year" />
          </tokens>
        </entity>
      </entities>
    </sentence>
  </sentences>
  <coreferences>
    <coreference id="2" type="NOMINAL">
      <referenced ids_tokens="10-11-12-13-14-15-16" string="the judges for this year 's Pounds" id_sentence="1" />
      <mentions>
        <mention ids_tokens="1-2" string="The judges" id_sentence="36" />
        <mention ids_tokens="1-4" string="The judges this year" id_sentence="37" />
      </mentions>
    </coreference>
    <coreference id="3" type="PROPER">
      <referenced ids_tokens="12-13-14" string="early this year" id_sentence="34" />
      <mentions>
        <mention ids_tokens="13-15" string="this year's" id_sentence="1" />
        <mention ids_tokens="1-3" string="This year's" id_sentence="4" />
        <mention ids_tokens="3-4" string="this year" id_sentence="37" />
      </mentions>
    </coreference>
    <coreference id="4" type="PROPER">
      <referenced ids_tokens="3-4" string="the House" id_sentence="27" />
      <mentions>
        <mention ids_tokens="27-28" string="the shortlist" id_sentence="1" />
        <mention ids_tokens="1-2" string="The shortlist" id_sentence="7" />
        <mention ids_tokens="5-14" string="Serenity House by Christopher Hope ( MacMillan Pounds 14.99 )" id_sentence="7" />
        <mention ids_tokens="5" string="it" id_sentence="28" />
      </mentions>
    </coreference>
    <coreference id="5" type="PROPER">
      <referenced ids_tokens="1-2-3-4-5-6-7-8-9-10-11-12-13-14" string="The Prize ( courtesy of Booker plc , the food and agri-business group )" id_sentence="2" />
      <mentions>
        <mention ids_tokens="1" string="Its" id_sentence="3" />
        <mention ids_tokens="3-4" string="the prize" id_sentence="39" />
      </mentions>
    </coreference>
    <coreference id="15" type="PROPER">
      <referenced ids_tokens="4-5" string="next Saturday" id_sentence="10" />
      <mentions>
        <mention ids_tokens="5" string="Saturday" id_sentence="26" />
      </mentions>
    </coreference>
    <coreference id="17" type="PROPER">
      <referenced ids_tokens="4-5-6-7-8-9-10-11-12" string="Ian McEwan ( Secker &amp;amp; Warburg Pounds 14.99 )" id_sentence="18" />
      <mentions>
        <mention ids_tokens="1-2" string="McEwan's" id_sentence="19" />
        <mention ids_tokens="1" string="He" id_sentence="20" />
        <mention ids_tokens="3-6" string="the novelist of misanthropy" id_sentence="20" />
      </mentions>
    </coreference>
    <coreference id="18" type="NOMINAL">
      <referenced ids_tokens="1-2-3-4-5" string="The ' English patient '" id_sentence="25" />
      <mentions>
        <mention ids_tokens="1-12" string="The English Patient by Michael Ondaatje ( Bloomsbury Pounds 14.99 ) ." id_sentence="22" />
      </mentions>
    </coreference>
    <coreference id="19" type="PROPER">
      <referenced ids_tokens="5-6-7-8-9-10-11" string="Michael Ondaatje ( Bloomsbury Pounds 14.99 )" id_sentence="22" />
      <mentions>
        <mention ids_tokens="6-7" string="novelist Ondaatje" id_sentence="23" />
        <mention ids_tokens="7" string="Ondaatje" id_sentence="23" />
      </mentions>
    </coreference>
    <coreference id="21" type="PROPER">
      <referenced ids_tokens="6-7-8-9-10-11-12" string="Michele Roberts ( Virago Pounds 14.99 )" id_sentence="27" />
      <mentions>
        <mention ids_tokens="1" string="Roberts" id_sentence="29" />
        <mention ids_tokens="11" string="her" id_sentence="29" />
      </mentions>
    </coreference>
    <coreference id="22" type="PROPER">
      <referenced ids_tokens="4-5-6-7-8-9-10-11" string="Barry Unsworth ( Hamish Hamilton Pounds 15.99 )" id_sentence="33" />
      <mentions>
        <mention ids_tokens="1-2" string="Unsworth's" id_sentence="34" />
      </mentions>
    </coreference>
  </coreferences>
</document>
