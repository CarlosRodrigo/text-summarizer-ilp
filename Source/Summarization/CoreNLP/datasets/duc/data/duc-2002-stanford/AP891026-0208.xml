<?xml version="1.0" encoding="UTF-8"?>
<document id="0" name="AP891026-0208">
  <sentences>
    <sentence id="1" has_coreference="true">
      <content>Japanese writer Kazuo Ishiguro won the 1989 Booker Prize, Britain&amp;apost;s top literary award, for his novel ``The Remains of the Day,&amp;apost;&amp;apost; judges announced Thursday.</content>
      <tokens>
        <token id="1" string="Japanese" lemma="japanese" stem="japanes" pos="JJ" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="true" is_refers="false" />
        <token id="2" string="writer" lemma="writer" stem="writer" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="Kazuo" lemma="Kazuo" stem="kazuo" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="4" string="Ishiguro" lemma="Ishiguro" stem="ishiguro" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="5" string="won" lemma="win" stem="won" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="1989" lemma="1989" stem="1989" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="8" string="Booker" lemma="Booker" stem="booker" pos="NNP" type="Word" isStopWord="false" ner="MISC" is_referenced="true" is_refers="false" />
        <token id="9" string="Prize" lemma="Prize" stem="prize" pos="NNP" type="Word" isStopWord="false" ner="MISC" is_referenced="true" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="Britain" lemma="Britain" stem="britain" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="12" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="top" lemma="top" stem="top" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="literary" lemma="literary" stem="literari" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="award" lemma="award" stem="award" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="19" string="novel" lemma="novel" stem="novel" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="20" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="Remains" lemma="remains" stem="remain" pos="NNS" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="23" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="24" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="25" string="Day" lemma="day" stem="dai" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="true" is_refers="false" />
        <token id="26" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="judges" lemma="judge" stem="judg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="announced" lemma="announce" stem="announc" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="Thursday" lemma="Thursday" stem="thursdai" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="31" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (JJ Japanese) (NN writer) (NNP Kazuo) (NNP Ishiguro)) (VP (VBD won) (NP (NP (DT the) (CD 1989) (NNP Booker) (NNP Prize)) (, ,) (NP (NP (NNP Britain) (POS 's)) (JJ top) (JJ literary) (NN award)) (, ,) (PP (IN for) (NP (PRP$ his) (JJ novel))) (`` ``) (NP (NP (DT The) (NNS Remains)) (PP (IN of) (NP (DT the) (NN Day))))))) (, ,) ('' '') (NP (NNS judges)) (VP (VBD announced) (NP-TMP (NNP Thursday))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Britain 's top literary award" type="NP">
          <tokens>
            <token id="11" string="Britain" />
            <token id="12" string="'s" />
            <token id="13" string="top" />
            <token id="14" string="literary" />
            <token id="15" string="award" />
          </tokens>
        </chunking>
        <chunking id="2" string="the 1989 Booker Prize , Britain 's top literary award , for his novel `` The Remains of the Day" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="1989" />
            <token id="8" string="Booker" />
            <token id="9" string="Prize" />
            <token id="10" string="," />
            <token id="11" string="Britain" />
            <token id="12" string="'s" />
            <token id="13" string="top" />
            <token id="14" string="literary" />
            <token id="15" string="award" />
            <token id="16" string="," />
            <token id="17" string="for" />
            <token id="18" string="his" />
            <token id="19" string="novel" />
            <token id="20" string="``" />
            <token id="21" string="The" />
            <token id="22" string="Remains" />
            <token id="23" string="of" />
            <token id="24" string="the" />
            <token id="25" string="Day" />
          </tokens>
        </chunking>
        <chunking id="3" string="the Day" type="NP">
          <tokens>
            <token id="24" string="the" />
            <token id="25" string="Day" />
          </tokens>
        </chunking>
        <chunking id="4" string="announced Thursday" type="VP">
          <tokens>
            <token id="29" string="announced" />
            <token id="30" string="Thursday" />
          </tokens>
        </chunking>
        <chunking id="5" string="Britain 's" type="NP">
          <tokens>
            <token id="11" string="Britain" />
            <token id="12" string="'s" />
          </tokens>
        </chunking>
        <chunking id="6" string="The Remains of the Day" type="NP">
          <tokens>
            <token id="21" string="The" />
            <token id="22" string="Remains" />
            <token id="23" string="of" />
            <token id="24" string="the" />
            <token id="25" string="Day" />
          </tokens>
        </chunking>
        <chunking id="7" string="judges" type="NP">
          <tokens>
            <token id="28" string="judges" />
          </tokens>
        </chunking>
        <chunking id="8" string="The Remains" type="NP">
          <tokens>
            <token id="21" string="The" />
            <token id="22" string="Remains" />
          </tokens>
        </chunking>
        <chunking id="9" string="Japanese writer Kazuo Ishiguro" type="NP">
          <tokens>
            <token id="1" string="Japanese" />
            <token id="2" string="writer" />
            <token id="3" string="Kazuo" />
            <token id="4" string="Ishiguro" />
          </tokens>
        </chunking>
        <chunking id="10" string="won the 1989 Booker Prize , Britain 's top literary award , for his novel `` The Remains of the Day" type="VP">
          <tokens>
            <token id="5" string="won" />
            <token id="6" string="the" />
            <token id="7" string="1989" />
            <token id="8" string="Booker" />
            <token id="9" string="Prize" />
            <token id="10" string="," />
            <token id="11" string="Britain" />
            <token id="12" string="'s" />
            <token id="13" string="top" />
            <token id="14" string="literary" />
            <token id="15" string="award" />
            <token id="16" string="," />
            <token id="17" string="for" />
            <token id="18" string="his" />
            <token id="19" string="novel" />
            <token id="20" string="``" />
            <token id="21" string="The" />
            <token id="22" string="Remains" />
            <token id="23" string="of" />
            <token id="24" string="the" />
            <token id="25" string="Day" />
          </tokens>
        </chunking>
        <chunking id="11" string="the 1989 Booker Prize" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="1989" />
            <token id="8" string="Booker" />
            <token id="9" string="Prize" />
          </tokens>
        </chunking>
        <chunking id="12" string="his novel" type="NP">
          <tokens>
            <token id="18" string="his" />
            <token id="19" string="novel" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="amod">
          <governor id="4">Ishiguro</governor>
          <dependent id="1">Japanese</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">Ishiguro</governor>
          <dependent id="2">writer</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="4">Ishiguro</governor>
          <dependent id="3">Kazuo</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">won</governor>
          <dependent id="4">Ishiguro</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="29">announced</governor>
          <dependent id="5">won</dependent>
        </dependency>
        <dependency type="det">
          <governor id="9">Prize</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="9">Prize</governor>
          <dependent id="7">1989</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">Prize</governor>
          <dependent id="8">Booker</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">won</governor>
          <dependent id="9">Prize</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="15">award</governor>
          <dependent id="11">Britain</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">Britain</governor>
          <dependent id="12">'s</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">award</governor>
          <dependent id="13">top</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">award</governor>
          <dependent id="14">literary</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="9">Prize</governor>
          <dependent id="15">award</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">novel</governor>
          <dependent id="17">for</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="19">novel</governor>
          <dependent id="18">his</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">Prize</governor>
          <dependent id="19">novel</dependent>
        </dependency>
        <dependency type="det">
          <governor id="22">Remains</governor>
          <dependent id="21">The</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="9">Prize</governor>
          <dependent id="22">Remains</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">Day</governor>
          <dependent id="23">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="25">Day</governor>
          <dependent id="24">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="22">Remains</governor>
          <dependent id="25">Day</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="29">announced</governor>
          <dependent id="28">judges</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="29">announced</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="29">announced</governor>
          <dependent id="30">Thursday</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Kazuo Ishiguro" type="PERSON" score="0.0">
          <tokens>
            <token id="3" string="Kazuo" />
            <token id="4" string="Ishiguro" />
          </tokens>
        </entity>
        <entity id="2" string="the Day" type="DATE" score="0.0">
          <tokens>
            <token id="24" string="the" />
            <token id="25" string="Day" />
          </tokens>
        </entity>
        <entity id="3" string="Thursday" type="DATE" score="0.0">
          <tokens>
            <token id="30" string="Thursday" />
          </tokens>
        </entity>
        <entity id="4" string="Booker Prize" type="MISC" score="0.0">
          <tokens>
            <token id="8" string="Booker" />
            <token id="9" string="Prize" />
          </tokens>
        </entity>
        <entity id="5" string="Britain" type="LOCATION" score="0.0">
          <tokens>
            <token id="11" string="Britain" />
          </tokens>
        </entity>
        <entity id="6" string="Japanese" type="NATIONALITY" score="0.0">
          <tokens>
            <token id="1" string="Japanese" />
          </tokens>
        </entity>
        <entity id="7" string="Remains of" type="MISC" score="0.0">
          <tokens>
            <token id="22" string="Remains" />
            <token id="23" string="of" />
          </tokens>
        </entity>
        <entity id="8" string="1989" type="DATE" score="0.0">
          <tokens>
            <token id="7" string="1989" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="2" has_coreference="true">
      <content>The prize is worth $32,000.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="prize" lemma="prize" stem="prize" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="worth" lemma="worth" stem="worth" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="$" lemma="$" stem="$" pos="$" type="Symbol" isStopWord="true" ner="MONEY" is_referenced="false" is_refers="false" />
        <token id="6" string="32,000" lemma="32,000" stem="32,000" pos="CD" type="Word" isStopWord="false" ner="MONEY" is_referenced="false" is_refers="false" />
        <token id="7" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NN prize)) (VP (VBZ is) (ADJP (JJ worth) (NP ($ $) (CD 32,000)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="$ 32,000" type="NP">
          <tokens>
            <token id="5" string="$" />
            <token id="6" string="32,000" />
          </tokens>
        </chunking>
        <chunking id="2" string="is worth $ 32,000" type="VP">
          <tokens>
            <token id="3" string="is" />
            <token id="4" string="worth" />
            <token id="5" string="$" />
            <token id="6" string="32,000" />
          </tokens>
        </chunking>
        <chunking id="3" string="worth $ 32,000" type="ADJP">
          <tokens>
            <token id="4" string="worth" />
            <token id="5" string="$" />
            <token id="6" string="32,000" />
          </tokens>
        </chunking>
        <chunking id="4" string="The prize" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="prize" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">prize</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">worth</governor>
          <dependent id="2">prize</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="4">worth</governor>
          <dependent id="3">is</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">worth</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="6">32,000</governor>
          <dependent id="5">$</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="4">worth</governor>
          <dependent id="6">32,000</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="$ 32,000" type="MONEY" score="0.0">
          <tokens>
            <token id="5" string="$" />
            <token id="6" string="32,000" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="3" has_coreference="true">
      <content>Judges praised the novel, a love story about a butler&amp;apost;s English vacation, as ``a cunningly structured and beautifully paced performance.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="Judges" lemma="Judges" stem="judg" pos="NNPS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="2" string="praised" lemma="praise" stem="prais" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="novel" lemma="novel" stem="novel" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="love" lemma="love" stem="love" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="story" lemma="story" stem="stori" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="butler" lemma="butler" stem="butler" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="English" lemma="English" stem="english" pos="NNP" type="Word" isStopWord="false" ner="MISC" is_referenced="true" is_refers="false" />
        <token id="14" string="vacation" lemma="vacation" stem="vacat" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="cunningly" lemma="cunningly" stem="cunningli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="structured" lemma="structure" stem="structur" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="beautifully" lemma="beautifully" stem="beautifulli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="paced" lemma="pace" stem="pace" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="performance" lemma="performance" stem="perform" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNPS Judges)) (VP (VP (VBD praised) (NP (NP (DT the) (JJ novel)) (, ,) (NP (NP (DT a) (NN love) (NN story)) (PP (IN about) (NP (NP (DT a) (NN butler) (POS 's)) (NNP English) (NN vacation)))) (, ,) (PP (IN as) (NP (NP (`` ``) (DT a)) (VP (ADVP (RB cunningly)) (VBN structured)))))) (CC and) (VP (ADVP (RB beautifully)) (VBD paced) (NP (NN performance)))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="the novel" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="novel" />
          </tokens>
        </chunking>
        <chunking id="2" string="cunningly structured" type="VP">
          <tokens>
            <token id="19" string="cunningly" />
            <token id="20" string="structured" />
          </tokens>
        </chunking>
        <chunking id="3" string="a butler 's" type="NP">
          <tokens>
            <token id="10" string="a" />
            <token id="11" string="butler" />
            <token id="12" string="'s" />
          </tokens>
        </chunking>
        <chunking id="4" string="praised the novel , a love story about a butler 's English vacation , as `` a cunningly structured and beautifully paced performance" type="VP">
          <tokens>
            <token id="2" string="praised" />
            <token id="3" string="the" />
            <token id="4" string="novel" />
            <token id="5" string="," />
            <token id="6" string="a" />
            <token id="7" string="love" />
            <token id="8" string="story" />
            <token id="9" string="about" />
            <token id="10" string="a" />
            <token id="11" string="butler" />
            <token id="12" string="'s" />
            <token id="13" string="English" />
            <token id="14" string="vacation" />
            <token id="15" string="," />
            <token id="16" string="as" />
            <token id="17" string="``" />
            <token id="18" string="a" />
            <token id="19" string="cunningly" />
            <token id="20" string="structured" />
            <token id="21" string="and" />
            <token id="22" string="beautifully" />
            <token id="23" string="paced" />
            <token id="24" string="performance" />
          </tokens>
        </chunking>
        <chunking id="5" string="praised the novel , a love story about a butler 's English vacation , as `` a cunningly structured" type="VP">
          <tokens>
            <token id="2" string="praised" />
            <token id="3" string="the" />
            <token id="4" string="novel" />
            <token id="5" string="," />
            <token id="6" string="a" />
            <token id="7" string="love" />
            <token id="8" string="story" />
            <token id="9" string="about" />
            <token id="10" string="a" />
            <token id="11" string="butler" />
            <token id="12" string="'s" />
            <token id="13" string="English" />
            <token id="14" string="vacation" />
            <token id="15" string="," />
            <token id="16" string="as" />
            <token id="17" string="``" />
            <token id="18" string="a" />
            <token id="19" string="cunningly" />
            <token id="20" string="structured" />
          </tokens>
        </chunking>
        <chunking id="6" string="performance" type="NP">
          <tokens>
            <token id="24" string="performance" />
          </tokens>
        </chunking>
        <chunking id="7" string="`` a" type="NP">
          <tokens>
            <token id="17" string="``" />
            <token id="18" string="a" />
          </tokens>
        </chunking>
        <chunking id="8" string="a love story about a butler 's English vacation" type="NP">
          <tokens>
            <token id="6" string="a" />
            <token id="7" string="love" />
            <token id="8" string="story" />
            <token id="9" string="about" />
            <token id="10" string="a" />
            <token id="11" string="butler" />
            <token id="12" string="'s" />
            <token id="13" string="English" />
            <token id="14" string="vacation" />
          </tokens>
        </chunking>
        <chunking id="9" string="a love story" type="NP">
          <tokens>
            <token id="6" string="a" />
            <token id="7" string="love" />
            <token id="8" string="story" />
          </tokens>
        </chunking>
        <chunking id="10" string="`` a cunningly structured" type="NP">
          <tokens>
            <token id="17" string="``" />
            <token id="18" string="a" />
            <token id="19" string="cunningly" />
            <token id="20" string="structured" />
          </tokens>
        </chunking>
        <chunking id="11" string="beautifully paced performance" type="VP">
          <tokens>
            <token id="22" string="beautifully" />
            <token id="23" string="paced" />
            <token id="24" string="performance" />
          </tokens>
        </chunking>
        <chunking id="12" string="the novel , a love story about a butler 's English vacation , as `` a cunningly structured" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="novel" />
            <token id="5" string="," />
            <token id="6" string="a" />
            <token id="7" string="love" />
            <token id="8" string="story" />
            <token id="9" string="about" />
            <token id="10" string="a" />
            <token id="11" string="butler" />
            <token id="12" string="'s" />
            <token id="13" string="English" />
            <token id="14" string="vacation" />
            <token id="15" string="," />
            <token id="16" string="as" />
            <token id="17" string="``" />
            <token id="18" string="a" />
            <token id="19" string="cunningly" />
            <token id="20" string="structured" />
          </tokens>
        </chunking>
        <chunking id="13" string="a butler 's English vacation" type="NP">
          <tokens>
            <token id="10" string="a" />
            <token id="11" string="butler" />
            <token id="12" string="'s" />
            <token id="13" string="English" />
            <token id="14" string="vacation" />
          </tokens>
        </chunking>
        <chunking id="14" string="Judges" type="NP">
          <tokens>
            <token id="1" string="Judges" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">praised</governor>
          <dependent id="1">Judges</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">praised</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">novel</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">praised</governor>
          <dependent id="4">novel</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">story</governor>
          <dependent id="6">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">story</governor>
          <dependent id="7">love</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="4">novel</governor>
          <dependent id="8">story</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">vacation</governor>
          <dependent id="9">about</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">butler</governor>
          <dependent id="10">a</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="14">vacation</governor>
          <dependent id="11">butler</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">butler</governor>
          <dependent id="12">'s</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">vacation</governor>
          <dependent id="13">English</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">story</governor>
          <dependent id="14">vacation</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">a</governor>
          <dependent id="16">as</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">novel</governor>
          <dependent id="18">a</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="20">structured</governor>
          <dependent id="19">cunningly</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="18">a</governor>
          <dependent id="20">structured</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="2">praised</governor>
          <dependent id="21">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="23">paced</governor>
          <dependent id="22">beautifully</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="2">praised</governor>
          <dependent id="23">paced</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="23">paced</governor>
          <dependent id="24">performance</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="English" type="MISC" score="0.0">
          <tokens>
            <token id="13" string="English" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="4" has_coreference="true">
      <content>Other finalists this year were Canadian Margaret Atwood for ``Cat&amp;apost;s Eye,&amp;apost;&amp;apost; Irish author John Banville for ``Book of Evidence,&amp;apost;&amp;apost; Scotland&amp;apost;s James Kelman for ``A Disaffection&amp;apost;&amp;apost; and English writers Sybille Bedford for ``Jigsaw&amp;apost;&amp;apost; and Rose Tremain for ``Restoration.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="Other" lemma="other" stem="other" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="finalists" lemma="finalist" stem="finalist" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="4" string="year" lemma="year" stem="year" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="5" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Canadian" lemma="canadian" stem="canadian" pos="JJ" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="true" is_refers="false" />
        <token id="7" string="Margaret" lemma="Margaret" stem="margaret" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="8" string="Atwood" lemma="Atwood" stem="atwood" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="9" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="Cat" lemma="cat" stem="cat" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="Eye" lemma="Eye" stem="eye" pos="NNP" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="16" string="Irish" lemma="irish" stem="irish" pos="JJ" type="Word" isStopWord="false" ner="NATIONALITY" is_referenced="true" is_refers="false" />
        <token id="17" string="author" lemma="author" stem="author" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="18" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="19" string="Banville" lemma="Banville" stem="banvil" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="20" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="21" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="22" string="Book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="23" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="24" string="Evidence" lemma="evidence" stem="evidenc" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="25" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="Scotland" lemma="Scotland" stem="scotland" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="28" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="James" lemma="James" stem="jame" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="30" string="Kelman" lemma="Kelman" stem="kelman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="31" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="A" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="Disaffection" lemma="disaffection" stem="disaffect" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="English" lemma="english" stem="english" pos="JJ" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="38" string="writers" lemma="writer" stem="writer" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="Sybille" lemma="Sybille" stem="sybil" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="40" string="Bedford" lemma="Bedford" stem="bedford" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="41" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="42" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="43" string="Jigsaw" lemma="Jigsaw" stem="jigsaw" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="44" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="45" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="46" string="Rose" lemma="Rose" stem="rose" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="47" string="Tremain" lemma="Tremain" stem="tremain" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="48" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="49" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="50" string="Restoration" lemma="Restoration" stem="restor" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="51" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="52" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (JJ Other) (NNS finalists)) (NP-TMP (DT this) (NN year))) (VP (VBD were) (NP (NP (NP (JJ Canadian) (NNP Margaret) (NNP Atwood)) (PP (IN for) (`` ``) (NP (NP (NP (NN Cat) (POS 's)) (NNP Eye)) (, ,) ('' '') (NP (NP (JJ Irish) (NN author) (NNP John) (NNP Banville)) (PP (IN for) (`` ``) (NP (NP (NN Book)) (PP (IN of) (NP (NN Evidence))))))))) (, ,) ('' '') (NP (NP (NNP Scotland) (POS 's)) (NP (NNP James) (NNP Kelman)) (PP (IN for) (`` ``) (NP (DT A) (NN Disaffection)) ('' '')) (CC and) (NP (NP (JJ English) (NNS writers)) (NP (NP (NNP Sybille) (NNP Bedford)) (PP (IN for) (`` ``) (NP (NNP Jigsaw)) ('' ''))))) (CC and) (NP (NP (NNP Rose) (NNP Tremain)) (PP (IN for) (`` ``) (NP (NNP Restoration)))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="Cat 's Eye" type="NP">
          <tokens>
            <token id="11" string="Cat" />
            <token id="12" string="'s" />
            <token id="13" string="Eye" />
          </tokens>
        </chunking>
        <chunking id="2" string="Canadian Margaret Atwood for `` Cat 's Eye , '' Irish author John Banville for `` Book of Evidence , '' Scotland 's James Kelman for `` A Disaffection '' and English writers Sybille Bedford for `` Jigsaw '' and Rose Tremain for `` Restoration" type="NP">
          <tokens>
            <token id="6" string="Canadian" />
            <token id="7" string="Margaret" />
            <token id="8" string="Atwood" />
            <token id="9" string="for" />
            <token id="10" string="``" />
            <token id="11" string="Cat" />
            <token id="12" string="'s" />
            <token id="13" string="Eye" />
            <token id="14" string="," />
            <token id="15" string="''" />
            <token id="16" string="Irish" />
            <token id="17" string="author" />
            <token id="18" string="John" />
            <token id="19" string="Banville" />
            <token id="20" string="for" />
            <token id="21" string="``" />
            <token id="22" string="Book" />
            <token id="23" string="of" />
            <token id="24" string="Evidence" />
            <token id="25" string="," />
            <token id="26" string="''" />
            <token id="27" string="Scotland" />
            <token id="28" string="'s" />
            <token id="29" string="James" />
            <token id="30" string="Kelman" />
            <token id="31" string="for" />
            <token id="32" string="``" />
            <token id="33" string="A" />
            <token id="34" string="Disaffection" />
            <token id="35" string="''" />
            <token id="36" string="and" />
            <token id="37" string="English" />
            <token id="38" string="writers" />
            <token id="39" string="Sybille" />
            <token id="40" string="Bedford" />
            <token id="41" string="for" />
            <token id="42" string="``" />
            <token id="43" string="Jigsaw" />
            <token id="44" string="''" />
            <token id="45" string="and" />
            <token id="46" string="Rose" />
            <token id="47" string="Tremain" />
            <token id="48" string="for" />
            <token id="49" string="``" />
            <token id="50" string="Restoration" />
          </tokens>
        </chunking>
        <chunking id="3" string="Other finalists" type="NP">
          <tokens>
            <token id="1" string="Other" />
            <token id="2" string="finalists" />
          </tokens>
        </chunking>
        <chunking id="4" string="English writers" type="NP">
          <tokens>
            <token id="37" string="English" />
            <token id="38" string="writers" />
          </tokens>
        </chunking>
        <chunking id="5" string="Scotland 's" type="NP">
          <tokens>
            <token id="27" string="Scotland" />
            <token id="28" string="'s" />
          </tokens>
        </chunking>
        <chunking id="6" string="were Canadian Margaret Atwood for `` Cat 's Eye , '' Irish author John Banville for `` Book of Evidence , '' Scotland 's James Kelman for `` A Disaffection '' and English writers Sybille Bedford for `` Jigsaw '' and Rose Tremain for `` Restoration" type="VP">
          <tokens>
            <token id="5" string="were" />
            <token id="6" string="Canadian" />
            <token id="7" string="Margaret" />
            <token id="8" string="Atwood" />
            <token id="9" string="for" />
            <token id="10" string="``" />
            <token id="11" string="Cat" />
            <token id="12" string="'s" />
            <token id="13" string="Eye" />
            <token id="14" string="," />
            <token id="15" string="''" />
            <token id="16" string="Irish" />
            <token id="17" string="author" />
            <token id="18" string="John" />
            <token id="19" string="Banville" />
            <token id="20" string="for" />
            <token id="21" string="``" />
            <token id="22" string="Book" />
            <token id="23" string="of" />
            <token id="24" string="Evidence" />
            <token id="25" string="," />
            <token id="26" string="''" />
            <token id="27" string="Scotland" />
            <token id="28" string="'s" />
            <token id="29" string="James" />
            <token id="30" string="Kelman" />
            <token id="31" string="for" />
            <token id="32" string="``" />
            <token id="33" string="A" />
            <token id="34" string="Disaffection" />
            <token id="35" string="''" />
            <token id="36" string="and" />
            <token id="37" string="English" />
            <token id="38" string="writers" />
            <token id="39" string="Sybille" />
            <token id="40" string="Bedford" />
            <token id="41" string="for" />
            <token id="42" string="``" />
            <token id="43" string="Jigsaw" />
            <token id="44" string="''" />
            <token id="45" string="and" />
            <token id="46" string="Rose" />
            <token id="47" string="Tremain" />
            <token id="48" string="for" />
            <token id="49" string="``" />
            <token id="50" string="Restoration" />
          </tokens>
        </chunking>
        <chunking id="7" string="A Disaffection" type="NP">
          <tokens>
            <token id="33" string="A" />
            <token id="34" string="Disaffection" />
          </tokens>
        </chunking>
        <chunking id="8" string="Cat 's" type="NP">
          <tokens>
            <token id="11" string="Cat" />
            <token id="12" string="'s" />
          </tokens>
        </chunking>
        <chunking id="9" string="Scotland 's James Kelman for `` A Disaffection '' and English writers Sybille Bedford for `` Jigsaw ''" type="NP">
          <tokens>
            <token id="27" string="Scotland" />
            <token id="28" string="'s" />
            <token id="29" string="James" />
            <token id="30" string="Kelman" />
            <token id="31" string="for" />
            <token id="32" string="``" />
            <token id="33" string="A" />
            <token id="34" string="Disaffection" />
            <token id="35" string="''" />
            <token id="36" string="and" />
            <token id="37" string="English" />
            <token id="38" string="writers" />
            <token id="39" string="Sybille" />
            <token id="40" string="Bedford" />
            <token id="41" string="for" />
            <token id="42" string="``" />
            <token id="43" string="Jigsaw" />
            <token id="44" string="''" />
          </tokens>
        </chunking>
        <chunking id="10" string="Canadian Margaret Atwood" type="NP">
          <tokens>
            <token id="6" string="Canadian" />
            <token id="7" string="Margaret" />
            <token id="8" string="Atwood" />
          </tokens>
        </chunking>
        <chunking id="11" string="Jigsaw" type="NP">
          <tokens>
            <token id="43" string="Jigsaw" />
          </tokens>
        </chunking>
        <chunking id="12" string="Book of Evidence" type="NP">
          <tokens>
            <token id="22" string="Book" />
            <token id="23" string="of" />
            <token id="24" string="Evidence" />
          </tokens>
        </chunking>
        <chunking id="13" string="Irish author John Banville" type="NP">
          <tokens>
            <token id="16" string="Irish" />
            <token id="17" string="author" />
            <token id="18" string="John" />
            <token id="19" string="Banville" />
          </tokens>
        </chunking>
        <chunking id="14" string="Sybille Bedford for `` Jigsaw ''" type="NP">
          <tokens>
            <token id="39" string="Sybille" />
            <token id="40" string="Bedford" />
            <token id="41" string="for" />
            <token id="42" string="``" />
            <token id="43" string="Jigsaw" />
            <token id="44" string="''" />
          </tokens>
        </chunking>
        <chunking id="15" string="Other finalists this year" type="NP">
          <tokens>
            <token id="1" string="Other" />
            <token id="2" string="finalists" />
            <token id="3" string="this" />
            <token id="4" string="year" />
          </tokens>
        </chunking>
        <chunking id="16" string="Rose Tremain" type="NP">
          <tokens>
            <token id="46" string="Rose" />
            <token id="47" string="Tremain" />
          </tokens>
        </chunking>
        <chunking id="17" string="Canadian Margaret Atwood for `` Cat 's Eye , '' Irish author John Banville for `` Book of Evidence" type="NP">
          <tokens>
            <token id="6" string="Canadian" />
            <token id="7" string="Margaret" />
            <token id="8" string="Atwood" />
            <token id="9" string="for" />
            <token id="10" string="``" />
            <token id="11" string="Cat" />
            <token id="12" string="'s" />
            <token id="13" string="Eye" />
            <token id="14" string="," />
            <token id="15" string="''" />
            <token id="16" string="Irish" />
            <token id="17" string="author" />
            <token id="18" string="John" />
            <token id="19" string="Banville" />
            <token id="20" string="for" />
            <token id="21" string="``" />
            <token id="22" string="Book" />
            <token id="23" string="of" />
            <token id="24" string="Evidence" />
          </tokens>
        </chunking>
        <chunking id="18" string="Rose Tremain for `` Restoration" type="NP">
          <tokens>
            <token id="46" string="Rose" />
            <token id="47" string="Tremain" />
            <token id="48" string="for" />
            <token id="49" string="``" />
            <token id="50" string="Restoration" />
          </tokens>
        </chunking>
        <chunking id="19" string="Sybille Bedford" type="NP">
          <tokens>
            <token id="39" string="Sybille" />
            <token id="40" string="Bedford" />
          </tokens>
        </chunking>
        <chunking id="20" string="James Kelman" type="NP">
          <tokens>
            <token id="29" string="James" />
            <token id="30" string="Kelman" />
          </tokens>
        </chunking>
        <chunking id="21" string="English writers Sybille Bedford for `` Jigsaw ''" type="NP">
          <tokens>
            <token id="37" string="English" />
            <token id="38" string="writers" />
            <token id="39" string="Sybille" />
            <token id="40" string="Bedford" />
            <token id="41" string="for" />
            <token id="42" string="``" />
            <token id="43" string="Jigsaw" />
            <token id="44" string="''" />
          </tokens>
        </chunking>
        <chunking id="22" string="Cat 's Eye , '' Irish author John Banville for `` Book of Evidence" type="NP">
          <tokens>
            <token id="11" string="Cat" />
            <token id="12" string="'s" />
            <token id="13" string="Eye" />
            <token id="14" string="," />
            <token id="15" string="''" />
            <token id="16" string="Irish" />
            <token id="17" string="author" />
            <token id="18" string="John" />
            <token id="19" string="Banville" />
            <token id="20" string="for" />
            <token id="21" string="``" />
            <token id="22" string="Book" />
            <token id="23" string="of" />
            <token id="24" string="Evidence" />
          </tokens>
        </chunking>
        <chunking id="23" string="Irish author John Banville for `` Book of Evidence" type="NP">
          <tokens>
            <token id="16" string="Irish" />
            <token id="17" string="author" />
            <token id="18" string="John" />
            <token id="19" string="Banville" />
            <token id="20" string="for" />
            <token id="21" string="``" />
            <token id="22" string="Book" />
            <token id="23" string="of" />
            <token id="24" string="Evidence" />
          </tokens>
        </chunking>
        <chunking id="24" string="Book" type="NP">
          <tokens>
            <token id="22" string="Book" />
          </tokens>
        </chunking>
        <chunking id="25" string="Restoration" type="NP">
          <tokens>
            <token id="50" string="Restoration" />
          </tokens>
        </chunking>
        <chunking id="26" string="Evidence" type="NP">
          <tokens>
            <token id="24" string="Evidence" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="amod">
          <governor id="2">finalists</governor>
          <dependent id="1">Other</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">Atwood</governor>
          <dependent id="2">finalists</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">year</governor>
          <dependent id="3">this</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="2">finalists</governor>
          <dependent id="4">year</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="8">Atwood</governor>
          <dependent id="5">were</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">Atwood</governor>
          <dependent id="6">Canadian</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">Atwood</governor>
          <dependent id="7">Margaret</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="8">Atwood</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">Eye</governor>
          <dependent id="9">for</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="13">Eye</governor>
          <dependent id="11">Cat</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">Cat</governor>
          <dependent id="12">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">Atwood</governor>
          <dependent id="13">Eye</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">Banville</governor>
          <dependent id="16">Irish</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">Banville</governor>
          <dependent id="17">author</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="19">Banville</governor>
          <dependent id="18">John</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="13">Eye</governor>
          <dependent id="19">Banville</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">Book</governor>
          <dependent id="20">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">Banville</governor>
          <dependent id="22">Book</dependent>
        </dependency>
        <dependency type="case">
          <governor id="24">Evidence</governor>
          <dependent id="23">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="22">Book</governor>
          <dependent id="24">Evidence</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="8">Atwood</governor>
          <dependent id="27">Scotland</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">Scotland</governor>
          <dependent id="28">'s</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="30">Kelman</governor>
          <dependent id="29">James</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="27">Scotland</governor>
          <dependent id="30">Kelman</dependent>
        </dependency>
        <dependency type="case">
          <governor id="34">Disaffection</governor>
          <dependent id="31">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="34">Disaffection</governor>
          <dependent id="33">A</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">Scotland</governor>
          <dependent id="34">Disaffection</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="27">Scotland</governor>
          <dependent id="36">and</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="38">writers</governor>
          <dependent id="37">English</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="27">Scotland</governor>
          <dependent id="38">writers</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="40">Bedford</governor>
          <dependent id="39">Sybille</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="38">writers</governor>
          <dependent id="40">Bedford</dependent>
        </dependency>
        <dependency type="case">
          <governor id="43">Jigsaw</governor>
          <dependent id="41">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="40">Bedford</governor>
          <dependent id="43">Jigsaw</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="8">Atwood</governor>
          <dependent id="45">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="47">Tremain</governor>
          <dependent id="46">Rose</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="8">Atwood</governor>
          <dependent id="47">Tremain</dependent>
        </dependency>
        <dependency type="case">
          <governor id="50">Restoration</governor>
          <dependent id="48">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="47">Tremain</governor>
          <dependent id="50">Restoration</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="James Kelman" type="PERSON" score="0.0">
          <tokens>
            <token id="29" string="James" />
            <token id="30" string="Kelman" />
          </tokens>
        </entity>
        <entity id="2" string="Canadian" type="NATIONALITY" score="0.0">
          <tokens>
            <token id="6" string="Canadian" />
          </tokens>
        </entity>
        <entity id="3" string="Scotland" type="LOCATION" score="0.0">
          <tokens>
            <token id="27" string="Scotland" />
          </tokens>
        </entity>
        <entity id="4" string="Rose Tremain" type="PERSON" score="0.0">
          <tokens>
            <token id="46" string="Rose" />
            <token id="47" string="Tremain" />
          </tokens>
        </entity>
        <entity id="5" string="Sybille Bedford" type="PERSON" score="0.0">
          <tokens>
            <token id="39" string="Sybille" />
            <token id="40" string="Bedford" />
          </tokens>
        </entity>
        <entity id="6" string="this year" type="DATE" score="0.0">
          <tokens>
            <token id="3" string="this" />
            <token id="4" string="year" />
          </tokens>
        </entity>
        <entity id="7" string="Margaret Atwood" type="PERSON" score="0.0">
          <tokens>
            <token id="7" string="Margaret" />
            <token id="8" string="Atwood" />
          </tokens>
        </entity>
        <entity id="8" string="John Banville" type="PERSON" score="0.0">
          <tokens>
            <token id="18" string="John" />
            <token id="19" string="Banville" />
          </tokens>
        </entity>
        <entity id="9" string="Irish" type="NATIONALITY" score="0.0">
          <tokens>
            <token id="16" string="Irish" />
          </tokens>
        </entity>
        <entity id="10" string="English" type="MISC" score="0.0">
          <tokens>
            <token id="37" string="English" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="5" has_coreference="true">
      <content>After receiving the prize, Ishiguro paid tribute to Salman Rushdie, the 1988 runner-up for the award, who went into hiding in February when he was threatened with death by Moslems who said his novel ``The Satanic Verses&amp;apost;&amp;apost; blasphemed their faith.</content>
      <tokens>
        <token id="1" string="After" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="receiving" lemma="receive" stem="receiv" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="prize" lemma="prize" stem="prize" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Ishiguro" lemma="Ishiguro" stem="ishiguro" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="7" string="paid" lemma="pay" stem="paid" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="tribute" lemma="tribute" stem="tribut" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="Salman" lemma="Salman" stem="salman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="11" string="Rushdie" lemma="Rushdie" stem="rushdi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="1988" lemma="1988" stem="1988" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="15" string="runner-up" lemma="runner-up" stem="runner-up" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="award" lemma="award" stem="award" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="went" lemma="go" stem="went" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="into" lemma="into" stem="into" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="hiding" lemma="hide" stem="hide" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="February" lemma="February" stem="februari" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="26" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="threatened" lemma="threaten" stem="threaten" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="death" lemma="death" stem="death" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="Moslems" lemma="Moslems" stem="moslem" pos="NNPS" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="34" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="36" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="37" string="novel" lemma="novel" stem="novel" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="38" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="39" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="40" string="Satanic" lemma="satanic" stem="satan" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="41" string="Verses" lemma="verse" stem="vers" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="42" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="43" string="blasphemed" lemma="blaspheme" stem="blasphem" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="44" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="45" string="faith" lemma="faith" stem="faith" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="46" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (IN After) (S (VP (VBG receiving) (NP (DT the) (NN prize))))) (, ,) (NP (NNP Ishiguro)) (VP (VBD paid) (NP (NN tribute)) (PP (TO to) (NP (NNP Salman) (NNP Rushdie))) (, ,) (NP (NP (DT the) (CD 1988) (NN runner-up)) (PP (IN for) (NP (NP (DT the) (NN award)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBD went) (PP (IN into) (S (VP (VBG hiding) (PP (IN in) (NP (NNP February))) (SBAR (WHADVP (WRB when)) (S (NP (PRP he)) (VP (VBD was) (VP (VBN threatened) (PP (IN with) (NP (NN death))) (PP (IN by) (NP (NP (NNPS Moslems)) (SBAR (WHNP (WP who)) (S (VP (VBD said) (NP (PRP$ his) (JJ novel)) (SBAR (S (`` ``) (NP (DT The) (JJ Satanic) (NNS Verses)) ('' '') (VP (VBD blasphemed) (NP (PRP$ their) (NN faith)))))))))))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="threatened with death by Moslems who said his novel `` The Satanic Verses '' blasphemed their faith" type="VP">
          <tokens>
            <token id="29" string="threatened" />
            <token id="30" string="with" />
            <token id="31" string="death" />
            <token id="32" string="by" />
            <token id="33" string="Moslems" />
            <token id="34" string="who" />
            <token id="35" string="said" />
            <token id="36" string="his" />
            <token id="37" string="novel" />
            <token id="38" string="``" />
            <token id="39" string="The" />
            <token id="40" string="Satanic" />
            <token id="41" string="Verses" />
            <token id="42" string="''" />
            <token id="43" string="blasphemed" />
            <token id="44" string="their" />
            <token id="45" string="faith" />
          </tokens>
        </chunking>
        <chunking id="2" string="Ishiguro" type="NP">
          <tokens>
            <token id="6" string="Ishiguro" />
          </tokens>
        </chunking>
        <chunking id="3" string="the prize" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="prize" />
          </tokens>
        </chunking>
        <chunking id="4" string="Salman Rushdie" type="NP">
          <tokens>
            <token id="10" string="Salman" />
            <token id="11" string="Rushdie" />
          </tokens>
        </chunking>
        <chunking id="5" string="he" type="NP">
          <tokens>
            <token id="27" string="he" />
          </tokens>
        </chunking>
        <chunking id="6" string="paid tribute to Salman Rushdie , the 1988 runner-up for the award , who went into hiding in February when he was threatened with death by Moslems who said his novel `` The Satanic Verses '' blasphemed their faith" type="VP">
          <tokens>
            <token id="7" string="paid" />
            <token id="8" string="tribute" />
            <token id="9" string="to" />
            <token id="10" string="Salman" />
            <token id="11" string="Rushdie" />
            <token id="12" string="," />
            <token id="13" string="the" />
            <token id="14" string="1988" />
            <token id="15" string="runner-up" />
            <token id="16" string="for" />
            <token id="17" string="the" />
            <token id="18" string="award" />
            <token id="19" string="," />
            <token id="20" string="who" />
            <token id="21" string="went" />
            <token id="22" string="into" />
            <token id="23" string="hiding" />
            <token id="24" string="in" />
            <token id="25" string="February" />
            <token id="26" string="when" />
            <token id="27" string="he" />
            <token id="28" string="was" />
            <token id="29" string="threatened" />
            <token id="30" string="with" />
            <token id="31" string="death" />
            <token id="32" string="by" />
            <token id="33" string="Moslems" />
            <token id="34" string="who" />
            <token id="35" string="said" />
            <token id="36" string="his" />
            <token id="37" string="novel" />
            <token id="38" string="``" />
            <token id="39" string="The" />
            <token id="40" string="Satanic" />
            <token id="41" string="Verses" />
            <token id="42" string="''" />
            <token id="43" string="blasphemed" />
            <token id="44" string="their" />
            <token id="45" string="faith" />
          </tokens>
        </chunking>
        <chunking id="7" string="tribute" type="NP">
          <tokens>
            <token id="8" string="tribute" />
          </tokens>
        </chunking>
        <chunking id="8" string="the 1988 runner-up" type="NP">
          <tokens>
            <token id="13" string="the" />
            <token id="14" string="1988" />
            <token id="15" string="runner-up" />
          </tokens>
        </chunking>
        <chunking id="9" string="the award , who went into hiding in February when he was threatened with death by Moslems who said his novel `` The Satanic Verses '' blasphemed their faith" type="NP">
          <tokens>
            <token id="17" string="the" />
            <token id="18" string="award" />
            <token id="19" string="," />
            <token id="20" string="who" />
            <token id="21" string="went" />
            <token id="22" string="into" />
            <token id="23" string="hiding" />
            <token id="24" string="in" />
            <token id="25" string="February" />
            <token id="26" string="when" />
            <token id="27" string="he" />
            <token id="28" string="was" />
            <token id="29" string="threatened" />
            <token id="30" string="with" />
            <token id="31" string="death" />
            <token id="32" string="by" />
            <token id="33" string="Moslems" />
            <token id="34" string="who" />
            <token id="35" string="said" />
            <token id="36" string="his" />
            <token id="37" string="novel" />
            <token id="38" string="``" />
            <token id="39" string="The" />
            <token id="40" string="Satanic" />
            <token id="41" string="Verses" />
            <token id="42" string="''" />
            <token id="43" string="blasphemed" />
            <token id="44" string="their" />
            <token id="45" string="faith" />
          </tokens>
        </chunking>
        <chunking id="10" string="Moslems" type="NP">
          <tokens>
            <token id="33" string="Moslems" />
          </tokens>
        </chunking>
        <chunking id="11" string="death" type="NP">
          <tokens>
            <token id="31" string="death" />
          </tokens>
        </chunking>
        <chunking id="12" string="went into hiding in February when he was threatened with death by Moslems who said his novel `` The Satanic Verses '' blasphemed their faith" type="VP">
          <tokens>
            <token id="21" string="went" />
            <token id="22" string="into" />
            <token id="23" string="hiding" />
            <token id="24" string="in" />
            <token id="25" string="February" />
            <token id="26" string="when" />
            <token id="27" string="he" />
            <token id="28" string="was" />
            <token id="29" string="threatened" />
            <token id="30" string="with" />
            <token id="31" string="death" />
            <token id="32" string="by" />
            <token id="33" string="Moslems" />
            <token id="34" string="who" />
            <token id="35" string="said" />
            <token id="36" string="his" />
            <token id="37" string="novel" />
            <token id="38" string="``" />
            <token id="39" string="The" />
            <token id="40" string="Satanic" />
            <token id="41" string="Verses" />
            <token id="42" string="''" />
            <token id="43" string="blasphemed" />
            <token id="44" string="their" />
            <token id="45" string="faith" />
          </tokens>
        </chunking>
        <chunking id="13" string="`` The Satanic Verses '' blasphemed their faith" type="SBAR">
          <tokens>
            <token id="38" string="``" />
            <token id="39" string="The" />
            <token id="40" string="Satanic" />
            <token id="41" string="Verses" />
            <token id="42" string="''" />
            <token id="43" string="blasphemed" />
            <token id="44" string="their" />
            <token id="45" string="faith" />
          </tokens>
        </chunking>
        <chunking id="14" string="their faith" type="NP">
          <tokens>
            <token id="44" string="their" />
            <token id="45" string="faith" />
          </tokens>
        </chunking>
        <chunking id="15" string="when he was threatened with death by Moslems who said his novel `` The Satanic Verses '' blasphemed their faith" type="SBAR">
          <tokens>
            <token id="26" string="when" />
            <token id="27" string="he" />
            <token id="28" string="was" />
            <token id="29" string="threatened" />
            <token id="30" string="with" />
            <token id="31" string="death" />
            <token id="32" string="by" />
            <token id="33" string="Moslems" />
            <token id="34" string="who" />
            <token id="35" string="said" />
            <token id="36" string="his" />
            <token id="37" string="novel" />
            <token id="38" string="``" />
            <token id="39" string="The" />
            <token id="40" string="Satanic" />
            <token id="41" string="Verses" />
            <token id="42" string="''" />
            <token id="43" string="blasphemed" />
            <token id="44" string="their" />
            <token id="45" string="faith" />
          </tokens>
        </chunking>
        <chunking id="16" string="Moslems who said his novel `` The Satanic Verses '' blasphemed their faith" type="NP">
          <tokens>
            <token id="33" string="Moslems" />
            <token id="34" string="who" />
            <token id="35" string="said" />
            <token id="36" string="his" />
            <token id="37" string="novel" />
            <token id="38" string="``" />
            <token id="39" string="The" />
            <token id="40" string="Satanic" />
            <token id="41" string="Verses" />
            <token id="42" string="''" />
            <token id="43" string="blasphemed" />
            <token id="44" string="their" />
            <token id="45" string="faith" />
          </tokens>
        </chunking>
        <chunking id="17" string="when" type="WHADVP">
          <tokens>
            <token id="26" string="when" />
          </tokens>
        </chunking>
        <chunking id="18" string="The Satanic Verses" type="NP">
          <tokens>
            <token id="39" string="The" />
            <token id="40" string="Satanic" />
            <token id="41" string="Verses" />
          </tokens>
        </chunking>
        <chunking id="19" string="hiding in February when he was threatened with death by Moslems who said his novel `` The Satanic Verses '' blasphemed their faith" type="VP">
          <tokens>
            <token id="23" string="hiding" />
            <token id="24" string="in" />
            <token id="25" string="February" />
            <token id="26" string="when" />
            <token id="27" string="he" />
            <token id="28" string="was" />
            <token id="29" string="threatened" />
            <token id="30" string="with" />
            <token id="31" string="death" />
            <token id="32" string="by" />
            <token id="33" string="Moslems" />
            <token id="34" string="who" />
            <token id="35" string="said" />
            <token id="36" string="his" />
            <token id="37" string="novel" />
            <token id="38" string="``" />
            <token id="39" string="The" />
            <token id="40" string="Satanic" />
            <token id="41" string="Verses" />
            <token id="42" string="''" />
            <token id="43" string="blasphemed" />
            <token id="44" string="their" />
            <token id="45" string="faith" />
          </tokens>
        </chunking>
        <chunking id="20" string="was threatened with death by Moslems who said his novel `` The Satanic Verses '' blasphemed their faith" type="VP">
          <tokens>
            <token id="28" string="was" />
            <token id="29" string="threatened" />
            <token id="30" string="with" />
            <token id="31" string="death" />
            <token id="32" string="by" />
            <token id="33" string="Moslems" />
            <token id="34" string="who" />
            <token id="35" string="said" />
            <token id="36" string="his" />
            <token id="37" string="novel" />
            <token id="38" string="``" />
            <token id="39" string="The" />
            <token id="40" string="Satanic" />
            <token id="41" string="Verses" />
            <token id="42" string="''" />
            <token id="43" string="blasphemed" />
            <token id="44" string="their" />
            <token id="45" string="faith" />
          </tokens>
        </chunking>
        <chunking id="21" string="blasphemed their faith" type="VP">
          <tokens>
            <token id="43" string="blasphemed" />
            <token id="44" string="their" />
            <token id="45" string="faith" />
          </tokens>
        </chunking>
        <chunking id="22" string="receiving the prize" type="VP">
          <tokens>
            <token id="2" string="receiving" />
            <token id="3" string="the" />
            <token id="4" string="prize" />
          </tokens>
        </chunking>
        <chunking id="23" string="the 1988 runner-up for the award , who went into hiding in February when he was threatened with death by Moslems who said his novel `` The Satanic Verses '' blasphemed their faith" type="NP">
          <tokens>
            <token id="13" string="the" />
            <token id="14" string="1988" />
            <token id="15" string="runner-up" />
            <token id="16" string="for" />
            <token id="17" string="the" />
            <token id="18" string="award" />
            <token id="19" string="," />
            <token id="20" string="who" />
            <token id="21" string="went" />
            <token id="22" string="into" />
            <token id="23" string="hiding" />
            <token id="24" string="in" />
            <token id="25" string="February" />
            <token id="26" string="when" />
            <token id="27" string="he" />
            <token id="28" string="was" />
            <token id="29" string="threatened" />
            <token id="30" string="with" />
            <token id="31" string="death" />
            <token id="32" string="by" />
            <token id="33" string="Moslems" />
            <token id="34" string="who" />
            <token id="35" string="said" />
            <token id="36" string="his" />
            <token id="37" string="novel" />
            <token id="38" string="``" />
            <token id="39" string="The" />
            <token id="40" string="Satanic" />
            <token id="41" string="Verses" />
            <token id="42" string="''" />
            <token id="43" string="blasphemed" />
            <token id="44" string="their" />
            <token id="45" string="faith" />
          </tokens>
        </chunking>
        <chunking id="24" string="the award" type="NP">
          <tokens>
            <token id="17" string="the" />
            <token id="18" string="award" />
          </tokens>
        </chunking>
        <chunking id="25" string="who said his novel `` The Satanic Verses '' blasphemed their faith" type="SBAR">
          <tokens>
            <token id="34" string="who" />
            <token id="35" string="said" />
            <token id="36" string="his" />
            <token id="37" string="novel" />
            <token id="38" string="``" />
            <token id="39" string="The" />
            <token id="40" string="Satanic" />
            <token id="41" string="Verses" />
            <token id="42" string="''" />
            <token id="43" string="blasphemed" />
            <token id="44" string="their" />
            <token id="45" string="faith" />
          </tokens>
        </chunking>
        <chunking id="26" string="said his novel `` The Satanic Verses '' blasphemed their faith" type="VP">
          <tokens>
            <token id="35" string="said" />
            <token id="36" string="his" />
            <token id="37" string="novel" />
            <token id="38" string="``" />
            <token id="39" string="The" />
            <token id="40" string="Satanic" />
            <token id="41" string="Verses" />
            <token id="42" string="''" />
            <token id="43" string="blasphemed" />
            <token id="44" string="their" />
            <token id="45" string="faith" />
          </tokens>
        </chunking>
        <chunking id="27" string="February" type="NP">
          <tokens>
            <token id="25" string="February" />
          </tokens>
        </chunking>
        <chunking id="28" string="who went into hiding in February when he was threatened with death by Moslems who said his novel `` The Satanic Verses '' blasphemed their faith" type="SBAR">
          <tokens>
            <token id="20" string="who" />
            <token id="21" string="went" />
            <token id="22" string="into" />
            <token id="23" string="hiding" />
            <token id="24" string="in" />
            <token id="25" string="February" />
            <token id="26" string="when" />
            <token id="27" string="he" />
            <token id="28" string="was" />
            <token id="29" string="threatened" />
            <token id="30" string="with" />
            <token id="31" string="death" />
            <token id="32" string="by" />
            <token id="33" string="Moslems" />
            <token id="34" string="who" />
            <token id="35" string="said" />
            <token id="36" string="his" />
            <token id="37" string="novel" />
            <token id="38" string="``" />
            <token id="39" string="The" />
            <token id="40" string="Satanic" />
            <token id="41" string="Verses" />
            <token id="42" string="''" />
            <token id="43" string="blasphemed" />
            <token id="44" string="their" />
            <token id="45" string="faith" />
          </tokens>
        </chunking>
        <chunking id="29" string="his novel" type="NP">
          <tokens>
            <token id="36" string="his" />
            <token id="37" string="novel" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="mark">
          <governor id="2">receiving</governor>
          <dependent id="1">After</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="7">paid</governor>
          <dependent id="2">receiving</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">prize</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">receiving</governor>
          <dependent id="4">prize</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">paid</governor>
          <dependent id="6">Ishiguro</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="7">paid</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">paid</governor>
          <dependent id="8">tribute</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">Rushdie</governor>
          <dependent id="9">to</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="11">Rushdie</governor>
          <dependent id="10">Salman</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">paid</governor>
          <dependent id="11">Rushdie</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">runner-up</governor>
          <dependent id="13">the</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="15">runner-up</governor>
          <dependent id="14">1988</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">paid</governor>
          <dependent id="15">runner-up</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">award</governor>
          <dependent id="16">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">award</governor>
          <dependent id="17">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">runner-up</governor>
          <dependent id="18">award</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="21">went</governor>
          <dependent id="20">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="18">award</governor>
          <dependent id="21">went</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="23">hiding</governor>
          <dependent id="22">into</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="21">went</governor>
          <dependent id="23">hiding</dependent>
        </dependency>
        <dependency type="case">
          <governor id="25">February</governor>
          <dependent id="24">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">hiding</governor>
          <dependent id="25">February</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="29">threatened</governor>
          <dependent id="26">when</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="29">threatened</governor>
          <dependent id="27">he</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="29">threatened</governor>
          <dependent id="28">was</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="23">hiding</governor>
          <dependent id="29">threatened</dependent>
        </dependency>
        <dependency type="case">
          <governor id="31">death</governor>
          <dependent id="30">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="29">threatened</governor>
          <dependent id="31">death</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">Moslems</governor>
          <dependent id="32">by</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="29">threatened</governor>
          <dependent id="33">Moslems</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="35">said</governor>
          <dependent id="34">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="33">Moslems</governor>
          <dependent id="35">said</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="37">novel</governor>
          <dependent id="36">his</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="35">said</governor>
          <dependent id="37">novel</dependent>
        </dependency>
        <dependency type="det">
          <governor id="41">Verses</governor>
          <dependent id="39">The</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="41">Verses</governor>
          <dependent id="40">Satanic</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="43">blasphemed</governor>
          <dependent id="41">Verses</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="35">said</governor>
          <dependent id="43">blasphemed</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="45">faith</governor>
          <dependent id="44">their</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="43">blasphemed</governor>
          <dependent id="45">faith</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="1988" type="DATE" score="0.0">
          <tokens>
            <token id="14" string="1988" />
          </tokens>
        </entity>
        <entity id="2" string="Ishiguro" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Ishiguro" />
          </tokens>
        </entity>
        <entity id="3" string="February" type="DATE" score="0.0">
          <tokens>
            <token id="25" string="February" />
          </tokens>
        </entity>
        <entity id="4" string="Salman Rushdie" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="Salman" />
            <token id="11" string="Rushdie" />
          </tokens>
        </entity>
        <entity id="5" string="Moslems" type="MISC" score="0.0">
          <tokens>
            <token id="33" string="Moslems" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="6" has_coreference="true">
      <content>``Now that circumstances have changed, when perhaps it is inconvenient for many people to support his writing, I just felt it was fitting that we should not turn our backs on him,&amp;apost;&amp;apost; Ishiguro said.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="Now" lemma="now" stem="now" pos="RB" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="3" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="circumstances" lemma="circumstance" stem="circumst" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="have" lemma="have" stem="have" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="changed" lemma="change" stem="chang" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="when" lemma="when" stem="when" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="perhaps" lemma="perhaps" stem="perhap" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="inconvenient" lemma="inconvenient" stem="inconveni" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="many" lemma="many" stem="mani" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="people" lemma="people" stem="peopl" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="support" lemma="support" stem="support" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="writing" lemma="writing" stem="write" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="22" string="just" lemma="just" stem="just" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="felt" lemma="feel" stem="felt" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="fitting" lemma="fitting" stem="fit" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="we" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="29" string="should" lemma="should" stem="should" pos="MD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="turn" lemma="turn" stem="turn" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="our" lemma="we" stem="our" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="33" string="backs" lemma="back" stem="back" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="him" lemma="he" stem="him" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="36" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="37" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="Ishiguro" lemma="Ishiguro" stem="ishiguro" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="39" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="40" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (SBAR (RB Now) (IN that) (S (NP (NNS circumstances)) (VP (VBP have) (VP (VBN changed))))) (, ,) (SBAR (WHADVP (WRB when)) (S (ADVP (RB perhaps)) (NP (PRP it)) (VP (VBZ is) (ADJP (JJ inconvenient) (PP (IN for) (NP (JJ many) (NNS people)))) (S (VP (TO to) (VP (VB support) (NP (PRP$ his) (NN writing)))))))) (, ,) (NP (PRP I)) (ADVP (RB just)) (VP (VBD felt) (SBAR (S (NP (PRP it)) (VP (VBD was) (ADJP (JJ fitting)) (SBAR (IN that) (S (NP (PRP we)) (VP (MD should) (RB not) (VP (VB turn) (NP (PRP$ our) (NNS backs)) (PP (IN on) (NP (PRP him)))))))))))) (, ,) ('' '') (NP (NNP Ishiguro)) (VP (VBD said)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="that we should not turn our backs on him" type="SBAR">
          <tokens>
            <token id="27" string="that" />
            <token id="28" string="we" />
            <token id="29" string="should" />
            <token id="30" string="not" />
            <token id="31" string="turn" />
            <token id="32" string="our" />
            <token id="33" string="backs" />
            <token id="34" string="on" />
            <token id="35" string="him" />
          </tokens>
        </chunking>
        <chunking id="2" string="circumstances" type="NP">
          <tokens>
            <token id="4" string="circumstances" />
          </tokens>
        </chunking>
        <chunking id="3" string="have changed" type="VP">
          <tokens>
            <token id="5" string="have" />
            <token id="6" string="changed" />
          </tokens>
        </chunking>
        <chunking id="4" string="to support his writing" type="VP">
          <tokens>
            <token id="16" string="to" />
            <token id="17" string="support" />
            <token id="18" string="his" />
            <token id="19" string="writing" />
          </tokens>
        </chunking>
        <chunking id="5" string="Ishiguro" type="NP">
          <tokens>
            <token id="38" string="Ishiguro" />
          </tokens>
        </chunking>
        <chunking id="6" string="it" type="NP">
          <tokens>
            <token id="10" string="it" />
          </tokens>
        </chunking>
        <chunking id="7" string="is inconvenient for many people to support his writing" type="VP">
          <tokens>
            <token id="11" string="is" />
            <token id="12" string="inconvenient" />
            <token id="13" string="for" />
            <token id="14" string="many" />
            <token id="15" string="people" />
            <token id="16" string="to" />
            <token id="17" string="support" />
            <token id="18" string="his" />
            <token id="19" string="writing" />
          </tokens>
        </chunking>
        <chunking id="8" string="support his writing" type="VP">
          <tokens>
            <token id="17" string="support" />
            <token id="18" string="his" />
            <token id="19" string="writing" />
          </tokens>
        </chunking>
        <chunking id="9" string="changed" type="VP">
          <tokens>
            <token id="6" string="changed" />
          </tokens>
        </chunking>
        <chunking id="10" string="inconvenient for many people" type="ADJP">
          <tokens>
            <token id="12" string="inconvenient" />
            <token id="13" string="for" />
            <token id="14" string="many" />
            <token id="15" string="people" />
          </tokens>
        </chunking>
        <chunking id="11" string="his writing" type="NP">
          <tokens>
            <token id="18" string="his" />
            <token id="19" string="writing" />
          </tokens>
        </chunking>
        <chunking id="12" string="when perhaps it is inconvenient for many people to support his writing" type="SBAR">
          <tokens>
            <token id="8" string="when" />
            <token id="9" string="perhaps" />
            <token id="10" string="it" />
            <token id="11" string="is" />
            <token id="12" string="inconvenient" />
            <token id="13" string="for" />
            <token id="14" string="many" />
            <token id="15" string="people" />
            <token id="16" string="to" />
            <token id="17" string="support" />
            <token id="18" string="his" />
            <token id="19" string="writing" />
          </tokens>
        </chunking>
        <chunking id="13" string="fitting" type="ADJP">
          <tokens>
            <token id="26" string="fitting" />
          </tokens>
        </chunking>
        <chunking id="14" string="should not turn our backs on him" type="VP">
          <tokens>
            <token id="29" string="should" />
            <token id="30" string="not" />
            <token id="31" string="turn" />
            <token id="32" string="our" />
            <token id="33" string="backs" />
            <token id="34" string="on" />
            <token id="35" string="him" />
          </tokens>
        </chunking>
        <chunking id="15" string="I" type="NP">
          <tokens>
            <token id="21" string="I" />
          </tokens>
        </chunking>
        <chunking id="16" string="Now that circumstances have changed" type="SBAR">
          <tokens>
            <token id="2" string="Now" />
            <token id="3" string="that" />
            <token id="4" string="circumstances" />
            <token id="5" string="have" />
            <token id="6" string="changed" />
          </tokens>
        </chunking>
        <chunking id="17" string="turn our backs on him" type="VP">
          <tokens>
            <token id="31" string="turn" />
            <token id="32" string="our" />
            <token id="33" string="backs" />
            <token id="34" string="on" />
            <token id="35" string="him" />
          </tokens>
        </chunking>
        <chunking id="18" string="him" type="NP">
          <tokens>
            <token id="35" string="him" />
          </tokens>
        </chunking>
        <chunking id="19" string="we" type="NP">
          <tokens>
            <token id="28" string="we" />
          </tokens>
        </chunking>
        <chunking id="20" string="when" type="WHADVP">
          <tokens>
            <token id="8" string="when" />
          </tokens>
        </chunking>
        <chunking id="21" string="it was fitting that we should not turn our backs on him" type="SBAR">
          <tokens>
            <token id="24" string="it" />
            <token id="25" string="was" />
            <token id="26" string="fitting" />
            <token id="27" string="that" />
            <token id="28" string="we" />
            <token id="29" string="should" />
            <token id="30" string="not" />
            <token id="31" string="turn" />
            <token id="32" string="our" />
            <token id="33" string="backs" />
            <token id="34" string="on" />
            <token id="35" string="him" />
          </tokens>
        </chunking>
        <chunking id="22" string="many people" type="NP">
          <tokens>
            <token id="14" string="many" />
            <token id="15" string="people" />
          </tokens>
        </chunking>
        <chunking id="23" string="felt it was fitting that we should not turn our backs on him" type="VP">
          <tokens>
            <token id="23" string="felt" />
            <token id="24" string="it" />
            <token id="25" string="was" />
            <token id="26" string="fitting" />
            <token id="27" string="that" />
            <token id="28" string="we" />
            <token id="29" string="should" />
            <token id="30" string="not" />
            <token id="31" string="turn" />
            <token id="32" string="our" />
            <token id="33" string="backs" />
            <token id="34" string="on" />
            <token id="35" string="him" />
          </tokens>
        </chunking>
        <chunking id="24" string="said" type="VP">
          <tokens>
            <token id="39" string="said" />
          </tokens>
        </chunking>
        <chunking id="25" string="was fitting that we should not turn our backs on him" type="VP">
          <tokens>
            <token id="25" string="was" />
            <token id="26" string="fitting" />
            <token id="27" string="that" />
            <token id="28" string="we" />
            <token id="29" string="should" />
            <token id="30" string="not" />
            <token id="31" string="turn" />
            <token id="32" string="our" />
            <token id="33" string="backs" />
            <token id="34" string="on" />
            <token id="35" string="him" />
          </tokens>
        </chunking>
        <chunking id="26" string="our backs" type="NP">
          <tokens>
            <token id="32" string="our" />
            <token id="33" string="backs" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="6">changed</governor>
          <dependent id="2">Now</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="6">changed</governor>
          <dependent id="3">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">changed</governor>
          <dependent id="4">circumstances</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="6">changed</governor>
          <dependent id="5">have</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="23">felt</governor>
          <dependent id="6">changed</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="12">inconvenient</governor>
          <dependent id="8">when</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="12">inconvenient</governor>
          <dependent id="9">perhaps</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">inconvenient</governor>
          <dependent id="10">it</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="12">inconvenient</governor>
          <dependent id="11">is</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="23">felt</governor>
          <dependent id="12">inconvenient</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">people</governor>
          <dependent id="13">for</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="15">people</governor>
          <dependent id="14">many</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">inconvenient</governor>
          <dependent id="15">people</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="17">support</governor>
          <dependent id="16">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="12">inconvenient</governor>
          <dependent id="17">support</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="19">writing</governor>
          <dependent id="18">his</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="17">support</governor>
          <dependent id="19">writing</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="23">felt</governor>
          <dependent id="21">I</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="23">felt</governor>
          <dependent id="22">just</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="39">said</governor>
          <dependent id="23">felt</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="26">fitting</governor>
          <dependent id="24">it</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="26">fitting</governor>
          <dependent id="25">was</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="23">felt</governor>
          <dependent id="26">fitting</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="31">turn</governor>
          <dependent id="27">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="31">turn</governor>
          <dependent id="28">we</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="31">turn</governor>
          <dependent id="29">should</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="31">turn</governor>
          <dependent id="30">not</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="26">fitting</governor>
          <dependent id="31">turn</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="33">backs</governor>
          <dependent id="32">our</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="31">turn</governor>
          <dependent id="33">backs</dependent>
        </dependency>
        <dependency type="case">
          <governor id="35">him</governor>
          <dependent id="34">on</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="31">turn</governor>
          <dependent id="35">him</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="39">said</governor>
          <dependent id="38">Ishiguro</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="39">said</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Now" type="DATE" score="0.0">
          <tokens>
            <token id="2" string="Now" />
          </tokens>
        </entity>
        <entity id="2" string="Ishiguro" type="PERSON" score="0.0">
          <tokens>
            <token id="38" string="Ishiguro" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="7" has_coreference="true">
      <content>Rushdie lives in England.</content>
      <tokens>
        <token id="1" string="Rushdie" lemma="Rushdie" stem="rushdi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="lives" lemma="live" stem="live" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="England" lemma="England" stem="england" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="5" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Rushdie)) (VP (VBZ lives) (PP (IN in) (NP (NNP England)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="lives in England" type="VP">
          <tokens>
            <token id="2" string="lives" />
            <token id="3" string="in" />
            <token id="4" string="England" />
          </tokens>
        </chunking>
        <chunking id="2" string="England" type="NP">
          <tokens>
            <token id="4" string="England" />
          </tokens>
        </chunking>
        <chunking id="3" string="Rushdie" type="NP">
          <tokens>
            <token id="1" string="Rushdie" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">lives</governor>
          <dependent id="1">Rushdie</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">lives</dependent>
        </dependency>
        <dependency type="case">
          <governor id="4">England</governor>
          <dependent id="3">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">lives</governor>
          <dependent id="4">England</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="England" type="LOCATION" score="0.0">
          <tokens>
            <token id="4" string="England" />
          </tokens>
        </entity>
        <entity id="2" string="Rushdie" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Rushdie" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="8" has_coreference="true">
      <content>Ishiguro, 39, arrived in England at age 6 and now lives in south London.</content>
      <tokens>
        <token id="1" string="Ishiguro" lemma="Ishiguro" stem="ishiguro" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="39" lemma="39" stem="39" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="true" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="arrived" lemma="arrive" stem="arriv" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="England" lemma="England" stem="england" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="8" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="age" lemma="age" stem="ag" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="6" lemma="6" stem="6" pos="CD" type="Number" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="11" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="now" lemma="now" stem="now" pos="RB" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="13" string="lives" lemma="live" stem="live" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="south" lemma="south" stem="south" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="London" lemma="London" stem="london" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="17" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Ishiguro)) (, ,) (NP (CD 39)) (, ,)) (VP (VP (VBN arrived) (PP (IN in) (NP (NNP England))) (PP (IN at) (NP (NN age) (CD 6)))) (CC and) (VP (ADVP (RB now)) (VBZ lives) (PP (IN in) (NP (JJ south) (NNP London))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="south London" type="NP">
          <tokens>
            <token id="15" string="south" />
            <token id="16" string="London" />
          </tokens>
        </chunking>
        <chunking id="2" string="arrived in England at age 6 and now lives in south London" type="VP">
          <tokens>
            <token id="5" string="arrived" />
            <token id="6" string="in" />
            <token id="7" string="England" />
            <token id="8" string="at" />
            <token id="9" string="age" />
            <token id="10" string="6" />
            <token id="11" string="and" />
            <token id="12" string="now" />
            <token id="13" string="lives" />
            <token id="14" string="in" />
            <token id="15" string="south" />
            <token id="16" string="London" />
          </tokens>
        </chunking>
        <chunking id="3" string="age 6" type="NP">
          <tokens>
            <token id="9" string="age" />
            <token id="10" string="6" />
          </tokens>
        </chunking>
        <chunking id="4" string="39" type="NP">
          <tokens>
            <token id="3" string="39" />
          </tokens>
        </chunking>
        <chunking id="5" string="arrived in England at age 6" type="VP">
          <tokens>
            <token id="5" string="arrived" />
            <token id="6" string="in" />
            <token id="7" string="England" />
            <token id="8" string="at" />
            <token id="9" string="age" />
            <token id="10" string="6" />
          </tokens>
        </chunking>
        <chunking id="6" string="Ishiguro" type="NP">
          <tokens>
            <token id="1" string="Ishiguro" />
          </tokens>
        </chunking>
        <chunking id="7" string="now lives in south London" type="VP">
          <tokens>
            <token id="12" string="now" />
            <token id="13" string="lives" />
            <token id="14" string="in" />
            <token id="15" string="south" />
            <token id="16" string="London" />
          </tokens>
        </chunking>
        <chunking id="8" string="England" type="NP">
          <tokens>
            <token id="7" string="England" />
          </tokens>
        </chunking>
        <chunking id="9" string="Ishiguro , 39 ," type="NP">
          <tokens>
            <token id="1" string="Ishiguro" />
            <token id="2" string="," />
            <token id="3" string="39" />
            <token id="4" string="," />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="5">arrived</governor>
          <dependent id="1">Ishiguro</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="1">Ishiguro</governor>
          <dependent id="3">39</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">arrived</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">England</governor>
          <dependent id="6">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">arrived</governor>
          <dependent id="7">England</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">age</governor>
          <dependent id="8">at</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">arrived</governor>
          <dependent id="9">age</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="9">age</governor>
          <dependent id="10">6</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="5">arrived</governor>
          <dependent id="11">and</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="13">lives</governor>
          <dependent id="12">now</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="5">arrived</governor>
          <dependent id="13">lives</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">London</governor>
          <dependent id="14">in</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="16">London</governor>
          <dependent id="15">south</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">lives</governor>
          <dependent id="16">London</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="now" type="DATE" score="0.0">
          <tokens>
            <token id="12" string="now" />
          </tokens>
        </entity>
        <entity id="2" string="39" type="NUMBER" score="0.0">
          <tokens>
            <token id="3" string="39" />
          </tokens>
        </entity>
        <entity id="3" string="6" type="NUMBER" score="0.0">
          <tokens>
            <token id="10" string="6" />
          </tokens>
        </entity>
        <entity id="4" string="Ishiguro" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Ishiguro" />
          </tokens>
        </entity>
        <entity id="5" string="London" type="LOCATION" score="0.0">
          <tokens>
            <token id="16" string="London" />
          </tokens>
        </entity>
        <entity id="6" string="England" type="LOCATION" score="0.0">
          <tokens>
            <token id="7" string="England" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="9" has_coreference="true">
      <content>He writes in English.</content>
      <tokens>
        <token id="1" string="He" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="writes" lemma="write" stem="write" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="English" lemma="English" stem="english" pos="NNP" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="false" />
        <token id="5" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP He)) (VP (VBZ writes) (PP (IN in) (NP (NNP English)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="He" type="NP">
          <tokens>
            <token id="1" string="He" />
          </tokens>
        </chunking>
        <chunking id="2" string="writes in English" type="VP">
          <tokens>
            <token id="2" string="writes" />
            <token id="3" string="in" />
            <token id="4" string="English" />
          </tokens>
        </chunking>
        <chunking id="3" string="English" type="NP">
          <tokens>
            <token id="4" string="English" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">writes</governor>
          <dependent id="1">He</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">writes</dependent>
        </dependency>
        <dependency type="case">
          <governor id="4">English</governor>
          <dependent id="3">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">writes</governor>
          <dependent id="4">English</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="English" type="MISC" score="0.0">
          <tokens>
            <token id="4" string="English" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="10" has_coreference="true">
      <content>In announcing the winner, the judges said: ```The Remains of the Day&amp;apost; renders with humor and pathos a memorable character and explores the large, vexed themes of class, tradition and duty.</content>
      <tokens>
        <token id="1" string="In" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="announcing" lemma="announce" stem="announc" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="winner" lemma="winner" stem="winner" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="judges" lemma="judge" stem="judg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="8" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string=":" lemma=":" stem=":" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="`" lemma="`" stem="`" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="Remains" lemma="remains" stem="remain" pos="NNS" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="true" />
        <token id="14" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" ner="MISC" is_referenced="false" is_refers="true" />
        <token id="15" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="16" string="Day" lemma="Day" stem="dai" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="17" string="'" lemma="'" stem="'" pos="POS" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="renders" lemma="render" stem="render" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="humor" lemma="humor" stem="humor" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="pathos" lemma="pathos" stem="patho" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="memorable" lemma="memorable" stem="memor" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="character" lemma="character" stem="charact" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="explores" lemma="explore" stem="explor" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="large" lemma="large" stem="larg" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="vexed" lemma="vexed" stem="vex" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="themes" lemma="theme" stem="theme" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="class" lemma="class" stem="class" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="tradition" lemma="tradition" stem="tradit" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="37" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="duty" lemma="duty" stem="duti" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="39" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (IN In) (S (VP (VBG announcing) (NP (DT the) (NN winner))))) (, ,) (NP (DT the) (NNS judges)) (VP (VBD said) (: :) (`` ``) (S (`` `) (NP (NP (DT The) (NNS Remains)) (PP (IN of) (NP (DT the) (NNP Day) (POS ')))) (VP (VP (VBZ renders) (PP (IN with) (NP (NP (NN humor) (CC and) (NN pathos)) (NP (DT a) (JJ memorable) (NN character))))) (CC and) (VP (VBZ explores) (NP (NP (DT the) (JJ large) (, ,) (JJ vexed) (NNS themes)) (PP (IN of) (NP (NN class) (, ,) (NN tradition) (CC and) (NN duty)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a memorable character" type="NP">
          <tokens>
            <token id="23" string="a" />
            <token id="24" string="memorable" />
            <token id="25" string="character" />
          </tokens>
        </chunking>
        <chunking id="2" string="humor and pathos a memorable character" type="NP">
          <tokens>
            <token id="20" string="humor" />
            <token id="21" string="and" />
            <token id="22" string="pathos" />
            <token id="23" string="a" />
            <token id="24" string="memorable" />
            <token id="25" string="character" />
          </tokens>
        </chunking>
        <chunking id="3" string="the Day '" type="NP">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="Day" />
            <token id="17" string="'" />
          </tokens>
        </chunking>
        <chunking id="4" string="The Remains of the Day '" type="NP">
          <tokens>
            <token id="12" string="The" />
            <token id="13" string="Remains" />
            <token id="14" string="of" />
            <token id="15" string="the" />
            <token id="16" string="Day" />
            <token id="17" string="'" />
          </tokens>
        </chunking>
        <chunking id="5" string="said : `` ` The Remains of the Day ' renders with humor and pathos a memorable character and explores the large , vexed themes of class , tradition and duty" type="VP">
          <tokens>
            <token id="8" string="said" />
            <token id="9" string=":" />
            <token id="10" string="``" />
            <token id="11" string="`" />
            <token id="12" string="The" />
            <token id="13" string="Remains" />
            <token id="14" string="of" />
            <token id="15" string="the" />
            <token id="16" string="Day" />
            <token id="17" string="'" />
            <token id="18" string="renders" />
            <token id="19" string="with" />
            <token id="20" string="humor" />
            <token id="21" string="and" />
            <token id="22" string="pathos" />
            <token id="23" string="a" />
            <token id="24" string="memorable" />
            <token id="25" string="character" />
            <token id="26" string="and" />
            <token id="27" string="explores" />
            <token id="28" string="the" />
            <token id="29" string="large" />
            <token id="30" string="," />
            <token id="31" string="vexed" />
            <token id="32" string="themes" />
            <token id="33" string="of" />
            <token id="34" string="class" />
            <token id="35" string="," />
            <token id="36" string="tradition" />
            <token id="37" string="and" />
            <token id="38" string="duty" />
          </tokens>
        </chunking>
        <chunking id="6" string="humor and pathos" type="NP">
          <tokens>
            <token id="20" string="humor" />
            <token id="21" string="and" />
            <token id="22" string="pathos" />
          </tokens>
        </chunking>
        <chunking id="7" string="explores the large , vexed themes of class , tradition and duty" type="VP">
          <tokens>
            <token id="27" string="explores" />
            <token id="28" string="the" />
            <token id="29" string="large" />
            <token id="30" string="," />
            <token id="31" string="vexed" />
            <token id="32" string="themes" />
            <token id="33" string="of" />
            <token id="34" string="class" />
            <token id="35" string="," />
            <token id="36" string="tradition" />
            <token id="37" string="and" />
            <token id="38" string="duty" />
          </tokens>
        </chunking>
        <chunking id="8" string="the large , vexed themes" type="NP">
          <tokens>
            <token id="28" string="the" />
            <token id="29" string="large" />
            <token id="30" string="," />
            <token id="31" string="vexed" />
            <token id="32" string="themes" />
          </tokens>
        </chunking>
        <chunking id="9" string="the winner" type="NP">
          <tokens>
            <token id="3" string="the" />
            <token id="4" string="winner" />
          </tokens>
        </chunking>
        <chunking id="10" string="renders with humor and pathos a memorable character" type="VP">
          <tokens>
            <token id="18" string="renders" />
            <token id="19" string="with" />
            <token id="20" string="humor" />
            <token id="21" string="and" />
            <token id="22" string="pathos" />
            <token id="23" string="a" />
            <token id="24" string="memorable" />
            <token id="25" string="character" />
          </tokens>
        </chunking>
        <chunking id="11" string="announcing the winner" type="VP">
          <tokens>
            <token id="2" string="announcing" />
            <token id="3" string="the" />
            <token id="4" string="winner" />
          </tokens>
        </chunking>
        <chunking id="12" string="the judges" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="judges" />
          </tokens>
        </chunking>
        <chunking id="13" string="the large , vexed themes of class , tradition and duty" type="NP">
          <tokens>
            <token id="28" string="the" />
            <token id="29" string="large" />
            <token id="30" string="," />
            <token id="31" string="vexed" />
            <token id="32" string="themes" />
            <token id="33" string="of" />
            <token id="34" string="class" />
            <token id="35" string="," />
            <token id="36" string="tradition" />
            <token id="37" string="and" />
            <token id="38" string="duty" />
          </tokens>
        </chunking>
        <chunking id="14" string="The Remains" type="NP">
          <tokens>
            <token id="12" string="The" />
            <token id="13" string="Remains" />
          </tokens>
        </chunking>
        <chunking id="15" string="class , tradition and duty" type="NP">
          <tokens>
            <token id="34" string="class" />
            <token id="35" string="," />
            <token id="36" string="tradition" />
            <token id="37" string="and" />
            <token id="38" string="duty" />
          </tokens>
        </chunking>
        <chunking id="16" string="renders with humor and pathos a memorable character and explores the large , vexed themes of class , tradition and duty" type="VP">
          <tokens>
            <token id="18" string="renders" />
            <token id="19" string="with" />
            <token id="20" string="humor" />
            <token id="21" string="and" />
            <token id="22" string="pathos" />
            <token id="23" string="a" />
            <token id="24" string="memorable" />
            <token id="25" string="character" />
            <token id="26" string="and" />
            <token id="27" string="explores" />
            <token id="28" string="the" />
            <token id="29" string="large" />
            <token id="30" string="," />
            <token id="31" string="vexed" />
            <token id="32" string="themes" />
            <token id="33" string="of" />
            <token id="34" string="class" />
            <token id="35" string="," />
            <token id="36" string="tradition" />
            <token id="37" string="and" />
            <token id="38" string="duty" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="mark">
          <governor id="2">announcing</governor>
          <dependent id="1">In</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="8">said</governor>
          <dependent id="2">announcing</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">winner</governor>
          <dependent id="3">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">announcing</governor>
          <dependent id="4">winner</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">judges</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">said</governor>
          <dependent id="7">judges</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="8">said</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">Remains</governor>
          <dependent id="12">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="18">renders</governor>
          <dependent id="13">Remains</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">Day</governor>
          <dependent id="14">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="16">Day</governor>
          <dependent id="15">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">Remains</governor>
          <dependent id="16">Day</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">Day</governor>
          <dependent id="17">'</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="8">said</governor>
          <dependent id="18">renders</dependent>
        </dependency>
        <dependency type="case">
          <governor id="20">humor</governor>
          <dependent id="19">with</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="18">renders</governor>
          <dependent id="20">humor</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="20">humor</governor>
          <dependent id="21">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="20">humor</governor>
          <dependent id="22">pathos</dependent>
        </dependency>
        <dependency type="det">
          <governor id="25">character</governor>
          <dependent id="23">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="25">character</governor>
          <dependent id="24">memorable</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="20">humor</governor>
          <dependent id="25">character</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="18">renders</governor>
          <dependent id="26">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="18">renders</governor>
          <dependent id="27">explores</dependent>
        </dependency>
        <dependency type="det">
          <governor id="32">themes</governor>
          <dependent id="28">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="32">themes</governor>
          <dependent id="29">large</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="32">themes</governor>
          <dependent id="31">vexed</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="27">explores</governor>
          <dependent id="32">themes</dependent>
        </dependency>
        <dependency type="case">
          <governor id="34">class</governor>
          <dependent id="33">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="32">themes</governor>
          <dependent id="34">class</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="34">class</governor>
          <dependent id="36">tradition</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="34">class</governor>
          <dependent id="37">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="34">class</governor>
          <dependent id="38">duty</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="the Day" type="DATE" score="0.0">
          <tokens>
            <token id="15" string="the" />
            <token id="16" string="Day" />
          </tokens>
        </entity>
        <entity id="2" string="Remains of" type="MISC" score="0.0">
          <tokens>
            <token id="13" string="Remains" />
            <token id="14" string="of" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="11" has_coreference="true">
      <content>It was narrowly preferred but universally admired.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="narrowly" lemma="narrowly" stem="narrowli" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="preferred" lemma="prefer" stem="prefer" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="universally" lemma="universally" stem="univers" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="admired" lemma="admire" stem="admir" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP It)) (VP (VP (VBD was) (ADJP (RB narrowly) (VBN preferred))) (CC but) (ADVP (RB universally)) (VP (VBD admired))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="was narrowly preferred" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="narrowly" />
            <token id="4" string="preferred" />
          </tokens>
        </chunking>
        <chunking id="2" string="admired" type="VP">
          <tokens>
            <token id="7" string="admired" />
          </tokens>
        </chunking>
        <chunking id="3" string="narrowly preferred" type="ADJP">
          <tokens>
            <token id="3" string="narrowly" />
            <token id="4" string="preferred" />
          </tokens>
        </chunking>
        <chunking id="4" string="It" type="NP">
          <tokens>
            <token id="1" string="It" />
          </tokens>
        </chunking>
        <chunking id="5" string="was narrowly preferred but universally admired" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="narrowly" />
            <token id="4" string="preferred" />
            <token id="5" string="but" />
            <token id="6" string="universally" />
            <token id="7" string="admired" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="4">preferred</governor>
          <dependent id="1">It</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="4">preferred</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">preferred</governor>
          <dependent id="3">narrowly</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">preferred</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">preferred</governor>
          <dependent id="5">but</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="7">admired</governor>
          <dependent id="6">universally</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">preferred</governor>
          <dependent id="7">admired</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="12" has_coreference="true">
      <content>Ishiguro was a finalist for the Booker Prize in 1986 for his novel ``An Artist of the Floating World.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="Ishiguro" lemma="Ishiguro" stem="ishiguro" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="finalist" lemma="finalist" stem="finalist" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="Booker" lemma="Booker" stem="booker" pos="NNP" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="true" />
        <token id="8" string="Prize" lemma="Prize" stem="prize" pos="NNP" type="Word" isStopWord="false" ner="MISC" is_referenced="false" is_refers="true" />
        <token id="9" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="1986" lemma="1986" stem="1986" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="true" />
        <token id="11" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="novel" lemma="novel" stem="novel" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="14" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="An" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="16" string="Artist" lemma="Artist" stem="artist" pos="NNP" type="Word" isStopWord="false" ner="TITLE" is_referenced="false" is_refers="true" />
        <token id="17" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="19" string="Floating" lemma="float" stem="float" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="20" string="World" lemma="world" stem="world" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="21" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Ishiguro)) (VP (VBD was) (NP (NP (DT a) (NN finalist)) (PP (IN for) (NP (DT the) (NNP Booker) (NNP Prize))) (PP (IN in) (NP (CD 1986))) (PP (IN for) (NP (PRP$ his) (JJ novel))) (`` ``) (NP (NP (DT An) (NNP Artist)) (PP (IN of) (NP (DT the) (VBG Floating) (NN World)))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="1986" type="NP">
          <tokens>
            <token id="10" string="1986" />
          </tokens>
        </chunking>
        <chunking id="2" string="the Floating World" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="Floating" />
            <token id="20" string="World" />
          </tokens>
        </chunking>
        <chunking id="3" string="a finalist" type="NP">
          <tokens>
            <token id="3" string="a" />
            <token id="4" string="finalist" />
          </tokens>
        </chunking>
        <chunking id="4" string="Ishiguro" type="NP">
          <tokens>
            <token id="1" string="Ishiguro" />
          </tokens>
        </chunking>
        <chunking id="5" string="An Artist" type="NP">
          <tokens>
            <token id="15" string="An" />
            <token id="16" string="Artist" />
          </tokens>
        </chunking>
        <chunking id="6" string="a finalist for the Booker Prize in 1986 for his novel `` An Artist of the Floating World" type="NP">
          <tokens>
            <token id="3" string="a" />
            <token id="4" string="finalist" />
            <token id="5" string="for" />
            <token id="6" string="the" />
            <token id="7" string="Booker" />
            <token id="8" string="Prize" />
            <token id="9" string="in" />
            <token id="10" string="1986" />
            <token id="11" string="for" />
            <token id="12" string="his" />
            <token id="13" string="novel" />
            <token id="14" string="``" />
            <token id="15" string="An" />
            <token id="16" string="Artist" />
            <token id="17" string="of" />
            <token id="18" string="the" />
            <token id="19" string="Floating" />
            <token id="20" string="World" />
          </tokens>
        </chunking>
        <chunking id="7" string="An Artist of the Floating World" type="NP">
          <tokens>
            <token id="15" string="An" />
            <token id="16" string="Artist" />
            <token id="17" string="of" />
            <token id="18" string="the" />
            <token id="19" string="Floating" />
            <token id="20" string="World" />
          </tokens>
        </chunking>
        <chunking id="8" string="the Booker Prize" type="NP">
          <tokens>
            <token id="6" string="the" />
            <token id="7" string="Booker" />
            <token id="8" string="Prize" />
          </tokens>
        </chunking>
        <chunking id="9" string="was a finalist for the Booker Prize in 1986 for his novel `` An Artist of the Floating World" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="a" />
            <token id="4" string="finalist" />
            <token id="5" string="for" />
            <token id="6" string="the" />
            <token id="7" string="Booker" />
            <token id="8" string="Prize" />
            <token id="9" string="in" />
            <token id="10" string="1986" />
            <token id="11" string="for" />
            <token id="12" string="his" />
            <token id="13" string="novel" />
            <token id="14" string="``" />
            <token id="15" string="An" />
            <token id="16" string="Artist" />
            <token id="17" string="of" />
            <token id="18" string="the" />
            <token id="19" string="Floating" />
            <token id="20" string="World" />
          </tokens>
        </chunking>
        <chunking id="10" string="his novel" type="NP">
          <tokens>
            <token id="12" string="his" />
            <token id="13" string="novel" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">finalist</governor>
          <dependent id="1">Ishiguro</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="4">finalist</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">finalist</governor>
          <dependent id="3">a</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">finalist</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">Prize</governor>
          <dependent id="5">for</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">Prize</governor>
          <dependent id="6">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">Prize</governor>
          <dependent id="7">Booker</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">finalist</governor>
          <dependent id="8">Prize</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">1986</governor>
          <dependent id="9">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">finalist</governor>
          <dependent id="10">1986</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">novel</governor>
          <dependent id="11">for</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="13">novel</governor>
          <dependent id="12">his</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">finalist</governor>
          <dependent id="13">novel</dependent>
        </dependency>
        <dependency type="det">
          <governor id="16">Artist</governor>
          <dependent id="15">An</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="4">finalist</governor>
          <dependent id="16">Artist</dependent>
        </dependency>
        <dependency type="case">
          <governor id="20">World</governor>
          <dependent id="17">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="20">World</governor>
          <dependent id="18">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="20">World</governor>
          <dependent id="19">Floating</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">Artist</governor>
          <dependent id="20">World</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Artist" type="TITLE" score="0.0">
          <tokens>
            <token id="16" string="Artist" />
          </tokens>
        </entity>
        <entity id="2" string="1986" type="DATE" score="0.0">
          <tokens>
            <token id="10" string="1986" />
          </tokens>
        </entity>
        <entity id="3" string="Ishiguro" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Ishiguro" />
          </tokens>
        </entity>
        <entity id="4" string="Booker Prize" type="MISC" score="0.0">
          <tokens>
            <token id="7" string="Booker" />
            <token id="8" string="Prize" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="13" has_coreference="true">
      <content>The Booker Prize is sponsored by Booker, an international food and agriculture business.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="Booker" lemma="Booker" stem="booker" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="Prize" lemma="Prize" stem="prize" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="4" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="sponsored" lemma="sponsor" stem="sponsor" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="Booker" lemma="Booker" stem="booker" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="international" lemma="international" stem="intern" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="food" lemma="food" stem="food" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="agriculture" lemma="agriculture" stem="agricultur" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="business" lemma="business" stem="busi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NNP Booker) (NNP Prize)) (VP (VBZ is) (VP (VBN sponsored) (PP (IN by) (NP (NP (NNP Booker)) (, ,) (NP (DT an) (JJ international) (NN food)) (CC and) (NP (NN agriculture) (NN business)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="an international food" type="NP">
          <tokens>
            <token id="9" string="an" />
            <token id="10" string="international" />
            <token id="11" string="food" />
          </tokens>
        </chunking>
        <chunking id="2" string="sponsored by Booker , an international food and agriculture business" type="VP">
          <tokens>
            <token id="5" string="sponsored" />
            <token id="6" string="by" />
            <token id="7" string="Booker" />
            <token id="8" string="," />
            <token id="9" string="an" />
            <token id="10" string="international" />
            <token id="11" string="food" />
            <token id="12" string="and" />
            <token id="13" string="agriculture" />
            <token id="14" string="business" />
          </tokens>
        </chunking>
        <chunking id="3" string="Booker" type="NP">
          <tokens>
            <token id="7" string="Booker" />
          </tokens>
        </chunking>
        <chunking id="4" string="Booker , an international food and agriculture business" type="NP">
          <tokens>
            <token id="7" string="Booker" />
            <token id="8" string="," />
            <token id="9" string="an" />
            <token id="10" string="international" />
            <token id="11" string="food" />
            <token id="12" string="and" />
            <token id="13" string="agriculture" />
            <token id="14" string="business" />
          </tokens>
        </chunking>
        <chunking id="5" string="The Booker Prize" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Booker" />
            <token id="3" string="Prize" />
          </tokens>
        </chunking>
        <chunking id="6" string="is sponsored by Booker , an international food and agriculture business" type="VP">
          <tokens>
            <token id="4" string="is" />
            <token id="5" string="sponsored" />
            <token id="6" string="by" />
            <token id="7" string="Booker" />
            <token id="8" string="," />
            <token id="9" string="an" />
            <token id="10" string="international" />
            <token id="11" string="food" />
            <token id="12" string="and" />
            <token id="13" string="agriculture" />
            <token id="14" string="business" />
          </tokens>
        </chunking>
        <chunking id="7" string="agriculture business" type="NP">
          <tokens>
            <token id="13" string="agriculture" />
            <token id="14" string="business" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">Prize</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">Prize</governor>
          <dependent id="2">Booker</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="5">sponsored</governor>
          <dependent id="3">Prize</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="5">sponsored</governor>
          <dependent id="4">is</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">sponsored</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">Booker</governor>
          <dependent id="6">by</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">sponsored</governor>
          <dependent id="7">Booker</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">food</governor>
          <dependent id="9">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">food</governor>
          <dependent id="10">international</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">Booker</governor>
          <dependent id="11">food</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="7">Booker</governor>
          <dependent id="12">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">business</governor>
          <dependent id="13">agriculture</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="7">Booker</governor>
          <dependent id="14">business</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Booker" type="PERSON" score="0.0">
          <tokens>
            <token id="7" string="Booker" />
          </tokens>
        </entity>
      </entities>
    </sentence>
  </sentences>
  <coreferences>
    <coreference id="1" type="PROPER">
      <referenced ids_tokens="8-9" string="Booker Prize" id_sentence="1" />
      <mentions>
        <mention ids_tokens="1-2" string="The prize" id_sentence="2" />
        <mention ids_tokens="3-4" string="the prize" id_sentence="5" />
        <mention ids_tokens="17-18" string="the award" id_sentence="5" />
        <mention ids_tokens="12-17" string="The Remains of the Day'" id_sentence="10" />
        <mention ids_tokens="1" string="It" id_sentence="11" />
      </mentions>
    </coreference>
    <coreference id="2" type="PROPER">
      <referenced ids_tokens="1-2-3-4" string="Japanese writer Kazuo Ishiguro" id_sentence="1" />
      <mentions>
        <mention ids_tokens="6" string="Ishiguro" id_sentence="5" />
        <mention ids_tokens="27" string="he" id_sentence="5" />
        <mention ids_tokens="36" string="his" id_sentence="5" />
        <mention ids_tokens="18" string="his" id_sentence="6" />
        <mention ids_tokens="35" string="him" id_sentence="6" />
        <mention ids_tokens="38" string="Ishiguro" id_sentence="6" />
        <mention ids_tokens="1-3" string="Ishiguro , 39" id_sentence="8" />
        <mention ids_tokens="1" string="Ishiguro" id_sentence="8" />
        <mention ids_tokens="1" string="He" id_sentence="9" />
        <mention ids_tokens="1" string="Ishiguro" id_sentence="12" />
        <mention ids_tokens="3-20" string="a finalist for the Booker Prize in 1986 for his novel `` An Artist of the Floating World" id_sentence="12" />
        <mention ids_tokens="12" string="his" id_sentence="12" />
      </mentions>
    </coreference>
    <coreference id="3" type="PROPER">
      <referenced ids_tokens="6-7-8-9" string="the 1989 Booker Prize" id_sentence="1" />
      <mentions>
        <mention ids_tokens="6-8" string="the Booker Prize" id_sentence="12" />
        <mention ids_tokens="1-3" string="The Booker Prize" id_sentence="13" />
      </mentions>
    </coreference>
    <coreference id="5" type="PROPER">
      <referenced ids_tokens="24-25" string="the Day" id_sentence="1" />
      <mentions>
        <mention ids_tokens="15-17" string="the Day'" id_sentence="10" />
      </mentions>
    </coreference>
    <coreference id="6" type="PROPER">
      <referenced ids_tokens="1" string="Judges" id_sentence="3" />
      <mentions>
        <mention ids_tokens="6-7" string="the judges" id_sentence="10" />
      </mentions>
    </coreference>
    <coreference id="8" type="PROPER">
      <referenced ids_tokens="6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-21-22-23-24" string="Canadian Margaret Atwood for `` Cat 's Eye , '' Irish author John Banville for `` Book of Evidence" id_sentence="4" />
      <mentions>
        <mention ids_tokens="28" string="we" id_sentence="6" />
        <mention ids_tokens="32" string="our" id_sentence="6" />
      </mentions>
    </coreference>
    <coreference id="9" type="PROPER">
      <referenced ids_tokens="10-11" string="Salman Rushdie" id_sentence="5" />
      <mentions>
        <mention ids_tokens="21" string="I" id_sentence="6" />
        <mention ids_tokens="1" string="Rushdie" id_sentence="7" />
      </mentions>
    </coreference>
  </coreferences>
</document>
