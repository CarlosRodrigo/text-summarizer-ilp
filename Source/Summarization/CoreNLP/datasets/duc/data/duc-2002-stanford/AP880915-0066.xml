<?xml version="1.0" encoding="UTF-8"?>
<document id="0" name="AP880915-0066">
  <sentences>
    <sentence id="1" has_coreference="true">
      <content>A biography that portrays John Lennon as a drug-addled, anorexic bisexual who raged his way from Liverpool to New York City is ``totally fiction,&amp;apost;&amp;apost; Yoko Ono said in a national radio broadcast.</content>
      <tokens>
        <token id="1" string="A" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="2" string="biography" lemma="biography" stem="biographi" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="3" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="4" string="portrays" lemma="portray" stem="portrai" pos="VBZ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="5" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="6" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="7" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="9" string="drug-addled" lemma="drug-addled" stem="drug-addl" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="anorexic" lemma="anorexic" stem="anorex" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="12" string="bisexual" lemma="bisexual" stem="bisexu" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="13" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="raged" lemma="rage" stem="rage" pos="VBD" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="16" string="way" lemma="way" stem="wai" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="17" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="18" string="Liverpool" lemma="Liverpool" stem="liverpool" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="19" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="20" string="New" lemma="New" stem="new" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="21" string="York" lemma="York" stem="york" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="22" string="City" lemma="City" stem="citi" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="23" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="totally" lemma="totally" stem="total" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="26" string="fiction" lemma="fiction" stem="fiction" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="Yoko" lemma="Yoko" stem="yoko" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="30" string="Ono" lemma="Ono" stem="ono" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="31" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="34" string="national" lemma="national" stem="nation" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="35" string="radio" lemma="radio" stem="radio" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="36" string="broadcast" lemma="broadcast" stem="broadcast" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="37" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (NP (DT A) (NN biography)) (SBAR (WHNP (WDT that)) (S (VP (VBZ portrays) (NP (NP (NNP John) (NNP Lennon)) (PP (IN as) (NP (NP (DT a) (JJ drug-addled) (, ,) (JJ anorexic) (NN bisexual)) (SBAR (WHNP (WP who)) (S (VP (VBD raged) (NP (PRP$ his) (NN way)) (PP (IN from) (NP (NNP Liverpool))) (PP (TO to) (NP (NNP New) (NNP York) (NNP City))))))))))))) (VP (VBZ is) (`` ``) (NP (RB totally) (NN fiction)))) (, ,) ('' '') (NP (NNP Yoko) (NNP Ono)) (VP (VBD said) (PP (IN in) (NP (DT a) (JJ national) (NN radio) (NN broadcast)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Liverpool" type="NP">
          <tokens>
            <token id="18" string="Liverpool" />
          </tokens>
        </chunking>
        <chunking id="2" string="totally fiction" type="NP">
          <tokens>
            <token id="25" string="totally" />
            <token id="26" string="fiction" />
          </tokens>
        </chunking>
        <chunking id="3" string="A biography" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="biography" />
          </tokens>
        </chunking>
        <chunking id="4" string="raged his way from Liverpool to New York City" type="VP">
          <tokens>
            <token id="14" string="raged" />
            <token id="15" string="his" />
            <token id="16" string="way" />
            <token id="17" string="from" />
            <token id="18" string="Liverpool" />
            <token id="19" string="to" />
            <token id="20" string="New" />
            <token id="21" string="York" />
            <token id="22" string="City" />
          </tokens>
        </chunking>
        <chunking id="5" string="a drug-addled , anorexic bisexual" type="NP">
          <tokens>
            <token id="8" string="a" />
            <token id="9" string="drug-addled" />
            <token id="10" string="," />
            <token id="11" string="anorexic" />
            <token id="12" string="bisexual" />
          </tokens>
        </chunking>
        <chunking id="6" string="John Lennon" type="NP">
          <tokens>
            <token id="5" string="John" />
            <token id="6" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="7" string="that portrays John Lennon as a drug-addled , anorexic bisexual who raged his way from Liverpool to New York City" type="SBAR">
          <tokens>
            <token id="3" string="that" />
            <token id="4" string="portrays" />
            <token id="5" string="John" />
            <token id="6" string="Lennon" />
            <token id="7" string="as" />
            <token id="8" string="a" />
            <token id="9" string="drug-addled" />
            <token id="10" string="," />
            <token id="11" string="anorexic" />
            <token id="12" string="bisexual" />
            <token id="13" string="who" />
            <token id="14" string="raged" />
            <token id="15" string="his" />
            <token id="16" string="way" />
            <token id="17" string="from" />
            <token id="18" string="Liverpool" />
            <token id="19" string="to" />
            <token id="20" string="New" />
            <token id="21" string="York" />
            <token id="22" string="City" />
          </tokens>
        </chunking>
        <chunking id="8" string="portrays John Lennon as a drug-addled , anorexic bisexual who raged his way from Liverpool to New York City" type="VP">
          <tokens>
            <token id="4" string="portrays" />
            <token id="5" string="John" />
            <token id="6" string="Lennon" />
            <token id="7" string="as" />
            <token id="8" string="a" />
            <token id="9" string="drug-addled" />
            <token id="10" string="," />
            <token id="11" string="anorexic" />
            <token id="12" string="bisexual" />
            <token id="13" string="who" />
            <token id="14" string="raged" />
            <token id="15" string="his" />
            <token id="16" string="way" />
            <token id="17" string="from" />
            <token id="18" string="Liverpool" />
            <token id="19" string="to" />
            <token id="20" string="New" />
            <token id="21" string="York" />
            <token id="22" string="City" />
          </tokens>
        </chunking>
        <chunking id="9" string="a national radio broadcast" type="NP">
          <tokens>
            <token id="33" string="a" />
            <token id="34" string="national" />
            <token id="35" string="radio" />
            <token id="36" string="broadcast" />
          </tokens>
        </chunking>
        <chunking id="10" string="A biography that portrays John Lennon as a drug-addled , anorexic bisexual who raged his way from Liverpool to New York City" type="NP">
          <tokens>
            <token id="1" string="A" />
            <token id="2" string="biography" />
            <token id="3" string="that" />
            <token id="4" string="portrays" />
            <token id="5" string="John" />
            <token id="6" string="Lennon" />
            <token id="7" string="as" />
            <token id="8" string="a" />
            <token id="9" string="drug-addled" />
            <token id="10" string="," />
            <token id="11" string="anorexic" />
            <token id="12" string="bisexual" />
            <token id="13" string="who" />
            <token id="14" string="raged" />
            <token id="15" string="his" />
            <token id="16" string="way" />
            <token id="17" string="from" />
            <token id="18" string="Liverpool" />
            <token id="19" string="to" />
            <token id="20" string="New" />
            <token id="21" string="York" />
            <token id="22" string="City" />
          </tokens>
        </chunking>
        <chunking id="11" string="John Lennon as a drug-addled , anorexic bisexual who raged his way from Liverpool to New York City" type="NP">
          <tokens>
            <token id="5" string="John" />
            <token id="6" string="Lennon" />
            <token id="7" string="as" />
            <token id="8" string="a" />
            <token id="9" string="drug-addled" />
            <token id="10" string="," />
            <token id="11" string="anorexic" />
            <token id="12" string="bisexual" />
            <token id="13" string="who" />
            <token id="14" string="raged" />
            <token id="15" string="his" />
            <token id="16" string="way" />
            <token id="17" string="from" />
            <token id="18" string="Liverpool" />
            <token id="19" string="to" />
            <token id="20" string="New" />
            <token id="21" string="York" />
            <token id="22" string="City" />
          </tokens>
        </chunking>
        <chunking id="12" string="is `` totally fiction" type="VP">
          <tokens>
            <token id="23" string="is" />
            <token id="24" string="``" />
            <token id="25" string="totally" />
            <token id="26" string="fiction" />
          </tokens>
        </chunking>
        <chunking id="13" string="New York City" type="NP">
          <tokens>
            <token id="20" string="New" />
            <token id="21" string="York" />
            <token id="22" string="City" />
          </tokens>
        </chunking>
        <chunking id="14" string="Yoko Ono" type="NP">
          <tokens>
            <token id="29" string="Yoko" />
            <token id="30" string="Ono" />
          </tokens>
        </chunking>
        <chunking id="15" string="said in a national radio broadcast" type="VP">
          <tokens>
            <token id="31" string="said" />
            <token id="32" string="in" />
            <token id="33" string="a" />
            <token id="34" string="national" />
            <token id="35" string="radio" />
            <token id="36" string="broadcast" />
          </tokens>
        </chunking>
        <chunking id="16" string="his way" type="NP">
          <tokens>
            <token id="15" string="his" />
            <token id="16" string="way" />
          </tokens>
        </chunking>
        <chunking id="17" string="who raged his way from Liverpool to New York City" type="SBAR">
          <tokens>
            <token id="13" string="who" />
            <token id="14" string="raged" />
            <token id="15" string="his" />
            <token id="16" string="way" />
            <token id="17" string="from" />
            <token id="18" string="Liverpool" />
            <token id="19" string="to" />
            <token id="20" string="New" />
            <token id="21" string="York" />
            <token id="22" string="City" />
          </tokens>
        </chunking>
        <chunking id="18" string="a drug-addled , anorexic bisexual who raged his way from Liverpool to New York City" type="NP">
          <tokens>
            <token id="8" string="a" />
            <token id="9" string="drug-addled" />
            <token id="10" string="," />
            <token id="11" string="anorexic" />
            <token id="12" string="bisexual" />
            <token id="13" string="who" />
            <token id="14" string="raged" />
            <token id="15" string="his" />
            <token id="16" string="way" />
            <token id="17" string="from" />
            <token id="18" string="Liverpool" />
            <token id="19" string="to" />
            <token id="20" string="New" />
            <token id="21" string="York" />
            <token id="22" string="City" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">biography</governor>
          <dependent id="1">A</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="26">fiction</governor>
          <dependent id="2">biography</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">portrays</governor>
          <dependent id="3">that</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="2">biography</governor>
          <dependent id="4">portrays</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="6">Lennon</governor>
          <dependent id="5">John</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">portrays</governor>
          <dependent id="6">Lennon</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">bisexual</governor>
          <dependent id="7">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="12">bisexual</governor>
          <dependent id="8">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">bisexual</governor>
          <dependent id="9">drug-addled</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="12">bisexual</governor>
          <dependent id="11">anorexic</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">Lennon</governor>
          <dependent id="12">bisexual</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="14">raged</governor>
          <dependent id="13">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="12">bisexual</governor>
          <dependent id="14">raged</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="16">way</governor>
          <dependent id="15">his</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="14">raged</governor>
          <dependent id="16">way</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">Liverpool</governor>
          <dependent id="17">from</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">raged</governor>
          <dependent id="18">Liverpool</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">City</governor>
          <dependent id="19">to</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="22">City</governor>
          <dependent id="20">New</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="22">City</governor>
          <dependent id="21">York</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">raged</governor>
          <dependent id="22">City</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="26">fiction</governor>
          <dependent id="23">is</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="26">fiction</governor>
          <dependent id="25">totally</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="31">said</governor>
          <dependent id="26">fiction</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="30">Ono</governor>
          <dependent id="29">Yoko</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="31">said</governor>
          <dependent id="30">Ono</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="31">said</dependent>
        </dependency>
        <dependency type="case">
          <governor id="36">broadcast</governor>
          <dependent id="32">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="36">broadcast</governor>
          <dependent id="33">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="36">broadcast</governor>
          <dependent id="34">national</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="36">broadcast</governor>
          <dependent id="35">radio</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="31">said</governor>
          <dependent id="36">broadcast</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="John" />
            <token id="6" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="Liverpool" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="18" string="Liverpool" />
          </tokens>
        </entity>
        <entity id="3" string="New York City" type="LOCATION" score="0.0">
          <tokens>
            <token id="20" string="New" />
            <token id="21" string="York" />
            <token id="22" string="City" />
          </tokens>
        </entity>
        <entity id="4" string="Yoko Ono" type="PERSON" score="0.0">
          <tokens>
            <token id="29" string="Yoko" />
            <token id="30" string="Ono" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="2" has_coreference="true">
      <content>``It&amp;apost;s amazing that somebody took such a poetic license.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="amazing" lemma="amazing" stem="amaz" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="somebody" lemma="somebody" stem="somebodi" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="took" lemma="take" stem="took" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="such" lemma="such" stem="such" pos="PDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="poetic" lemma="poetic" stem="poetic" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="license" lemma="license" stem="licens" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (PRP It)) (VP (VBZ 's) (ADJP (JJ amazing)) (SBAR (IN that) (S (NP (NN somebody)) (VP (VBD took) (NP (PDT such) (DT a) (JJ poetic) (NN license)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="amazing" type="ADJP">
          <tokens>
            <token id="4" string="amazing" />
          </tokens>
        </chunking>
        <chunking id="2" string="'s amazing that somebody took such a poetic license" type="VP">
          <tokens>
            <token id="3" string="'s" />
            <token id="4" string="amazing" />
            <token id="5" string="that" />
            <token id="6" string="somebody" />
            <token id="7" string="took" />
            <token id="8" string="such" />
            <token id="9" string="a" />
            <token id="10" string="poetic" />
            <token id="11" string="license" />
          </tokens>
        </chunking>
        <chunking id="3" string="It" type="NP">
          <tokens>
            <token id="2" string="It" />
          </tokens>
        </chunking>
        <chunking id="4" string="somebody" type="NP">
          <tokens>
            <token id="6" string="somebody" />
          </tokens>
        </chunking>
        <chunking id="5" string="that somebody took such a poetic license" type="SBAR">
          <tokens>
            <token id="5" string="that" />
            <token id="6" string="somebody" />
            <token id="7" string="took" />
            <token id="8" string="such" />
            <token id="9" string="a" />
            <token id="10" string="poetic" />
            <token id="11" string="license" />
          </tokens>
        </chunking>
        <chunking id="6" string="took such a poetic license" type="VP">
          <tokens>
            <token id="7" string="took" />
            <token id="8" string="such" />
            <token id="9" string="a" />
            <token id="10" string="poetic" />
            <token id="11" string="license" />
          </tokens>
        </chunking>
        <chunking id="7" string="such a poetic license" type="NP">
          <tokens>
            <token id="8" string="such" />
            <token id="9" string="a" />
            <token id="10" string="poetic" />
            <token id="11" string="license" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">amazing</governor>
          <dependent id="2">It</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="4">amazing</governor>
          <dependent id="3">'s</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">amazing</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="7">took</governor>
          <dependent id="5">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="7">took</governor>
          <dependent id="6">somebody</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="4">amazing</governor>
          <dependent id="7">took</dependent>
        </dependency>
        <dependency type="det:predet">
          <governor id="11">license</governor>
          <dependent id="8">such</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">license</governor>
          <dependent id="9">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">license</governor>
          <dependent id="10">poetic</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="7">took</governor>
          <dependent id="11">license</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="3" has_coreference="true">
      <content>I don&amp;apost;t see John in there ... It&amp;apost;s a very dramatically described something, but it&amp;apost;s not about John,&amp;apost;&amp;apost; Ono said Wednesday night in her first broadcast comments on the book by Albert Goldman.</content>
      <tokens>
        <token id="1" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="do" lemma="do" stem="do" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="n't" lemma="not" stem="n't" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="see" lemma="see" stem="see" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="6" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="there" lemma="there" stem="there" pos="EX" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="..." lemma="..." stem="..." pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="12" string="very" lemma="very" stem="veri" pos="RB" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="dramatically" lemma="dramatically" stem="dramat" pos="RB" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="14" string="described" lemma="describe" stem="describ" pos="VBD" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="15" string="something" lemma="something" stem="someth" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="about" lemma="about" stem="about" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="23" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="Ono" lemma="Ono" stem="ono" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="26" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="Wednesday" lemma="Wednesday" stem="wednesdai" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="28" string="night" lemma="night" stem="night" pos="NN" type="Word" isStopWord="false" ner="TIME" is_referenced="false" is_refers="false" />
        <token id="29" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="her" lemma="she" stem="her" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="31" string="first" lemma="first" stem="first" pos="JJ" type="Word" isStopWord="false" ner="ORDINAL" is_referenced="false" is_refers="false" />
        <token id="32" string="broadcast" lemma="broadcast" stem="broadcast" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="comments" lemma="comment" stem="comment" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="36" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="37" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="38" string="Albert" lemma="Albert" stem="albert" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="39" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="40" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (S (S (NP (PRP I)) (VP (VBP do) (RB n't) (VP (VB see) (NP (NP (NNP John)) (PP (IN in) (NP (EX there))))))) (: ...) (S (NP (PRP It)) (VP (VBZ 's) (NP (NP (DT a) (ADJP (RB very) (RB dramatically))) (SBAR (S (VP (VBD described) (NP (NN something))))))))) (, ,) (CC but) (S (NP (PRP it)) (VP (VBZ 's) (RB not) (PP (IN about) (NP (NNP John)))))) (, ,) ('' '') (NP (NNP Ono)) (VP (VBD said) (NP-TMP (NNP Wednesday) (NN night)) (PP (IN in) (NP (PRP$ her) (JJ first) (NN broadcast) (NNS comments))) (PP (IN on) (NP (DT the) (NN book))) (PP (IN by) (NP (NNP Albert) (NNP Goldman)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="very dramatically" type="ADJP">
          <tokens>
            <token id="12" string="very" />
            <token id="13" string="dramatically" />
          </tokens>
        </chunking>
        <chunking id="2" string="a very dramatically described something" type="NP">
          <tokens>
            <token id="11" string="a" />
            <token id="12" string="very" />
            <token id="13" string="dramatically" />
            <token id="14" string="described" />
            <token id="15" string="something" />
          </tokens>
        </chunking>
        <chunking id="3" string="I" type="NP">
          <tokens>
            <token id="1" string="I" />
          </tokens>
        </chunking>
        <chunking id="4" string="John" type="NP">
          <tokens>
            <token id="5" string="John" />
          </tokens>
        </chunking>
        <chunking id="5" string="It" type="NP">
          <tokens>
            <token id="9" string="It" />
          </tokens>
        </chunking>
        <chunking id="6" string="described something" type="SBAR">
          <tokens>
            <token id="14" string="described" />
            <token id="15" string="something" />
          </tokens>
        </chunking>
        <chunking id="7" string="it" type="NP">
          <tokens>
            <token id="18" string="it" />
          </tokens>
        </chunking>
        <chunking id="8" string="the book" type="NP">
          <tokens>
            <token id="35" string="the" />
            <token id="36" string="book" />
          </tokens>
        </chunking>
        <chunking id="9" string="something" type="NP">
          <tokens>
            <token id="15" string="something" />
          </tokens>
        </chunking>
        <chunking id="10" string="Ono" type="NP">
          <tokens>
            <token id="25" string="Ono" />
          </tokens>
        </chunking>
        <chunking id="11" string="there" type="NP">
          <tokens>
            <token id="7" string="there" />
          </tokens>
        </chunking>
        <chunking id="12" string="'s a very dramatically described something" type="VP">
          <tokens>
            <token id="10" string="'s" />
            <token id="11" string="a" />
            <token id="12" string="very" />
            <token id="13" string="dramatically" />
            <token id="14" string="described" />
            <token id="15" string="something" />
          </tokens>
        </chunking>
        <chunking id="13" string="her first broadcast comments" type="NP">
          <tokens>
            <token id="30" string="her" />
            <token id="31" string="first" />
            <token id="32" string="broadcast" />
            <token id="33" string="comments" />
          </tokens>
        </chunking>
        <chunking id="14" string="do n't see John in there" type="VP">
          <tokens>
            <token id="2" string="do" />
            <token id="3" string="n't" />
            <token id="4" string="see" />
            <token id="5" string="John" />
            <token id="6" string="in" />
            <token id="7" string="there" />
          </tokens>
        </chunking>
        <chunking id="15" string="see John in there" type="VP">
          <tokens>
            <token id="4" string="see" />
            <token id="5" string="John" />
            <token id="6" string="in" />
            <token id="7" string="there" />
          </tokens>
        </chunking>
        <chunking id="16" string="said Wednesday night in her first broadcast comments on the book by Albert Goldman" type="VP">
          <tokens>
            <token id="26" string="said" />
            <token id="27" string="Wednesday" />
            <token id="28" string="night" />
            <token id="29" string="in" />
            <token id="30" string="her" />
            <token id="31" string="first" />
            <token id="32" string="broadcast" />
            <token id="33" string="comments" />
            <token id="34" string="on" />
            <token id="35" string="the" />
            <token id="36" string="book" />
            <token id="37" string="by" />
            <token id="38" string="Albert" />
            <token id="39" string="Goldman" />
          </tokens>
        </chunking>
        <chunking id="17" string="a very dramatically" type="NP">
          <tokens>
            <token id="11" string="a" />
            <token id="12" string="very" />
            <token id="13" string="dramatically" />
          </tokens>
        </chunking>
        <chunking id="18" string="Albert Goldman" type="NP">
          <tokens>
            <token id="38" string="Albert" />
            <token id="39" string="Goldman" />
          </tokens>
        </chunking>
        <chunking id="19" string="John in there" type="NP">
          <tokens>
            <token id="5" string="John" />
            <token id="6" string="in" />
            <token id="7" string="there" />
          </tokens>
        </chunking>
        <chunking id="20" string="'s not about John" type="VP">
          <tokens>
            <token id="19" string="'s" />
            <token id="20" string="not" />
            <token id="21" string="about" />
            <token id="22" string="John" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">see</governor>
          <dependent id="1">I</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="4">see</governor>
          <dependent id="2">do</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="4">see</governor>
          <dependent id="3">n't</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="26">said</governor>
          <dependent id="4">see</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">see</governor>
          <dependent id="5">John</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">there</governor>
          <dependent id="6">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">John</governor>
          <dependent id="7">there</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="13">dramatically</governor>
          <dependent id="9">It</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="13">dramatically</governor>
          <dependent id="10">'s</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">dramatically</governor>
          <dependent id="11">a</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="13">dramatically</governor>
          <dependent id="12">very</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="4">see</governor>
          <dependent id="13">dramatically</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="13">dramatically</governor>
          <dependent id="14">described</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="14">described</governor>
          <dependent id="15">something</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">see</governor>
          <dependent id="17">but</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">John</governor>
          <dependent id="18">it</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="22">John</governor>
          <dependent id="19">'s</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="22">John</governor>
          <dependent id="20">not</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">John</governor>
          <dependent id="21">about</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">see</governor>
          <dependent id="22">John</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="26">said</governor>
          <dependent id="25">Ono</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="26">said</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="28">night</governor>
          <dependent id="27">Wednesday</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="26">said</governor>
          <dependent id="28">night</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">comments</governor>
          <dependent id="29">in</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="33">comments</governor>
          <dependent id="30">her</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="33">comments</governor>
          <dependent id="31">first</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="33">comments</governor>
          <dependent id="32">broadcast</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">said</governor>
          <dependent id="33">comments</dependent>
        </dependency>
        <dependency type="case">
          <governor id="36">book</governor>
          <dependent id="34">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="36">book</governor>
          <dependent id="35">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">said</governor>
          <dependent id="36">book</dependent>
        </dependency>
        <dependency type="case">
          <governor id="39">Goldman</governor>
          <dependent id="37">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="39">Goldman</governor>
          <dependent id="38">Albert</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">said</governor>
          <dependent id="39">Goldman</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="first" type="ORDINAL" score="0.0">
          <tokens>
            <token id="31" string="first" />
          </tokens>
        </entity>
        <entity id="2" string="Wednesday" type="DATE" score="0.0">
          <tokens>
            <token id="27" string="Wednesday" />
          </tokens>
        </entity>
        <entity id="3" string="night" type="TIME" score="0.0">
          <tokens>
            <token id="28" string="night" />
          </tokens>
        </entity>
        <entity id="4" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="John" />
          </tokens>
        </entity>
        <entity id="5" string="Albert Goldman" type="PERSON" score="0.0">
          <tokens>
            <token id="38" string="Albert" />
            <token id="39" string="Goldman" />
          </tokens>
        </entity>
        <entity id="6" string="Ono" type="PERSON" score="0.0">
          <tokens>
            <token id="25" string="Ono" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="4" has_coreference="true">
      <content>The usually reclusive Ono agreed to the interview to counter Goldman&amp;apost;s ``The Lives of John Lennon,&amp;apost;&amp;apost; which presents the ex-Beatle as a violent, drug-addicted drunk who wanted to dump his wife.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="usually" lemma="usually" stem="usual" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="reclusive" lemma="reclusive" stem="reclus" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="Ono" lemma="Ono" stem="ono" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="5" string="agreed" lemma="agree" stem="agre" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="interview" lemma="interview" stem="interview" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="9" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="counter" lemma="counter" stem="counter" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="12" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="13" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="Lives" lemma="life" stem="live" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="18" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="19" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="which" lemma="which" stem="which" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="presents" lemma="present" stem="present" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="ex-Beatle" lemma="ex-beatle" stem="ex-beatl" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="27" string="violent" lemma="violent" stem="violent" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="28" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="29" string="drug-addicted" lemma="drug-addicted" stem="drug-addict" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="30" string="drunk" lemma="drunk" stem="drunk" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="31" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="wanted" lemma="want" stem="want" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="34" string="dump" lemma="dump" stem="dump" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="35" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="36" string="wife" lemma="wife" stem="wife" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="37" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (ADJP (RB usually) (JJ reclusive)) (NNP Ono)) (VP (VBD agreed) (PP (TO to) (NP (DT the) (NN interview))) (S (VP (TO to) (VP (VB counter) (NP (NP (NP (NNP Goldman) (POS 's)) (NX (`` ``) (NP (DT The) (NNS Lives)) (PP (IN of) (NP (NNP John) (NNP Lennon))) (, ,) ('' ''))) (SBAR (WHNP (WDT which)) (S (VP (VBZ presents) (NP (NP (DT the) (NN ex-Beatle)) (PP (IN as) (NP (DT a) (ADJP (JJ violent) (, ,) (JJ drug-addicted)) (JJ drunk))) (SBAR (WHNP (WP who)) (S (VP (VBD wanted) (S (VP (TO to) (VP (VB dump) (NP (PRP$ his) (NN wife))))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a violent , drug-addicted drunk" type="NP">
          <tokens>
            <token id="26" string="a" />
            <token id="27" string="violent" />
            <token id="28" string="," />
            <token id="29" string="drug-addicted" />
            <token id="30" string="drunk" />
          </tokens>
        </chunking>
        <chunking id="2" string="dump his wife" type="VP">
          <tokens>
            <token id="34" string="dump" />
            <token id="35" string="his" />
            <token id="36" string="wife" />
          </tokens>
        </chunking>
        <chunking id="3" string="to dump his wife" type="VP">
          <tokens>
            <token id="33" string="to" />
            <token id="34" string="dump" />
            <token id="35" string="his" />
            <token id="36" string="wife" />
          </tokens>
        </chunking>
        <chunking id="4" string="usually reclusive" type="ADJP">
          <tokens>
            <token id="2" string="usually" />
            <token id="3" string="reclusive" />
          </tokens>
        </chunking>
        <chunking id="5" string="The Lives" type="NP">
          <tokens>
            <token id="14" string="The" />
            <token id="15" string="Lives" />
          </tokens>
        </chunking>
        <chunking id="6" string="wanted to dump his wife" type="VP">
          <tokens>
            <token id="32" string="wanted" />
            <token id="33" string="to" />
            <token id="34" string="dump" />
            <token id="35" string="his" />
            <token id="36" string="wife" />
          </tokens>
        </chunking>
        <chunking id="7" string="to counter Goldman 's `` The Lives of John Lennon , '' which presents the ex-Beatle as a violent , drug-addicted drunk who wanted to dump his wife" type="VP">
          <tokens>
            <token id="9" string="to" />
            <token id="10" string="counter" />
            <token id="11" string="Goldman" />
            <token id="12" string="'s" />
            <token id="13" string="``" />
            <token id="14" string="The" />
            <token id="15" string="Lives" />
            <token id="16" string="of" />
            <token id="17" string="John" />
            <token id="18" string="Lennon" />
            <token id="19" string="," />
            <token id="20" string="''" />
            <token id="21" string="which" />
            <token id="22" string="presents" />
            <token id="23" string="the" />
            <token id="24" string="ex-Beatle" />
            <token id="25" string="as" />
            <token id="26" string="a" />
            <token id="27" string="violent" />
            <token id="28" string="," />
            <token id="29" string="drug-addicted" />
            <token id="30" string="drunk" />
            <token id="31" string="who" />
            <token id="32" string="wanted" />
            <token id="33" string="to" />
            <token id="34" string="dump" />
            <token id="35" string="his" />
            <token id="36" string="wife" />
          </tokens>
        </chunking>
        <chunking id="8" string="the ex-Beatle" type="NP">
          <tokens>
            <token id="23" string="the" />
            <token id="24" string="ex-Beatle" />
          </tokens>
        </chunking>
        <chunking id="9" string="who wanted to dump his wife" type="SBAR">
          <tokens>
            <token id="31" string="who" />
            <token id="32" string="wanted" />
            <token id="33" string="to" />
            <token id="34" string="dump" />
            <token id="35" string="his" />
            <token id="36" string="wife" />
          </tokens>
        </chunking>
        <chunking id="10" string="the ex-Beatle as a violent , drug-addicted drunk who wanted to dump his wife" type="NP">
          <tokens>
            <token id="23" string="the" />
            <token id="24" string="ex-Beatle" />
            <token id="25" string="as" />
            <token id="26" string="a" />
            <token id="27" string="violent" />
            <token id="28" string="," />
            <token id="29" string="drug-addicted" />
            <token id="30" string="drunk" />
            <token id="31" string="who" />
            <token id="32" string="wanted" />
            <token id="33" string="to" />
            <token id="34" string="dump" />
            <token id="35" string="his" />
            <token id="36" string="wife" />
          </tokens>
        </chunking>
        <chunking id="11" string="presents the ex-Beatle as a violent , drug-addicted drunk who wanted to dump his wife" type="VP">
          <tokens>
            <token id="22" string="presents" />
            <token id="23" string="the" />
            <token id="24" string="ex-Beatle" />
            <token id="25" string="as" />
            <token id="26" string="a" />
            <token id="27" string="violent" />
            <token id="28" string="," />
            <token id="29" string="drug-addicted" />
            <token id="30" string="drunk" />
            <token id="31" string="who" />
            <token id="32" string="wanted" />
            <token id="33" string="to" />
            <token id="34" string="dump" />
            <token id="35" string="his" />
            <token id="36" string="wife" />
          </tokens>
        </chunking>
        <chunking id="12" string="John Lennon" type="NP">
          <tokens>
            <token id="17" string="John" />
            <token id="18" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="13" string="violent , drug-addicted" type="ADJP">
          <tokens>
            <token id="27" string="violent" />
            <token id="28" string="," />
            <token id="29" string="drug-addicted" />
          </tokens>
        </chunking>
        <chunking id="14" string="counter Goldman 's `` The Lives of John Lennon , '' which presents the ex-Beatle as a violent , drug-addicted drunk who wanted to dump his wife" type="VP">
          <tokens>
            <token id="10" string="counter" />
            <token id="11" string="Goldman" />
            <token id="12" string="'s" />
            <token id="13" string="``" />
            <token id="14" string="The" />
            <token id="15" string="Lives" />
            <token id="16" string="of" />
            <token id="17" string="John" />
            <token id="18" string="Lennon" />
            <token id="19" string="," />
            <token id="20" string="''" />
            <token id="21" string="which" />
            <token id="22" string="presents" />
            <token id="23" string="the" />
            <token id="24" string="ex-Beatle" />
            <token id="25" string="as" />
            <token id="26" string="a" />
            <token id="27" string="violent" />
            <token id="28" string="," />
            <token id="29" string="drug-addicted" />
            <token id="30" string="drunk" />
            <token id="31" string="who" />
            <token id="32" string="wanted" />
            <token id="33" string="to" />
            <token id="34" string="dump" />
            <token id="35" string="his" />
            <token id="36" string="wife" />
          </tokens>
        </chunking>
        <chunking id="15" string="Goldman 's `` The Lives of John Lennon , '' which presents the ex-Beatle as a violent , drug-addicted drunk who wanted to dump his wife" type="NP">
          <tokens>
            <token id="11" string="Goldman" />
            <token id="12" string="'s" />
            <token id="13" string="``" />
            <token id="14" string="The" />
            <token id="15" string="Lives" />
            <token id="16" string="of" />
            <token id="17" string="John" />
            <token id="18" string="Lennon" />
            <token id="19" string="," />
            <token id="20" string="''" />
            <token id="21" string="which" />
            <token id="22" string="presents" />
            <token id="23" string="the" />
            <token id="24" string="ex-Beatle" />
            <token id="25" string="as" />
            <token id="26" string="a" />
            <token id="27" string="violent" />
            <token id="28" string="," />
            <token id="29" string="drug-addicted" />
            <token id="30" string="drunk" />
            <token id="31" string="who" />
            <token id="32" string="wanted" />
            <token id="33" string="to" />
            <token id="34" string="dump" />
            <token id="35" string="his" />
            <token id="36" string="wife" />
          </tokens>
        </chunking>
        <chunking id="16" string="Goldman 's" type="NP">
          <tokens>
            <token id="11" string="Goldman" />
            <token id="12" string="'s" />
          </tokens>
        </chunking>
        <chunking id="17" string="The usually reclusive Ono" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="usually" />
            <token id="3" string="reclusive" />
            <token id="4" string="Ono" />
          </tokens>
        </chunking>
        <chunking id="18" string="Goldman 's `` The Lives of John Lennon , ''" type="NP">
          <tokens>
            <token id="11" string="Goldman" />
            <token id="12" string="'s" />
            <token id="13" string="``" />
            <token id="14" string="The" />
            <token id="15" string="Lives" />
            <token id="16" string="of" />
            <token id="17" string="John" />
            <token id="18" string="Lennon" />
            <token id="19" string="," />
            <token id="20" string="''" />
          </tokens>
        </chunking>
        <chunking id="19" string="agreed to the interview to counter Goldman 's `` The Lives of John Lennon , '' which presents the ex-Beatle as a violent , drug-addicted drunk who wanted to dump his wife" type="VP">
          <tokens>
            <token id="5" string="agreed" />
            <token id="6" string="to" />
            <token id="7" string="the" />
            <token id="8" string="interview" />
            <token id="9" string="to" />
            <token id="10" string="counter" />
            <token id="11" string="Goldman" />
            <token id="12" string="'s" />
            <token id="13" string="``" />
            <token id="14" string="The" />
            <token id="15" string="Lives" />
            <token id="16" string="of" />
            <token id="17" string="John" />
            <token id="18" string="Lennon" />
            <token id="19" string="," />
            <token id="20" string="''" />
            <token id="21" string="which" />
            <token id="22" string="presents" />
            <token id="23" string="the" />
            <token id="24" string="ex-Beatle" />
            <token id="25" string="as" />
            <token id="26" string="a" />
            <token id="27" string="violent" />
            <token id="28" string="," />
            <token id="29" string="drug-addicted" />
            <token id="30" string="drunk" />
            <token id="31" string="who" />
            <token id="32" string="wanted" />
            <token id="33" string="to" />
            <token id="34" string="dump" />
            <token id="35" string="his" />
            <token id="36" string="wife" />
          </tokens>
        </chunking>
        <chunking id="20" string="which presents the ex-Beatle as a violent , drug-addicted drunk who wanted to dump his wife" type="SBAR">
          <tokens>
            <token id="21" string="which" />
            <token id="22" string="presents" />
            <token id="23" string="the" />
            <token id="24" string="ex-Beatle" />
            <token id="25" string="as" />
            <token id="26" string="a" />
            <token id="27" string="violent" />
            <token id="28" string="," />
            <token id="29" string="drug-addicted" />
            <token id="30" string="drunk" />
            <token id="31" string="who" />
            <token id="32" string="wanted" />
            <token id="33" string="to" />
            <token id="34" string="dump" />
            <token id="35" string="his" />
            <token id="36" string="wife" />
          </tokens>
        </chunking>
        <chunking id="21" string="the interview" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="interview" />
          </tokens>
        </chunking>
        <chunking id="22" string="his wife" type="NP">
          <tokens>
            <token id="35" string="his" />
            <token id="36" string="wife" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="4">Ono</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="3">reclusive</governor>
          <dependent id="2">usually</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="4">Ono</governor>
          <dependent id="3">reclusive</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">agreed</governor>
          <dependent id="4">Ono</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">agreed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">interview</governor>
          <dependent id="6">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">interview</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">agreed</governor>
          <dependent id="8">interview</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="10">counter</governor>
          <dependent id="9">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="5">agreed</governor>
          <dependent id="10">counter</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="15">Lives</governor>
          <dependent id="11">Goldman</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">Goldman</governor>
          <dependent id="12">'s</dependent>
        </dependency>
        <dependency type="det">
          <governor id="15">Lives</governor>
          <dependent id="14">The</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="10">counter</governor>
          <dependent id="15">Lives</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">Lennon</governor>
          <dependent id="16">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="18">Lennon</governor>
          <dependent id="17">John</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="15">Lives</governor>
          <dependent id="18">Lennon</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="22">presents</governor>
          <dependent id="21">which</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="15">Lives</governor>
          <dependent id="22">presents</dependent>
        </dependency>
        <dependency type="det">
          <governor id="24">ex-Beatle</governor>
          <dependent id="23">the</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="22">presents</governor>
          <dependent id="24">ex-Beatle</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">drunk</governor>
          <dependent id="25">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="30">drunk</governor>
          <dependent id="26">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="29">drug-addicted</governor>
          <dependent id="27">violent</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="30">drunk</governor>
          <dependent id="29">drug-addicted</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="24">ex-Beatle</governor>
          <dependent id="30">drunk</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="32">wanted</governor>
          <dependent id="31">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="24">ex-Beatle</governor>
          <dependent id="32">wanted</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="34">dump</governor>
          <dependent id="33">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="32">wanted</governor>
          <dependent id="34">dump</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="36">wife</governor>
          <dependent id="35">his</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="34">dump</governor>
          <dependent id="36">wife</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="17" string="John" />
            <token id="18" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="Goldman" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="11" string="Goldman" />
          </tokens>
        </entity>
        <entity id="3" string="Ono" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="Ono" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="5" has_coreference="true">
      <content>As for Ono, Goldman shows her as a gold-digger who snorted heroin up to the time of Lennon&amp;apost;s death.</content>
      <tokens>
        <token id="1" string="As" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="Ono" lemma="Ono" stem="ono" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="6" string="shows" lemma="show" stem="show" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="her" lemma="she" stem="her" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="8" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="gold-digger" lemma="gold-digger" stem="gold-digg" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="snorted" lemma="snort" stem="snort" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="heroin" lemma="heroin" stem="heroin" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="up" lemma="up" stem="up" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="time" lemma="time" stem="time" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="20" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="21" string="death" lemma="death" stem="death" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (IN As) (PP (IN for) (NP (NNP Ono)))) (, ,) (NP (NNP Goldman)) (VP (VBZ shows) (NP (PRP her)) (PP (IN as) (NP (NP (DT a) (NN gold-digger)) (SBAR (WHNP (WP who)) (S (VP (VBD snorted) (NP (NN heroin)) (ADVP (IN up) (PP (TO to) (NP (NP (DT the) (NN time)) (PP (IN of) (NP (NP (NNP Lennon) (POS 's)) (NN death)))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="shows her as a gold-digger who snorted heroin up to the time of Lennon 's death" type="VP">
          <tokens>
            <token id="6" string="shows" />
            <token id="7" string="her" />
            <token id="8" string="as" />
            <token id="9" string="a" />
            <token id="10" string="gold-digger" />
            <token id="11" string="who" />
            <token id="12" string="snorted" />
            <token id="13" string="heroin" />
            <token id="14" string="up" />
            <token id="15" string="to" />
            <token id="16" string="the" />
            <token id="17" string="time" />
            <token id="18" string="of" />
            <token id="19" string="Lennon" />
            <token id="20" string="'s" />
            <token id="21" string="death" />
          </tokens>
        </chunking>
        <chunking id="2" string="a gold-digger" type="NP">
          <tokens>
            <token id="9" string="a" />
            <token id="10" string="gold-digger" />
          </tokens>
        </chunking>
        <chunking id="3" string="heroin" type="NP">
          <tokens>
            <token id="13" string="heroin" />
          </tokens>
        </chunking>
        <chunking id="4" string="a gold-digger who snorted heroin up to the time of Lennon 's death" type="NP">
          <tokens>
            <token id="9" string="a" />
            <token id="10" string="gold-digger" />
            <token id="11" string="who" />
            <token id="12" string="snorted" />
            <token id="13" string="heroin" />
            <token id="14" string="up" />
            <token id="15" string="to" />
            <token id="16" string="the" />
            <token id="17" string="time" />
            <token id="18" string="of" />
            <token id="19" string="Lennon" />
            <token id="20" string="'s" />
            <token id="21" string="death" />
          </tokens>
        </chunking>
        <chunking id="5" string="Lennon 's" type="NP">
          <tokens>
            <token id="19" string="Lennon" />
            <token id="20" string="'s" />
          </tokens>
        </chunking>
        <chunking id="6" string="Ono" type="NP">
          <tokens>
            <token id="3" string="Ono" />
          </tokens>
        </chunking>
        <chunking id="7" string="snorted heroin up to the time of Lennon 's death" type="VP">
          <tokens>
            <token id="12" string="snorted" />
            <token id="13" string="heroin" />
            <token id="14" string="up" />
            <token id="15" string="to" />
            <token id="16" string="the" />
            <token id="17" string="time" />
            <token id="18" string="of" />
            <token id="19" string="Lennon" />
            <token id="20" string="'s" />
            <token id="21" string="death" />
          </tokens>
        </chunking>
        <chunking id="8" string="her" type="NP">
          <tokens>
            <token id="7" string="her" />
          </tokens>
        </chunking>
        <chunking id="9" string="Lennon 's death" type="NP">
          <tokens>
            <token id="19" string="Lennon" />
            <token id="20" string="'s" />
            <token id="21" string="death" />
          </tokens>
        </chunking>
        <chunking id="10" string="the time" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="time" />
          </tokens>
        </chunking>
        <chunking id="11" string="who snorted heroin up to the time of Lennon 's death" type="SBAR">
          <tokens>
            <token id="11" string="who" />
            <token id="12" string="snorted" />
            <token id="13" string="heroin" />
            <token id="14" string="up" />
            <token id="15" string="to" />
            <token id="16" string="the" />
            <token id="17" string="time" />
            <token id="18" string="of" />
            <token id="19" string="Lennon" />
            <token id="20" string="'s" />
            <token id="21" string="death" />
          </tokens>
        </chunking>
        <chunking id="12" string="Goldman" type="NP">
          <tokens>
            <token id="5" string="Goldman" />
          </tokens>
        </chunking>
        <chunking id="13" string="the time of Lennon 's death" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="time" />
            <token id="18" string="of" />
            <token id="19" string="Lennon" />
            <token id="20" string="'s" />
            <token id="21" string="death" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="case">
          <governor id="3">Ono</governor>
          <dependent id="1">As</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="1">As</governor>
          <dependent id="2">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">shows</governor>
          <dependent id="3">Ono</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">shows</governor>
          <dependent id="5">Goldman</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="6">shows</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="6">shows</governor>
          <dependent id="7">her</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">gold-digger</governor>
          <dependent id="8">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">gold-digger</governor>
          <dependent id="9">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">shows</governor>
          <dependent id="10">gold-digger</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">snorted</governor>
          <dependent id="11">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="10">gold-digger</governor>
          <dependent id="12">snorted</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="12">snorted</governor>
          <dependent id="13">heroin</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="12">snorted</governor>
          <dependent id="14">up</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">time</governor>
          <dependent id="15">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="17">time</governor>
          <dependent id="16">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">up</governor>
          <dependent id="17">time</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">death</governor>
          <dependent id="18">of</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="21">death</governor>
          <dependent id="19">Lennon</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">Lennon</governor>
          <dependent id="20">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">time</governor>
          <dependent id="21">death</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="19" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="Goldman" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="Goldman" />
          </tokens>
        </entity>
        <entity id="3" string="Ono" type="PERSON" score="0.0">
          <tokens>
            <token id="3" string="Ono" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="6" has_coreference="true">
      <content>Goldman also charges she was responsible for Paul McCartney&amp;apost;s 1980 marijuana bust in Japan, had affairs while they were married and was just as intent on a divorce as Lennon.</content>
      <tokens>
        <token id="1" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="charges" lemma="charge" stem="charg" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="5" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="responsible" lemma="responsible" stem="respons" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Paul" lemma="Paul" stem="paul" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="9" string="McCartney" lemma="McCartney" stem="mccartnei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="10" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="1980" lemma="1980" stem="1980" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="12" string="marijuana" lemma="marijuana" stem="marijuana" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="bust" lemma="bust" stem="bust" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="Japan" lemma="Japan" stem="japan" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="16" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="affairs" lemma="affair" stem="affair" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="while" lemma="while" stem="while" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="they" lemma="they" stem="thei" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="married" lemma="marry" stem="marri" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="just" lemma="just" stem="just" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="intent" lemma="intent" stem="intent" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="divorce" lemma="divorce" stem="divorc" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="33" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Goldman)) (ADVP (RB also)) (VP (VBZ charges) (SBAR (S (NP (PRP she)) (VP (VP (VBD was) (ADJP (JJ responsible) (PP (IN for) (NP (NP (NNP Paul) (NNP McCartney) (POS 's)) (CD 1980) (NN marijuana) (NN bust)))) (PP (IN in) (NP (NNP Japan)))) (, ,) (VP (VBD had) (NP (NP (NNS affairs)) (SBAR (IN while) (S (NP (PRP they)) (VP (VBD were) (VP (VBN married))))))) (CC and) (VP (VBD was) (ADVP (RB just)) (PP (IN as) (NP (NP (NN intent)) (PP (IN on) (NP (NP (DT a) (NN divorce)) (PP (IN as) (NP (NNP Lennon)))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="was responsible for Paul McCartney 's 1980 marijuana bust in Japan , had affairs while they were married and was just as intent on a divorce as Lennon" type="VP">
          <tokens>
            <token id="5" string="was" />
            <token id="6" string="responsible" />
            <token id="7" string="for" />
            <token id="8" string="Paul" />
            <token id="9" string="McCartney" />
            <token id="10" string="'s" />
            <token id="11" string="1980" />
            <token id="12" string="marijuana" />
            <token id="13" string="bust" />
            <token id="14" string="in" />
            <token id="15" string="Japan" />
            <token id="16" string="," />
            <token id="17" string="had" />
            <token id="18" string="affairs" />
            <token id="19" string="while" />
            <token id="20" string="they" />
            <token id="21" string="were" />
            <token id="22" string="married" />
            <token id="23" string="and" />
            <token id="24" string="was" />
            <token id="25" string="just" />
            <token id="26" string="as" />
            <token id="27" string="intent" />
            <token id="28" string="on" />
            <token id="29" string="a" />
            <token id="30" string="divorce" />
            <token id="31" string="as" />
            <token id="32" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="2" string="was just as intent on a divorce as Lennon" type="VP">
          <tokens>
            <token id="24" string="was" />
            <token id="25" string="just" />
            <token id="26" string="as" />
            <token id="27" string="intent" />
            <token id="28" string="on" />
            <token id="29" string="a" />
            <token id="30" string="divorce" />
            <token id="31" string="as" />
            <token id="32" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="3" string="Lennon" type="NP">
          <tokens>
            <token id="32" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="4" string="Japan" type="NP">
          <tokens>
            <token id="15" string="Japan" />
          </tokens>
        </chunking>
        <chunking id="5" string="were married" type="VP">
          <tokens>
            <token id="21" string="were" />
            <token id="22" string="married" />
          </tokens>
        </chunking>
        <chunking id="6" string="affairs while they were married" type="NP">
          <tokens>
            <token id="18" string="affairs" />
            <token id="19" string="while" />
            <token id="20" string="they" />
            <token id="21" string="were" />
            <token id="22" string="married" />
          </tokens>
        </chunking>
        <chunking id="7" string="she was responsible for Paul McCartney 's 1980 marijuana bust in Japan , had affairs while they were married and was just as intent on a divorce as Lennon" type="SBAR">
          <tokens>
            <token id="4" string="she" />
            <token id="5" string="was" />
            <token id="6" string="responsible" />
            <token id="7" string="for" />
            <token id="8" string="Paul" />
            <token id="9" string="McCartney" />
            <token id="10" string="'s" />
            <token id="11" string="1980" />
            <token id="12" string="marijuana" />
            <token id="13" string="bust" />
            <token id="14" string="in" />
            <token id="15" string="Japan" />
            <token id="16" string="," />
            <token id="17" string="had" />
            <token id="18" string="affairs" />
            <token id="19" string="while" />
            <token id="20" string="they" />
            <token id="21" string="were" />
            <token id="22" string="married" />
            <token id="23" string="and" />
            <token id="24" string="was" />
            <token id="25" string="just" />
            <token id="26" string="as" />
            <token id="27" string="intent" />
            <token id="28" string="on" />
            <token id="29" string="a" />
            <token id="30" string="divorce" />
            <token id="31" string="as" />
            <token id="32" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="8" string="intent" type="NP">
          <tokens>
            <token id="27" string="intent" />
          </tokens>
        </chunking>
        <chunking id="9" string="intent on a divorce as Lennon" type="NP">
          <tokens>
            <token id="27" string="intent" />
            <token id="28" string="on" />
            <token id="29" string="a" />
            <token id="30" string="divorce" />
            <token id="31" string="as" />
            <token id="32" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="10" string="she" type="NP">
          <tokens>
            <token id="4" string="she" />
          </tokens>
        </chunking>
        <chunking id="11" string="while they were married" type="SBAR">
          <tokens>
            <token id="19" string="while" />
            <token id="20" string="they" />
            <token id="21" string="were" />
            <token id="22" string="married" />
          </tokens>
        </chunking>
        <chunking id="12" string="they" type="NP">
          <tokens>
            <token id="20" string="they" />
          </tokens>
        </chunking>
        <chunking id="13" string="responsible for Paul McCartney 's 1980 marijuana bust" type="ADJP">
          <tokens>
            <token id="6" string="responsible" />
            <token id="7" string="for" />
            <token id="8" string="Paul" />
            <token id="9" string="McCartney" />
            <token id="10" string="'s" />
            <token id="11" string="1980" />
            <token id="12" string="marijuana" />
            <token id="13" string="bust" />
          </tokens>
        </chunking>
        <chunking id="14" string="Paul McCartney 's 1980 marijuana bust" type="NP">
          <tokens>
            <token id="8" string="Paul" />
            <token id="9" string="McCartney" />
            <token id="10" string="'s" />
            <token id="11" string="1980" />
            <token id="12" string="marijuana" />
            <token id="13" string="bust" />
          </tokens>
        </chunking>
        <chunking id="15" string="a divorce" type="NP">
          <tokens>
            <token id="29" string="a" />
            <token id="30" string="divorce" />
          </tokens>
        </chunking>
        <chunking id="16" string="charges she was responsible for Paul McCartney 's 1980 marijuana bust in Japan , had affairs while they were married and was just as intent on a divorce as Lennon" type="VP">
          <tokens>
            <token id="3" string="charges" />
            <token id="4" string="she" />
            <token id="5" string="was" />
            <token id="6" string="responsible" />
            <token id="7" string="for" />
            <token id="8" string="Paul" />
            <token id="9" string="McCartney" />
            <token id="10" string="'s" />
            <token id="11" string="1980" />
            <token id="12" string="marijuana" />
            <token id="13" string="bust" />
            <token id="14" string="in" />
            <token id="15" string="Japan" />
            <token id="16" string="," />
            <token id="17" string="had" />
            <token id="18" string="affairs" />
            <token id="19" string="while" />
            <token id="20" string="they" />
            <token id="21" string="were" />
            <token id="22" string="married" />
            <token id="23" string="and" />
            <token id="24" string="was" />
            <token id="25" string="just" />
            <token id="26" string="as" />
            <token id="27" string="intent" />
            <token id="28" string="on" />
            <token id="29" string="a" />
            <token id="30" string="divorce" />
            <token id="31" string="as" />
            <token id="32" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="17" string="had affairs while they were married" type="VP">
          <tokens>
            <token id="17" string="had" />
            <token id="18" string="affairs" />
            <token id="19" string="while" />
            <token id="20" string="they" />
            <token id="21" string="were" />
            <token id="22" string="married" />
          </tokens>
        </chunking>
        <chunking id="18" string="a divorce as Lennon" type="NP">
          <tokens>
            <token id="29" string="a" />
            <token id="30" string="divorce" />
            <token id="31" string="as" />
            <token id="32" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="19" string="Paul McCartney 's" type="NP">
          <tokens>
            <token id="8" string="Paul" />
            <token id="9" string="McCartney" />
            <token id="10" string="'s" />
          </tokens>
        </chunking>
        <chunking id="20" string="was responsible for Paul McCartney 's 1980 marijuana bust in Japan" type="VP">
          <tokens>
            <token id="5" string="was" />
            <token id="6" string="responsible" />
            <token id="7" string="for" />
            <token id="8" string="Paul" />
            <token id="9" string="McCartney" />
            <token id="10" string="'s" />
            <token id="11" string="1980" />
            <token id="12" string="marijuana" />
            <token id="13" string="bust" />
            <token id="14" string="in" />
            <token id="15" string="Japan" />
          </tokens>
        </chunking>
        <chunking id="21" string="married" type="VP">
          <tokens>
            <token id="22" string="married" />
          </tokens>
        </chunking>
        <chunking id="22" string="Goldman" type="NP">
          <tokens>
            <token id="1" string="Goldman" />
          </tokens>
        </chunking>
        <chunking id="23" string="affairs" type="NP">
          <tokens>
            <token id="18" string="affairs" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">charges</governor>
          <dependent id="1">Goldman</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="3">charges</governor>
          <dependent id="2">also</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">charges</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">responsible</governor>
          <dependent id="4">she</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="6">responsible</governor>
          <dependent id="5">was</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">charges</governor>
          <dependent id="6">responsible</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">bust</governor>
          <dependent id="7">for</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="9">McCartney</governor>
          <dependent id="8">Paul</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="13">bust</governor>
          <dependent id="9">McCartney</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">McCartney</governor>
          <dependent id="10">'s</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="13">bust</governor>
          <dependent id="11">1980</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="13">bust</governor>
          <dependent id="12">marijuana</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">responsible</governor>
          <dependent id="13">bust</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">Japan</governor>
          <dependent id="14">in</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="6">responsible</governor>
          <dependent id="15">Japan</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="6">responsible</governor>
          <dependent id="17">had</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="17">had</governor>
          <dependent id="18">affairs</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="22">married</governor>
          <dependent id="19">while</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="22">married</governor>
          <dependent id="20">they</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="22">married</governor>
          <dependent id="21">were</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="18">affairs</governor>
          <dependent id="22">married</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="6">responsible</governor>
          <dependent id="23">and</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="27">intent</governor>
          <dependent id="24">was</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="27">intent</governor>
          <dependent id="25">just</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">intent</governor>
          <dependent id="26">as</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="6">responsible</governor>
          <dependent id="27">intent</dependent>
        </dependency>
        <dependency type="case">
          <governor id="30">divorce</governor>
          <dependent id="28">on</dependent>
        </dependency>
        <dependency type="det">
          <governor id="30">divorce</governor>
          <dependent id="29">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="27">intent</governor>
          <dependent id="30">divorce</dependent>
        </dependency>
        <dependency type="case">
          <governor id="32">Lennon</governor>
          <dependent id="31">as</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="30">divorce</governor>
          <dependent id="32">Lennon</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="32" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="Japan" type="LOCATION" score="0.0">
          <tokens>
            <token id="15" string="Japan" />
          </tokens>
        </entity>
        <entity id="3" string="1980" type="DATE" score="0.0">
          <tokens>
            <token id="11" string="1980" />
          </tokens>
        </entity>
        <entity id="4" string="Goldman" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Goldman" />
          </tokens>
        </entity>
        <entity id="5" string="Paul McCartney" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Paul" />
            <token id="9" string="McCartney" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="7" has_coreference="true">
      <content>Ono refuted virtually all of these charges, and countered through taped interviews with Lennon friends and employees that Goldman&amp;apost;s book was based on unreliable sources and misquotes.</content>
      <tokens>
        <token id="1" string="Ono" lemma="Ono" stem="ono" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="refuted" lemma="refute" stem="refut" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="virtually" lemma="virtually" stem="virtual" pos="RB" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="6" string="these" lemma="these" stem="these" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="7" string="charges" lemma="charge" stem="charg" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="8" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="countered" lemma="counter" stem="counter" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="through" lemma="through" stem="through" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="taped" lemma="tape" stem="tape" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="interviews" lemma="interview" stem="interview" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="16" string="friends" lemma="friend" stem="friend" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="17" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="employees" lemma="employee" stem="employe" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="19" string="that" lemma="that" stem="that" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="true" />
        <token id="21" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="22" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="23" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="based" lemma="base" stem="base" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="unreliable" lemma="unreliable" stem="unreli" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="27" string="sources" lemma="source" stem="sourc" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="28" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="29" string="misquotes" lemma="misquote" stem="misquot" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="30" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Ono)) (VP (VP (VBD refuted) (NP (NP (RB virtually) (DT all)) (PP (IN of) (NP (DT these) (NNS charges))))) (, ,) (CC and) (VP (VBD countered) (PP (IN through) (NP (NP (VBN taped) (NNS interviews)) (PP (IN with) (NP (NNP Lennon) (NNS friends) (CC and) (NNS employees))) (SBAR (WHNP (WDT that)) (S (NP (NP (NNP Goldman) (POS 's)) (NN book)) (VP (VBD was) (VP (VBN based) (PP (IN on) (NP (JJ unreliable) (NNS sources) (CC and) (NNS misquotes))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="taped interviews with Lennon friends and employees that Goldman 's book was based on unreliable sources and misquotes" type="NP">
          <tokens>
            <token id="12" string="taped" />
            <token id="13" string="interviews" />
            <token id="14" string="with" />
            <token id="15" string="Lennon" />
            <token id="16" string="friends" />
            <token id="17" string="and" />
            <token id="18" string="employees" />
            <token id="19" string="that" />
            <token id="20" string="Goldman" />
            <token id="21" string="'s" />
            <token id="22" string="book" />
            <token id="23" string="was" />
            <token id="24" string="based" />
            <token id="25" string="on" />
            <token id="26" string="unreliable" />
            <token id="27" string="sources" />
            <token id="28" string="and" />
            <token id="29" string="misquotes" />
          </tokens>
        </chunking>
        <chunking id="2" string="based on unreliable sources and misquotes" type="VP">
          <tokens>
            <token id="24" string="based" />
            <token id="25" string="on" />
            <token id="26" string="unreliable" />
            <token id="27" string="sources" />
            <token id="28" string="and" />
            <token id="29" string="misquotes" />
          </tokens>
        </chunking>
        <chunking id="3" string="taped interviews" type="NP">
          <tokens>
            <token id="12" string="taped" />
            <token id="13" string="interviews" />
          </tokens>
        </chunking>
        <chunking id="4" string="Goldman 's book" type="NP">
          <tokens>
            <token id="20" string="Goldman" />
            <token id="21" string="'s" />
            <token id="22" string="book" />
          </tokens>
        </chunking>
        <chunking id="5" string="virtually all" type="NP">
          <tokens>
            <token id="3" string="virtually" />
            <token id="4" string="all" />
          </tokens>
        </chunking>
        <chunking id="6" string="Lennon friends and employees" type="NP">
          <tokens>
            <token id="15" string="Lennon" />
            <token id="16" string="friends" />
            <token id="17" string="and" />
            <token id="18" string="employees" />
          </tokens>
        </chunking>
        <chunking id="7" string="unreliable sources and misquotes" type="NP">
          <tokens>
            <token id="26" string="unreliable" />
            <token id="27" string="sources" />
            <token id="28" string="and" />
            <token id="29" string="misquotes" />
          </tokens>
        </chunking>
        <chunking id="8" string="Ono" type="NP">
          <tokens>
            <token id="1" string="Ono" />
          </tokens>
        </chunking>
        <chunking id="9" string="refuted virtually all of these charges" type="VP">
          <tokens>
            <token id="2" string="refuted" />
            <token id="3" string="virtually" />
            <token id="4" string="all" />
            <token id="5" string="of" />
            <token id="6" string="these" />
            <token id="7" string="charges" />
          </tokens>
        </chunking>
        <chunking id="10" string="was based on unreliable sources and misquotes" type="VP">
          <tokens>
            <token id="23" string="was" />
            <token id="24" string="based" />
            <token id="25" string="on" />
            <token id="26" string="unreliable" />
            <token id="27" string="sources" />
            <token id="28" string="and" />
            <token id="29" string="misquotes" />
          </tokens>
        </chunking>
        <chunking id="11" string="Goldman 's" type="NP">
          <tokens>
            <token id="20" string="Goldman" />
            <token id="21" string="'s" />
          </tokens>
        </chunking>
        <chunking id="12" string="these charges" type="NP">
          <tokens>
            <token id="6" string="these" />
            <token id="7" string="charges" />
          </tokens>
        </chunking>
        <chunking id="13" string="virtually all of these charges" type="NP">
          <tokens>
            <token id="3" string="virtually" />
            <token id="4" string="all" />
            <token id="5" string="of" />
            <token id="6" string="these" />
            <token id="7" string="charges" />
          </tokens>
        </chunking>
        <chunking id="14" string="refuted virtually all of these charges , and countered through taped interviews with Lennon friends and employees that Goldman 's book was based on unreliable sources and misquotes" type="VP">
          <tokens>
            <token id="2" string="refuted" />
            <token id="3" string="virtually" />
            <token id="4" string="all" />
            <token id="5" string="of" />
            <token id="6" string="these" />
            <token id="7" string="charges" />
            <token id="8" string="," />
            <token id="9" string="and" />
            <token id="10" string="countered" />
            <token id="11" string="through" />
            <token id="12" string="taped" />
            <token id="13" string="interviews" />
            <token id="14" string="with" />
            <token id="15" string="Lennon" />
            <token id="16" string="friends" />
            <token id="17" string="and" />
            <token id="18" string="employees" />
            <token id="19" string="that" />
            <token id="20" string="Goldman" />
            <token id="21" string="'s" />
            <token id="22" string="book" />
            <token id="23" string="was" />
            <token id="24" string="based" />
            <token id="25" string="on" />
            <token id="26" string="unreliable" />
            <token id="27" string="sources" />
            <token id="28" string="and" />
            <token id="29" string="misquotes" />
          </tokens>
        </chunking>
        <chunking id="15" string="that Goldman 's book was based on unreliable sources and misquotes" type="SBAR">
          <tokens>
            <token id="19" string="that" />
            <token id="20" string="Goldman" />
            <token id="21" string="'s" />
            <token id="22" string="book" />
            <token id="23" string="was" />
            <token id="24" string="based" />
            <token id="25" string="on" />
            <token id="26" string="unreliable" />
            <token id="27" string="sources" />
            <token id="28" string="and" />
            <token id="29" string="misquotes" />
          </tokens>
        </chunking>
        <chunking id="16" string="countered through taped interviews with Lennon friends and employees that Goldman 's book was based on unreliable sources and misquotes" type="VP">
          <tokens>
            <token id="10" string="countered" />
            <token id="11" string="through" />
            <token id="12" string="taped" />
            <token id="13" string="interviews" />
            <token id="14" string="with" />
            <token id="15" string="Lennon" />
            <token id="16" string="friends" />
            <token id="17" string="and" />
            <token id="18" string="employees" />
            <token id="19" string="that" />
            <token id="20" string="Goldman" />
            <token id="21" string="'s" />
            <token id="22" string="book" />
            <token id="23" string="was" />
            <token id="24" string="based" />
            <token id="25" string="on" />
            <token id="26" string="unreliable" />
            <token id="27" string="sources" />
            <token id="28" string="and" />
            <token id="29" string="misquotes" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">refuted</governor>
          <dependent id="1">Ono</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">refuted</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">all</governor>
          <dependent id="3">virtually</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">refuted</governor>
          <dependent id="4">all</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">charges</governor>
          <dependent id="5">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">charges</governor>
          <dependent id="6">these</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">all</governor>
          <dependent id="7">charges</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="2">refuted</governor>
          <dependent id="9">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="2">refuted</governor>
          <dependent id="10">countered</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">interviews</governor>
          <dependent id="11">through</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">interviews</governor>
          <dependent id="12">taped</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">countered</governor>
          <dependent id="13">interviews</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">friends</governor>
          <dependent id="14">with</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="16">friends</governor>
          <dependent id="15">Lennon</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="13">interviews</governor>
          <dependent id="16">friends</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="16">friends</governor>
          <dependent id="17">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="16">friends</governor>
          <dependent id="18">employees</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="24">based</governor>
          <dependent id="19">that</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="22">book</governor>
          <dependent id="20">Goldman</dependent>
        </dependency>
        <dependency type="case">
          <governor id="20">Goldman</governor>
          <dependent id="21">'s</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="24">based</governor>
          <dependent id="22">book</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="24">based</governor>
          <dependent id="23">was</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="13">interviews</governor>
          <dependent id="24">based</dependent>
        </dependency>
        <dependency type="case">
          <governor id="27">sources</governor>
          <dependent id="25">on</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="27">sources</governor>
          <dependent id="26">unreliable</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="24">based</governor>
          <dependent id="27">sources</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="27">sources</governor>
          <dependent id="28">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="27">sources</governor>
          <dependent id="29">misquotes</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="15" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="Goldman" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="20" string="Goldman" />
          </tokens>
        </entity>
        <entity id="3" string="Ono" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Ono" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="8" has_coreference="true">
      <content>``It&amp;apost;s totally fiction,&amp;apost;&amp;apost; Yoko said during the hour-long program, which opened with Lennon&amp;apost;s song, ``Gimme Some Truth,&amp;apost;&amp;apost; and coincided with the book&amp;apost;s national release Wednesday.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="totally" lemma="totally" stem="total" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="fiction" lemma="fiction" stem="fiction" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Yoko" lemma="Yoko" stem="yoko" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="9" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="during" lemma="during" stem="dure" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="hour-long" lemma="hour-long" stem="hour-long" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="program" lemma="program" stem="program" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="which" lemma="which" stem="which" pos="WDT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="opened" lemma="open" stem="open" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="19" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="20" string="song" lemma="song" stem="song" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="Gim" lemma="Gim" stem="gim" pos="NNP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="me" lemma="I" stem="me" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="25" string="Some" lemma="some" stem="some" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="26" string="Truth" lemma="truth" stem="truth" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="27" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="29" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="coincided" lemma="coincide" stem="coincid" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="32" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="33" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="34" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="35" string="national" lemma="national" stem="nation" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="36" string="release" lemma="release" stem="releas" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="37" string="Wednesday" lemma="Wednesday" stem="wednesdai" pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="38" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (S (NP (PRP It)) (VP (VBZ 's) (ADVP (RB totally)) (NP (NN fiction)))) (, ,) ('' '') (NP (NNP Yoko)) (VP (VP (VBD said) (PP (IN during) (NP (NP (DT the) (JJ hour-long) (NN program)) (, ,) (SBAR (WHNP (WDT which)) (S (VP (VBD opened) (PP (IN with) (NP (NP (NNP Lennon) (POS 's)) (NN song))) (, ,) (`` ``) (S (NP (NNP Gim)) (NP (NP (PRP me)) (NP (DT Some) (NN Truth)) (, ,) ('' ''))))))))) (CC and) (VP (VBN coincided) (PP (IN with) (NP (NP (DT the) (NN book) (POS 's)) (JJ national) (NN release))) (NP-TMP (NNP Wednesday)))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="'s totally fiction" type="VP">
          <tokens>
            <token id="3" string="'s" />
            <token id="4" string="totally" />
            <token id="5" string="fiction" />
          </tokens>
        </chunking>
        <chunking id="2" string="opened with Lennon 's song , `` Gim me Some Truth , ''" type="VP">
          <tokens>
            <token id="16" string="opened" />
            <token id="17" string="with" />
            <token id="18" string="Lennon" />
            <token id="19" string="'s" />
            <token id="20" string="song" />
            <token id="21" string="," />
            <token id="22" string="``" />
            <token id="23" string="Gim" />
            <token id="24" string="me" />
            <token id="25" string="Some" />
            <token id="26" string="Truth" />
            <token id="27" string="," />
            <token id="28" string="''" />
          </tokens>
        </chunking>
        <chunking id="3" string="fiction" type="NP">
          <tokens>
            <token id="5" string="fiction" />
          </tokens>
        </chunking>
        <chunking id="4" string="coincided with the book 's national release Wednesday" type="VP">
          <tokens>
            <token id="30" string="coincided" />
            <token id="31" string="with" />
            <token id="32" string="the" />
            <token id="33" string="book" />
            <token id="34" string="'s" />
            <token id="35" string="national" />
            <token id="36" string="release" />
            <token id="37" string="Wednesday" />
          </tokens>
        </chunking>
        <chunking id="5" string="the book 's" type="NP">
          <tokens>
            <token id="32" string="the" />
            <token id="33" string="book" />
            <token id="34" string="'s" />
          </tokens>
        </chunking>
        <chunking id="6" string="the hour-long program" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="hour-long" />
            <token id="13" string="program" />
          </tokens>
        </chunking>
        <chunking id="7" string="It" type="NP">
          <tokens>
            <token id="2" string="It" />
          </tokens>
        </chunking>
        <chunking id="8" string="Lennon 's" type="NP">
          <tokens>
            <token id="18" string="Lennon" />
            <token id="19" string="'s" />
          </tokens>
        </chunking>
        <chunking id="9" string="Gim" type="NP">
          <tokens>
            <token id="23" string="Gim" />
          </tokens>
        </chunking>
        <chunking id="10" string="me Some Truth , ''" type="NP">
          <tokens>
            <token id="24" string="me" />
            <token id="25" string="Some" />
            <token id="26" string="Truth" />
            <token id="27" string="," />
            <token id="28" string="''" />
          </tokens>
        </chunking>
        <chunking id="11" string="said during the hour-long program , which opened with Lennon 's song , `` Gim me Some Truth , '' and coincided with the book 's national release Wednesday" type="VP">
          <tokens>
            <token id="9" string="said" />
            <token id="10" string="during" />
            <token id="11" string="the" />
            <token id="12" string="hour-long" />
            <token id="13" string="program" />
            <token id="14" string="," />
            <token id="15" string="which" />
            <token id="16" string="opened" />
            <token id="17" string="with" />
            <token id="18" string="Lennon" />
            <token id="19" string="'s" />
            <token id="20" string="song" />
            <token id="21" string="," />
            <token id="22" string="``" />
            <token id="23" string="Gim" />
            <token id="24" string="me" />
            <token id="25" string="Some" />
            <token id="26" string="Truth" />
            <token id="27" string="," />
            <token id="28" string="''" />
            <token id="29" string="and" />
            <token id="30" string="coincided" />
            <token id="31" string="with" />
            <token id="32" string="the" />
            <token id="33" string="book" />
            <token id="34" string="'s" />
            <token id="35" string="national" />
            <token id="36" string="release" />
            <token id="37" string="Wednesday" />
          </tokens>
        </chunking>
        <chunking id="12" string="said during the hour-long program , which opened with Lennon 's song , `` Gim me Some Truth , ''" type="VP">
          <tokens>
            <token id="9" string="said" />
            <token id="10" string="during" />
            <token id="11" string="the" />
            <token id="12" string="hour-long" />
            <token id="13" string="program" />
            <token id="14" string="," />
            <token id="15" string="which" />
            <token id="16" string="opened" />
            <token id="17" string="with" />
            <token id="18" string="Lennon" />
            <token id="19" string="'s" />
            <token id="20" string="song" />
            <token id="21" string="," />
            <token id="22" string="``" />
            <token id="23" string="Gim" />
            <token id="24" string="me" />
            <token id="25" string="Some" />
            <token id="26" string="Truth" />
            <token id="27" string="," />
            <token id="28" string="''" />
          </tokens>
        </chunking>
        <chunking id="13" string="Lennon 's song" type="NP">
          <tokens>
            <token id="18" string="Lennon" />
            <token id="19" string="'s" />
            <token id="20" string="song" />
          </tokens>
        </chunking>
        <chunking id="14" string="Yoko" type="NP">
          <tokens>
            <token id="8" string="Yoko" />
          </tokens>
        </chunking>
        <chunking id="15" string="the hour-long program , which opened with Lennon 's song , `` Gim me Some Truth , ''" type="NP">
          <tokens>
            <token id="11" string="the" />
            <token id="12" string="hour-long" />
            <token id="13" string="program" />
            <token id="14" string="," />
            <token id="15" string="which" />
            <token id="16" string="opened" />
            <token id="17" string="with" />
            <token id="18" string="Lennon" />
            <token id="19" string="'s" />
            <token id="20" string="song" />
            <token id="21" string="," />
            <token id="22" string="``" />
            <token id="23" string="Gim" />
            <token id="24" string="me" />
            <token id="25" string="Some" />
            <token id="26" string="Truth" />
            <token id="27" string="," />
            <token id="28" string="''" />
          </tokens>
        </chunking>
        <chunking id="16" string="me" type="NP">
          <tokens>
            <token id="24" string="me" />
          </tokens>
        </chunking>
        <chunking id="17" string="the book 's national release" type="NP">
          <tokens>
            <token id="32" string="the" />
            <token id="33" string="book" />
            <token id="34" string="'s" />
            <token id="35" string="national" />
            <token id="36" string="release" />
          </tokens>
        </chunking>
        <chunking id="18" string="which opened with Lennon 's song , `` Gim me Some Truth , ''" type="SBAR">
          <tokens>
            <token id="15" string="which" />
            <token id="16" string="opened" />
            <token id="17" string="with" />
            <token id="18" string="Lennon" />
            <token id="19" string="'s" />
            <token id="20" string="song" />
            <token id="21" string="," />
            <token id="22" string="``" />
            <token id="23" string="Gim" />
            <token id="24" string="me" />
            <token id="25" string="Some" />
            <token id="26" string="Truth" />
            <token id="27" string="," />
            <token id="28" string="''" />
          </tokens>
        </chunking>
        <chunking id="19" string="Some Truth" type="NP">
          <tokens>
            <token id="25" string="Some" />
            <token id="26" string="Truth" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="5">fiction</governor>
          <dependent id="2">It</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="5">fiction</governor>
          <dependent id="3">'s</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">fiction</governor>
          <dependent id="4">totally</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="9">said</governor>
          <dependent id="5">fiction</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">said</governor>
          <dependent id="8">Yoko</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="9">said</dependent>
        </dependency>
        <dependency type="case">
          <governor id="13">program</governor>
          <dependent id="10">during</dependent>
        </dependency>
        <dependency type="det">
          <governor id="13">program</governor>
          <dependent id="11">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">program</governor>
          <dependent id="12">hour-long</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">said</governor>
          <dependent id="13">program</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="16">opened</governor>
          <dependent id="15">which</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="13">program</governor>
          <dependent id="16">opened</dependent>
        </dependency>
        <dependency type="case">
          <governor id="20">song</governor>
          <dependent id="17">with</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="20">song</governor>
          <dependent id="18">Lennon</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">Lennon</governor>
          <dependent id="19">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="16">opened</governor>
          <dependent id="20">song</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="24">me</governor>
          <dependent id="23">Gim</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="16">opened</governor>
          <dependent id="24">me</dependent>
        </dependency>
        <dependency type="det">
          <governor id="26">Truth</governor>
          <dependent id="25">Some</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="24">me</governor>
          <dependent id="26">Truth</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="9">said</governor>
          <dependent id="29">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="9">said</governor>
          <dependent id="30">coincided</dependent>
        </dependency>
        <dependency type="case">
          <governor id="36">release</governor>
          <dependent id="31">with</dependent>
        </dependency>
        <dependency type="det">
          <governor id="33">book</governor>
          <dependent id="32">the</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="36">release</governor>
          <dependent id="33">book</dependent>
        </dependency>
        <dependency type="case">
          <governor id="33">book</governor>
          <dependent id="34">'s</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="36">release</governor>
          <dependent id="35">national</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="30">coincided</governor>
          <dependent id="36">release</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="30">coincided</governor>
          <dependent id="37">Wednesday</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Yoko" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Yoko" />
          </tokens>
        </entity>
        <entity id="2" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="18" string="Lennon" />
          </tokens>
        </entity>
        <entity id="3" string="Wednesday" type="DATE" score="0.0">
          <tokens>
            <token id="37" string="Wednesday" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="9" has_coreference="true">
      <content>``These people in this book are not us.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="These" lemma="these" stem="these" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="3" string="people" lemma="people" stem="peopl" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="4" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="5" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="6" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="true" />
        <token id="7" string="are" lemma="be" stem="ar" pos="VBP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="us" lemma="we" stem="u" pos="PRP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (NP (DT These) (NNS people)) (PP (IN in) (NP (DT this) (NN book)))) (VP (VBP are) (RB not) (NP (PRP us))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="These people" type="NP">
          <tokens>
            <token id="2" string="These" />
            <token id="3" string="people" />
          </tokens>
        </chunking>
        <chunking id="2" string="this book" type="NP">
          <tokens>
            <token id="5" string="this" />
            <token id="6" string="book" />
          </tokens>
        </chunking>
        <chunking id="3" string="are not us" type="VP">
          <tokens>
            <token id="7" string="are" />
            <token id="8" string="not" />
            <token id="9" string="us" />
          </tokens>
        </chunking>
        <chunking id="4" string="These people in this book" type="NP">
          <tokens>
            <token id="2" string="These" />
            <token id="3" string="people" />
            <token id="4" string="in" />
            <token id="5" string="this" />
            <token id="6" string="book" />
          </tokens>
        </chunking>
        <chunking id="5" string="us" type="NP">
          <tokens>
            <token id="9" string="us" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">people</governor>
          <dependent id="2">These</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="9">us</governor>
          <dependent id="3">people</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">book</governor>
          <dependent id="4">in</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">book</governor>
          <dependent id="5">this</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">people</governor>
          <dependent id="6">book</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="9">us</governor>
          <dependent id="7">are</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="9">us</governor>
          <dependent id="8">not</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="9">us</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="10" has_coreference="true">
      <content>It&amp;apost;s not John and me,&amp;apost;&amp;apost; Ono continued.</content>
      <tokens>
        <token id="1" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="5" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="6" string="me" lemma="I" stem="me" pos="PRP" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="Ono" lemma="Ono" stem="ono" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="10" string="continued" lemma="continue" stem="continu" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (PRP It)) (VP (VBZ 's) (RB not) (NP (NP (NNP John)) (CC and) (NP (PRP me))))) (, ,) ('' '') (NP (NNP Ono)) (VP (VBD continued)) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="'s not John and me" type="VP">
          <tokens>
            <token id="2" string="'s" />
            <token id="3" string="not" />
            <token id="4" string="John" />
            <token id="5" string="and" />
            <token id="6" string="me" />
          </tokens>
        </chunking>
        <chunking id="2" string="me" type="NP">
          <tokens>
            <token id="6" string="me" />
          </tokens>
        </chunking>
        <chunking id="3" string="continued" type="VP">
          <tokens>
            <token id="10" string="continued" />
          </tokens>
        </chunking>
        <chunking id="4" string="John" type="NP">
          <tokens>
            <token id="4" string="John" />
          </tokens>
        </chunking>
        <chunking id="5" string="It" type="NP">
          <tokens>
            <token id="1" string="It" />
          </tokens>
        </chunking>
        <chunking id="6" string="John and me" type="NP">
          <tokens>
            <token id="4" string="John" />
            <token id="5" string="and" />
            <token id="6" string="me" />
          </tokens>
        </chunking>
        <chunking id="7" string="Ono" type="NP">
          <tokens>
            <token id="9" string="Ono" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">John</governor>
          <dependent id="1">It</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="4">John</governor>
          <dependent id="2">'s</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="4">John</governor>
          <dependent id="3">not</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="10">continued</governor>
          <dependent id="4">John</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">John</governor>
          <dependent id="5">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">John</governor>
          <dependent id="6">me</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="10">continued</governor>
          <dependent id="9">Ono</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="10">continued</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="John" />
          </tokens>
        </entity>
        <entity id="2" string="Ono" type="PERSON" score="0.0">
          <tokens>
            <token id="9" string="Ono" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="11" has_coreference="true">
      <content>``It&amp;apost;s unfair he&amp;apost;s been assaulted and can&amp;apost;t hit back.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="unfair" lemma="unfair" stem="unfair" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="'s" lemma="be" stem="'" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="7" string="been" lemma="be" stem="been" pos="VBN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="assaulted" lemma="assault" stem="assault" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="ca" lemma="can" stem="ca" pos="MD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="n't" lemma="not" stem="n't" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="hit" lemma="hit" stem="hit" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="back" lemma="back" stem="back" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (`` ``) (NP (PRP It)) (VP (VBZ 's) (ADJP (JJ unfair) (SBAR (S (NP (PRP he)) (VP (VP (VBZ 's) (VP (VBN been) (VP (VBN assaulted)))) (CC and) (VP (MD ca) (RB n't) (VP (VB hit) (ADVP (RB back))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="assaulted" type="VP">
          <tokens>
            <token id="8" string="assaulted" />
          </tokens>
        </chunking>
        <chunking id="2" string="'s unfair he 's been assaulted and ca n't hit back" type="VP">
          <tokens>
            <token id="3" string="'s" />
            <token id="4" string="unfair" />
            <token id="5" string="he" />
            <token id="6" string="'s" />
            <token id="7" string="been" />
            <token id="8" string="assaulted" />
            <token id="9" string="and" />
            <token id="10" string="ca" />
            <token id="11" string="n't" />
            <token id="12" string="hit" />
            <token id="13" string="back" />
          </tokens>
        </chunking>
        <chunking id="3" string="hit back" type="VP">
          <tokens>
            <token id="12" string="hit" />
            <token id="13" string="back" />
          </tokens>
        </chunking>
        <chunking id="4" string="'s been assaulted" type="VP">
          <tokens>
            <token id="6" string="'s" />
            <token id="7" string="been" />
            <token id="8" string="assaulted" />
          </tokens>
        </chunking>
        <chunking id="5" string="It" type="NP">
          <tokens>
            <token id="2" string="It" />
          </tokens>
        </chunking>
        <chunking id="6" string="he 's been assaulted and ca n't hit back" type="SBAR">
          <tokens>
            <token id="5" string="he" />
            <token id="6" string="'s" />
            <token id="7" string="been" />
            <token id="8" string="assaulted" />
            <token id="9" string="and" />
            <token id="10" string="ca" />
            <token id="11" string="n't" />
            <token id="12" string="hit" />
            <token id="13" string="back" />
          </tokens>
        </chunking>
        <chunking id="7" string="been assaulted" type="VP">
          <tokens>
            <token id="7" string="been" />
            <token id="8" string="assaulted" />
          </tokens>
        </chunking>
        <chunking id="8" string="ca n't hit back" type="VP">
          <tokens>
            <token id="10" string="ca" />
            <token id="11" string="n't" />
            <token id="12" string="hit" />
            <token id="13" string="back" />
          </tokens>
        </chunking>
        <chunking id="9" string="unfair he 's been assaulted and ca n't hit back" type="ADJP">
          <tokens>
            <token id="4" string="unfair" />
            <token id="5" string="he" />
            <token id="6" string="'s" />
            <token id="7" string="been" />
            <token id="8" string="assaulted" />
            <token id="9" string="and" />
            <token id="10" string="ca" />
            <token id="11" string="n't" />
            <token id="12" string="hit" />
            <token id="13" string="back" />
          </tokens>
        </chunking>
        <chunking id="10" string="he" type="NP">
          <tokens>
            <token id="5" string="he" />
          </tokens>
        </chunking>
        <chunking id="11" string="'s been assaulted and ca n't hit back" type="VP">
          <tokens>
            <token id="6" string="'s" />
            <token id="7" string="been" />
            <token id="8" string="assaulted" />
            <token id="9" string="and" />
            <token id="10" string="ca" />
            <token id="11" string="n't" />
            <token id="12" string="hit" />
            <token id="13" string="back" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="4">unfair</governor>
          <dependent id="2">It</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="4">unfair</governor>
          <dependent id="3">'s</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">unfair</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="8">assaulted</governor>
          <dependent id="5">he</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="8">assaulted</governor>
          <dependent id="6">'s</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="8">assaulted</governor>
          <dependent id="7">been</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="4">unfair</governor>
          <dependent id="8">assaulted</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="8">assaulted</governor>
          <dependent id="9">and</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="12">hit</governor>
          <dependent id="10">ca</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="12">hit</governor>
          <dependent id="11">n't</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="8">assaulted</governor>
          <dependent id="12">hit</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="12">hit</governor>
          <dependent id="13">back</dependent>
        </dependency>
      </dependencies>
    </sentence>
    <sentence id="12" has_coreference="true">
      <content>... For some reason this book is attempting to rob both John and I of the basic human dignity that we&amp;apost;re entitled to.&amp;apost;&amp;apost;</content>
      <tokens>
        <token id="1" string="..." lemma="..." stem="..." pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="For" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="some" lemma="some" stem="some" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="reason" lemma="reason" stem="reason" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="this" lemma="this" stem="thi" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="attempting" lemma="attempt" stem="attempt" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="rob" lemma="rob" stem="rob" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="both" lemma="both" stem="both" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="John" lemma="John" stem="john" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="13" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="I" lemma="I" stem="i" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="basic" lemma="basic" stem="basic" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="human" lemma="human" stem="human" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="dignity" lemma="dignity" stem="digniti" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="we" lemma="we" stem="we" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="22" string="'re" lemma="be" stem="'re" pos="VBP" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="entitled" lemma="entitle" stem="entitl" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (: ...) (PP (IN For) (NP (DT some) (NN reason))) (NP (DT this) (NN book)) (VP (VBZ is) (VP (VBG attempting) (S (VP (TO to) (VP (VB rob) (NP (CC both) (NP (NNP John)) (CC and) (NP (PRP I))) (PP (IN of) (NP (DT the) (JJ basic) (JJ human) (NN dignity))) (SBAR (IN that) (S (NP (PRP we)) (VP (VBP 're) (ADJP (VBN entitled) (PP (TO to))))))))))) (. .) ('' '')))</syntactictree>
      <chunkings>
        <chunking id="1" string="rob both John and I of the basic human dignity that we 're entitled to" type="VP">
          <tokens>
            <token id="10" string="rob" />
            <token id="11" string="both" />
            <token id="12" string="John" />
            <token id="13" string="and" />
            <token id="14" string="I" />
            <token id="15" string="of" />
            <token id="16" string="the" />
            <token id="17" string="basic" />
            <token id="18" string="human" />
            <token id="19" string="dignity" />
            <token id="20" string="that" />
            <token id="21" string="we" />
            <token id="22" string="'re" />
            <token id="23" string="entitled" />
            <token id="24" string="to" />
          </tokens>
        </chunking>
        <chunking id="2" string="'re entitled to" type="VP">
          <tokens>
            <token id="22" string="'re" />
            <token id="23" string="entitled" />
            <token id="24" string="to" />
          </tokens>
        </chunking>
        <chunking id="3" string="entitled to" type="ADJP">
          <tokens>
            <token id="23" string="entitled" />
            <token id="24" string="to" />
          </tokens>
        </chunking>
        <chunking id="4" string="I" type="NP">
          <tokens>
            <token id="14" string="I" />
          </tokens>
        </chunking>
        <chunking id="5" string="John" type="NP">
          <tokens>
            <token id="12" string="John" />
          </tokens>
        </chunking>
        <chunking id="6" string="that we 're entitled to" type="SBAR">
          <tokens>
            <token id="20" string="that" />
            <token id="21" string="we" />
            <token id="22" string="'re" />
            <token id="23" string="entitled" />
            <token id="24" string="to" />
          </tokens>
        </chunking>
        <chunking id="7" string="we" type="NP">
          <tokens>
            <token id="21" string="we" />
          </tokens>
        </chunking>
        <chunking id="8" string="attempting to rob both John and I of the basic human dignity that we 're entitled to" type="VP">
          <tokens>
            <token id="8" string="attempting" />
            <token id="9" string="to" />
            <token id="10" string="rob" />
            <token id="11" string="both" />
            <token id="12" string="John" />
            <token id="13" string="and" />
            <token id="14" string="I" />
            <token id="15" string="of" />
            <token id="16" string="the" />
            <token id="17" string="basic" />
            <token id="18" string="human" />
            <token id="19" string="dignity" />
            <token id="20" string="that" />
            <token id="21" string="we" />
            <token id="22" string="'re" />
            <token id="23" string="entitled" />
            <token id="24" string="to" />
          </tokens>
        </chunking>
        <chunking id="9" string="is attempting to rob both John and I of the basic human dignity that we 're entitled to" type="VP">
          <tokens>
            <token id="7" string="is" />
            <token id="8" string="attempting" />
            <token id="9" string="to" />
            <token id="10" string="rob" />
            <token id="11" string="both" />
            <token id="12" string="John" />
            <token id="13" string="and" />
            <token id="14" string="I" />
            <token id="15" string="of" />
            <token id="16" string="the" />
            <token id="17" string="basic" />
            <token id="18" string="human" />
            <token id="19" string="dignity" />
            <token id="20" string="that" />
            <token id="21" string="we" />
            <token id="22" string="'re" />
            <token id="23" string="entitled" />
            <token id="24" string="to" />
          </tokens>
        </chunking>
        <chunking id="10" string="this book" type="NP">
          <tokens>
            <token id="5" string="this" />
            <token id="6" string="book" />
          </tokens>
        </chunking>
        <chunking id="11" string="the basic human dignity" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="basic" />
            <token id="18" string="human" />
            <token id="19" string="dignity" />
          </tokens>
        </chunking>
        <chunking id="12" string="some reason" type="NP">
          <tokens>
            <token id="3" string="some" />
            <token id="4" string="reason" />
          </tokens>
        </chunking>
        <chunking id="13" string="both John and I" type="NP">
          <tokens>
            <token id="11" string="both" />
            <token id="12" string="John" />
            <token id="13" string="and" />
            <token id="14" string="I" />
          </tokens>
        </chunking>
        <chunking id="14" string="to rob both John and I of the basic human dignity that we 're entitled to" type="VP">
          <tokens>
            <token id="9" string="to" />
            <token id="10" string="rob" />
            <token id="11" string="both" />
            <token id="12" string="John" />
            <token id="13" string="and" />
            <token id="14" string="I" />
            <token id="15" string="of" />
            <token id="16" string="the" />
            <token id="17" string="basic" />
            <token id="18" string="human" />
            <token id="19" string="dignity" />
            <token id="20" string="that" />
            <token id="21" string="we" />
            <token id="22" string="'re" />
            <token id="23" string="entitled" />
            <token id="24" string="to" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="case">
          <governor id="4">reason</governor>
          <dependent id="2">For</dependent>
        </dependency>
        <dependency type="det">
          <governor id="4">reason</governor>
          <dependent id="3">some</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">attempting</governor>
          <dependent id="4">reason</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">book</governor>
          <dependent id="5">this</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="8">attempting</governor>
          <dependent id="6">book</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="8">attempting</governor>
          <dependent id="7">is</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="8">attempting</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="10">rob</governor>
          <dependent id="9">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="8">attempting</governor>
          <dependent id="10">rob</dependent>
        </dependency>
        <dependency type="cc:preconj">
          <governor id="12">John</governor>
          <dependent id="11">both</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="10">rob</governor>
          <dependent id="12">John</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="12">John</governor>
          <dependent id="13">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="12">John</governor>
          <dependent id="14">I</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">dignity</governor>
          <dependent id="15">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">dignity</governor>
          <dependent id="16">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">dignity</governor>
          <dependent id="17">basic</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="19">dignity</governor>
          <dependent id="18">human</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="10">rob</governor>
          <dependent id="19">dignity</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="23">entitled</governor>
          <dependent id="20">that</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="23">entitled</governor>
          <dependent id="21">we</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="23">entitled</governor>
          <dependent id="22">'re</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="10">rob</governor>
          <dependent id="23">entitled</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">entitled</governor>
          <dependent id="24">to</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="John" type="PERSON" score="0.0">
          <tokens>
            <token id="12" string="John" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="13" has_coreference="true">
      <content>The Ono interview was taped Aug. 28 at the Dakota, where the Lennons lived _ and Yoko still lives with their son, Sean.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="Ono" lemma="Ono" stem="ono" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="interview" lemma="interview" stem="interview" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="taped" lemma="tape" stem="tape" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="Aug." lemma="Aug." stem="aug." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="7" string="28" lemma="28" stem="28" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="8" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="10" string="Dakota" lemma="Dakota" stem="dakota" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="true" is_refers="false" />
        <token id="11" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="where" lemma="where" stem="where" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="Lennons" lemma="Lennons" stem="lennon" pos="NNPS" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="15" string="lived" lemma="live" stem="live" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="_" lemma="_" stem="_" pos="NN" type="Symbol" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="17" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="18" string="Yoko" lemma="Yoko" stem="yoko" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="19" string="still" lemma="still" stem="still" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="lives" lemma="live" stem="live" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="their" lemma="they" stem="their" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="son" lemma="son" stem="son" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="Sean" lemma="Sean" stem="sean" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="26" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (DT The) (NNP Ono) (NN interview)) (VP (VBD was) (VP (VBN taped) (NP-TMP (NNP Aug.) (CD 28)) (PP (IN at) (NP (NP (DT the) (NNP Dakota)) (, ,) (SBAR (WHADVP (WRB where)) (S (NP (DT the) (NNPS Lennons)) (VP (VBD lived) (SBAR (S (NP (NP (NN _)) (CC and) (NP (NNP Yoko))) (ADVP (RB still)) (VP (VBZ lives) (PP (IN with) (NP (NP (PRP$ their) (NN son)) (, ,) (NP (NNP Sean)))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="The Ono interview" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="Ono" />
            <token id="3" string="interview" />
          </tokens>
        </chunking>
        <chunking id="2" string="lived _ and Yoko still lives with their son , Sean" type="VP">
          <tokens>
            <token id="15" string="lived" />
            <token id="16" string="_" />
            <token id="17" string="and" />
            <token id="18" string="Yoko" />
            <token id="19" string="still" />
            <token id="20" string="lives" />
            <token id="21" string="with" />
            <token id="22" string="their" />
            <token id="23" string="son" />
            <token id="24" string="," />
            <token id="25" string="Sean" />
          </tokens>
        </chunking>
        <chunking id="3" string="_ and Yoko" type="NP">
          <tokens>
            <token id="16" string="_" />
            <token id="17" string="and" />
            <token id="18" string="Yoko" />
          </tokens>
        </chunking>
        <chunking id="4" string="lives with their son , Sean" type="VP">
          <tokens>
            <token id="20" string="lives" />
            <token id="21" string="with" />
            <token id="22" string="their" />
            <token id="23" string="son" />
            <token id="24" string="," />
            <token id="25" string="Sean" />
          </tokens>
        </chunking>
        <chunking id="5" string="_ and Yoko still lives with their son , Sean" type="SBAR">
          <tokens>
            <token id="16" string="_" />
            <token id="17" string="and" />
            <token id="18" string="Yoko" />
            <token id="19" string="still" />
            <token id="20" string="lives" />
            <token id="21" string="with" />
            <token id="22" string="their" />
            <token id="23" string="son" />
            <token id="24" string="," />
            <token id="25" string="Sean" />
          </tokens>
        </chunking>
        <chunking id="6" string="taped Aug. 28 at the Dakota , where the Lennons lived _ and Yoko still lives with their son , Sean" type="VP">
          <tokens>
            <token id="5" string="taped" />
            <token id="6" string="Aug." />
            <token id="7" string="28" />
            <token id="8" string="at" />
            <token id="9" string="the" />
            <token id="10" string="Dakota" />
            <token id="11" string="," />
            <token id="12" string="where" />
            <token id="13" string="the" />
            <token id="14" string="Lennons" />
            <token id="15" string="lived" />
            <token id="16" string="_" />
            <token id="17" string="and" />
            <token id="18" string="Yoko" />
            <token id="19" string="still" />
            <token id="20" string="lives" />
            <token id="21" string="with" />
            <token id="22" string="their" />
            <token id="23" string="son" />
            <token id="24" string="," />
            <token id="25" string="Sean" />
          </tokens>
        </chunking>
        <chunking id="7" string="the Dakota , where the Lennons lived _ and Yoko still lives with their son , Sean" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="Dakota" />
            <token id="11" string="," />
            <token id="12" string="where" />
            <token id="13" string="the" />
            <token id="14" string="Lennons" />
            <token id="15" string="lived" />
            <token id="16" string="_" />
            <token id="17" string="and" />
            <token id="18" string="Yoko" />
            <token id="19" string="still" />
            <token id="20" string="lives" />
            <token id="21" string="with" />
            <token id="22" string="their" />
            <token id="23" string="son" />
            <token id="24" string="," />
            <token id="25" string="Sean" />
          </tokens>
        </chunking>
        <chunking id="8" string="Yoko" type="NP">
          <tokens>
            <token id="18" string="Yoko" />
          </tokens>
        </chunking>
        <chunking id="9" string="where the Lennons lived _ and Yoko still lives with their son , Sean" type="SBAR">
          <tokens>
            <token id="12" string="where" />
            <token id="13" string="the" />
            <token id="14" string="Lennons" />
            <token id="15" string="lived" />
            <token id="16" string="_" />
            <token id="17" string="and" />
            <token id="18" string="Yoko" />
            <token id="19" string="still" />
            <token id="20" string="lives" />
            <token id="21" string="with" />
            <token id="22" string="their" />
            <token id="23" string="son" />
            <token id="24" string="," />
            <token id="25" string="Sean" />
          </tokens>
        </chunking>
        <chunking id="10" string="the Dakota" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="Dakota" />
          </tokens>
        </chunking>
        <chunking id="11" string="Sean" type="NP">
          <tokens>
            <token id="25" string="Sean" />
          </tokens>
        </chunking>
        <chunking id="12" string="the Lennons" type="NP">
          <tokens>
            <token id="13" string="the" />
            <token id="14" string="Lennons" />
          </tokens>
        </chunking>
        <chunking id="13" string="where" type="WHADVP">
          <tokens>
            <token id="12" string="where" />
          </tokens>
        </chunking>
        <chunking id="14" string="their son , Sean" type="NP">
          <tokens>
            <token id="22" string="their" />
            <token id="23" string="son" />
            <token id="24" string="," />
            <token id="25" string="Sean" />
          </tokens>
        </chunking>
        <chunking id="15" string="was taped Aug. 28 at the Dakota , where the Lennons lived _ and Yoko still lives with their son , Sean" type="VP">
          <tokens>
            <token id="4" string="was" />
            <token id="5" string="taped" />
            <token id="6" string="Aug." />
            <token id="7" string="28" />
            <token id="8" string="at" />
            <token id="9" string="the" />
            <token id="10" string="Dakota" />
            <token id="11" string="," />
            <token id="12" string="where" />
            <token id="13" string="the" />
            <token id="14" string="Lennons" />
            <token id="15" string="lived" />
            <token id="16" string="_" />
            <token id="17" string="and" />
            <token id="18" string="Yoko" />
            <token id="19" string="still" />
            <token id="20" string="lives" />
            <token id="21" string="with" />
            <token id="22" string="their" />
            <token id="23" string="son" />
            <token id="24" string="," />
            <token id="25" string="Sean" />
          </tokens>
        </chunking>
        <chunking id="16" string="_" type="NP">
          <tokens>
            <token id="16" string="_" />
          </tokens>
        </chunking>
        <chunking id="17" string="their son" type="NP">
          <tokens>
            <token id="22" string="their" />
            <token id="23" string="son" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="3">interview</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="3">interview</governor>
          <dependent id="2">Ono</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="5">taped</governor>
          <dependent id="3">interview</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="5">taped</governor>
          <dependent id="4">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">taped</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="5">taped</governor>
          <dependent id="6">Aug.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="6">Aug.</governor>
          <dependent id="7">28</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">Dakota</governor>
          <dependent id="8">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">Dakota</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">taped</governor>
          <dependent id="10">Dakota</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="15">lived</governor>
          <dependent id="12">where</dependent>
        </dependency>
        <dependency type="det">
          <governor id="14">Lennons</governor>
          <dependent id="13">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">lived</governor>
          <dependent id="14">Lennons</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="10">Dakota</governor>
          <dependent id="15">lived</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="20">lives</governor>
          <dependent id="16">_</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="16">_</governor>
          <dependent id="17">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="16">_</governor>
          <dependent id="18">Yoko</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="20">lives</governor>
          <dependent id="19">still</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="15">lived</governor>
          <dependent id="20">lives</dependent>
        </dependency>
        <dependency type="case">
          <governor id="23">son</governor>
          <dependent id="21">with</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="23">son</governor>
          <dependent id="22">their</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="20">lives</governor>
          <dependent id="23">son</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="23">son</governor>
          <dependent id="25">Sean</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Yoko" type="PERSON" score="0.0">
          <tokens>
            <token id="18" string="Yoko" />
          </tokens>
        </entity>
        <entity id="2" string="Aug. 28" type="DATE" score="0.0">
          <tokens>
            <token id="6" string="Aug." />
            <token id="7" string="28" />
          </tokens>
        </entity>
        <entity id="3" string="Sean" type="PERSON" score="0.0">
          <tokens>
            <token id="25" string="Sean" />
          </tokens>
        </entity>
        <entity id="4" string="Ono" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Ono" />
          </tokens>
        </entity>
        <entity id="5" string="Lennons" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="14" string="Lennons" />
          </tokens>
        </entity>
        <entity id="6" string="Dakota" type="LOCATION" score="0.0">
          <tokens>
            <token id="10" string="Dakota" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="14" has_coreference="true">
      <content>Lennon was murdered by Mark David Chapman outside the Dakota on Dec. 8, 1980.</content>
      <tokens>
        <token id="1" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="murdered" lemma="murder" stem="murder" pos="VBN" type="Word" isStopWord="false" ner="CAUSE_OF_DEATH" is_referenced="false" is_refers="false" />
        <token id="4" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="Mark" lemma="Mark" stem="mark" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="6" string="David" lemma="David" stem="david" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="7" string="Chapman" lemma="Chapman" stem="chapman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="8" string="outside" lemma="outside" stem="outsid" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="Dakota" lemma="Dakota" stem="dakota" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="11" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="Dec." lemma="Dec." stem="dec." pos="NNP" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="13" string="8" lemma="8" stem="8" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="14" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="15" string="1980" lemma="1980" stem="1980" pos="CD" type="Number" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="16" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Lennon)) (VP (VBD was) (VP (VBN murdered) (PP (IN by) (NP (NP (NNP Mark) (NNP David) (NNP Chapman)) (PP (IN outside) (NP (DT the) (NNP Dakota))))) (PP (IN on) (NP (NNP Dec.) (CD 8) (, ,) (CD 1980))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Dec. 8 , 1980" type="NP">
          <tokens>
            <token id="12" string="Dec." />
            <token id="13" string="8" />
            <token id="14" string="," />
            <token id="15" string="1980" />
          </tokens>
        </chunking>
        <chunking id="2" string="was murdered by Mark David Chapman outside the Dakota on Dec. 8 , 1980" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="murdered" />
            <token id="4" string="by" />
            <token id="5" string="Mark" />
            <token id="6" string="David" />
            <token id="7" string="Chapman" />
            <token id="8" string="outside" />
            <token id="9" string="the" />
            <token id="10" string="Dakota" />
            <token id="11" string="on" />
            <token id="12" string="Dec." />
            <token id="13" string="8" />
            <token id="14" string="," />
            <token id="15" string="1980" />
          </tokens>
        </chunking>
        <chunking id="3" string="Lennon" type="NP">
          <tokens>
            <token id="1" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="4" string="Mark David Chapman outside the Dakota" type="NP">
          <tokens>
            <token id="5" string="Mark" />
            <token id="6" string="David" />
            <token id="7" string="Chapman" />
            <token id="8" string="outside" />
            <token id="9" string="the" />
            <token id="10" string="Dakota" />
          </tokens>
        </chunking>
        <chunking id="5" string="murdered by Mark David Chapman outside the Dakota on Dec. 8 , 1980" type="VP">
          <tokens>
            <token id="3" string="murdered" />
            <token id="4" string="by" />
            <token id="5" string="Mark" />
            <token id="6" string="David" />
            <token id="7" string="Chapman" />
            <token id="8" string="outside" />
            <token id="9" string="the" />
            <token id="10" string="Dakota" />
            <token id="11" string="on" />
            <token id="12" string="Dec." />
            <token id="13" string="8" />
            <token id="14" string="," />
            <token id="15" string="1980" />
          </tokens>
        </chunking>
        <chunking id="6" string="Mark David Chapman" type="NP">
          <tokens>
            <token id="5" string="Mark" />
            <token id="6" string="David" />
            <token id="7" string="Chapman" />
          </tokens>
        </chunking>
        <chunking id="7" string="the Dakota" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="Dakota" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="3">murdered</governor>
          <dependent id="1">Lennon</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="3">murdered</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">murdered</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">Chapman</governor>
          <dependent id="4">by</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">Chapman</governor>
          <dependent id="5">Mark</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">Chapman</governor>
          <dependent id="6">David</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">murdered</governor>
          <dependent id="7">Chapman</dependent>
        </dependency>
        <dependency type="case">
          <governor id="10">Dakota</governor>
          <dependent id="8">outside</dependent>
        </dependency>
        <dependency type="det">
          <governor id="10">Dakota</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="7">Chapman</governor>
          <dependent id="10">Dakota</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">Dec.</governor>
          <dependent id="11">on</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">murdered</governor>
          <dependent id="12">Dec.</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="12">Dec.</governor>
          <dependent id="13">8</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="12">Dec.</governor>
          <dependent id="15">1980</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Dec. 8 , 1980" type="DATE" score="0.0">
          <tokens>
            <token id="12" string="Dec." />
            <token id="13" string="8" />
            <token id="14" string="," />
            <token id="15" string="1980" />
          </tokens>
        </entity>
        <entity id="2" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Lennon" />
          </tokens>
        </entity>
        <entity id="3" string="murdered" type="CAUSE_OF_DEATH" score="0.0">
          <tokens>
            <token id="3" string="murdered" />
          </tokens>
        </entity>
        <entity id="4" string="Mark David Chapman" type="PERSON" score="0.0">
          <tokens>
            <token id="5" string="Mark" />
            <token id="6" string="David" />
            <token id="7" string="Chapman" />
          </tokens>
        </entity>
        <entity id="5" string="Dakota" type="LOCATION" score="0.0">
          <tokens>
            <token id="10" string="Dakota" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="15" has_coreference="true">
      <content>The interview was broadcast nationwide via the Westwood One Network; spokeswoman Laurie Wilde said an exact figure on how many stations picked up the broadcast was not available.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="interview" lemma="interview" stem="interview" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="broadcast" lemma="broadcast" stem="broadcast" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="nationwide" lemma="nationwide" stem="nationwid" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="via" lemma="via" stem="via" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="8" string="Westwood" lemma="Westwood" stem="westwood" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="9" string="One" lemma="one" stem="one" pos="CD" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="false" />
        <token id="10" string="Network" lemma="Network" stem="network" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="false" />
        <token id="11" string=";" lemma=";" stem=";" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="spokeswoman" lemma="spokeswoman" stem="spokeswoman" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="Laurie" lemma="Laurie" stem="lauri" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="14" string="Wilde" lemma="Wilde" stem="wild" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="15" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="exact" lemma="exact" stem="exact" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="figure" lemma="figure" stem="figur" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="19" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="20" string="how" lemma="how" stem="how" pos="WRB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="many" lemma="many" stem="mani" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="22" string="stations" lemma="station" stem="station" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="picked" lemma="pick" stem="pick" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="up" lemma="up" stem="up" pos="RP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="26" string="broadcast" lemma="broadcast" stem="broadcast" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="27" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="available" lemma="available" stem="avail" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (DT The) (NN interview)) (VP (VBD was) (VP (VBN broadcast) (ADVP (JJ nationwide) (PP (IN via) (NP (DT the) (NNP Westwood) (CD One)) (NP-TMP (NNP Network))))))) (: ;) (S (NP (NN spokeswoman) (NNP Laurie) (NNP Wilde)) (VP (VBD said) (NP (DT an) (JJ exact) (NN figure)) (PP (IN on) (SBAR (WHNP (WHADJP (WRB how) (JJ many)) (NNS stations)) (S (VP (VBD picked) (PRT (RP up)) (SBAR (S (NP (DT the) (NN broadcast)) (VP (VBD was) (RB not) (ADJP (JJ available))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="spokeswoman Laurie Wilde" type="NP">
          <tokens>
            <token id="12" string="spokeswoman" />
            <token id="13" string="Laurie" />
            <token id="14" string="Wilde" />
          </tokens>
        </chunking>
        <chunking id="2" string="the Westwood One" type="NP">
          <tokens>
            <token id="7" string="the" />
            <token id="8" string="Westwood" />
            <token id="9" string="One" />
          </tokens>
        </chunking>
        <chunking id="3" string="the broadcast was not available" type="SBAR">
          <tokens>
            <token id="25" string="the" />
            <token id="26" string="broadcast" />
            <token id="27" string="was" />
            <token id="28" string="not" />
            <token id="29" string="available" />
          </tokens>
        </chunking>
        <chunking id="4" string="the broadcast" type="NP">
          <tokens>
            <token id="25" string="the" />
            <token id="26" string="broadcast" />
          </tokens>
        </chunking>
        <chunking id="5" string="was broadcast nationwide via the Westwood One Network" type="VP">
          <tokens>
            <token id="3" string="was" />
            <token id="4" string="broadcast" />
            <token id="5" string="nationwide" />
            <token id="6" string="via" />
            <token id="7" string="the" />
            <token id="8" string="Westwood" />
            <token id="9" string="One" />
            <token id="10" string="Network" />
          </tokens>
        </chunking>
        <chunking id="6" string="how many stations picked up the broadcast was not available" type="SBAR">
          <tokens>
            <token id="20" string="how" />
            <token id="21" string="many" />
            <token id="22" string="stations" />
            <token id="23" string="picked" />
            <token id="24" string="up" />
            <token id="25" string="the" />
            <token id="26" string="broadcast" />
            <token id="27" string="was" />
            <token id="28" string="not" />
            <token id="29" string="available" />
          </tokens>
        </chunking>
        <chunking id="7" string="available" type="ADJP">
          <tokens>
            <token id="29" string="available" />
          </tokens>
        </chunking>
        <chunking id="8" string="an exact figure" type="NP">
          <tokens>
            <token id="16" string="an" />
            <token id="17" string="exact" />
            <token id="18" string="figure" />
          </tokens>
        </chunking>
        <chunking id="9" string="was not available" type="VP">
          <tokens>
            <token id="27" string="was" />
            <token id="28" string="not" />
            <token id="29" string="available" />
          </tokens>
        </chunking>
        <chunking id="10" string="broadcast nationwide via the Westwood One Network" type="VP">
          <tokens>
            <token id="4" string="broadcast" />
            <token id="5" string="nationwide" />
            <token id="6" string="via" />
            <token id="7" string="the" />
            <token id="8" string="Westwood" />
            <token id="9" string="One" />
            <token id="10" string="Network" />
          </tokens>
        </chunking>
        <chunking id="11" string="picked up the broadcast was not available" type="VP">
          <tokens>
            <token id="23" string="picked" />
            <token id="24" string="up" />
            <token id="25" string="the" />
            <token id="26" string="broadcast" />
            <token id="27" string="was" />
            <token id="28" string="not" />
            <token id="29" string="available" />
          </tokens>
        </chunking>
        <chunking id="12" string="said an exact figure on how many stations picked up the broadcast was not available" type="VP">
          <tokens>
            <token id="15" string="said" />
            <token id="16" string="an" />
            <token id="17" string="exact" />
            <token id="18" string="figure" />
            <token id="19" string="on" />
            <token id="20" string="how" />
            <token id="21" string="many" />
            <token id="22" string="stations" />
            <token id="23" string="picked" />
            <token id="24" string="up" />
            <token id="25" string="the" />
            <token id="26" string="broadcast" />
            <token id="27" string="was" />
            <token id="28" string="not" />
            <token id="29" string="available" />
          </tokens>
        </chunking>
        <chunking id="13" string="The interview" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="interview" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">interview</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="4">broadcast</governor>
          <dependent id="2">interview</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="4">broadcast</governor>
          <dependent id="3">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">broadcast</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="4">broadcast</governor>
          <dependent id="5">nationwide</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">Westwood</governor>
          <dependent id="6">via</dependent>
        </dependency>
        <dependency type="det">
          <governor id="8">Westwood</governor>
          <dependent id="7">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">nationwide</governor>
          <dependent id="8">Westwood</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="8">Westwood</governor>
          <dependent id="9">One</dependent>
        </dependency>
        <dependency type="nmod:tmod">
          <governor id="8">Westwood</governor>
          <dependent id="10">Network</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">Wilde</governor>
          <dependent id="12">spokeswoman</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">Wilde</governor>
          <dependent id="13">Laurie</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="15">said</governor>
          <dependent id="14">Wilde</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="4">broadcast</governor>
          <dependent id="15">said</dependent>
        </dependency>
        <dependency type="det">
          <governor id="18">figure</governor>
          <dependent id="16">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="18">figure</governor>
          <dependent id="17">exact</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">said</governor>
          <dependent id="18">figure</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="23">picked</governor>
          <dependent id="19">on</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="21">many</governor>
          <dependent id="20">how</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="22">stations</governor>
          <dependent id="21">many</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="23">picked</governor>
          <dependent id="22">stations</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="15">said</governor>
          <dependent id="23">picked</dependent>
        </dependency>
        <dependency type="compound:prt">
          <governor id="23">picked</governor>
          <dependent id="24">up</dependent>
        </dependency>
        <dependency type="det">
          <governor id="26">broadcast</governor>
          <dependent id="25">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="29">available</governor>
          <dependent id="26">broadcast</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="29">available</governor>
          <dependent id="27">was</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="29">available</governor>
          <dependent id="28">not</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="23">picked</governor>
          <dependent id="29">available</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Westwood One Network" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="8" string="Westwood" />
            <token id="9" string="One" />
            <token id="10" string="Network" />
          </tokens>
        </entity>
        <entity id="2" string="Laurie Wilde" type="PERSON" score="0.0">
          <tokens>
            <token id="13" string="Laurie" />
            <token id="14" string="Wilde" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="16" has_coreference="true">
      <content>Also appearing were Sean and Lennon other son, Julian.</content>
      <tokens>
        <token id="1" string="Also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="appearing" lemma="appear" stem="appear" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="Sean" lemma="Sean" stem="sean" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="5" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="7" string="other" lemma="other" stem="other" pos="JJ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="son" lemma="son" stem="son" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="Julian" lemma="Julian" stem="julian" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="11" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (SINV (ADVP (RB Also)) (VP (VBG appearing)) (VP (VBD were) (NP (NNP Sean) (CC and) (NNP Lennon))) (NP (NP (JJ other) (NN son)) (, ,) (NP (NNP Julian))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Sean and Lennon" type="NP">
          <tokens>
            <token id="4" string="Sean" />
            <token id="5" string="and" />
            <token id="6" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="2" string="were Sean and Lennon" type="VP">
          <tokens>
            <token id="3" string="were" />
            <token id="4" string="Sean" />
            <token id="5" string="and" />
            <token id="6" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="3" string="appearing" type="VP">
          <tokens>
            <token id="2" string="appearing" />
          </tokens>
        </chunking>
        <chunking id="4" string="other son" type="NP">
          <tokens>
            <token id="7" string="other" />
            <token id="8" string="son" />
          </tokens>
        </chunking>
        <chunking id="5" string="Julian" type="NP">
          <tokens>
            <token id="10" string="Julian" />
          </tokens>
        </chunking>
        <chunking id="6" string="other son , Julian" type="NP">
          <tokens>
            <token id="7" string="other" />
            <token id="8" string="son" />
            <token id="9" string="," />
            <token id="10" string="Julian" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="advmod">
          <governor id="2">appearing</governor>
          <dependent id="1">Also</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">appearing</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="4">Sean</governor>
          <dependent id="3">were</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="2">appearing</governor>
          <dependent id="4">Sean</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="4">Sean</governor>
          <dependent id="5">and</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="4">Sean</governor>
          <dependent id="6">Lennon</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">son</governor>
          <dependent id="7">other</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="2">appearing</governor>
          <dependent id="8">son</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="8">son</governor>
          <dependent id="10">Julian</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="6" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="Sean" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="Sean" />
          </tokens>
        </entity>
        <entity id="3" string="Julian" type="PERSON" score="0.0">
          <tokens>
            <token id="10" string="Julian" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="17" has_coreference="true">
      <content>Ono was interviewed by longtime associate Elliot Mintz, who is described in Goldman&amp;apost;s book as a spy for Yoko _ a charge Mintz denied during the show.</content>
      <tokens>
        <token id="1" string="Ono" lemma="Ono" stem="ono" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="interviewed" lemma="interview" stem="interview" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="longtime" lemma="longtime" stem="longtim" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="associate" lemma="associate" stem="associ" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="Elliot" lemma="Elliot" stem="elliot" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="8" string="Mintz" lemma="Mintz" stem="mintz" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="false" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="described" lemma="describe" stem="describ" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="15" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="16" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="18" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="spy" lemma="spy" stem="spy" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="Yoko" lemma="Yoko" stem="yoko" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="22" string="_" lemma="_" stem="_" pos="NNP" type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="23" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="24" string="charge" lemma="charge" stem="charg" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="25" string="Mintz" lemma="Mintz" stem="mintz" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="26" string="denied" lemma="deny" stem="deni" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="27" string="during" lemma="during" stem="dure" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="29" string="show" lemma="show" stem="show" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="30" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Ono)) (VP (VBD was) (VP (VBN interviewed) (PP (IN by) (NP (JJ longtime) (JJ associate) (NNP Elliot) (NNP Mintz))) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBZ is) (VP (VBN described) (PP (IN in) (NP (NP (NNP Goldman) (POS 's)) (NN book))) (PP (IN as) (NP (NP (DT a) (NN spy)) (PP (IN for) (NP (NP (NNP Yoko) (NNP _)) (SBAR (S (NP (DT a) (NN charge) (NNP Mintz)) (VP (VBD denied) (PP (IN during) (NP (DT the) (NN show)))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a charge Mintz" type="NP">
          <tokens>
            <token id="23" string="a" />
            <token id="24" string="charge" />
            <token id="25" string="Mintz" />
          </tokens>
        </chunking>
        <chunking id="2" string="was interviewed by longtime associate Elliot Mintz , who is described in Goldman 's book as a spy for Yoko _ a charge Mintz denied during the show" type="VP">
          <tokens>
            <token id="2" string="was" />
            <token id="3" string="interviewed" />
            <token id="4" string="by" />
            <token id="5" string="longtime" />
            <token id="6" string="associate" />
            <token id="7" string="Elliot" />
            <token id="8" string="Mintz" />
            <token id="9" string="," />
            <token id="10" string="who" />
            <token id="11" string="is" />
            <token id="12" string="described" />
            <token id="13" string="in" />
            <token id="14" string="Goldman" />
            <token id="15" string="'s" />
            <token id="16" string="book" />
            <token id="17" string="as" />
            <token id="18" string="a" />
            <token id="19" string="spy" />
            <token id="20" string="for" />
            <token id="21" string="Yoko" />
            <token id="22" string="_" />
            <token id="23" string="a" />
            <token id="24" string="charge" />
            <token id="25" string="Mintz" />
            <token id="26" string="denied" />
            <token id="27" string="during" />
            <token id="28" string="the" />
            <token id="29" string="show" />
          </tokens>
        </chunking>
        <chunking id="3" string="is described in Goldman 's book as a spy for Yoko _ a charge Mintz denied during the show" type="VP">
          <tokens>
            <token id="11" string="is" />
            <token id="12" string="described" />
            <token id="13" string="in" />
            <token id="14" string="Goldman" />
            <token id="15" string="'s" />
            <token id="16" string="book" />
            <token id="17" string="as" />
            <token id="18" string="a" />
            <token id="19" string="spy" />
            <token id="20" string="for" />
            <token id="21" string="Yoko" />
            <token id="22" string="_" />
            <token id="23" string="a" />
            <token id="24" string="charge" />
            <token id="25" string="Mintz" />
            <token id="26" string="denied" />
            <token id="27" string="during" />
            <token id="28" string="the" />
            <token id="29" string="show" />
          </tokens>
        </chunking>
        <chunking id="4" string="Goldman 's book" type="NP">
          <tokens>
            <token id="14" string="Goldman" />
            <token id="15" string="'s" />
            <token id="16" string="book" />
          </tokens>
        </chunking>
        <chunking id="5" string="who is described in Goldman 's book as a spy for Yoko _ a charge Mintz denied during the show" type="SBAR">
          <tokens>
            <token id="10" string="who" />
            <token id="11" string="is" />
            <token id="12" string="described" />
            <token id="13" string="in" />
            <token id="14" string="Goldman" />
            <token id="15" string="'s" />
            <token id="16" string="book" />
            <token id="17" string="as" />
            <token id="18" string="a" />
            <token id="19" string="spy" />
            <token id="20" string="for" />
            <token id="21" string="Yoko" />
            <token id="22" string="_" />
            <token id="23" string="a" />
            <token id="24" string="charge" />
            <token id="25" string="Mintz" />
            <token id="26" string="denied" />
            <token id="27" string="during" />
            <token id="28" string="the" />
            <token id="29" string="show" />
          </tokens>
        </chunking>
        <chunking id="6" string="described in Goldman 's book as a spy for Yoko _ a charge Mintz denied during the show" type="VP">
          <tokens>
            <token id="12" string="described" />
            <token id="13" string="in" />
            <token id="14" string="Goldman" />
            <token id="15" string="'s" />
            <token id="16" string="book" />
            <token id="17" string="as" />
            <token id="18" string="a" />
            <token id="19" string="spy" />
            <token id="20" string="for" />
            <token id="21" string="Yoko" />
            <token id="22" string="_" />
            <token id="23" string="a" />
            <token id="24" string="charge" />
            <token id="25" string="Mintz" />
            <token id="26" string="denied" />
            <token id="27" string="during" />
            <token id="28" string="the" />
            <token id="29" string="show" />
          </tokens>
        </chunking>
        <chunking id="7" string="a charge Mintz denied during the show" type="SBAR">
          <tokens>
            <token id="23" string="a" />
            <token id="24" string="charge" />
            <token id="25" string="Mintz" />
            <token id="26" string="denied" />
            <token id="27" string="during" />
            <token id="28" string="the" />
            <token id="29" string="show" />
          </tokens>
        </chunking>
        <chunking id="8" string="Yoko _ a charge Mintz denied during the show" type="NP">
          <tokens>
            <token id="21" string="Yoko" />
            <token id="22" string="_" />
            <token id="23" string="a" />
            <token id="24" string="charge" />
            <token id="25" string="Mintz" />
            <token id="26" string="denied" />
            <token id="27" string="during" />
            <token id="28" string="the" />
            <token id="29" string="show" />
          </tokens>
        </chunking>
        <chunking id="9" string="the show" type="NP">
          <tokens>
            <token id="28" string="the" />
            <token id="29" string="show" />
          </tokens>
        </chunking>
        <chunking id="10" string="Ono" type="NP">
          <tokens>
            <token id="1" string="Ono" />
          </tokens>
        </chunking>
        <chunking id="11" string="longtime associate Elliot Mintz" type="NP">
          <tokens>
            <token id="5" string="longtime" />
            <token id="6" string="associate" />
            <token id="7" string="Elliot" />
            <token id="8" string="Mintz" />
          </tokens>
        </chunking>
        <chunking id="12" string="Goldman 's" type="NP">
          <tokens>
            <token id="14" string="Goldman" />
            <token id="15" string="'s" />
          </tokens>
        </chunking>
        <chunking id="13" string="interviewed by longtime associate Elliot Mintz , who is described in Goldman 's book as a spy for Yoko _ a charge Mintz denied during the show" type="VP">
          <tokens>
            <token id="3" string="interviewed" />
            <token id="4" string="by" />
            <token id="5" string="longtime" />
            <token id="6" string="associate" />
            <token id="7" string="Elliot" />
            <token id="8" string="Mintz" />
            <token id="9" string="," />
            <token id="10" string="who" />
            <token id="11" string="is" />
            <token id="12" string="described" />
            <token id="13" string="in" />
            <token id="14" string="Goldman" />
            <token id="15" string="'s" />
            <token id="16" string="book" />
            <token id="17" string="as" />
            <token id="18" string="a" />
            <token id="19" string="spy" />
            <token id="20" string="for" />
            <token id="21" string="Yoko" />
            <token id="22" string="_" />
            <token id="23" string="a" />
            <token id="24" string="charge" />
            <token id="25" string="Mintz" />
            <token id="26" string="denied" />
            <token id="27" string="during" />
            <token id="28" string="the" />
            <token id="29" string="show" />
          </tokens>
        </chunking>
        <chunking id="14" string="Yoko _" type="NP">
          <tokens>
            <token id="21" string="Yoko" />
            <token id="22" string="_" />
          </tokens>
        </chunking>
        <chunking id="15" string="denied during the show" type="VP">
          <tokens>
            <token id="26" string="denied" />
            <token id="27" string="during" />
            <token id="28" string="the" />
            <token id="29" string="show" />
          </tokens>
        </chunking>
        <chunking id="16" string="a spy" type="NP">
          <tokens>
            <token id="18" string="a" />
            <token id="19" string="spy" />
          </tokens>
        </chunking>
        <chunking id="17" string="a spy for Yoko _ a charge Mintz denied during the show" type="NP">
          <tokens>
            <token id="18" string="a" />
            <token id="19" string="spy" />
            <token id="20" string="for" />
            <token id="21" string="Yoko" />
            <token id="22" string="_" />
            <token id="23" string="a" />
            <token id="24" string="charge" />
            <token id="25" string="Mintz" />
            <token id="26" string="denied" />
            <token id="27" string="during" />
            <token id="28" string="the" />
            <token id="29" string="show" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="3">interviewed</governor>
          <dependent id="1">Ono</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="3">interviewed</governor>
          <dependent id="2">was</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">interviewed</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">Mintz</governor>
          <dependent id="4">by</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">Mintz</governor>
          <dependent id="5">longtime</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="8">Mintz</governor>
          <dependent id="6">associate</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="8">Mintz</governor>
          <dependent id="7">Elliot</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="3">interviewed</governor>
          <dependent id="8">Mintz</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="12">described</governor>
          <dependent id="10">who</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="12">described</governor>
          <dependent id="11">is</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="3">interviewed</governor>
          <dependent id="12">described</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">book</governor>
          <dependent id="13">in</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="16">book</governor>
          <dependent id="14">Goldman</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">Goldman</governor>
          <dependent id="15">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">described</governor>
          <dependent id="16">book</dependent>
        </dependency>
        <dependency type="case">
          <governor id="19">spy</governor>
          <dependent id="17">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">spy</governor>
          <dependent id="18">a</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">described</governor>
          <dependent id="19">spy</dependent>
        </dependency>
        <dependency type="case">
          <governor id="22">_</governor>
          <dependent id="20">for</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="22">_</governor>
          <dependent id="21">Yoko</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">spy</governor>
          <dependent id="22">_</dependent>
        </dependency>
        <dependency type="det">
          <governor id="25">Mintz</governor>
          <dependent id="23">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="25">Mintz</governor>
          <dependent id="24">charge</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="26">denied</governor>
          <dependent id="25">Mintz</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="22">_</governor>
          <dependent id="26">denied</dependent>
        </dependency>
        <dependency type="case">
          <governor id="29">show</governor>
          <dependent id="27">during</dependent>
        </dependency>
        <dependency type="det">
          <governor id="29">show</governor>
          <dependent id="28">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="26">denied</governor>
          <dependent id="29">show</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Yoko" type="PERSON" score="0.0">
          <tokens>
            <token id="21" string="Yoko" />
          </tokens>
        </entity>
        <entity id="2" string="Elliot Mintz" type="PERSON" score="0.0">
          <tokens>
            <token id="7" string="Elliot" />
            <token id="8" string="Mintz" />
          </tokens>
        </entity>
        <entity id="3" string="Mintz" type="PERSON" score="0.0">
          <tokens>
            <token id="25" string="Mintz" />
          </tokens>
        </entity>
        <entity id="4" string="Goldman" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="14" string="Goldman" />
          </tokens>
        </entity>
        <entity id="5" string="Ono" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Ono" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="18" has_coreference="true">
      <content>``It is, for one, very upsetting, but most of all it is lies, untruths ... The whole thing is just sickening,&amp;apost;&amp;apost; said Julian, who also refuted Goldman&amp;apost;s tales of Lennon as a violent father.</content>
      <tokens>
        <token id="1" string="``" lemma="``" stem="``" pos="``" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="It" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="one" lemma="one" stem="on" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="7" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="very" lemma="very" stem="veri" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="upsetting" lemma="upset" stem="upset" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="10" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="12" string="most" lemma="most" stem="most" pos="JJS" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="13" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="14" string="all" lemma="all" stem="all" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="15" string="it" lemma="it" stem="it" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="lies" lemma="lie" stem="li" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="untruths" lemma="untruth" stem="untruth" pos="VBZ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="..." lemma="..." stem="..." pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="whole" lemma="whole" stem="whole" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="thing" lemma="thing" stem="thing" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="24" string="is" lemma="be" stem="i" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="just" lemma="just" stem="just" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="sickening" lemma="sickening" stem="sicken" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="27" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="28" string="''" lemma="''" stem="''" pos="''" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="said" lemma="say" stem="said" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="Julian" lemma="Julian" stem="julian" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="31" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="32" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="33" string="also" lemma="also" stem="also" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="34" string="refuted" lemma="refute" stem="refut" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="35" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="false" is_refers="true" />
        <token id="36" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="37" string="tales" lemma="tale" stem="tale" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="38" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="39" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="40" string="as" lemma="as" stem="a" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="41" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="42" string="violent" lemma="violent" stem="violent" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="43" string="father" lemma="father" stem="father" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="44" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (SINV (`` ``) (S (S (NP (PRP It)) (VP (VBZ is) (, ,) (PP (IN for) (NP (CD one))) (, ,) (ADJP (RB very) (S (VP (VBG upsetting)))))) (, ,) (CC but) (S (NP (NP (JJS most)) (PP (IN of) (NP (DT all)))) (PRN (S (NP (PRP it)) (VP (VBZ is) (VP (VBZ lies)))) (, ,)) (VP (VBZ untruths))) (: ...) (S (NP (DT The) (JJ whole) (NN thing)) (VP (VBZ is) (ADJP (RB just) (JJ sickening))))) (, ,) ('' '') (VP (VBD said)) (NP (NP (NNP Julian)) (, ,) (SBAR (WHNP (WP who)) (S (ADVP (RB also)) (VP (VBD refuted) (NP (NP (NP (NNP Goldman) (POS 's)) (NNS tales)) (PP (IN of) (NP (NNP Lennon)))) (PP (IN as) (NP (DT a) (JJ violent) (NN father))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="all" type="NP">
          <tokens>
            <token id="14" string="all" />
          </tokens>
        </chunking>
        <chunking id="2" string="one" type="NP">
          <tokens>
            <token id="6" string="one" />
          </tokens>
        </chunking>
        <chunking id="3" string="The whole thing" type="NP">
          <tokens>
            <token id="21" string="The" />
            <token id="22" string="whole" />
            <token id="23" string="thing" />
          </tokens>
        </chunking>
        <chunking id="4" string="a violent father" type="NP">
          <tokens>
            <token id="41" string="a" />
            <token id="42" string="violent" />
            <token id="43" string="father" />
          </tokens>
        </chunking>
        <chunking id="5" string="just sickening" type="ADJP">
          <tokens>
            <token id="25" string="just" />
            <token id="26" string="sickening" />
          </tokens>
        </chunking>
        <chunking id="6" string="it" type="NP">
          <tokens>
            <token id="15" string="it" />
          </tokens>
        </chunking>
        <chunking id="7" string="most" type="NP">
          <tokens>
            <token id="12" string="most" />
          </tokens>
        </chunking>
        <chunking id="8" string="is just sickening" type="VP">
          <tokens>
            <token id="24" string="is" />
            <token id="25" string="just" />
            <token id="26" string="sickening" />
          </tokens>
        </chunking>
        <chunking id="9" string="Goldman 's" type="NP">
          <tokens>
            <token id="35" string="Goldman" />
            <token id="36" string="'s" />
          </tokens>
        </chunking>
        <chunking id="10" string="Goldman 's tales" type="NP">
          <tokens>
            <token id="35" string="Goldman" />
            <token id="36" string="'s" />
            <token id="37" string="tales" />
          </tokens>
        </chunking>
        <chunking id="11" string="upsetting" type="VP">
          <tokens>
            <token id="9" string="upsetting" />
          </tokens>
        </chunking>
        <chunking id="12" string="Julian" type="NP">
          <tokens>
            <token id="30" string="Julian" />
          </tokens>
        </chunking>
        <chunking id="13" string="is lies" type="VP">
          <tokens>
            <token id="16" string="is" />
            <token id="17" string="lies" />
          </tokens>
        </chunking>
        <chunking id="14" string="Lennon" type="NP">
          <tokens>
            <token id="39" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="15" string="refuted Goldman 's tales of Lennon as a violent father" type="VP">
          <tokens>
            <token id="34" string="refuted" />
            <token id="35" string="Goldman" />
            <token id="36" string="'s" />
            <token id="37" string="tales" />
            <token id="38" string="of" />
            <token id="39" string="Lennon" />
            <token id="40" string="as" />
            <token id="41" string="a" />
            <token id="42" string="violent" />
            <token id="43" string="father" />
          </tokens>
        </chunking>
        <chunking id="16" string="Goldman 's tales of Lennon" type="NP">
          <tokens>
            <token id="35" string="Goldman" />
            <token id="36" string="'s" />
            <token id="37" string="tales" />
            <token id="38" string="of" />
            <token id="39" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="17" string="untruths" type="VP">
          <tokens>
            <token id="19" string="untruths" />
          </tokens>
        </chunking>
        <chunking id="18" string="most of all" type="NP">
          <tokens>
            <token id="12" string="most" />
            <token id="13" string="of" />
            <token id="14" string="all" />
          </tokens>
        </chunking>
        <chunking id="19" string="It" type="NP">
          <tokens>
            <token id="2" string="It" />
          </tokens>
        </chunking>
        <chunking id="20" string="very upsetting" type="ADJP">
          <tokens>
            <token id="8" string="very" />
            <token id="9" string="upsetting" />
          </tokens>
        </chunking>
        <chunking id="21" string="Julian , who also refuted Goldman 's tales of Lennon as a violent father" type="NP">
          <tokens>
            <token id="30" string="Julian" />
            <token id="31" string="," />
            <token id="32" string="who" />
            <token id="33" string="also" />
            <token id="34" string="refuted" />
            <token id="35" string="Goldman" />
            <token id="36" string="'s" />
            <token id="37" string="tales" />
            <token id="38" string="of" />
            <token id="39" string="Lennon" />
            <token id="40" string="as" />
            <token id="41" string="a" />
            <token id="42" string="violent" />
            <token id="43" string="father" />
          </tokens>
        </chunking>
        <chunking id="22" string="lies" type="VP">
          <tokens>
            <token id="17" string="lies" />
          </tokens>
        </chunking>
        <chunking id="23" string="is , for one , very upsetting" type="VP">
          <tokens>
            <token id="3" string="is" />
            <token id="4" string="," />
            <token id="5" string="for" />
            <token id="6" string="one" />
            <token id="7" string="," />
            <token id="8" string="very" />
            <token id="9" string="upsetting" />
          </tokens>
        </chunking>
        <chunking id="24" string="who also refuted Goldman 's tales of Lennon as a violent father" type="SBAR">
          <tokens>
            <token id="32" string="who" />
            <token id="33" string="also" />
            <token id="34" string="refuted" />
            <token id="35" string="Goldman" />
            <token id="36" string="'s" />
            <token id="37" string="tales" />
            <token id="38" string="of" />
            <token id="39" string="Lennon" />
            <token id="40" string="as" />
            <token id="41" string="a" />
            <token id="42" string="violent" />
            <token id="43" string="father" />
          </tokens>
        </chunking>
        <chunking id="25" string="said" type="VP">
          <tokens>
            <token id="29" string="said" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="8">very</governor>
          <dependent id="2">It</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="8">very</governor>
          <dependent id="3">is</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">one</governor>
          <dependent id="5">for</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">very</governor>
          <dependent id="6">one</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="29">said</governor>
          <dependent id="8">very</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="8">very</governor>
          <dependent id="9">upsetting</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="8">very</governor>
          <dependent id="11">but</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="19">untruths</governor>
          <dependent id="12">most</dependent>
        </dependency>
        <dependency type="case">
          <governor id="14">all</governor>
          <dependent id="13">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">most</governor>
          <dependent id="14">all</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="17">lies</governor>
          <dependent id="15">it</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="17">lies</governor>
          <dependent id="16">is</dependent>
        </dependency>
        <dependency type="parataxis">
          <governor id="19">untruths</governor>
          <dependent id="17">lies</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="8">very</governor>
          <dependent id="19">untruths</dependent>
        </dependency>
        <dependency type="det">
          <governor id="23">thing</governor>
          <dependent id="21">The</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="23">thing</governor>
          <dependent id="22">whole</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="26">sickening</governor>
          <dependent id="23">thing</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="26">sickening</governor>
          <dependent id="24">is</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="26">sickening</governor>
          <dependent id="25">just</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="8">very</governor>
          <dependent id="26">sickening</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="29">said</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="29">said</governor>
          <dependent id="30">Julian</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="34">refuted</governor>
          <dependent id="32">who</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="34">refuted</governor>
          <dependent id="33">also</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="30">Julian</governor>
          <dependent id="34">refuted</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="37">tales</governor>
          <dependent id="35">Goldman</dependent>
        </dependency>
        <dependency type="case">
          <governor id="35">Goldman</governor>
          <dependent id="36">'s</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="34">refuted</governor>
          <dependent id="37">tales</dependent>
        </dependency>
        <dependency type="case">
          <governor id="39">Lennon</governor>
          <dependent id="38">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="37">tales</governor>
          <dependent id="39">Lennon</dependent>
        </dependency>
        <dependency type="case">
          <governor id="43">father</governor>
          <dependent id="40">as</dependent>
        </dependency>
        <dependency type="det">
          <governor id="43">father</governor>
          <dependent id="41">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="43">father</governor>
          <dependent id="42">violent</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="34">refuted</governor>
          <dependent id="43">father</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="one" type="NUMBER" score="0.0">
          <tokens>
            <token id="6" string="one" />
          </tokens>
        </entity>
        <entity id="2" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="39" string="Lennon" />
          </tokens>
        </entity>
        <entity id="3" string="Goldman" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="35" string="Goldman" />
          </tokens>
        </entity>
        <entity id="4" string="Julian" type="PERSON" score="0.0">
          <tokens>
            <token id="30" string="Julian" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="19" has_coreference="true">
      <content>Mintz read excerpts from the book to Yoko, who then responded to the allegations point by point.</content>
      <tokens>
        <token id="1" string="Mintz" lemma="Mintz" stem="mintz" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="read" lemma="read" stem="read" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="excerpts" lemma="excerpt" stem="excerpt" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="7" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="Yoko" lemma="Yoko" stem="yoko" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="9" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="then" lemma="then" stem="then" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="12" string="responded" lemma="respond" stem="respond" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="13" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="14" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="15" string="allegations" lemma="allegation" stem="alleg" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="16" string="point" lemma="point" stem="point" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="17" string="by" lemma="by" stem="by" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="18" string="point" lemma="point" stem="point" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="19" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNP Mintz)) (VP (VBD read) (NP (NNS excerpts)) (PP (IN from) (NP (DT the) (NN book))) (PP (TO to) (NP (NP (NNP Yoko)) (, ,) (SBAR (WHNP (WP who)) (S (ADVP (RB then)) (VP (VBD responded) (PP (TO to) (NP (DT the) (NNS allegations) (NN point))) (PP (IN by) (NP (NN point))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Yoko , who then responded to the allegations point by point" type="NP">
          <tokens>
            <token id="8" string="Yoko" />
            <token id="9" string="," />
            <token id="10" string="who" />
            <token id="11" string="then" />
            <token id="12" string="responded" />
            <token id="13" string="to" />
            <token id="14" string="the" />
            <token id="15" string="allegations" />
            <token id="16" string="point" />
            <token id="17" string="by" />
            <token id="18" string="point" />
          </tokens>
        </chunking>
        <chunking id="2" string="who then responded to the allegations point by point" type="SBAR">
          <tokens>
            <token id="10" string="who" />
            <token id="11" string="then" />
            <token id="12" string="responded" />
            <token id="13" string="to" />
            <token id="14" string="the" />
            <token id="15" string="allegations" />
            <token id="16" string="point" />
            <token id="17" string="by" />
            <token id="18" string="point" />
          </tokens>
        </chunking>
        <chunking id="3" string="point" type="NP">
          <tokens>
            <token id="18" string="point" />
          </tokens>
        </chunking>
        <chunking id="4" string="Yoko" type="NP">
          <tokens>
            <token id="8" string="Yoko" />
          </tokens>
        </chunking>
        <chunking id="5" string="excerpts" type="NP">
          <tokens>
            <token id="3" string="excerpts" />
          </tokens>
        </chunking>
        <chunking id="6" string="responded to the allegations point by point" type="VP">
          <tokens>
            <token id="12" string="responded" />
            <token id="13" string="to" />
            <token id="14" string="the" />
            <token id="15" string="allegations" />
            <token id="16" string="point" />
            <token id="17" string="by" />
            <token id="18" string="point" />
          </tokens>
        </chunking>
        <chunking id="7" string="the allegations point" type="NP">
          <tokens>
            <token id="14" string="the" />
            <token id="15" string="allegations" />
            <token id="16" string="point" />
          </tokens>
        </chunking>
        <chunking id="8" string="read excerpts from the book to Yoko , who then responded to the allegations point by point" type="VP">
          <tokens>
            <token id="2" string="read" />
            <token id="3" string="excerpts" />
            <token id="4" string="from" />
            <token id="5" string="the" />
            <token id="6" string="book" />
            <token id="7" string="to" />
            <token id="8" string="Yoko" />
            <token id="9" string="," />
            <token id="10" string="who" />
            <token id="11" string="then" />
            <token id="12" string="responded" />
            <token id="13" string="to" />
            <token id="14" string="the" />
            <token id="15" string="allegations" />
            <token id="16" string="point" />
            <token id="17" string="by" />
            <token id="18" string="point" />
          </tokens>
        </chunking>
        <chunking id="9" string="Mintz" type="NP">
          <tokens>
            <token id="1" string="Mintz" />
          </tokens>
        </chunking>
        <chunking id="10" string="the book" type="NP">
          <tokens>
            <token id="5" string="the" />
            <token id="6" string="book" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">read</governor>
          <dependent id="1">Mintz</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">read</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="2">read</governor>
          <dependent id="3">excerpts</dependent>
        </dependency>
        <dependency type="case">
          <governor id="6">book</governor>
          <dependent id="4">from</dependent>
        </dependency>
        <dependency type="det">
          <governor id="6">book</governor>
          <dependent id="5">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">read</governor>
          <dependent id="6">book</dependent>
        </dependency>
        <dependency type="case">
          <governor id="8">Yoko</governor>
          <dependent id="7">to</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="2">read</governor>
          <dependent id="8">Yoko</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="12">responded</governor>
          <dependent id="10">who</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="12">responded</governor>
          <dependent id="11">then</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="8">Yoko</governor>
          <dependent id="12">responded</dependent>
        </dependency>
        <dependency type="case">
          <governor id="16">point</governor>
          <dependent id="13">to</dependent>
        </dependency>
        <dependency type="det">
          <governor id="16">point</governor>
          <dependent id="14">the</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="16">point</governor>
          <dependent id="15">allegations</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">responded</governor>
          <dependent id="16">point</dependent>
        </dependency>
        <dependency type="case">
          <governor id="18">point</governor>
          <dependent id="17">by</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="12">responded</governor>
          <dependent id="18">point</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Yoko" type="PERSON" score="0.0">
          <tokens>
            <token id="8" string="Yoko" />
          </tokens>
        </entity>
        <entity id="2" string="Mintz" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Mintz" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="20" has_coreference="true">
      <content>She denied that Lennon was homosexual; that he was using drugs while living at the Dakota with his son, Sean; that he had become a reclusive rock &amp;apost;n&amp;apost; roll Howard Hughes.</content>
      <tokens>
        <token id="1" string="She" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="denied" lemma="deny" stem="deni" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="3" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="5" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="homosexual" lemma="homosexual" stem="homosexu" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string=";" lemma=";" stem=";" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="11" string="using" lemma="use" stem="us" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="drugs" lemma="drug" stem="drug" pos="NNS" type="Word" isStopWord="false" ner="CAUSE_OF_DEATH" is_referenced="false" is_refers="false" />
        <token id="13" string="while" lemma="while" stem="while" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="living" lemma="live" stem="live" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="at" lemma="at" stem="at" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="Dakota" lemma="Dakota" stem="dakota" pos="NNP" type="Word" isStopWord="false" ner="LOCATION" is_referenced="false" is_refers="false" />
        <token id="18" string="with" lemma="with" stem="with" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="20" string="son" lemma="son" stem="son" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="21" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="22" string="Sean" lemma="Sean" stem="sean" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="23" string=";" lemma=";" stem=";" pos=":" type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="25" string="he" lemma="he" stem="he" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="26" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="27" string="become" lemma="become" stem="becom" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="28" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="29" string="reclusive" lemma="reclusive" stem="reclus" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="30" string="rock" lemma="rock" stem="rock" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="31" string="'n'" lemma="'n'" stem="'n'" pos="CC" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="roll" lemma="roll" stem="roll" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="33" string="Howard" lemma="Howard" stem="howard" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="34" string="Hughes" lemma="Hughes" stem="hugh" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="35" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP She)) (VP (VBD denied) (SBAR (SBAR (IN that) (S (NP (NNP Lennon)) (VP (VBD was) (ADJP (JJ homosexual))))) (: ;) (SBAR (IN that) (S (NP (PRP he)) (VP (VBD was) (VP (VBG using) (NP (NNS drugs)) (PP (IN while) (S (VP (VBG living) (PP (IN at) (NP (DT the) (NNP Dakota))) (PP (IN with) (NP (NP (PRP$ his) (NN son)) (, ,) (NP (NNP Sean))))))))))) (: ;) (SBAR (IN that) (S (NP (PRP he)) (VP (VBD had) (VP (VBN become) (NP (NP (DT a) (JJ reclusive) (NN rock)) (CC 'n') (NP (NN roll) (NNP Howard) (NNP Hughes))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="his son , Sean" type="NP">
          <tokens>
            <token id="19" string="his" />
            <token id="20" string="son" />
            <token id="21" string="," />
            <token id="22" string="Sean" />
          </tokens>
        </chunking>
        <chunking id="2" string="Lennon" type="NP">
          <tokens>
            <token id="4" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="3" string="drugs" type="NP">
          <tokens>
            <token id="12" string="drugs" />
          </tokens>
        </chunking>
        <chunking id="4" string="living at the Dakota with his son , Sean" type="VP">
          <tokens>
            <token id="14" string="living" />
            <token id="15" string="at" />
            <token id="16" string="the" />
            <token id="17" string="Dakota" />
            <token id="18" string="with" />
            <token id="19" string="his" />
            <token id="20" string="son" />
            <token id="21" string="," />
            <token id="22" string="Sean" />
          </tokens>
        </chunking>
        <chunking id="5" string="was homosexual" type="VP">
          <tokens>
            <token id="5" string="was" />
            <token id="6" string="homosexual" />
          </tokens>
        </chunking>
        <chunking id="6" string="denied that Lennon was homosexual ; that he was using drugs while living at the Dakota with his son , Sean ; that he had become a reclusive rock 'n' roll Howard Hughes" type="VP">
          <tokens>
            <token id="2" string="denied" />
            <token id="3" string="that" />
            <token id="4" string="Lennon" />
            <token id="5" string="was" />
            <token id="6" string="homosexual" />
            <token id="7" string=";" />
            <token id="8" string="that" />
            <token id="9" string="he" />
            <token id="10" string="was" />
            <token id="11" string="using" />
            <token id="12" string="drugs" />
            <token id="13" string="while" />
            <token id="14" string="living" />
            <token id="15" string="at" />
            <token id="16" string="the" />
            <token id="17" string="Dakota" />
            <token id="18" string="with" />
            <token id="19" string="his" />
            <token id="20" string="son" />
            <token id="21" string="," />
            <token id="22" string="Sean" />
            <token id="23" string=";" />
            <token id="24" string="that" />
            <token id="25" string="he" />
            <token id="26" string="had" />
            <token id="27" string="become" />
            <token id="28" string="a" />
            <token id="29" string="reclusive" />
            <token id="30" string="rock" />
            <token id="31" string="'n'" />
            <token id="32" string="roll" />
            <token id="33" string="Howard" />
            <token id="34" string="Hughes" />
          </tokens>
        </chunking>
        <chunking id="7" string="a reclusive rock 'n' roll Howard Hughes" type="NP">
          <tokens>
            <token id="28" string="a" />
            <token id="29" string="reclusive" />
            <token id="30" string="rock" />
            <token id="31" string="'n'" />
            <token id="32" string="roll" />
            <token id="33" string="Howard" />
            <token id="34" string="Hughes" />
          </tokens>
        </chunking>
        <chunking id="8" string="that Lennon was homosexual ; that he was using drugs while living at the Dakota with his son , Sean ; that he had become a reclusive rock 'n' roll Howard Hughes" type="SBAR">
          <tokens>
            <token id="3" string="that" />
            <token id="4" string="Lennon" />
            <token id="5" string="was" />
            <token id="6" string="homosexual" />
            <token id="7" string=";" />
            <token id="8" string="that" />
            <token id="9" string="he" />
            <token id="10" string="was" />
            <token id="11" string="using" />
            <token id="12" string="drugs" />
            <token id="13" string="while" />
            <token id="14" string="living" />
            <token id="15" string="at" />
            <token id="16" string="the" />
            <token id="17" string="Dakota" />
            <token id="18" string="with" />
            <token id="19" string="his" />
            <token id="20" string="son" />
            <token id="21" string="," />
            <token id="22" string="Sean" />
            <token id="23" string=";" />
            <token id="24" string="that" />
            <token id="25" string="he" />
            <token id="26" string="had" />
            <token id="27" string="become" />
            <token id="28" string="a" />
            <token id="29" string="reclusive" />
            <token id="30" string="rock" />
            <token id="31" string="'n'" />
            <token id="32" string="roll" />
            <token id="33" string="Howard" />
            <token id="34" string="Hughes" />
          </tokens>
        </chunking>
        <chunking id="9" string="She" type="NP">
          <tokens>
            <token id="1" string="She" />
          </tokens>
        </chunking>
        <chunking id="10" string="that Lennon was homosexual" type="SBAR">
          <tokens>
            <token id="3" string="that" />
            <token id="4" string="Lennon" />
            <token id="5" string="was" />
            <token id="6" string="homosexual" />
          </tokens>
        </chunking>
        <chunking id="11" string="had become a reclusive rock 'n' roll Howard Hughes" type="VP">
          <tokens>
            <token id="26" string="had" />
            <token id="27" string="become" />
            <token id="28" string="a" />
            <token id="29" string="reclusive" />
            <token id="30" string="rock" />
            <token id="31" string="'n'" />
            <token id="32" string="roll" />
            <token id="33" string="Howard" />
            <token id="34" string="Hughes" />
          </tokens>
        </chunking>
        <chunking id="12" string="a reclusive rock" type="NP">
          <tokens>
            <token id="28" string="a" />
            <token id="29" string="reclusive" />
            <token id="30" string="rock" />
          </tokens>
        </chunking>
        <chunking id="13" string="that he had become a reclusive rock 'n' roll Howard Hughes" type="SBAR">
          <tokens>
            <token id="24" string="that" />
            <token id="25" string="he" />
            <token id="26" string="had" />
            <token id="27" string="become" />
            <token id="28" string="a" />
            <token id="29" string="reclusive" />
            <token id="30" string="rock" />
            <token id="31" string="'n'" />
            <token id="32" string="roll" />
            <token id="33" string="Howard" />
            <token id="34" string="Hughes" />
          </tokens>
        </chunking>
        <chunking id="14" string="homosexual" type="ADJP">
          <tokens>
            <token id="6" string="homosexual" />
          </tokens>
        </chunking>
        <chunking id="15" string="the Dakota" type="NP">
          <tokens>
            <token id="16" string="the" />
            <token id="17" string="Dakota" />
          </tokens>
        </chunking>
        <chunking id="16" string="Sean" type="NP">
          <tokens>
            <token id="22" string="Sean" />
          </tokens>
        </chunking>
        <chunking id="17" string="his son" type="NP">
          <tokens>
            <token id="19" string="his" />
            <token id="20" string="son" />
          </tokens>
        </chunking>
        <chunking id="18" string="he" type="NP">
          <tokens>
            <token id="9" string="he" />
          </tokens>
        </chunking>
        <chunking id="19" string="become a reclusive rock 'n' roll Howard Hughes" type="VP">
          <tokens>
            <token id="27" string="become" />
            <token id="28" string="a" />
            <token id="29" string="reclusive" />
            <token id="30" string="rock" />
            <token id="31" string="'n'" />
            <token id="32" string="roll" />
            <token id="33" string="Howard" />
            <token id="34" string="Hughes" />
          </tokens>
        </chunking>
        <chunking id="20" string="roll Howard Hughes" type="NP">
          <tokens>
            <token id="32" string="roll" />
            <token id="33" string="Howard" />
            <token id="34" string="Hughes" />
          </tokens>
        </chunking>
        <chunking id="21" string="using drugs while living at the Dakota with his son , Sean" type="VP">
          <tokens>
            <token id="11" string="using" />
            <token id="12" string="drugs" />
            <token id="13" string="while" />
            <token id="14" string="living" />
            <token id="15" string="at" />
            <token id="16" string="the" />
            <token id="17" string="Dakota" />
            <token id="18" string="with" />
            <token id="19" string="his" />
            <token id="20" string="son" />
            <token id="21" string="," />
            <token id="22" string="Sean" />
          </tokens>
        </chunking>
        <chunking id="22" string="was using drugs while living at the Dakota with his son , Sean" type="VP">
          <tokens>
            <token id="10" string="was" />
            <token id="11" string="using" />
            <token id="12" string="drugs" />
            <token id="13" string="while" />
            <token id="14" string="living" />
            <token id="15" string="at" />
            <token id="16" string="the" />
            <token id="17" string="Dakota" />
            <token id="18" string="with" />
            <token id="19" string="his" />
            <token id="20" string="son" />
            <token id="21" string="," />
            <token id="22" string="Sean" />
          </tokens>
        </chunking>
        <chunking id="23" string="that he was using drugs while living at the Dakota with his son , Sean" type="SBAR">
          <tokens>
            <token id="8" string="that" />
            <token id="9" string="he" />
            <token id="10" string="was" />
            <token id="11" string="using" />
            <token id="12" string="drugs" />
            <token id="13" string="while" />
            <token id="14" string="living" />
            <token id="15" string="at" />
            <token id="16" string="the" />
            <token id="17" string="Dakota" />
            <token id="18" string="with" />
            <token id="19" string="his" />
            <token id="20" string="son" />
            <token id="21" string="," />
            <token id="22" string="Sean" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="2">denied</governor>
          <dependent id="1">She</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="2">denied</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="6">homosexual</governor>
          <dependent id="3">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="6">homosexual</governor>
          <dependent id="4">Lennon</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="6">homosexual</governor>
          <dependent id="5">was</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="2">denied</governor>
          <dependent id="6">homosexual</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="11">using</governor>
          <dependent id="8">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="11">using</governor>
          <dependent id="9">he</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="11">using</governor>
          <dependent id="10">was</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="6">homosexual</governor>
          <dependent id="11">using</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="11">using</governor>
          <dependent id="12">drugs</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="14">living</governor>
          <dependent id="13">while</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="11">using</governor>
          <dependent id="14">living</dependent>
        </dependency>
        <dependency type="case">
          <governor id="17">Dakota</governor>
          <dependent id="15">at</dependent>
        </dependency>
        <dependency type="det">
          <governor id="17">Dakota</governor>
          <dependent id="16">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">living</governor>
          <dependent id="17">Dakota</dependent>
        </dependency>
        <dependency type="case">
          <governor id="20">son</governor>
          <dependent id="18">with</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="20">son</governor>
          <dependent id="19">his</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="14">living</governor>
          <dependent id="20">son</dependent>
        </dependency>
        <dependency type="appos">
          <governor id="20">son</governor>
          <dependent id="22">Sean</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="27">become</governor>
          <dependent id="24">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="27">become</governor>
          <dependent id="25">he</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="27">become</governor>
          <dependent id="26">had</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="6">homosexual</governor>
          <dependent id="27">become</dependent>
        </dependency>
        <dependency type="det">
          <governor id="30">rock</governor>
          <dependent id="28">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="30">rock</governor>
          <dependent id="29">reclusive</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="27">become</governor>
          <dependent id="30">rock</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="30">rock</governor>
          <dependent id="31">'n'</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="34">Hughes</governor>
          <dependent id="32">roll</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="34">Hughes</governor>
          <dependent id="33">Howard</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="30">rock</governor>
          <dependent id="34">Hughes</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="4" string="Lennon" />
          </tokens>
        </entity>
        <entity id="2" string="drugs" type="CAUSE_OF_DEATH" score="0.0">
          <tokens>
            <token id="12" string="drugs" />
          </tokens>
        </entity>
        <entity id="3" string="Sean" type="PERSON" score="0.0">
          <tokens>
            <token id="22" string="Sean" />
          </tokens>
        </entity>
        <entity id="4" string="Dakota" type="LOCATION" score="0.0">
          <tokens>
            <token id="17" string="Dakota" />
          </tokens>
        </entity>
        <entity id="5" string="Howard Hughes" type="PERSON" score="0.0">
          <tokens>
            <token id="33" string="Howard" />
            <token id="34" string="Hughes" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="21" has_coreference="true">
      <content>She did acknowledge having a heroin problem during the late 1970s, but rejected Goldman&amp;apost;s assertion that she had a $5,000-a-week habit.</content>
      <tokens>
        <token id="1" string="She" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="did" lemma="do" stem="did" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="acknowledge" lemma="acknowledge" stem="acknowledg" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="4" string="having" lemma="have" stem="have" pos="VBG" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="5" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="heroin" lemma="heroin" stem="heroin" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="problem" lemma="problem" stem="problem" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="during" lemma="during" stem="dure" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="late" lemma="late" stem="late" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="1970s" lemma="1970" stem="1970" pos="NNS" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="12" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="but" lemma="but" stem="but" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="rejected" lemma="reject" stem="reject" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="15" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="16" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="17" string="assertion" lemma="assertion" stem="assert" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="that" lemma="that" stem="that" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="she" lemma="she" stem="she" pos="PRP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="20" string="had" lemma="have" stem="had" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="22" string="$" lemma="$" stem="$" pos="$" type="Symbol" isStopWord="true" ner="MONEY" is_referenced="false" is_refers="false" />
        <token id="23" string="5,000-a-week" lemma="5,000-a-week" stem="5,000-a-week" pos="JJ" type="Word" isStopWord="false" ner="MONEY" is_referenced="false" is_refers="false" />
        <token id="24" string="habit" lemma="habit" stem="habit" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (PRP She)) (VP (VP (VBD did) (VP (VB acknowledge) (S (VP (VBG having) (NP (DT a) (NN heroin) (NN problem)) (PP (IN during) (NP (DT the) (JJ late) (NNS 1970s))))))) (, ,) (CC but) (VP (VBD rejected) (NP (NP (NNP Goldman) (POS 's)) (NN assertion) (SBAR (IN that) (S (NP (PRP she)) (VP (VBD had) (NP (DT a) (ADJP ($ $) (JJ 5,000-a-week)) (NN habit)))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a heroin problem" type="NP">
          <tokens>
            <token id="5" string="a" />
            <token id="6" string="heroin" />
            <token id="7" string="problem" />
          </tokens>
        </chunking>
        <chunking id="2" string="$ 5,000-a-week" type="ADJP">
          <tokens>
            <token id="22" string="$" />
            <token id="23" string="5,000-a-week" />
          </tokens>
        </chunking>
        <chunking id="3" string="Goldman 's assertion that she had a $ 5,000-a-week habit" type="NP">
          <tokens>
            <token id="15" string="Goldman" />
            <token id="16" string="'s" />
            <token id="17" string="assertion" />
            <token id="18" string="that" />
            <token id="19" string="she" />
            <token id="20" string="had" />
            <token id="21" string="a" />
            <token id="22" string="$" />
            <token id="23" string="5,000-a-week" />
            <token id="24" string="habit" />
          </tokens>
        </chunking>
        <chunking id="4" string="that she had a $ 5,000-a-week habit" type="SBAR">
          <tokens>
            <token id="18" string="that" />
            <token id="19" string="she" />
            <token id="20" string="had" />
            <token id="21" string="a" />
            <token id="22" string="$" />
            <token id="23" string="5,000-a-week" />
            <token id="24" string="habit" />
          </tokens>
        </chunking>
        <chunking id="5" string="had a $ 5,000-a-week habit" type="VP">
          <tokens>
            <token id="20" string="had" />
            <token id="21" string="a" />
            <token id="22" string="$" />
            <token id="23" string="5,000-a-week" />
            <token id="24" string="habit" />
          </tokens>
        </chunking>
        <chunking id="6" string="She" type="NP">
          <tokens>
            <token id="1" string="She" />
          </tokens>
        </chunking>
        <chunking id="7" string="she" type="NP">
          <tokens>
            <token id="19" string="she" />
          </tokens>
        </chunking>
        <chunking id="8" string="the late 1970s" type="NP">
          <tokens>
            <token id="9" string="the" />
            <token id="10" string="late" />
            <token id="11" string="1970s" />
          </tokens>
        </chunking>
        <chunking id="9" string="did acknowledge having a heroin problem during the late 1970s" type="VP">
          <tokens>
            <token id="2" string="did" />
            <token id="3" string="acknowledge" />
            <token id="4" string="having" />
            <token id="5" string="a" />
            <token id="6" string="heroin" />
            <token id="7" string="problem" />
            <token id="8" string="during" />
            <token id="9" string="the" />
            <token id="10" string="late" />
            <token id="11" string="1970s" />
          </tokens>
        </chunking>
        <chunking id="10" string="having a heroin problem during the late 1970s" type="VP">
          <tokens>
            <token id="4" string="having" />
            <token id="5" string="a" />
            <token id="6" string="heroin" />
            <token id="7" string="problem" />
            <token id="8" string="during" />
            <token id="9" string="the" />
            <token id="10" string="late" />
            <token id="11" string="1970s" />
          </tokens>
        </chunking>
        <chunking id="11" string="Goldman 's" type="NP">
          <tokens>
            <token id="15" string="Goldman" />
            <token id="16" string="'s" />
          </tokens>
        </chunking>
        <chunking id="12" string="rejected Goldman 's assertion that she had a $ 5,000-a-week habit" type="VP">
          <tokens>
            <token id="14" string="rejected" />
            <token id="15" string="Goldman" />
            <token id="16" string="'s" />
            <token id="17" string="assertion" />
            <token id="18" string="that" />
            <token id="19" string="she" />
            <token id="20" string="had" />
            <token id="21" string="a" />
            <token id="22" string="$" />
            <token id="23" string="5,000-a-week" />
            <token id="24" string="habit" />
          </tokens>
        </chunking>
        <chunking id="13" string="a $ 5,000-a-week habit" type="NP">
          <tokens>
            <token id="21" string="a" />
            <token id="22" string="$" />
            <token id="23" string="5,000-a-week" />
            <token id="24" string="habit" />
          </tokens>
        </chunking>
        <chunking id="14" string="did acknowledge having a heroin problem during the late 1970s , but rejected Goldman 's assertion that she had a $ 5,000-a-week habit" type="VP">
          <tokens>
            <token id="2" string="did" />
            <token id="3" string="acknowledge" />
            <token id="4" string="having" />
            <token id="5" string="a" />
            <token id="6" string="heroin" />
            <token id="7" string="problem" />
            <token id="8" string="during" />
            <token id="9" string="the" />
            <token id="10" string="late" />
            <token id="11" string="1970s" />
            <token id="12" string="," />
            <token id="13" string="but" />
            <token id="14" string="rejected" />
            <token id="15" string="Goldman" />
            <token id="16" string="'s" />
            <token id="17" string="assertion" />
            <token id="18" string="that" />
            <token id="19" string="she" />
            <token id="20" string="had" />
            <token id="21" string="a" />
            <token id="22" string="$" />
            <token id="23" string="5,000-a-week" />
            <token id="24" string="habit" />
          </tokens>
        </chunking>
        <chunking id="15" string="acknowledge having a heroin problem during the late 1970s" type="VP">
          <tokens>
            <token id="3" string="acknowledge" />
            <token id="4" string="having" />
            <token id="5" string="a" />
            <token id="6" string="heroin" />
            <token id="7" string="problem" />
            <token id="8" string="during" />
            <token id="9" string="the" />
            <token id="10" string="late" />
            <token id="11" string="1970s" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="3">acknowledge</governor>
          <dependent id="1">She</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="3">acknowledge</governor>
          <dependent id="2">did</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="3">acknowledge</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="3">acknowledge</governor>
          <dependent id="4">having</dependent>
        </dependency>
        <dependency type="det">
          <governor id="7">problem</governor>
          <dependent id="5">a</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="7">problem</governor>
          <dependent id="6">heroin</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">having</governor>
          <dependent id="7">problem</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">1970s</governor>
          <dependent id="8">during</dependent>
        </dependency>
        <dependency type="det">
          <governor id="11">1970s</governor>
          <dependent id="9">the</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="11">1970s</governor>
          <dependent id="10">late</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">having</governor>
          <dependent id="11">1970s</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="3">acknowledge</governor>
          <dependent id="13">but</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="3">acknowledge</governor>
          <dependent id="14">rejected</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="17">assertion</governor>
          <dependent id="15">Goldman</dependent>
        </dependency>
        <dependency type="case">
          <governor id="15">Goldman</governor>
          <dependent id="16">'s</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="14">rejected</governor>
          <dependent id="17">assertion</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="20">had</governor>
          <dependent id="18">that</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="20">had</governor>
          <dependent id="19">she</dependent>
        </dependency>
        <dependency type="ccomp">
          <governor id="17">assertion</governor>
          <dependent id="20">had</dependent>
        </dependency>
        <dependency type="det">
          <governor id="24">habit</governor>
          <dependent id="21">a</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="24">habit</governor>
          <dependent id="22">$</dependent>
        </dependency>
        <dependency type="dep">
          <governor id="22">$</governor>
          <dependent id="23">5,000-a-week</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="20">had</governor>
          <dependent id="24">habit</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="$ 5,000-a-week" type="MONEY" score="0.0">
          <tokens>
            <token id="22" string="$" />
            <token id="23" string="5,000-a-week" />
          </tokens>
        </entity>
        <entity id="2" string="1970s" type="DATE" score="0.0">
          <tokens>
            <token id="11" string="1970s" />
          </tokens>
        </entity>
        <entity id="3" string="Goldman" type="PERSON" score="0.0">
          <tokens>
            <token id="15" string="Goldman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="22" has_coreference="true">
      <content>The book has already drawn angry responses from friends of Lennon and Ono McCartney, who contradicted one anecdote involving Lennon smashing a painting in Paul&amp;apost;s home, has urged a boycott of the book.</content>
      <tokens>
        <token id="1" string="The" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="2" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="3" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="already" lemma="already" stem="alreadi" pos="RB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="drawn" lemma="draw" stem="drawn" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="6" string="angry" lemma="angry" stem="angri" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="responses" lemma="response" stem="respons" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="8" string="from" lemma="from" stem="from" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="9" string="friends" lemma="friend" stem="friend" pos="NNS" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="10" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="11" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="true" is_refers="true" />
        <token id="12" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="13" string="Ono" lemma="Ono" stem="ono" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="14" string="McCartney" lemma="McCartney" stem="mccartnei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="15" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="16" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="contradicted" lemma="contradict" stem="contradict" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="one" lemma="one" stem="on" pos="CD" type="Word" isStopWord="true" ner="NUMBER" is_referenced="false" is_refers="false" />
        <token id="19" string="anecdote" lemma="anecdote" stem="anecdot" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="involving" lemma="involve" stem="involv" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="21" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="22" string="smashing" lemma="smash" stem="smash" pos="VBG" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="23" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="24" string="painting" lemma="painting" stem="paint" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="25" string="in" lemma="in" stem="in" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="26" string="Paul" lemma="Paul" stem="paul" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="27" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="28" string="home" lemma="home" stem="home" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="29" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="30" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="31" string="urged" lemma="urge" stem="urg" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="32" string="a" lemma="a" stem="a" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="33" string="boycott" lemma="boycott" stem="boycott" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="34" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="35" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="36" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="37" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (S (NP (DT The) (NN book)) (VP (VBZ has) (ADVP (RB already)) (VP (VBN drawn) (NP (JJ angry) (NNS responses)) (PP (IN from) (NP (NP (NNS friends)) (PP (IN of) (NP (NNP Lennon)))))))) (CC and) (S (NP (NP (NNP Ono) (NNP McCartney)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBD contradicted) (NP (NP (CD one) (NN anecdote)) (PP (VBG involving) (NP (NP (NNP Lennon)) (VP (VBG smashing) (NP (NP (DT a) (NN painting)) (PP (IN in) (NP (NP (NNP Paul) (POS 's)) (NN home))))))))))) (, ,)) (VP (VBZ has) (VP (VBN urged) (NP (NP (DT a) (NN boycott)) (PP (IN of) (NP (DT the) (NN book))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="a boycott" type="NP">
          <tokens>
            <token id="32" string="a" />
            <token id="33" string="boycott" />
          </tokens>
        </chunking>
        <chunking id="2" string="a painting in Paul 's home" type="NP">
          <tokens>
            <token id="23" string="a" />
            <token id="24" string="painting" />
            <token id="25" string="in" />
            <token id="26" string="Paul" />
            <token id="27" string="'s" />
            <token id="28" string="home" />
          </tokens>
        </chunking>
        <chunking id="3" string="The book" type="NP">
          <tokens>
            <token id="1" string="The" />
            <token id="2" string="book" />
          </tokens>
        </chunking>
        <chunking id="4" string="Lennon" type="NP">
          <tokens>
            <token id="11" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="5" string="one anecdote involving Lennon smashing a painting in Paul 's home" type="NP">
          <tokens>
            <token id="18" string="one" />
            <token id="19" string="anecdote" />
            <token id="20" string="involving" />
            <token id="21" string="Lennon" />
            <token id="22" string="smashing" />
            <token id="23" string="a" />
            <token id="24" string="painting" />
            <token id="25" string="in" />
            <token id="26" string="Paul" />
            <token id="27" string="'s" />
            <token id="28" string="home" />
          </tokens>
        </chunking>
        <chunking id="6" string="angry responses" type="NP">
          <tokens>
            <token id="6" string="angry" />
            <token id="7" string="responses" />
          </tokens>
        </chunking>
        <chunking id="7" string="friends of Lennon" type="NP">
          <tokens>
            <token id="9" string="friends" />
            <token id="10" string="of" />
            <token id="11" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="8" string="the book" type="NP">
          <tokens>
            <token id="35" string="the" />
            <token id="36" string="book" />
          </tokens>
        </chunking>
        <chunking id="9" string="contradicted one anecdote involving Lennon smashing a painting in Paul 's home" type="VP">
          <tokens>
            <token id="17" string="contradicted" />
            <token id="18" string="one" />
            <token id="19" string="anecdote" />
            <token id="20" string="involving" />
            <token id="21" string="Lennon" />
            <token id="22" string="smashing" />
            <token id="23" string="a" />
            <token id="24" string="painting" />
            <token id="25" string="in" />
            <token id="26" string="Paul" />
            <token id="27" string="'s" />
            <token id="28" string="home" />
          </tokens>
        </chunking>
        <chunking id="10" string="one anecdote" type="NP">
          <tokens>
            <token id="18" string="one" />
            <token id="19" string="anecdote" />
          </tokens>
        </chunking>
        <chunking id="11" string="friends" type="NP">
          <tokens>
            <token id="9" string="friends" />
          </tokens>
        </chunking>
        <chunking id="12" string="smashing a painting in Paul 's home" type="VP">
          <tokens>
            <token id="22" string="smashing" />
            <token id="23" string="a" />
            <token id="24" string="painting" />
            <token id="25" string="in" />
            <token id="26" string="Paul" />
            <token id="27" string="'s" />
            <token id="28" string="home" />
          </tokens>
        </chunking>
        <chunking id="13" string="a painting" type="NP">
          <tokens>
            <token id="23" string="a" />
            <token id="24" string="painting" />
          </tokens>
        </chunking>
        <chunking id="14" string="a boycott of the book" type="NP">
          <tokens>
            <token id="32" string="a" />
            <token id="33" string="boycott" />
            <token id="34" string="of" />
            <token id="35" string="the" />
            <token id="36" string="book" />
          </tokens>
        </chunking>
        <chunking id="15" string="Ono McCartney" type="NP">
          <tokens>
            <token id="13" string="Ono" />
            <token id="14" string="McCartney" />
          </tokens>
        </chunking>
        <chunking id="16" string="has already drawn angry responses from friends of Lennon" type="VP">
          <tokens>
            <token id="3" string="has" />
            <token id="4" string="already" />
            <token id="5" string="drawn" />
            <token id="6" string="angry" />
            <token id="7" string="responses" />
            <token id="8" string="from" />
            <token id="9" string="friends" />
            <token id="10" string="of" />
            <token id="11" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="17" string="who contradicted one anecdote involving Lennon smashing a painting in Paul 's home" type="SBAR">
          <tokens>
            <token id="16" string="who" />
            <token id="17" string="contradicted" />
            <token id="18" string="one" />
            <token id="19" string="anecdote" />
            <token id="20" string="involving" />
            <token id="21" string="Lennon" />
            <token id="22" string="smashing" />
            <token id="23" string="a" />
            <token id="24" string="painting" />
            <token id="25" string="in" />
            <token id="26" string="Paul" />
            <token id="27" string="'s" />
            <token id="28" string="home" />
          </tokens>
        </chunking>
        <chunking id="18" string="drawn angry responses from friends of Lennon" type="VP">
          <tokens>
            <token id="5" string="drawn" />
            <token id="6" string="angry" />
            <token id="7" string="responses" />
            <token id="8" string="from" />
            <token id="9" string="friends" />
            <token id="10" string="of" />
            <token id="11" string="Lennon" />
          </tokens>
        </chunking>
        <chunking id="19" string="Paul 's" type="NP">
          <tokens>
            <token id="26" string="Paul" />
            <token id="27" string="'s" />
          </tokens>
        </chunking>
        <chunking id="20" string="Ono McCartney , who contradicted one anecdote involving Lennon smashing a painting in Paul 's home ," type="NP">
          <tokens>
            <token id="13" string="Ono" />
            <token id="14" string="McCartney" />
            <token id="15" string="," />
            <token id="16" string="who" />
            <token id="17" string="contradicted" />
            <token id="18" string="one" />
            <token id="19" string="anecdote" />
            <token id="20" string="involving" />
            <token id="21" string="Lennon" />
            <token id="22" string="smashing" />
            <token id="23" string="a" />
            <token id="24" string="painting" />
            <token id="25" string="in" />
            <token id="26" string="Paul" />
            <token id="27" string="'s" />
            <token id="28" string="home" />
            <token id="29" string="," />
          </tokens>
        </chunking>
        <chunking id="21" string="Lennon smashing a painting in Paul 's home" type="NP">
          <tokens>
            <token id="21" string="Lennon" />
            <token id="22" string="smashing" />
            <token id="23" string="a" />
            <token id="24" string="painting" />
            <token id="25" string="in" />
            <token id="26" string="Paul" />
            <token id="27" string="'s" />
            <token id="28" string="home" />
          </tokens>
        </chunking>
        <chunking id="22" string="urged a boycott of the book" type="VP">
          <tokens>
            <token id="31" string="urged" />
            <token id="32" string="a" />
            <token id="33" string="boycott" />
            <token id="34" string="of" />
            <token id="35" string="the" />
            <token id="36" string="book" />
          </tokens>
        </chunking>
        <chunking id="23" string="Paul 's home" type="NP">
          <tokens>
            <token id="26" string="Paul" />
            <token id="27" string="'s" />
            <token id="28" string="home" />
          </tokens>
        </chunking>
        <chunking id="24" string="has urged a boycott of the book" type="VP">
          <tokens>
            <token id="30" string="has" />
            <token id="31" string="urged" />
            <token id="32" string="a" />
            <token id="33" string="boycott" />
            <token id="34" string="of" />
            <token id="35" string="the" />
            <token id="36" string="book" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="det">
          <governor id="2">book</governor>
          <dependent id="1">The</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="5">drawn</governor>
          <dependent id="2">book</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="5">drawn</governor>
          <dependent id="3">has</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="5">drawn</governor>
          <dependent id="4">already</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="5">drawn</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">responses</governor>
          <dependent id="6">angry</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="5">drawn</governor>
          <dependent id="7">responses</dependent>
        </dependency>
        <dependency type="case">
          <governor id="9">friends</governor>
          <dependent id="8">from</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="5">drawn</governor>
          <dependent id="9">friends</dependent>
        </dependency>
        <dependency type="case">
          <governor id="11">Lennon</governor>
          <dependent id="10">of</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="9">friends</governor>
          <dependent id="11">Lennon</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="5">drawn</governor>
          <dependent id="12">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="14">McCartney</governor>
          <dependent id="13">Ono</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="31">urged</governor>
          <dependent id="14">McCartney</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="17">contradicted</governor>
          <dependent id="16">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="14">McCartney</governor>
          <dependent id="17">contradicted</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="19">anecdote</governor>
          <dependent id="18">one</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="17">contradicted</governor>
          <dependent id="19">anecdote</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">Lennon</governor>
          <dependent id="20">involving</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="19">anecdote</governor>
          <dependent id="21">Lennon</dependent>
        </dependency>
        <dependency type="acl">
          <governor id="21">Lennon</governor>
          <dependent id="22">smashing</dependent>
        </dependency>
        <dependency type="det">
          <governor id="24">painting</governor>
          <dependent id="23">a</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="22">smashing</governor>
          <dependent id="24">painting</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">home</governor>
          <dependent id="25">in</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="28">home</governor>
          <dependent id="26">Paul</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">Paul</governor>
          <dependent id="27">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="24">painting</governor>
          <dependent id="28">home</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="31">urged</governor>
          <dependent id="30">has</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="5">drawn</governor>
          <dependent id="31">urged</dependent>
        </dependency>
        <dependency type="det">
          <governor id="33">boycott</governor>
          <dependent id="32">a</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="31">urged</governor>
          <dependent id="33">boycott</dependent>
        </dependency>
        <dependency type="case">
          <governor id="36">book</governor>
          <dependent id="34">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="36">book</governor>
          <dependent id="35">the</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="33">boycott</governor>
          <dependent id="36">book</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Ono McCartney" type="PERSON" score="0.0">
          <tokens>
            <token id="13" string="Ono" />
            <token id="14" string="McCartney" />
          </tokens>
        </entity>
        <entity id="2" string="Paul" type="PERSON" score="0.0">
          <tokens>
            <token id="26" string="Paul" />
          </tokens>
        </entity>
        <entity id="3" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="11" string="Lennon" />
          </tokens>
        </entity>
        <entity id="4" string="one" type="NUMBER" score="0.0">
          <tokens>
            <token id="18" string="one" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="23" has_coreference="true">
      <content>After Yoko&amp;apost;s rebuttal, listeners were invited to dial 900-digit telephone numbers to vote on whether the book was an accurate portrayal of the Lennons&amp;apost; life.</content>
      <tokens>
        <token id="1" string="After" lemma="after" stem="after" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="2" string="Yoko" lemma="Yoko" stem="yoko" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="3" string="'s" lemma="'s" stem="'" pos="POS" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="rebuttal" lemma="rebuttal" stem="rebutt" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="listeners" lemma="listener" stem="listen" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="7" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="8" string="invited" lemma="invite" stem="invit" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="9" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="10" string="dial" lemma="dial" stem="dial" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="11" string="900-digit" lemma="900-digit" stem="900-digit" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="12" string="telephone" lemma="telephone" stem="telephon" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="13" string="numbers" lemma="number" stem="number" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="14" string="to" lemma="to" stem="to" pos="TO" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="vote" lemma="vote" stem="vote" pos="VB" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="on" lemma="on" stem="on" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="17" string="whether" lemma="whether" stem="whether" pos="IN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="was" lemma="be" stem="wa" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="21" string="an" lemma="a" stem="an" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="22" string="accurate" lemma="accurate" stem="accur" pos="JJ" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="23" string="portrayal" lemma="portrayal" stem="portray" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="24" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="true" is_refers="false" />
        <token id="25" string="the" lemma="the" stem="the" pos="DT" type="Word" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="26" string="Lennons" lemma="Lennons" stem="lennon" pos="NNPS" type="Word" isStopWord="false" ner="ORGANIZATION" is_referenced="true" is_refers="true" />
        <token id="27" string="'" lemma="'" stem="'" pos="POS" type="Symbol" isStopWord="true" is_referenced="true" is_refers="true" />
        <token id="28" string="life" lemma="life" stem="life" pos="NN" type="Word" isStopWord="false" is_referenced="true" is_refers="false" />
        <token id="29" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (PP (IN After) (NP (NP (NNP Yoko) (POS 's)) (NN rebuttal))) (, ,) (NP (NNS listeners)) (VP (VBD were) (VP (VBN invited) (S (VP (TO to) (VP (VB dial) (NP (JJ 900-digit) (NN telephone) (NNS numbers)) (S (VP (TO to) (VP (VB vote) (PP (IN on) (SBAR (IN whether) (S (NP (DT the) (NN book)) (VP (VBD was) (NP (NP (DT an) (ADJP (JJ accurate)) (NN portrayal)) (PP (IN of) (NP (NP (DT the) (NNPS Lennons) (POS ')) (NN life)))))))))))))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="the Lennons ' life" type="NP">
          <tokens>
            <token id="25" string="the" />
            <token id="26" string="Lennons" />
            <token id="27" string="'" />
            <token id="28" string="life" />
          </tokens>
        </chunking>
        <chunking id="2" string="accurate" type="ADJP">
          <tokens>
            <token id="22" string="accurate" />
          </tokens>
        </chunking>
        <chunking id="3" string="listeners" type="NP">
          <tokens>
            <token id="6" string="listeners" />
          </tokens>
        </chunking>
        <chunking id="4" string="were invited to dial 900-digit telephone numbers to vote on whether the book was an accurate portrayal of the Lennons ' life" type="VP">
          <tokens>
            <token id="7" string="were" />
            <token id="8" string="invited" />
            <token id="9" string="to" />
            <token id="10" string="dial" />
            <token id="11" string="900-digit" />
            <token id="12" string="telephone" />
            <token id="13" string="numbers" />
            <token id="14" string="to" />
            <token id="15" string="vote" />
            <token id="16" string="on" />
            <token id="17" string="whether" />
            <token id="18" string="the" />
            <token id="19" string="book" />
            <token id="20" string="was" />
            <token id="21" string="an" />
            <token id="22" string="accurate" />
            <token id="23" string="portrayal" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="Lennons" />
            <token id="27" string="'" />
            <token id="28" string="life" />
          </tokens>
        </chunking>
        <chunking id="5" string="whether the book was an accurate portrayal of the Lennons ' life" type="SBAR">
          <tokens>
            <token id="17" string="whether" />
            <token id="18" string="the" />
            <token id="19" string="book" />
            <token id="20" string="was" />
            <token id="21" string="an" />
            <token id="22" string="accurate" />
            <token id="23" string="portrayal" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="Lennons" />
            <token id="27" string="'" />
            <token id="28" string="life" />
          </tokens>
        </chunking>
        <chunking id="6" string="was an accurate portrayal of the Lennons ' life" type="VP">
          <tokens>
            <token id="20" string="was" />
            <token id="21" string="an" />
            <token id="22" string="accurate" />
            <token id="23" string="portrayal" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="Lennons" />
            <token id="27" string="'" />
            <token id="28" string="life" />
          </tokens>
        </chunking>
        <chunking id="7" string="an accurate portrayal of the Lennons ' life" type="NP">
          <tokens>
            <token id="21" string="an" />
            <token id="22" string="accurate" />
            <token id="23" string="portrayal" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="Lennons" />
            <token id="27" string="'" />
            <token id="28" string="life" />
          </tokens>
        </chunking>
        <chunking id="8" string="the book" type="NP">
          <tokens>
            <token id="18" string="the" />
            <token id="19" string="book" />
          </tokens>
        </chunking>
        <chunking id="9" string="900-digit telephone numbers" type="NP">
          <tokens>
            <token id="11" string="900-digit" />
            <token id="12" string="telephone" />
            <token id="13" string="numbers" />
          </tokens>
        </chunking>
        <chunking id="10" string="Yoko 's rebuttal" type="NP">
          <tokens>
            <token id="2" string="Yoko" />
            <token id="3" string="'s" />
            <token id="4" string="rebuttal" />
          </tokens>
        </chunking>
        <chunking id="11" string="to vote on whether the book was an accurate portrayal of the Lennons ' life" type="VP">
          <tokens>
            <token id="14" string="to" />
            <token id="15" string="vote" />
            <token id="16" string="on" />
            <token id="17" string="whether" />
            <token id="18" string="the" />
            <token id="19" string="book" />
            <token id="20" string="was" />
            <token id="21" string="an" />
            <token id="22" string="accurate" />
            <token id="23" string="portrayal" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="Lennons" />
            <token id="27" string="'" />
            <token id="28" string="life" />
          </tokens>
        </chunking>
        <chunking id="12" string="invited to dial 900-digit telephone numbers to vote on whether the book was an accurate portrayal of the Lennons ' life" type="VP">
          <tokens>
            <token id="8" string="invited" />
            <token id="9" string="to" />
            <token id="10" string="dial" />
            <token id="11" string="900-digit" />
            <token id="12" string="telephone" />
            <token id="13" string="numbers" />
            <token id="14" string="to" />
            <token id="15" string="vote" />
            <token id="16" string="on" />
            <token id="17" string="whether" />
            <token id="18" string="the" />
            <token id="19" string="book" />
            <token id="20" string="was" />
            <token id="21" string="an" />
            <token id="22" string="accurate" />
            <token id="23" string="portrayal" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="Lennons" />
            <token id="27" string="'" />
            <token id="28" string="life" />
          </tokens>
        </chunking>
        <chunking id="13" string="vote on whether the book was an accurate portrayal of the Lennons ' life" type="VP">
          <tokens>
            <token id="15" string="vote" />
            <token id="16" string="on" />
            <token id="17" string="whether" />
            <token id="18" string="the" />
            <token id="19" string="book" />
            <token id="20" string="was" />
            <token id="21" string="an" />
            <token id="22" string="accurate" />
            <token id="23" string="portrayal" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="Lennons" />
            <token id="27" string="'" />
            <token id="28" string="life" />
          </tokens>
        </chunking>
        <chunking id="14" string="Yoko 's" type="NP">
          <tokens>
            <token id="2" string="Yoko" />
            <token id="3" string="'s" />
          </tokens>
        </chunking>
        <chunking id="15" string="to dial 900-digit telephone numbers to vote on whether the book was an accurate portrayal of the Lennons ' life" type="VP">
          <tokens>
            <token id="9" string="to" />
            <token id="10" string="dial" />
            <token id="11" string="900-digit" />
            <token id="12" string="telephone" />
            <token id="13" string="numbers" />
            <token id="14" string="to" />
            <token id="15" string="vote" />
            <token id="16" string="on" />
            <token id="17" string="whether" />
            <token id="18" string="the" />
            <token id="19" string="book" />
            <token id="20" string="was" />
            <token id="21" string="an" />
            <token id="22" string="accurate" />
            <token id="23" string="portrayal" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="Lennons" />
            <token id="27" string="'" />
            <token id="28" string="life" />
          </tokens>
        </chunking>
        <chunking id="16" string="dial 900-digit telephone numbers to vote on whether the book was an accurate portrayal of the Lennons ' life" type="VP">
          <tokens>
            <token id="10" string="dial" />
            <token id="11" string="900-digit" />
            <token id="12" string="telephone" />
            <token id="13" string="numbers" />
            <token id="14" string="to" />
            <token id="15" string="vote" />
            <token id="16" string="on" />
            <token id="17" string="whether" />
            <token id="18" string="the" />
            <token id="19" string="book" />
            <token id="20" string="was" />
            <token id="21" string="an" />
            <token id="22" string="accurate" />
            <token id="23" string="portrayal" />
            <token id="24" string="of" />
            <token id="25" string="the" />
            <token id="26" string="Lennons" />
            <token id="27" string="'" />
            <token id="28" string="life" />
          </tokens>
        </chunking>
        <chunking id="17" string="an accurate portrayal" type="NP">
          <tokens>
            <token id="21" string="an" />
            <token id="22" string="accurate" />
            <token id="23" string="portrayal" />
          </tokens>
        </chunking>
        <chunking id="18" string="the Lennons '" type="NP">
          <tokens>
            <token id="25" string="the" />
            <token id="26" string="Lennons" />
            <token id="27" string="'" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="case">
          <governor id="4">rebuttal</governor>
          <dependent id="1">After</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="4">rebuttal</governor>
          <dependent id="2">Yoko</dependent>
        </dependency>
        <dependency type="case">
          <governor id="2">Yoko</governor>
          <dependent id="3">'s</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="8">invited</governor>
          <dependent id="4">rebuttal</dependent>
        </dependency>
        <dependency type="nsubjpass">
          <governor id="8">invited</governor>
          <dependent id="6">listeners</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="8">invited</governor>
          <dependent id="7">were</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="8">invited</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="10">dial</governor>
          <dependent id="9">to</dependent>
        </dependency>
        <dependency type="xcomp">
          <governor id="8">invited</governor>
          <dependent id="10">dial</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="13">numbers</governor>
          <dependent id="11">900-digit</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="13">numbers</governor>
          <dependent id="12">telephone</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="10">dial</governor>
          <dependent id="13">numbers</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="15">vote</governor>
          <dependent id="14">to</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="10">dial</governor>
          <dependent id="15">vote</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="23">portrayal</governor>
          <dependent id="16">on</dependent>
        </dependency>
        <dependency type="mark">
          <governor id="23">portrayal</governor>
          <dependent id="17">whether</dependent>
        </dependency>
        <dependency type="det">
          <governor id="19">book</governor>
          <dependent id="18">the</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="23">portrayal</governor>
          <dependent id="19">book</dependent>
        </dependency>
        <dependency type="cop">
          <governor id="23">portrayal</governor>
          <dependent id="20">was</dependent>
        </dependency>
        <dependency type="det">
          <governor id="23">portrayal</governor>
          <dependent id="21">an</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="23">portrayal</governor>
          <dependent id="22">accurate</dependent>
        </dependency>
        <dependency type="advcl">
          <governor id="15">vote</governor>
          <dependent id="23">portrayal</dependent>
        </dependency>
        <dependency type="case">
          <governor id="28">life</governor>
          <dependent id="24">of</dependent>
        </dependency>
        <dependency type="det">
          <governor id="26">Lennons</governor>
          <dependent id="25">the</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="28">life</governor>
          <dependent id="26">Lennons</dependent>
        </dependency>
        <dependency type="case">
          <governor id="26">Lennons</governor>
          <dependent id="27">'</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="23">portrayal</governor>
          <dependent id="28">life</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="Yoko" type="PERSON" score="0.0">
          <tokens>
            <token id="2" string="Yoko" />
          </tokens>
        </entity>
        <entity id="2" string="Lennons" type="ORGANIZATION" score="0.0">
          <tokens>
            <token id="26" string="Lennons" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="24" has_coreference="false">
      <content>Results were not expected until later today.</content>
      <tokens>
        <token id="1" string="Results" lemma="result" stem="result" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="2" string="were" lemma="be" stem="were" pos="VBD" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="3" string="not" lemma="not" stem="not" pos="RB" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="4" string="expected" lemma="expect" stem="expect" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="5" string="until" lemma="until" stem="until" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="6" string="later" lemma="later" stem="later" pos="JJ" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="7" string="today" lemma="today" stem="todai" pos="NN" type="Word" isStopWord="false" ner="DATE" is_referenced="false" is_refers="false" />
        <token id="8" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NNS Results)) (VP (VBD were) (RB not) (VP (VBN expected) (PP (IN until) (NP-TMP (JJ later) (NN today))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="Results" type="NP">
          <tokens>
            <token id="1" string="Results" />
          </tokens>
        </chunking>
        <chunking id="2" string="were not expected until later today" type="VP">
          <tokens>
            <token id="2" string="were" />
            <token id="3" string="not" />
            <token id="4" string="expected" />
            <token id="5" string="until" />
            <token id="6" string="later" />
            <token id="7" string="today" />
          </tokens>
        </chunking>
        <chunking id="3" string="expected until later today" type="VP">
          <tokens>
            <token id="4" string="expected" />
            <token id="5" string="until" />
            <token id="6" string="later" />
            <token id="7" string="today" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubjpass">
          <governor id="4">expected</governor>
          <dependent id="1">Results</dependent>
        </dependency>
        <dependency type="auxpass">
          <governor id="4">expected</governor>
          <dependent id="2">were</dependent>
        </dependency>
        <dependency type="neg">
          <governor id="4">expected</governor>
          <dependent id="3">not</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="4">expected</dependent>
        </dependency>
        <dependency type="case">
          <governor id="7">today</governor>
          <dependent id="5">until</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="7">today</governor>
          <dependent id="6">later</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">expected</governor>
          <dependent id="7">today</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="later today" type="DATE" score="0.0">
          <tokens>
            <token id="6" string="later" />
            <token id="7" string="today" />
          </tokens>
        </entity>
      </entities>
    </sentence>
    <sentence id="25" has_coreference="true">
      <content>Goldman, who conducted more than 1,200 interviews for his Lennon book, has written controversial biographies of comedian Lenny Bruce and Elvis Presley.</content>
      <tokens>
        <token id="1" string="Goldman" lemma="Goldman" stem="goldman" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="2" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="3" string="who" lemma="who" stem="who" pos="WP" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="4" string="conducted" lemma="conduct" stem="conduct" pos="VBD" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="5" string="more" lemma="more" stem="more" pos="JJR" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="6" string="than" lemma="than" stem="than" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="7" string="1,200" lemma="1,200" stem="1,200" pos="CD" type="Word" isStopWord="false" ner="NUMBER" is_referenced="false" is_refers="true" />
        <token id="8" string="interviews" lemma="interview" stem="interview" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="9" string="for" lemma="for" stem="for" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="10" string="his" lemma="he" stem="hi" pos="PRP$" type="Word" isStopWord="true" is_referenced="false" is_refers="true" />
        <token id="11" string="Lennon" lemma="Lennon" stem="lennon" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="true" />
        <token id="12" string="book" lemma="book" stem="book" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="true" />
        <token id="13" string="," lemma="," stem="," pos="," type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="14" string="has" lemma="have" stem="ha" pos="VBZ" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="15" string="written" lemma="write" stem="written" pos="VBN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="16" string="controversial" lemma="controversial" stem="controversi" pos="JJ" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="17" string="biographies" lemma="biography" stem="biographi" pos="NNS" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="18" string="of" lemma="of" stem="of" pos="IN" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="19" string="comedian" lemma="comedian" stem="comedian" pos="NN" type="Word" isStopWord="false" is_referenced="false" is_refers="false" />
        <token id="20" string="Lenny" lemma="Lenny" stem="lenni" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="21" string="Bruce" lemma="Bruce" stem="bruce" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="22" string="and" lemma="and" stem="and" pos="CC" type="Word" isStopWord="true" is_referenced="false" is_refers="false" />
        <token id="23" string="Elvis" lemma="Elvis" stem="elvi" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="24" string="Presley" lemma="Presley" stem="preslei" pos="NNP" type="Word" isStopWord="false" ner="PERSON" is_referenced="false" is_refers="false" />
        <token id="25" string="." lemma="." stem="." pos="." type="Symbol" isStopWord="true" is_referenced="false" is_refers="false" />
      </tokens>
      <syntactictree>(ROOT (S (NP (NP (NNP Goldman)) (, ,) (SBAR (WHNP (WP who)) (S (VP (VBD conducted) (NP (QP (JJR more) (IN than) (CD 1,200)) (NNS interviews)) (PP (IN for) (NP (PRP$ his) (NNP Lennon) (NN book)))))) (, ,)) (VP (VBZ has) (VP (VBN written) (NP (NP (JJ controversial) (NNS biographies)) (PP (IN of) (NP (NN comedian) (NNP Lenny) (NNP Bruce) (CC and) (NNP Elvis) (NNP Presley)))))) (. .)))</syntactictree>
      <chunkings>
        <chunking id="1" string="who conducted more than 1,200 interviews for his Lennon book" type="SBAR">
          <tokens>
            <token id="3" string="who" />
            <token id="4" string="conducted" />
            <token id="5" string="more" />
            <token id="6" string="than" />
            <token id="7" string="1,200" />
            <token id="8" string="interviews" />
            <token id="9" string="for" />
            <token id="10" string="his" />
            <token id="11" string="Lennon" />
            <token id="12" string="book" />
          </tokens>
        </chunking>
        <chunking id="2" string="comedian Lenny Bruce and Elvis Presley" type="NP">
          <tokens>
            <token id="19" string="comedian" />
            <token id="20" string="Lenny" />
            <token id="21" string="Bruce" />
            <token id="22" string="and" />
            <token id="23" string="Elvis" />
            <token id="24" string="Presley" />
          </tokens>
        </chunking>
        <chunking id="3" string="has written controversial biographies of comedian Lenny Bruce and Elvis Presley" type="VP">
          <tokens>
            <token id="14" string="has" />
            <token id="15" string="written" />
            <token id="16" string="controversial" />
            <token id="17" string="biographies" />
            <token id="18" string="of" />
            <token id="19" string="comedian" />
            <token id="20" string="Lenny" />
            <token id="21" string="Bruce" />
            <token id="22" string="and" />
            <token id="23" string="Elvis" />
            <token id="24" string="Presley" />
          </tokens>
        </chunking>
        <chunking id="4" string="written controversial biographies of comedian Lenny Bruce and Elvis Presley" type="VP">
          <tokens>
            <token id="15" string="written" />
            <token id="16" string="controversial" />
            <token id="17" string="biographies" />
            <token id="18" string="of" />
            <token id="19" string="comedian" />
            <token id="20" string="Lenny" />
            <token id="21" string="Bruce" />
            <token id="22" string="and" />
            <token id="23" string="Elvis" />
            <token id="24" string="Presley" />
          </tokens>
        </chunking>
        <chunking id="5" string="Goldman , who conducted more than 1,200 interviews for his Lennon book ," type="NP">
          <tokens>
            <token id="1" string="Goldman" />
            <token id="2" string="," />
            <token id="3" string="who" />
            <token id="4" string="conducted" />
            <token id="5" string="more" />
            <token id="6" string="than" />
            <token id="7" string="1,200" />
            <token id="8" string="interviews" />
            <token id="9" string="for" />
            <token id="10" string="his" />
            <token id="11" string="Lennon" />
            <token id="12" string="book" />
            <token id="13" string="," />
          </tokens>
        </chunking>
        <chunking id="6" string="conducted more than 1,200 interviews for his Lennon book" type="VP">
          <tokens>
            <token id="4" string="conducted" />
            <token id="5" string="more" />
            <token id="6" string="than" />
            <token id="7" string="1,200" />
            <token id="8" string="interviews" />
            <token id="9" string="for" />
            <token id="10" string="his" />
            <token id="11" string="Lennon" />
            <token id="12" string="book" />
          </tokens>
        </chunking>
        <chunking id="7" string="Goldman" type="NP">
          <tokens>
            <token id="1" string="Goldman" />
          </tokens>
        </chunking>
        <chunking id="8" string="more than 1,200 interviews" type="NP">
          <tokens>
            <token id="5" string="more" />
            <token id="6" string="than" />
            <token id="7" string="1,200" />
            <token id="8" string="interviews" />
          </tokens>
        </chunking>
        <chunking id="9" string="controversial biographies of comedian Lenny Bruce and Elvis Presley" type="NP">
          <tokens>
            <token id="16" string="controversial" />
            <token id="17" string="biographies" />
            <token id="18" string="of" />
            <token id="19" string="comedian" />
            <token id="20" string="Lenny" />
            <token id="21" string="Bruce" />
            <token id="22" string="and" />
            <token id="23" string="Elvis" />
            <token id="24" string="Presley" />
          </tokens>
        </chunking>
        <chunking id="10" string="controversial biographies" type="NP">
          <tokens>
            <token id="16" string="controversial" />
            <token id="17" string="biographies" />
          </tokens>
        </chunking>
        <chunking id="11" string="his Lennon book" type="NP">
          <tokens>
            <token id="10" string="his" />
            <token id="11" string="Lennon" />
            <token id="12" string="book" />
          </tokens>
        </chunking>
      </chunkings>
      <dependencies>
        <dependency type="nsubj">
          <governor id="15">written</governor>
          <dependent id="1">Goldman</dependent>
        </dependency>
        <dependency type="nsubj">
          <governor id="4">conducted</governor>
          <dependent id="3">who</dependent>
        </dependency>
        <dependency type="acl:relcl">
          <governor id="1">Goldman</governor>
          <dependent id="4">conducted</dependent>
        </dependency>
        <dependency type="advmod">
          <governor id="7">1,200</governor>
          <dependent id="5">more</dependent>
        </dependency>
        <dependency type="mwe">
          <governor id="5">more</governor>
          <dependent id="6">than</dependent>
        </dependency>
        <dependency type="nummod">
          <governor id="8">interviews</governor>
          <dependent id="7">1,200</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="4">conducted</governor>
          <dependent id="8">interviews</dependent>
        </dependency>
        <dependency type="case">
          <governor id="12">book</governor>
          <dependent id="9">for</dependent>
        </dependency>
        <dependency type="nmod:poss">
          <governor id="12">book</governor>
          <dependent id="10">his</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="12">book</governor>
          <dependent id="11">Lennon</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="4">conducted</governor>
          <dependent id="12">book</dependent>
        </dependency>
        <dependency type="aux">
          <governor id="15">written</governor>
          <dependent id="14">has</dependent>
        </dependency>
        <dependency type="root">
          <governor id="0">ROOT</governor>
          <dependent id="15">written</dependent>
        </dependency>
        <dependency type="amod">
          <governor id="17">biographies</governor>
          <dependent id="16">controversial</dependent>
        </dependency>
        <dependency type="dobj">
          <governor id="15">written</governor>
          <dependent id="17">biographies</dependent>
        </dependency>
        <dependency type="case">
          <governor id="21">Bruce</governor>
          <dependent id="18">of</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="21">Bruce</governor>
          <dependent id="19">comedian</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="21">Bruce</governor>
          <dependent id="20">Lenny</dependent>
        </dependency>
        <dependency type="nmod">
          <governor id="17">biographies</governor>
          <dependent id="21">Bruce</dependent>
        </dependency>
        <dependency type="cc">
          <governor id="21">Bruce</governor>
          <dependent id="22">and</dependent>
        </dependency>
        <dependency type="compound">
          <governor id="24">Presley</governor>
          <dependent id="23">Elvis</dependent>
        </dependency>
        <dependency type="conj">
          <governor id="21">Bruce</governor>
          <dependent id="24">Presley</dependent>
        </dependency>
      </dependencies>
      <entities>
        <entity id="1" string="1,200" type="NUMBER" score="0.0">
          <tokens>
            <token id="7" string="1,200" />
          </tokens>
        </entity>
        <entity id="2" string="Lennon" type="PERSON" score="0.0">
          <tokens>
            <token id="11" string="Lennon" />
          </tokens>
        </entity>
        <entity id="3" string="Elvis Presley" type="PERSON" score="0.0">
          <tokens>
            <token id="23" string="Elvis" />
            <token id="24" string="Presley" />
          </tokens>
        </entity>
        <entity id="4" string="Lenny Bruce" type="PERSON" score="0.0">
          <tokens>
            <token id="20" string="Lenny" />
            <token id="21" string="Bruce" />
          </tokens>
        </entity>
        <entity id="5" string="Goldman" type="PERSON" score="0.0">
          <tokens>
            <token id="1" string="Goldman" />
          </tokens>
        </entity>
      </entities>
    </sentence>
  </sentences>
  <coreferences>
    <coreference id="1" type="PROPER">
      <referenced ids_tokens="29-30" string="Yoko Ono" id_sentence="1" />
      <mentions>
        <mention ids_tokens="25" string="Ono" id_sentence="3" />
        <mention ids_tokens="30" string="her" id_sentence="3" />
        <mention ids_tokens="3" string="Ono" id_sentence="5" />
        <mention ids_tokens="7" string="her" id_sentence="5" />
        <mention ids_tokens="4" string="she" id_sentence="6" />
        <mention ids_tokens="1" string="Ono" id_sentence="7" />
        <mention ids_tokens="9" string="Ono" id_sentence="10" />
        <mention ids_tokens="2" string="Ono" id_sentence="13" />
        <mention ids_tokens="1" string="Ono" id_sentence="17" />
      </mentions>
    </coreference>
    <coreference id="3" type="PROPER">
      <referenced ids_tokens="5-6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-21-22" string="John Lennon as a drug-addled , anorexic bisexual who raged his way from Liverpool to New York City" id_sentence="1" />
      <mentions>
        <mention ids_tokens="17-18" string="John Lennon" id_sentence="4" />
        <mention ids_tokens="19-20" string="Lennon's" id_sentence="5" />
        <mention ids_tokens="32" string="Lennon" id_sentence="6" />
        <mention ids_tokens="15" string="Lennon" id_sentence="7" />
        <mention ids_tokens="18-19" string="Lennon's" id_sentence="8" />
        <mention ids_tokens="24-28" string="me Some Truth ,''" id_sentence="8" />
        <mention ids_tokens="1" string="Lennon" id_sentence="14" />
        <mention ids_tokens="6" string="Lennon" id_sentence="16" />
        <mention ids_tokens="39" string="Lennon" id_sentence="18" />
        <mention ids_tokens="4" string="Lennon" id_sentence="20" />
        <mention ids_tokens="9" string="he" id_sentence="20" />
        <mention ids_tokens="19" string="his" id_sentence="20" />
        <mention ids_tokens="25" string="he" id_sentence="20" />
        <mention ids_tokens="11" string="Lennon" id_sentence="22" />
        <mention ids_tokens="11" string="Lennon" id_sentence="25" />
      </mentions>
    </coreference>
    <coreference id="4" type="NOMINAL">
      <referenced ids_tokens="11-12-13-14-15" string="a very dramatically described something" id_sentence="3" />
      <mentions>
        <mention ids_tokens="15-16" string="his way" id_sentence="1" />
        <mention ids_tokens="2" string="It" id_sentence="2" />
      </mentions>
    </coreference>
    <coreference id="5" type="NOMINAL">
      <referenced ids_tokens="33-34-35-36" string="a national radio broadcast" id_sentence="1" />
      <mentions>
        <mention ids_tokens="25-26" string="the broadcast" id_sentence="15" />
      </mentions>
    </coreference>
    <coreference id="6" type="PROPER">
      <referenced ids_tokens="38-39" string="Albert Goldman" id_sentence="3" />
      <mentions>
        <mention ids_tokens="11-12" string="Goldman's" id_sentence="4" />
        <mention ids_tokens="5" string="Goldman" id_sentence="5" />
        <mention ids_tokens="1" string="Goldman" id_sentence="6" />
        <mention ids_tokens="20-21" string="Goldman's" id_sentence="7" />
        <mention ids_tokens="14-15" string="Goldman's" id_sentence="17" />
        <mention ids_tokens="35-36" string="Goldman's" id_sentence="18" />
        <mention ids_tokens="15-16" string="Goldman's" id_sentence="21" />
        <mention ids_tokens="1-12" string="Goldman , who conducted more than 1,200 interviews for his Lennon book" id_sentence="25" />
        <mention ids_tokens="1" string="Goldman" id_sentence="25" />
        <mention ids_tokens="10" string="his" id_sentence="25" />
      </mentions>
    </coreference>
    <coreference id="7" type="PROPER">
      <referenced ids_tokens="5-6-7" string="John in there" id_sentence="3" />
      <mentions>
        <mention ids_tokens="4" string="John" id_sentence="10" />
        <mention ids_tokens="5" string="he" id_sentence="11" />
        <mention ids_tokens="12" string="John" id_sentence="12" />
      </mentions>
    </coreference>
    <coreference id="8" type="NOMINAL">
      <referenced ids_tokens="21-22-23-24-25-26-27-28" string="an accurate portrayal of the Lennons ' life" id_sentence="23" />
      <mentions>
        <mention ids_tokens="35-36" string="the book" id_sentence="3" />
        <mention ids_tokens="32-34" string="the book's" id_sentence="8" />
        <mention ids_tokens="5-6" string="this book" id_sentence="9" />
        <mention ids_tokens="1" string="It" id_sentence="10" />
        <mention ids_tokens="2" string="It" id_sentence="11" />
        <mention ids_tokens="5-6" string="this book" id_sentence="12" />
        <mention ids_tokens="5-6" string="the book" id_sentence="19" />
        <mention ids_tokens="1-2" string="The book" id_sentence="22" />
        <mention ids_tokens="35-36" string="the book" id_sentence="22" />
      </mentions>
    </coreference>
    <coreference id="11" type="PROPER">
      <referenced ids_tokens="8-9-10" string="Paul McCartney 's" id_sentence="6" />
      <mentions>
        <mention ids_tokens="26-27" string="Paul's" id_sentence="22" />
      </mentions>
    </coreference>
    <coreference id="12" type="NOMINAL">
      <referenced ids_tokens="3-4-5-6-7" string="virtually all of these charges" id_sentence="7" />
      <mentions>
        <mention ids_tokens="2" string="It" id_sentence="8" />
      </mentions>
    </coreference>
    <coreference id="13" type="NOMINAL">
      <referenced ids_tokens="9-10-11" string="friends of Lennon" id_sentence="22" />
      <mentions>
        <mention ids_tokens="15-18" string="Lennon friends and employees" id_sentence="7" />
      </mentions>
    </coreference>
    <coreference id="15" type="LIST">
      <referenced ids_tokens="26-27-28-29" string="unreliable sources and misquotes" id_sentence="7" />
      <mentions>
        <mention ids_tokens="6" string="me" id_sentence="10" />
        <mention ids_tokens="14" string="I" id_sentence="12" />
      </mentions>
    </coreference>
    <coreference id="16" type="PROPER">
      <referenced ids_tokens="8" string="Yoko" id_sentence="8" />
      <mentions>
        <mention ids_tokens="21-29" string="Yoko _ a charge Mintz denied during the show" id_sentence="17" />
        <mention ids_tokens="8-18" string="Yoko , who then responded to the allegations point by point" id_sentence="19" />
        <mention ids_tokens="1" string="She" id_sentence="20" />
        <mention ids_tokens="1" string="She" id_sentence="21" />
        <mention ids_tokens="19" string="she" id_sentence="21" />
        <mention ids_tokens="2-3" string="Yoko's" id_sentence="23" />
      </mentions>
    </coreference>
    <coreference id="18" type="LIST">
      <referenced ids_tokens="4-5-6" string="John and me" id_sentence="10" />
      <mentions>
        <mention ids_tokens="21" string="we" id_sentence="12" />
      </mentions>
    </coreference>
    <coreference id="20" type="PROPER">
      <referenced ids_tokens="13-14" string="the Lennons" id_sentence="13" />
      <mentions>
        <mention ids_tokens="25-27" string="the Lennons'" id_sentence="23" />
      </mentions>
    </coreference>
    <coreference id="22" type="PROPER">
      <referenced ids_tokens="25" string="Sean" id_sentence="13" />
      <mentions>
        <mention ids_tokens="19-22" string="his son , Sean" id_sentence="20" />
        <mention ids_tokens="19-20" string="his son" id_sentence="20" />
      </mentions>
    </coreference>
    <coreference id="24" type="PROPER">
      <referenced ids_tokens="10" string="Julian" id_sentence="16" />
      <mentions>
        <mention ids_tokens="30-43" string="Julian , who also refuted Goldman's tales of Lennon as a violent father" id_sentence="18" />
      </mentions>
    </coreference>
    <coreference id="25" type="PROPER">
      <referenced ids_tokens="7-8" string="Elliot Mintz" id_sentence="17" />
      <mentions>
        <mention ids_tokens="1" string="Mintz" id_sentence="19" />
      </mentions>
    </coreference>
  </coreferences>
</document>
